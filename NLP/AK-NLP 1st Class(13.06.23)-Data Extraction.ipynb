{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cdd95c",
   "metadata": {},
   "source": [
    "# Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc45c99",
   "metadata": {},
   "source": [
    "# Extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dedddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\achyu\\anaconda3\\lib\\site-packages (20221105)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba3b746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\achyu\\\\Learnbay\\\\Deep Learning\\\\ML Real Time Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d666607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_pages,extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c531541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTFigure(Im0) 0.000,0.000,504.000,661.440 matrix=[504.00,0.00,0.00,661.44, (0.00,0.00)]>\n",
      "<LTTextBoxHorizontal(0) 73.459,519.532,301.066,596.671 'Mastering MLOps \\nwith Dataiku\\n'>\n",
      "<LTTextBoxHorizontal(1) 177.796,465.541,425.334,497.194 'Dataiku is the only platform that provides one simple, consistent UI \\nfor data connection, wrangling, mining, visualization, machine \\nlearning, deployment, and model monitoring, all at enterprise scale.\\n'>\n",
      "<LTTextBoxHorizontal(2) 97.192,431.417,320.174,439.942 'KEY FEATURES FOR A SCALABLE MLOPS STRATEGY INCLUDE:\\n'>\n",
      "<LTTextBoxHorizontal(3) 99.360,416.174,103.509,423.717 '1\\n'>\n",
      "<LTTextBoxHorizontal(4) 99.360,391.734,103.509,399.277 '2\\n'>\n",
      "<LTTextBoxHorizontal(5) 110.468,407.381,428.838,424.149 'Model input drift detection that looks at the recent data the model has had to score and statistically \\ncompares it with the data on which the model was evaluated.\\n'>\n",
      "<LTTextBoxHorizontal(6) 110.468,373.679,417.814,399.638 'Easier creation of validation feedback loops via Dataiku Evaluation Recipes to compute the true \\nperformance of a saved model against a new validation dataset, plus automated retraining and \\nredeployment.\\n'>\n",
      "<LTTextBoxHorizontal(7) 88.950,355.527,185.005,373.726 ' Everyday AI, \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.150,358.713,103.299,366.257 '3\\n'>\n",
      "<LTTextBoxHorizontal(9) 110.468,358.184,332.747,365.935 'Dashboard interfaces dedicated to the monitoring of global pipelines.\\n'>\n",
      "<LTTextBoxHorizontal(10) 99.150,342.897,103.299,350.440 '4\\n'>\n",
      "<LTTextBoxHorizontal(11) 110.468,342.863,336.240,350.614 '...and more! See MLOps features with Dataiku in action at dataiku.com\\n'>\n",
      "<LTTextBoxHorizontal(12) 88.950,328.101,247.592,346.300 ' Extraordinary People \\n'>\n",
      "<LTTextBoxHorizontal(13) 326.216,297.570,388.958,301.672 'Elastic Architecture Built for the Cloud\\n'>\n",
      "<LTTextBoxHorizontal(14) 148.630,202.572,194.067,209.018 'Machine Learning\\n'>\n",
      "<LTTextBoxHorizontal(15) 245.505,202.404,278.566,208.850 'Visualization\\n'>\n",
      "<LTTextBoxHorizontal(16) 339.396,202.746,382.751,209.192 'Data Preparation\\n'>\n",
      "<LTTextBoxHorizontal(17) 371.920,173.780,373.809,174.981 'Age\\n'>\n",
      "<LTTextBoxHorizontal(18) 372.196,169.977,375.196,170.993 'Integer\\n'>\n",
      "<LTTextBoxHorizontal(19) 371.920,166.807,373.113,168.008 '22\\n'>\n",
      "<LTTextBoxHorizontal(20) 371.920,164.499,373.113,165.700 '38\\n'>\n",
      "<LTTextBoxHorizontal(21) 371.920,162.191,373.113,163.392 '26\\n'>\n",
      "<LTTextBoxHorizontal(22) 371.920,159.883,373.113,161.084 '35\\n'>\n",
      "<LTTextBoxHorizontal(23) 371.920,157.575,373.113,158.776 '35\\n'>\n",
      "<LTTextBoxHorizontal(24) 371.920,152.959,373.113,154.160 '29\\n'>\n",
      "<LTTextBoxHorizontal(25) 327.564,173.780,330.595,174.981 'Name\\n'>\n",
      "<LTTextBoxHorizontal(26) 327.842,169.977,333.281,170.993 'Natural lang.\\n'>\n",
      "<LTTextBoxHorizontal(27) 327.564,166.918,339.703,168.118 'Braund, Mr. Owen Harris\\n'>\n",
      "<LTTextBoxHorizontal(28) 327.564,164.610,336.310,165.810 'Moran, Mr. James\\n'>\n",
      "<LTTextBoxHorizontal(29) 327.564,162.302,338.625,163.502 'Heikkinen, Miss. Laina\\n'>\n",
      "<LTTextBoxHorizontal(30) 336.161,161.729,350.409,162.957 'Remove rows containing Mr.\\n'>\n",
      "<LTTextBoxHorizontal(31) 351.231,173.780,353.062,174.981 'Sex\\n'>\n",
      "<LTTextBoxHorizontal(32) 351.507,169.977,354.606,170.993 'Gender\\n'>\n",
      "<LTTextBoxHorizontal(33) 351.301,166.806,353.802,168.006 'male\\n'>\n",
      "<LTTextBoxHorizontal(34) 351.301,164.498,353.802,165.698 'male\\n'>\n",
      "<LTTextBoxHorizontal(35) 351.301,162.190,354.782,163.390 'female\\n'>\n",
      "<LTTextBoxHorizontal(36) 327.564,159.994,341.643,161.194 'Futrelle, Mrs. Jacques Heath\\n'>\n",
      "<LTTextBoxHorizontal(37) 336.161,159.144,351.349,160.371 'Keep only rows containing Mr.\\n'>\n",
      "<LTTextBoxHorizontal(38) 351.301,159.882,354.782,161.082 'female\\n'>\n",
      "<LTTextBoxHorizontal(39) 327.564,157.686,339.407,158.886 'Allen, Mr. William Henry\\n'>\n",
      "<LTTextBoxHorizontal(40) 327.564,155.378,337.972,156.578 'McCarthy, Mr. Robert\\n'>\n",
      "<LTTextBoxHorizontal(41) 336.161,155.797,345.981,157.024 'Split column on Mr.\\n'>\n",
      "<LTTextBoxHorizontal(42) 327.564,153.070,343.492,154.270 'Hewlett, Mrs (Mary D Kingcome)\\n'>\n",
      "<LTTextBoxHorizontal(43) 336.161,153.211,344.678,154.439 'Replace Mr. by ...\\n'>\n",
      "<LTTextBoxHorizontal(44) 351.301,157.574,353.802,158.774 'male\\n'>\n",
      "<LTTextBoxHorizontal(45) 351.301,155.266,353.802,156.466 'male\\n'>\n",
      "<LTTextBoxHorizontal(46) 336.161,149.603,356.418,150.831 'Remove rows equal to Moran, Mr. James\\n'>\n",
      "<LTTextBoxHorizontal(47) 336.161,147.018,357.355,148.246 'Keep only rows equal to Moran, Mr. James\\n'>\n",
      "<LTTextBoxHorizontal(48) 336.161,144.433,354.723,145.661 'Clear cells equal to Moran, Mr. James\\n'>\n",
      "<LTTextBoxHorizontal(49) 336.159,140.614,349.287,141.814 'Filter on Moran, Mr. James\\n'>\n",
      "<LTTextBoxHorizontal(50) 336.159,138.028,342.062,139.229 'Filter on Mr.\\n'>\n",
      "<LTTextBoxHorizontal(51) 336.159,135.443,346.366,136.643 'Toggle row highlight\\n'>\n",
      "<LTTextBoxHorizontal(52) 336.159,132.858,346.848,134.058 'Show complete value\\n'>\n",
      "<LTTextBoxHorizontal(53) 147.624,107.972,169.591,114.418 'DataOps\\n'>\n",
      "<LTTextBoxHorizontal(54) 240.522,108.172,295.493,114.617 'Governance & MLOps\\n'>\n",
      "<LTTextBoxHorizontal(55) 338.771,108.816,370.626,115.262 'Applications\\n'>\n",
      "<LTTextBoxHorizontal(56) 88.791,6.365,125.043,23.530 '450+\\n'>\n",
      "<LTTextBoxHorizontal(57) 88.791,-0.900,130.605,6.798 'CUSTOMERS\\n'>\n",
      "<LTTextBoxHorizontal(58) 148.748,6.365,208.276,23.530 '45,000+\\n'>\n",
      "<LTTextBoxHorizontal(59) 148.757,-0.900,196.545,6.798 'ACTIVE USERS\\n'>\n",
      "<LTTextBoxHorizontal(60) 212.566,5.800,291.436,12.949 '©Dataiku2021 | dataiku.com\\n'>\n",
      "<LTTextBoxHorizontal(61) 88.067,-18.255,310.477,-12.021 'Dataiku is the world’s leading platform for Everyday AI, systemizing the use of data for \\n'>\n",
      "<LTTextBoxHorizontal(62) 88.067,-26.982,321.842,-20.748 'exceptional business results. Organizations that use Dataiku elevate their people (whether \\n'>\n",
      "<LTTextBoxHorizontal(63) 88.067,-35.710,321.374,-29.476 'technical and working in code or on the business side and low- or no-code) to extraordinary, \\n'>\n",
      "<LTTextBoxHorizontal(64) 88.067,-44.437,283.316,-38.203 'arming them with the ability to make better day-to-day decisions with data.\\n'>\n",
      "<LTTextBoxHorizontal(65) 222.829,-73.650,278.491,-68.662 '©2021 dataiku | dataiku.com\\n'>\n",
      "<LTRect 0.000,19.947,504.000,326.240>\n",
      "<LTCurve 419.878,380.685,423.783,387.828>\n",
      "<LTCurve 420.997,381.529,422.782,385.394>\n",
      "<LTCurve 424.695,380.685,428.815,386.238>\n",
      "<LTCurve 425.754,381.470,427.363,383.609>\n",
      "<LTCurve 429.070,380.763,431.719,387.602>\n",
      "<LTCurve 432.161,380.685,436.281,386.238>\n",
      "<LTCurve 433.219,381.470,434.829,383.609>\n",
      "<LTCurve 427.000,372.088,430.699,377.504>\n",
      "<LTCurve 400.538,371.907,416.640,388.009>\n",
      "<LTRect 408.925,377.717,412.968,378.344>\n",
      "<LTCurve 403.738,375.262,413.001,384.224>\n",
      "<LTCurve 411.081,383.349,411.719,383.986>\n",
      "<LTLine 235.853,189.416,235.853,198.485>\n",
      "<LTLine 329.314,189.416,329.314,198.485>\n",
      "<LTLine 329.433,268.796,329.433,277.865>\n",
      "<LTLine 106.153,92.607,329.902,92.607>\n",
      "<LTCurve 99.821,92.636,105.125,98.093>\n",
      "<LTLine 99.806,99.131,99.806,180.122>\n",
      "<LTCurve 99.835,181.151,105.292,186.455>\n",
      "<LTLine 106.328,186.470,388.084,186.470>\n",
      "<LTCurve 389.111,186.499,394.417,191.956>\n",
      "<LTLine 394.433,192.994,394.461,259.830>\n",
      "<LTCurve 388.978,260.859,394.433,266.163>\n",
      "<LTLine 310.215,266.178,387.944,266.178>\n",
      "<LTLine 330.252,92.607,330.516,92.607>\n",
      "<LTCurve 105.458,92.607,105.976,92.613>\n",
      "<LTCurve 99.806,98.259,99.811,98.779>\n",
      "<LTCurve 99.806,180.299,99.811,180.819>\n",
      "<LTCurve 105.458,186.465,105.976,186.470>\n",
      "<LTCurve 388.259,186.470,388.777,186.476>\n",
      "<LTCurve 394.427,192.123,394.432,192.641>\n",
      "<LTCurve 394.455,260.007,394.461,260.526>\n",
      "<LTCurve 388.294,266.172,388.811,266.178>\n",
      "<LTLine 309.777,266.178,310.041,266.178>\n",
      "<LTFigure(Fm0) 224.422,86.447,527.990,128.608 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTRect 228.251,45.923,299.713,82.619>\n",
      "<LTFigure(Im0) 227.584,54.851,298.238,83.266 matrix=[70.65,0.00,0.00,28.41, (227.58,54.85)]>\n",
      "<LTFigure(Fm1) 229.023,180.826,519.538,306.893 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im1) 242.785,151.932,286.344,176.640 matrix=[43.56,0.00,0.00,24.71, (242.78,151.93)]>\n",
      "<LTRect 250.416,135.193,269.636,154.413>\n",
      "<LTRect 257.891,137.041,257.946,137.421>\n",
      "<LTRect 257.891,138.019,257.946,138.291>\n",
      "<LTRect 251.694,151.283,252.021,152.588>\n",
      "<LTRect 252.130,151.283,252.457,152.316>\n",
      "<LTRect 252.564,151.283,252.891,151.773>\n",
      "<LTRect 252.999,151.283,253.325,152.860>\n",
      "<LTRect 256.369,151.175,256.586,152.044>\n",
      "<LTRect 256.696,151.447,256.913,152.045>\n",
      "<LTRect 257.022,151.663,257.239,152.044>\n",
      "<LTRect 257.348,152.099,257.565,152.479>\n",
      "<LTRect 257.674,152.099,257.891,152.751>\n",
      "<LTRect 258.000,152.099,258.217,152.968>\n",
      "<LTRect 266.753,151.283,267.242,152.153>\n",
      "<LTRect 266.100,151.283,266.589,151.773>\n",
      "<LTRect 260.936,152.589,262.295,152.915>\n",
      "<LTRect 260.935,152.153,262.566,152.479>\n",
      "<LTRect 260.935,151.718,262.077,152.045>\n",
      "<LTRect 260.935,151.283,261.751,151.609>\n",
      "<LTCurve 252.407,147.149,253.305,147.898>\n",
      "<LTCurve 252.584,146.444,253.315,147.163>\n",
      "<LTCurve 251.769,146.357,252.899,147.882>\n",
      "<LTCurve 257.395,146.313,258.094,147.211>\n",
      "<LTCurve 256.640,146.303,257.359,146.837>\n",
      "<LTCurve 256.553,146.719,258.078,147.905>\n",
      "<LTRect 265.909,146.375,267.540,146.430>\n",
      "<LTCurve 265.909,146.842,267.538,147.831>\n",
      "<LTCurve 265.909,146.538,267.540,147.402>\n",
      "<LTCurve 261.022,142.677,261.511,143.166>\n",
      "<LTCurve 261.697,142.092,262.404,142.798>\n",
      "<LTCurve 261.403,141.416,262.001,142.014>\n",
      "<LTCurve 265.725,142.779,266.075,143.129>\n",
      "<LTCurve 265.751,142.258,266.050,142.558>\n",
      "<LTCurve 266.250,142.258,266.549,142.558>\n",
      "<LTCurve 266.749,142.258,267.048,142.558>\n",
      "<LTCurve 267.251,142.258,267.550,142.558>\n",
      "<LTCurve 265.801,141.856,266.000,142.055>\n",
      "<LTCurve 266.299,141.856,266.499,142.055>\n",
      "<LTCurve 266.799,141.856,266.998,142.055>\n",
      "<LTCurve 265.850,141.556,265.950,141.656>\n",
      "<LTCurve 266.349,141.556,266.449,141.656>\n",
      "<LTCurve 266.224,142.779,266.574,143.129>\n",
      "<LTCurve 266.723,142.779,267.073,143.129>\n",
      "<LTCurve 267.223,142.779,267.572,143.129>\n",
      "<LTRect 251.694,137.911,252.021,138.237>\n",
      "<LTRect 251.694,137.585,252.021,137.911>\n",
      "<LTRect 251.694,137.258,252.021,137.584>\n",
      "<LTRect 251.694,136.931,252.021,137.258>\n",
      "<LTRect 251.694,136.606,252.021,136.932>\n",
      "<LTRect 252.673,137.911,252.999,138.237>\n",
      "<LTRect 252.673,137.585,252.999,137.911>\n",
      "<LTRect 252.673,137.258,252.999,137.584>\n",
      "<LTRect 252.021,137.911,252.348,138.237>\n",
      "<LTRect 252.021,137.585,252.348,137.911>\n",
      "<LTRect 252.021,137.258,252.348,137.584>\n",
      "<LTRect 252.021,136.931,252.348,137.258>\n",
      "<LTRect 252.021,136.606,252.348,136.932>\n",
      "<LTRect 252.999,137.911,253.325,138.237>\n",
      "<LTRect 252.999,137.585,253.325,137.911>\n",
      "<LTRect 252.999,137.258,253.325,137.584>\n",
      "<LTRect 252.347,137.911,252.673,138.237>\n",
      "<LTRect 252.347,137.585,252.673,137.911>\n",
      "<LTRect 252.347,137.258,252.673,137.584>\n",
      "<LTRect 252.347,136.931,252.673,137.258>\n",
      "<LTRect 252.347,136.606,252.673,136.932>\n",
      "<LTRect 252.673,136.931,252.999,137.258>\n",
      "<LTRect 252.673,136.606,252.999,136.932>\n",
      "<LTRect 252.999,136.931,253.325,137.258>\n",
      "<LTRect 252.999,136.606,253.325,136.932>\n",
      "<LTCurve 260.990,146.396,262.403,147.810>\n",
      "<LTCurve 260.882,146.288,262.512,147.918>\n",
      "<LTCurve 261.643,146.818,261.937,147.647>\n",
      "<LTCurve 256.213,136.612,258.263,137.475>\n",
      "<LTRect 266.753,152.153,267.242,152.913>\n",
      "<LTRect 266.100,151.772,266.589,152.370>\n",
      "<LTCurve 258.055,142.450,258.163,142.558>\n",
      "<LTCurve 258.053,142.641,258.161,142.749>\n",
      "<LTCurve 258.379,142.641,258.487,142.749>\n",
      "<LTCurve 258.381,143.207,258.489,143.315>\n",
      "<LTCurve 258.053,143.017,258.161,143.125>\n",
      "<LTCurve 257.997,143.335,258.106,143.443>\n",
      "<LTCurve 257.727,142.829,257.835,142.937>\n",
      "<LTCurve 257.396,142.453,257.504,142.562>\n",
      "<LTCurve 257.070,142.265,257.178,142.373>\n",
      "<LTCurve 257.070,142.079,257.178,142.188>\n",
      "<LTCurve 257.399,141.894,257.507,142.002>\n",
      "<LTCurve 256.745,142.084,256.853,142.193>\n",
      "<LTCurve 256.745,141.706,256.853,141.814>\n",
      "<LTCurve 256.419,141.709,256.527,141.817>\n",
      "<LTCurve 256.091,141.518,256.199,141.626>\n",
      "<LTCurve 256.091,141.140,256.200,141.248>\n",
      "<LTRect 251.694,141.444,253.325,141.498>\n",
      "<LTCurve 251.695,142.056,253.326,143.130>\n",
      "<LTCurve 251.695,141.607,253.326,142.812>\n",
      "<LTRect 257.783,136.986,258.055,137.040>\n",
      "<LTRect 257.783,138.290,258.055,138.345>\n",
      "<LTRect 257.728,137.367,257.783,138.020>\n",
      "<LTRect 258.054,137.367,258.108,138.020>\n",
      "<LTRect 257.728,138.019,258.109,138.074>\n",
      "<LTRect 257.728,137.367,258.109,137.421>\n",
      "<LTRect 257.728,137.585,258.109,137.639>\n",
      "<LTRect 265.991,136.931,266.045,137.312>\n",
      "<LTRect 265.991,137.911,266.045,138.183>\n",
      "<LTRect 265.882,136.877,266.154,136.931>\n",
      "<LTRect 265.882,138.182,266.154,138.236>\n",
      "<LTRect 265.828,137.258,265.882,137.910>\n",
      "<LTRect 266.154,137.258,266.209,137.910>\n",
      "<LTRect 265.828,137.911,266.209,137.965>\n",
      "<LTRect 265.828,137.258,266.209,137.312>\n",
      "<LTRect 265.828,137.475,266.209,137.530>\n",
      "<LTRect 266.589,136.552,266.643,136.932>\n",
      "<LTRect 266.589,137.748,266.643,137.911>\n",
      "<LTRect 266.481,136.497,266.753,136.552>\n",
      "<LTRect 266.481,137.911,266.753,137.965>\n",
      "<LTRect 266.426,136.877,266.480,137.801>\n",
      "<LTRect 266.753,136.877,266.807,137.801>\n",
      "<LTRect 266.426,137.748,266.807,137.802>\n",
      "<LTRect 266.426,136.877,266.807,136.931>\n",
      "<LTRect 266.426,137.203,266.807,137.258>\n",
      "<LTRect 267.187,136.606,267.241,137.042>\n",
      "<LTRect 267.187,138.019,267.241,138.400>\n",
      "<LTRect 267.079,136.606,267.351,136.660>\n",
      "<LTRect 267.079,138.345,267.351,138.399>\n",
      "<LTRect 267.025,137.095,267.080,138.073>\n",
      "<LTRect 267.350,137.095,267.404,138.073>\n",
      "<LTRect 267.025,138.019,267.405,138.074>\n",
      "<LTRect 267.025,137.041,267.405,137.095>\n",
      "<LTRect 267.025,137.475,267.405,137.530>\n",
      "<LTCurve 261.118,137.855,261.646,138.290>\n",
      "<LTCurve 260.696,137.583,261.226,138.073>\n",
      "<LTCurve 261.120,137.366,261.648,137.802>\n",
      "<LTCurve 260.698,137.095,261.228,137.584>\n",
      "<LTCurve 261.121,136.877,261.646,137.312>\n",
      "<LTCurve 260.700,136.605,261.230,137.095>\n",
      "<LTCurve 261.121,136.388,261.650,136.822>\n",
      "<LTCurve 262.384,137.583,262.916,138.073>\n",
      "<LTCurve 261.962,137.855,262.494,138.290>\n",
      "<LTCurve 261.539,137.583,262.070,138.019>\n",
      "<LTCurve 261.962,137.366,262.491,137.802>\n",
      "<LTCurve 261.961,138.345,262.493,138.780>\n",
      "<LTCurve 261.539,138.073,262.069,138.508>\n",
      "<LTCurve 261.541,137.095,262.068,137.584>\n",
      "<LTCurve 261.960,136.877,262.490,137.312>\n",
      "<LTCurve 261.544,136.605,262.068,137.095>\n",
      "<LTFigure(Im2) 233.163,130.295,258.462,168.385 matrix=[25.30,0.00,0.00,38.09, (233.16,130.30)]>\n",
      "<LTFigure(Im3) 242.714,154.332,258.458,168.381 matrix=[15.74,0.00,0.00,14.05, (242.71,154.33)]>\n",
      "<LTFigure(Im4) 253.516,135.101,258.462,154.463 matrix=[4.95,0.00,0.00,19.36, (253.52,135.10)]>\n",
      "<LTFigure(Im5) 257.826,136.938,258.038,137.433 matrix=[0.21,0.00,0.00,0.49, (257.83,136.94)]>\n",
      "<LTFigure(Im6) 257.826,137.998,258.038,138.351 matrix=[0.21,0.00,0.00,0.35, (257.83,138.00)]>\n",
      "<LTFigure(Im7) 256.272,151.071,256.625,152.131 matrix=[0.35,0.00,0.00,1.06, (256.27,151.07)]>\n",
      "<LTFigure(Im8) 256.625,151.354,256.978,152.131 matrix=[0.35,0.00,0.00,0.78, (256.62,151.35)]>\n",
      "<LTFigure(Im9) 256.978,151.566,257.332,152.131 matrix=[0.35,0.00,0.00,0.57, (256.98,151.57)]>\n",
      "<LTFigure(Im10) 257.261,152.061,257.614,152.555 matrix=[0.35,0.00,0.00,0.49, (257.26,152.06)]>\n",
      "<LTFigure(Im11) 257.614,152.061,257.968,152.838 matrix=[0.35,0.00,0.00,0.78, (257.61,152.06)]>\n",
      "<LTFigure(Im12) 257.897,152.061,258.321,153.050 matrix=[0.42,0.00,0.00,0.99, (257.90,152.06)]>\n",
      "<LTFigure(Im13) 257.332,146.266,258.180,147.255 matrix=[0.85,0.00,0.00,0.99, (257.33,146.27)]>\n",
      "<LTFigure(Im14) 256.554,146.195,257.402,146.902 matrix=[0.85,0.00,0.00,0.71, (256.55,146.20)]>\n",
      "<LTFigure(Im15) 256.413,146.619,258.180,147.962 matrix=[1.77,0.00,0.00,1.34, (256.41,146.62)]>\n",
      "<LTFigure(Im16) 256.130,136.514,258.321,137.574 matrix=[2.19,0.00,0.00,1.06, (256.13,136.51)]>\n",
      "<LTFigure(Im17) 257.968,142.379,258.250,142.662 matrix=[0.28,0.00,0.00,0.28, (257.97,142.38)]>\n",
      "<LTFigure(Im17) 257.968,142.591,258.250,142.803 matrix=[0.28,0.00,0.00,0.21, (257.97,142.59)]>\n",
      "<LTFigure(Im18) 258.321,142.591,258.533,142.803 matrix=[0.21,0.00,0.00,0.21, (258.32,142.59)]>\n",
      "<LTFigure(Im18) 258.321,143.157,258.533,143.369 matrix=[0.21,0.00,0.00,0.21, (258.32,143.16)]>\n",
      "<LTFigure(Im17) 257.968,142.945,258.250,143.227 matrix=[0.28,0.00,0.00,0.28, (257.97,142.94)]>\n",
      "<LTFigure(Im17) 257.897,143.298,258.180,143.510 matrix=[0.28,0.00,0.00,0.21, (257.90,143.30)]>\n",
      "<LTFigure(Im17) 257.685,142.733,257.897,143.015 matrix=[0.21,0.00,0.00,0.28, (257.69,142.73)]>\n",
      "<LTFigure(Im19) 257.332,142.379,257.544,142.662 matrix=[0.21,0.00,0.00,0.28, (257.33,142.38)]>\n",
      "<LTFigure(Im20) 256.978,142.167,257.261,142.450 matrix=[0.28,0.00,0.00,0.28, (256.98,142.17)]>\n",
      "<LTFigure(Im20) 256.978,142.026,257.261,142.238 matrix=[0.28,0.00,0.00,0.21, (256.98,142.03)]>\n",
      "<LTFigure(Im19) 257.332,141.814,257.544,142.097 matrix=[0.21,0.00,0.00,0.28, (257.33,141.81)]>\n",
      "<LTFigure(Im21) 256.696,142.026,256.908,142.238 matrix=[0.21,0.00,0.00,0.21, (256.70,142.03)]>\n",
      "<LTFigure(Im22) 257.685,137.362,258.109,137.645 matrix=[0.42,0.00,0.00,0.28, (257.69,137.36)]>\n",
      "<LTFigure(Im23) 257.685,137.574,258.109,138.069 matrix=[0.42,0.00,0.00,0.49, (257.68,137.57)]>\n",
      "<LTFigure(Im24) 256.696,141.602,256.908,141.885 matrix=[0.21,0.00,0.00,0.28, (256.70,141.60)]>\n",
      "<LTFigure(Im25) 256.342,141.673,256.625,141.885 matrix=[0.28,0.00,0.00,0.21, (256.34,141.67)]>\n",
      "<LTFigure(Im26) 255.989,141.461,256.272,141.673 matrix=[0.28,0.00,0.00,0.21, (255.99,141.46)]>\n",
      "<LTFigure(Im26) 255.989,141.037,256.272,141.319 matrix=[0.28,0.00,0.00,0.28, (255.99,141.04)]>\n",
      "<LTFigure(Im27) 257.685,136.867,258.109,137.079 matrix=[0.42,0.00,0.00,0.21, (257.68,136.87)]>\n",
      "<LTFigure(Im27) 257.685,138.210,258.109,138.422 matrix=[0.42,0.00,0.00,0.21, (257.68,138.21)]>\n",
      "<LTFigure(Im28) 257.685,137.291,257.897,138.069 matrix=[0.21,0.00,0.00,0.78, (257.68,137.29)]>\n",
      "<LTFigure(Im28) 257.968,137.291,258.180,138.069 matrix=[0.21,0.00,0.00,0.78, (257.97,137.29)]>\n",
      "<LTFigure(Im29) 257.685,137.927,258.180,138.139 matrix=[0.49,0.00,0.00,0.21, (257.68,137.93)]>\n",
      "<LTFigure(Im29) 257.685,137.291,258.180,137.503 matrix=[0.49,0.00,0.00,0.21, (257.68,137.29)]>\n",
      "<LTFigure(Im29) 257.685,137.503,258.180,137.715 matrix=[0.49,0.00,0.00,0.21, (257.68,137.50)]>\n",
      "<LTFigure(Im30) 234.904,135.086,253.657,166.639 matrix=[18.75,0.00,0.00,31.55, (234.90,135.09)]>\n",
      "<LTFigure(Im31) 242.729,151.886,253.657,166.639 matrix=[10.93,0.00,0.00,14.75, (242.73,151.89)]>\n",
      "<LTFigure(Im32) 250.344,135.157,253.657,154.498 matrix=[3.31,0.00,0.00,19.34, (250.34,135.16)]>\n",
      "<LTFigure(Im33) 251.613,151.181,252.106,152.663 matrix=[0.49,0.00,0.00,1.48, (251.61,151.18)]>\n",
      "<LTFigure(Im34) 252.036,151.181,252.529,152.381 matrix=[0.49,0.00,0.00,1.20, (252.04,151.18)]>\n",
      "<LTFigure(Im35) 252.459,151.181,252.952,151.816 matrix=[0.49,0.00,0.00,0.64, (252.46,151.18)]>\n",
      "<LTFigure(Im36) 252.952,151.181,253.375,152.945 matrix=[0.42,0.00,0.00,1.76, (252.95,151.18)]>\n",
      "<LTFigure(Im37) 252.318,147.086,253.375,148.004 matrix=[1.06,0.00,0.00,0.92, (252.32,147.09)]>\n",
      "<LTFigure(Im38) 252.529,146.381,253.375,147.228 matrix=[0.85,0.00,0.00,0.85, (252.53,146.38)]>\n",
      "<LTFigure(Im39) 251.613,146.239,252.952,147.933 matrix=[1.34,0.00,0.00,1.69, (251.61,146.24)]>\n",
      "<LTFigure(Im35) 251.613,137.839,252.106,138.333 matrix=[0.49,0.00,0.00,0.49, (251.61,137.84)]>\n",
      "<LTFigure(Im35) 251.613,137.486,252.106,137.981 matrix=[0.49,0.00,0.00,0.49, (251.61,137.49)]>\n",
      "<LTFigure(Im22) 251.613,137.204,252.106,137.628 matrix=[0.49,0.00,0.00,0.42, (251.61,137.20)]>\n",
      "<LTFigure(Im35) 251.613,136.851,252.106,137.345 matrix=[0.49,0.00,0.00,0.49, (251.61,136.85)]>\n",
      "<LTFigure(Im40) 251.613,136.569,252.106,136.992 matrix=[0.49,0.00,0.00,0.42, (251.61,136.57)]>\n",
      "<LTFigure(Im35) 252.600,137.839,253.093,138.333 matrix=[0.49,0.00,0.00,0.49, (252.60,137.84)]>\n",
      "<LTFigure(Im41) 252.600,137.486,253.093,137.981 matrix=[0.49,0.00,0.00,0.49, (252.60,137.49)]>\n",
      "<LTFigure(Im42) 252.600,137.204,253.093,137.628 matrix=[0.49,0.00,0.00,0.42, (252.60,137.20)]>\n",
      "<LTFigure(Im23) 251.965,137.839,252.388,138.333 matrix=[0.42,0.00,0.00,0.49, (251.97,137.84)]>\n",
      "<LTFigure(Im23) 251.965,137.486,252.388,137.981 matrix=[0.42,0.00,0.00,0.49, (251.97,137.49)]>\n",
      "<LTFigure(Im43) 251.965,137.204,252.388,137.628 matrix=[0.42,0.00,0.00,0.42, (251.97,137.20)]>\n",
      "<LTFigure(Im44) 251.965,136.851,252.388,137.345 matrix=[0.42,0.00,0.00,0.49, (251.97,136.85)]>\n",
      "<LTFigure(Im45) 251.965,136.569,252.388,136.992 matrix=[0.42,0.00,0.00,0.42, (251.97,136.57)]>\n",
      "<LTFigure(Im23) 252.952,137.839,253.375,138.333 matrix=[0.42,0.00,0.00,0.49, (252.95,137.84)]>\n",
      "<LTFigure(Im23) 252.952,137.486,253.375,137.981 matrix=[0.42,0.00,0.00,0.49, (252.95,137.49)]>\n",
      "<LTFigure(Im43) 252.952,137.204,253.375,137.628 matrix=[0.42,0.00,0.00,0.42, (252.95,137.20)]>\n",
      "<LTFigure(Im35) 252.247,137.839,252.741,138.333 matrix=[0.49,0.00,0.00,0.49, (252.25,137.84)]>\n",
      "<LTFigure(Im46) 252.247,137.486,252.741,137.981 matrix=[0.49,0.00,0.00,0.49, (252.25,137.49)]>\n",
      "<LTFigure(Im47) 252.247,137.204,252.741,137.628 matrix=[0.49,0.00,0.00,0.42, (252.25,137.20)]>\n",
      "<LTFigure(Im48) 252.247,136.851,252.741,137.345 matrix=[0.49,0.00,0.00,0.49, (252.25,136.85)]>\n",
      "<LTFigure(Im49) 252.247,136.569,252.741,136.992 matrix=[0.49,0.00,0.00,0.42, (252.25,136.57)]>\n",
      "<LTFigure(Im50) 252.600,136.851,253.093,137.345 matrix=[0.49,0.00,0.00,0.49, (252.60,136.85)]>\n",
      "<LTFigure(Im51) 252.600,136.569,253.093,136.992 matrix=[0.49,0.00,0.00,0.42, (252.60,136.57)]>\n",
      "<LTFigure(Im52) 252.952,136.851,253.375,137.345 matrix=[0.42,0.00,0.00,0.49, (252.95,136.85)]>\n",
      "<LTFigure(Im53) 252.952,136.569,253.375,136.992 matrix=[0.42,0.00,0.00,0.42, (252.95,136.57)]>\n",
      "<LTFigure(Im54) 251.613,141.369,253.375,141.581 matrix=[1.76,0.00,0.00,0.21, (251.61,141.37)]>\n",
      "<LTFigure(Im55) 251.613,142.004,253.375,143.204 matrix=[1.76,0.00,0.00,1.20, (251.61,142.00)]>\n",
      "<LTFigure(Im56) 251.613,141.510,253.375,142.851 matrix=[1.76,0.00,0.00,1.34, (251.61,141.51)]>\n",
      "<LTFigure(Fm2) 132.243,87.546,335.553,128.562 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im57) 137.171,45.285,198.942,82.618 matrix=[61.77,0.00,0.00,37.33, (137.17,45.28)]>\n",
      "<LTCurve 326.902,263.735,331.964,268.797>\n",
      "<LTCurve 136.881,90.076,141.943,95.138>\n",
      "<LTCurve 326.637,184.278,331.699,189.340>\n",
      "<LTCurve 326.902,89.945,331.964,95.007>\n",
      "<LTCurve 232.669,90.076,237.731,95.138>\n",
      "<LTFigure(Fm3) 142.647,307.972,352.918,549.488 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Fm4) 99.095,313.588,289.267,536.099 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im58) 120.143,237.630,313.593,294.726 matrix=[193.45,0.00,0.00,57.10, (120.14,237.63)]>\n",
      "<LTRect 323.357,201.749,327.224,211.010>\n",
      "<LTRect 327.224,208.689,331.516,211.010>\n",
      "<LTRect 331.515,208.689,335.271,211.010>\n",
      "<LTRect 331.515,201.749,335.271,208.689>\n",
      "<LTRect 327.224,201.749,331.516,208.689>\n",
      "<LTLine 323.356,208.689,327.224,208.689>\n",
      "<LTLine 323.358,206.302,335.109,206.302>\n",
      "<LTLine 323.358,204.036,335.109,204.036>\n",
      "<LTFigure(Fm5) 322.076,180.841,704.746,308.120 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im59) 326.267,152.615,378.504,176.668 matrix=[52.24,0.00,0.00,24.05, (326.27,152.62)]>\n",
      "<LTRect 326.680,152.707,378.424,176.294>\n",
      "<LTCurve 326.634,152.706,378.424,176.340>\n",
      "<LTRect 326.727,169.878,378.425,172.740>\n",
      "<LTRect 326.633,154.927,378.425,157.235>\n",
      "<LTCurve 326.587,154.881,378.425,157.281>\n",
      "<LTRect 326.633,159.543,378.425,161.851>\n",
      "<LTCurve 326.587,159.496,378.425,161.897>\n",
      "<LTRect 326.633,164.158,378.425,166.466>\n",
      "<LTCurve 326.587,164.112,378.425,166.512>\n",
      "<LTRect 331.342,164.616,332.819,166.093>\n",
      "<LTRect 326.634,168.955,350.360,169.878>\n",
      "<LTRect 371.039,168.955,378.424,169.878>\n",
      "<LTRect 350.360,168.955,370.947,169.878>\n",
      "<LTRect 350.267,152.707,350.360,176.341>\n",
      "<LTRect 370.947,152.707,371.040,176.341>\n",
      "<LTRect 371.040,154.923,378.425,157.323>\n",
      "<LTFigure(Im60) 332.252,152.606,362.945,164.190 matrix=[30.69,0.00,0.00,11.58, (332.25,152.61)]>\n",
      "<LTFigure(Im61) 332.252,154.894,362.945,157.255 matrix=[30.69,0.00,0.00,2.36, (332.25,154.89)]>\n",
      "<LTFigure(Im62) 332.252,154.820,362.945,157.328 matrix=[30.69,0.00,0.00,2.51, (332.25,154.82)]>\n",
      "<LTFigure(Im61) 332.252,159.542,362.945,161.903 matrix=[30.69,0.00,0.00,2.36, (332.25,159.54)]>\n",
      "<LTFigure(Im62) 332.252,159.394,362.945,161.977 matrix=[30.69,0.00,0.00,2.58, (332.25,159.39)]>\n",
      "<LTFigure(Im63) 332.252,164.116,362.945,165.002 matrix=[30.69,0.00,0.00,0.89, (332.25,164.12)]>\n",
      "<LTFigure(Im64) 332.252,164.042,362.945,164.264 matrix=[30.69,0.00,0.00,0.22, (332.25,164.04)]>\n",
      "<LTFigure(Im65) 332.252,164.559,332.916,165.002 matrix=[0.66,0.00,0.00,0.44, (332.25,164.56)]>\n",
      "<LTFigure(Im66) 332.252,162.345,338.597,163.600 matrix=[6.35,0.00,0.00,1.25, (332.25,162.35)]>\n",
      "<LTFigure(Im67) 332.252,162.345,338.597,163.600 matrix=[6.35,0.00,0.00,1.25, (332.25,162.35)]>\n",
      "<LTFigure(Im68) 332.252,160.058,332.769,161.239 matrix=[0.52,0.00,0.00,1.18, (332.25,160.06)]>\n",
      "<LTFigure(Im69) 332.252,160.058,332.769,161.239 matrix=[0.52,0.00,0.00,1.18, (332.25,160.06)]>\n",
      "<LTFigure(Im70) 332.842,160.280,333.285,161.017 matrix=[0.44,0.00,0.00,0.74, (332.84,160.28)]>\n",
      "<LTFigure(Im71) 332.842,160.280,333.285,161.017 matrix=[0.44,0.00,0.00,0.74, (332.84,160.28)]>\n",
      "<LTFigure(Im72) 333.211,160.280,335.425,161.239 matrix=[2.21,0.00,0.00,0.96, (333.21,160.28)]>\n",
      "<LTFigure(Im73) 333.211,160.280,335.425,161.239 matrix=[2.21,0.00,0.00,0.96, (333.21,160.28)]>\n",
      "<LTFigure(Im74) 335.425,160.280,336.015,161.017 matrix=[0.59,0.00,0.00,0.74, (335.42,160.28)]>\n",
      "<LTFigure(Im74) 335.425,160.280,336.015,161.017 matrix=[0.59,0.00,0.00,0.74, (335.42,160.28)]>\n",
      "<LTFigure(Im75) 335.941,160.058,339.409,161.239 matrix=[3.47,0.00,0.00,1.18, (335.94,160.06)]>\n",
      "<LTFigure(Im75) 335.941,160.058,339.409,161.239 matrix=[3.47,0.00,0.00,1.18, (335.94,160.06)]>\n",
      "<LTFigure(Im76) 339.409,160.280,340.589,161.017 matrix=[1.18,0.00,0.00,0.74, (339.41,160.28)]>\n",
      "<LTFigure(Im76) 339.409,160.280,340.589,161.017 matrix=[1.18,0.00,0.00,0.74, (339.41,160.28)]>\n",
      "<LTFigure(Im76) 340.516,160.280,341.622,161.239 matrix=[1.11,0.00,0.00,0.96, (340.52,160.28)]>\n",
      "<LTFigure(Im76) 340.516,160.280,341.622,161.239 matrix=[1.11,0.00,0.00,0.96, (340.52,160.28)]>\n",
      "<LTFigure(Im77) 332.252,157.919,338.376,158.952 matrix=[6.12,0.00,0.00,1.03, (332.25,157.92)]>\n",
      "<LTFigure(Im78) 332.252,157.919,338.376,158.952 matrix=[6.12,0.00,0.00,1.03, (332.25,157.92)]>\n",
      "<LTFigure(Im79) 338.450,157.992,338.892,158.730 matrix=[0.44,0.00,0.00,0.74, (338.45,157.99)]>\n",
      "<LTFigure(Im79) 338.450,157.992,338.892,158.730 matrix=[0.44,0.00,0.00,0.74, (338.45,157.99)]>\n",
      "<LTFigure(Im80) 338.819,157.697,339.483,158.730 matrix=[0.66,0.00,0.00,1.03, (338.82,157.70)]>\n",
      "<LTFigure(Im80) 338.819,157.697,339.483,158.730 matrix=[0.66,0.00,0.00,1.03, (338.82,157.70)]>\n",
      "<LTFigure(Im81) 332.252,155.484,333.654,156.591 matrix=[1.40,0.00,0.00,1.11, (332.25,155.48)]>\n",
      "<LTFigure(Im82) 332.252,155.484,333.654,156.591 matrix=[1.40,0.00,0.00,1.11, (332.25,155.48)]>\n",
      "<LTFigure(Im79) 333.728,155.631,334.170,156.369 matrix=[0.44,0.00,0.00,0.74, (333.73,155.63)]>\n",
      "<LTFigure(Im79) 333.728,155.631,334.170,156.369 matrix=[0.44,0.00,0.00,0.74, (333.73,155.63)]>\n",
      "<LTFigure(Im83) 334.023,155.631,334.392,155.927 matrix=[0.37,0.00,0.00,0.30, (334.02,155.63)]>\n",
      "<LTFigure(Im83) 334.023,155.631,334.392,155.927 matrix=[0.37,0.00,0.00,0.30, (334.02,155.63)]>\n",
      "<LTFigure(Im74) 334.613,155.631,335.277,156.591 matrix=[0.66,0.00,0.00,0.96, (334.61,155.63)]>\n",
      "<LTFigure(Im74) 334.613,155.631,335.277,156.591 matrix=[0.66,0.00,0.00,0.96, (334.61,155.63)]>\n",
      "<LTFigure(Im84) 335.203,155.631,338.007,156.664 matrix=[2.80,0.00,0.00,1.03, (335.20,155.63)]>\n",
      "<LTFigure(Im84) 335.203,155.631,338.007,156.664 matrix=[2.80,0.00,0.00,1.03, (335.20,155.63)]>\n",
      "<LTFigure(Im85) 332.252,153.123,332.842,154.303 matrix=[0.59,0.00,0.00,1.18, (332.25,153.12)]>\n",
      "<LTFigure(Im86) 332.252,153.123,332.842,154.303 matrix=[0.59,0.00,0.00,1.18, (332.25,153.12)]>\n",
      "<LTFigure(Im87) 332.916,153.344,333.433,154.082 matrix=[0.52,0.00,0.00,0.74, (332.92,153.34)]>\n",
      "<LTFigure(Im88) 332.916,153.344,333.433,154.082 matrix=[0.52,0.00,0.00,0.74, (332.92,153.34)]>\n",
      "<LTFigure(Im89) 333.285,153.123,335.867,154.377 matrix=[2.58,0.00,0.00,1.25, (333.29,153.12)]>\n",
      "<LTFigure(Im90) 333.285,153.123,335.867,154.377 matrix=[2.58,0.00,0.00,1.25, (333.29,153.12)]>\n",
      "<LTFigure(Im79) 335.941,153.344,336.384,154.082 matrix=[0.44,0.00,0.00,0.74, (335.94,153.34)]>\n",
      "<LTFigure(Im79) 335.941,153.344,336.384,154.082 matrix=[0.44,0.00,0.00,0.74, (335.94,153.34)]>\n",
      "<LTFigure(Im75) 336.310,153.123,339.778,154.303 matrix=[3.47,0.00,0.00,1.18, (336.31,153.12)]>\n",
      "<LTFigure(Im75) 336.310,153.123,339.778,154.303 matrix=[3.47,0.00,0.00,1.18, (336.31,153.12)]>\n",
      "<LTFigure(Im80) 339.778,153.049,340.442,154.082 matrix=[0.66,0.00,0.00,1.03, (339.78,153.05)]>\n",
      "<LTFigure(Im80) 339.778,153.049,340.442,154.082 matrix=[0.66,0.00,0.00,1.03, (339.78,153.05)]>\n",
      "<LTFigure(Im74) 340.368,153.344,340.958,154.082 matrix=[0.59,0.00,0.00,0.74, (340.37,153.34)]>\n",
      "<LTFigure(Im74) 340.368,153.344,340.958,154.082 matrix=[0.59,0.00,0.00,0.74, (340.37,153.34)]>\n",
      "<LTFigure(Im90) 340.884,153.123,343.467,154.377 matrix=[2.58,0.00,0.00,1.25, (340.88,153.12)]>\n",
      "<LTFigure(Im90) 340.884,153.123,343.467,154.377 matrix=[2.58,0.00,0.00,1.25, (340.88,153.12)]>\n",
      "<LTFigure(Im91) 351.361,164.780,352.247,165.002 matrix=[0.89,0.00,0.00,0.22, (351.36,164.78)]>\n",
      "<LTFigure(Im92) 351.361,164.780,352.247,165.002 matrix=[0.89,0.00,0.00,0.22, (351.36,164.78)]>\n",
      "<LTFigure(Im91) 352.321,164.780,353.280,165.002 matrix=[0.96,0.00,0.00,0.22, (352.32,164.78)]>\n",
      "<LTFigure(Im92) 352.321,164.780,353.280,165.002 matrix=[0.96,0.00,0.00,0.22, (352.32,164.78)]>\n",
      "<LTFigure(Im93) 353.280,164.780,353.870,165.002 matrix=[0.59,0.00,0.00,0.22, (353.28,164.78)]>\n",
      "<LTFigure(Im94) 353.280,164.780,353.870,165.002 matrix=[0.59,0.00,0.00,0.22, (353.28,164.78)]>\n",
      "<LTFigure(Im79) 351.288,162.493,351.730,163.452 matrix=[0.44,0.00,0.00,0.96, (351.29,162.49)]>\n",
      "<LTFigure(Im79) 351.288,162.493,351.730,163.452 matrix=[0.44,0.00,0.00,0.96, (351.29,162.49)]>\n",
      "<LTFigure(Im95) 351.656,162.419,354.239,163.452 matrix=[2.58,0.00,0.00,1.03, (351.66,162.42)]>\n",
      "<LTFigure(Im95) 351.656,162.419,354.239,163.452 matrix=[2.58,0.00,0.00,1.03, (351.66,162.42)]>\n",
      "<LTFigure(Im74) 354.239,162.419,354.903,163.231 matrix=[0.66,0.00,0.00,0.81, (354.24,162.42)]>\n",
      "<LTFigure(Im74) 354.239,162.419,354.903,163.231 matrix=[0.66,0.00,0.00,0.81, (354.24,162.42)]>\n",
      "<LTFigure(Im96) 351.288,160.132,351.730,161.165 matrix=[0.44,0.00,0.00,1.03, (351.29,160.13)]>\n",
      "<LTFigure(Im96) 351.288,160.132,351.730,161.165 matrix=[0.44,0.00,0.00,1.03, (351.29,160.13)]>\n",
      "<LTFigure(Im95) 351.656,160.132,354.239,161.165 matrix=[2.58,0.00,0.00,1.03, (351.66,160.13)]>\n",
      "<LTFigure(Im95) 351.656,160.132,354.239,161.165 matrix=[2.58,0.00,0.00,1.03, (351.66,160.13)]>\n",
      "<LTFigure(Im74) 354.239,160.132,354.903,160.870 matrix=[0.66,0.00,0.00,0.74, (354.24,160.13)]>\n",
      "<LTFigure(Im74) 354.239,160.132,354.903,160.870 matrix=[0.66,0.00,0.00,0.74, (354.24,160.13)]>\n",
      "<LTFigure(Im97) 351.361,157.845,352.247,158.583 matrix=[0.89,0.00,0.00,0.74, (351.36,157.84)]>\n",
      "<LTFigure(Im97) 351.361,157.845,352.247,158.583 matrix=[0.89,0.00,0.00,0.74, (351.36,157.84)]>\n",
      "<LTFigure(Im98) 352.321,157.845,353.280,158.878 matrix=[0.96,0.00,0.00,1.03, (352.32,157.84)]>\n",
      "<LTFigure(Im98) 352.321,157.845,353.280,158.878 matrix=[0.96,0.00,0.00,1.03, (352.32,157.84)]>\n",
      "<LTFigure(Im74) 353.280,157.845,353.870,158.583 matrix=[0.59,0.00,0.00,0.74, (353.28,157.84)]>\n",
      "<LTFigure(Im74) 353.280,157.845,353.870,158.583 matrix=[0.59,0.00,0.00,0.74, (353.28,157.84)]>\n",
      "<LTFigure(Im97) 351.361,155.558,352.247,156.295 matrix=[0.89,0.00,0.00,0.74, (351.36,155.56)]>\n",
      "<LTFigure(Im97) 351.361,155.558,352.247,156.295 matrix=[0.89,0.00,0.00,0.74, (351.36,155.56)]>\n",
      "<LTFigure(Im97) 352.321,155.558,353.280,156.517 matrix=[0.96,0.00,0.00,0.96, (352.32,155.56)]>\n",
      "<LTFigure(Im97) 352.321,155.558,353.280,156.517 matrix=[0.96,0.00,0.00,0.96, (352.32,155.56)]>\n",
      "<LTFigure(Im74) 353.280,155.558,353.870,156.295 matrix=[0.59,0.00,0.00,0.74, (353.28,155.56)]>\n",
      "<LTFigure(Im74) 353.280,155.558,353.870,156.295 matrix=[0.59,0.00,0.00,0.74, (353.28,155.56)]>\n",
      "<LTFigure(Im99) 350.181,152.606,350.402,165.002 matrix=[0.22,0.00,0.00,12.40, (350.18,152.61)]>\n",
      "<LTRect 332.926,131.567,362.191,164.432>\n",
      "<LTRect 332.926,152.664,362.191,155.249>\n",
      "<LTRect 334.182,140.965,334.782,141.806>\n",
      "<LTCurve 334.277,141.440,334.686,141.745>\n",
      "<LTCurve 334.518,141.079,334.722,141.691>\n",
      "<LTCurve 334.241,141.079,334.445,141.691>\n",
      "<LTCurve 334.277,141.025,334.686,141.331>\n",
      "<LTRect 334.182,138.380,334.782,139.221>\n",
      "<LTCurve 334.277,138.854,334.686,139.161>\n",
      "<LTCurve 334.518,138.495,334.722,139.107>\n",
      "<LTCurve 334.241,138.495,334.445,139.107>\n",
      "<LTCurve 334.277,138.441,334.686,138.747>\n",
      "<LTRect 334.353,135.796,334.953,136.637>\n",
      "<LTCurve 334.450,136.270,334.858,136.576>\n",
      "<LTCurve 334.689,135.909,334.894,136.522>\n",
      "<LTCurve 334.413,135.909,334.617,136.522>\n",
      "<LTCurve 334.450,135.855,334.858,136.161>\n",
      "<LTRect 334.053,133.210,334.653,134.051>\n",
      "<LTCurve 334.149,133.684,334.557,133.990>\n",
      "<LTCurve 334.389,133.324,334.594,133.937>\n",
      "<LTCurve 334.113,133.324,334.318,133.937>\n",
      "<LTCurve 334.149,133.270,334.557,133.576>\n",
      "<LTRect 332.926,158.203,362.191,158.296>\n",
      "<LTRect 332.926,151.858,362.191,151.950>\n",
      "<LTRect 332.926,143.246,362.191,143.338>\n",
      "<LTCurve 346.094,151.759,347.492,153.569>\n",
      "<LTCurve 346.505,153.487,346.669,153.569>\n",
      "<LTCurve 346.341,152.583,346.505,153.487>\n",
      "<LTCurve 346.669,152.007,347.493,153.485>\n",
      "<LTCurve 346.093,152.583,346.341,152.829>\n",
      "<LTCurve 346.176,152.172,346.423,152.584>\n",
      "<LTCurve 346.423,151.759,347.328,152.173>\n",
      "<LTCurve 233.234,184.278,238.296,189.340>\n",
      "<LTCurve 228.237,200.934,243.210,211.824>\n",
      "<LTCurve 231.640,202.295,239.807,210.462>\n",
      "<LTCurve 233.681,204.337,237.765,208.421>\n",
      "<LTCurve 136.603,113.338,143.547,115.990>\n",
      "<LTCurve 136.603,111.065,143.547,112.328>\n",
      "<LTCurve 136.603,108.666,143.547,114.727>\n",
      "<LTCurve 136.603,106.519,143.547,110.055>\n",
      "<LTCurve 130.794,112.328,137.865,119.525>\n",
      "<LTCurve 132.771,114.324,135.889,117.485>\n",
      "<LTCurve 232.484,106.571,238.340,112.425>\n",
      "<LTRect 224.781,107.032,229.403,109.498>\n",
      "<LTCurve 227.694,114.308,231.112,117.725>\n",
      "<LTCurve 227.091,109.498,229.403,114.308>\n",
      "<LTCurve 231.112,112.425,235.413,116.017>\n",
      "<LTLine 229.403,108.266,232.484,108.266>\n",
      "<LTCurve 234.035,108.476,236.789,110.520>\n",
      "<LTCurve 323.107,109.111,336.227,117.736>\n",
      "<LTLine 329.667,106.559,329.667,109.111>\n",
      "<LTLine 327.723,106.559,331.610,106.559>\n",
      "<LTRect 326.508,110.934,327.845,112.513>\n",
      "<LTRect 329.180,110.934,330.517,113.849>\n",
      "<LTRect 331.610,110.933,332.947,115.913>\n",
      "<LTCurve 323.107,109.111,336.227,117.736>\n",
      "<LTLine 329.667,106.559,329.667,109.111>\n",
      "<LTRect 326.508,110.934,327.845,112.513>\n",
      "<LTRect 329.180,110.934,330.517,113.849>\n",
      "<LTRect 331.610,110.933,332.947,115.913>\n",
      "<LTLine 235.200,95.138,235.200,104.207>\n",
      "<LTLine 139.559,95.138,139.559,104.207>\n",
      "<LTLine 329.579,95.138,329.579,104.207>\n",
      "<LTFigure(Im100) 326.215,279.746,350.612,287.483 matrix=[24.40,0.00,0.00,7.74, (326.22,279.75)]>\n",
      "<LTLine 139.292,189.243,139.292,198.312>\n",
      "<LTFigure(Fm6) 133.114,181.135,333.705,311.701 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTRect 137.171,134.605,196.568,177.079>\n",
      "<LTFigure(Im101) 136.988,133.973,196.385,176.077 matrix=[59.40,0.00,0.00,42.10, (136.99,133.97)]>\n",
      "<LTCurve 136.761,184.105,141.823,189.167>\n",
      "<LTCurve 135.649,201.041,138.763,211.856>\n",
      "<LTCurve 133.941,202.528,135.471,210.426>\n",
      "<LTCurve 135.471,208.738,138.763,212.136>\n",
      "<LTCurve 134.436,207.334,135.427,208.548>\n",
      "<LTCurve 139.587,200.915,144.669,212.101>\n",
      "<LTLine 141.164,204.068,141.164,205.814>\n",
      "<LTCurve 140.681,201.919,141.692,202.931>\n",
      "<LTCurve 140.681,206.017,141.692,207.028>\n",
      "<LTCurve 142.373,208.449,143.385,209.461>\n",
      "<LTCurve 142.373,204.596,143.385,205.608>\n",
      "<LTCurve 140.530,209.992,141.541,211.004>\n",
      "<LTLine 141.187,205.101,142.373,205.101>\n",
      "<LTLine 142.879,205.608,142.879,208.449>\n",
      "<LTCurve 141.036,208.955,142.373,209.992>\n",
      "<LTLine 142.879,207.028,145.109,207.028>\n",
      "<LTLine 141.164,202.931,141.164,204.068>\n",
      "<LTLine 139.646,206.522,140.681,206.522>\n",
      "<LTFigure(Fm7) 322.140,88.309,706.221,132.481 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTRect 327.830,50.002,378.294,82.618>\n",
      "<LTFigure(Im102) 327.711,49.378,378.942,83.248 matrix=[51.23,0.00,0.00,33.87, (327.71,49.38)]>\n",
      "<LTFigure(Fm8) 346.204,60.233,710.212,100.119 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im103) 349.684,43.268,360.543,56.737 matrix=[10.86,0.00,0.00,13.47, (349.68,43.27)]>\n",
      "<LTFigure(Fm9) 368.185,60.233,754.174,100.119 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im104) 371.665,43.273,382.524,56.742 matrix=[10.86,0.00,0.00,13.47, (371.66,43.27)]>\n",
      "<LTFigure(Fm10) 357.195,60.233,732.194,100.119 matrix=[1.00,0.00,0.00,1.00, (0.00,0.00)]>\n",
      "<LTFigure(Im105) 360.685,43.253,371.543,56.757 matrix=[10.86,0.00,0.00,13.50, (360.68,43.25)]>\n",
      "<LTCurve 356.983,282.731,359.077,285.262>\n",
      "<LTCurve 357.457,283.086,358.498,283.894>\n",
      "<LTCurve 359.197,282.791,362.478,285.197>\n",
      "<LTCurve 362.670,282.721,364.456,285.267>\n",
      "<LTCurve 356.682,280.396,364.011,282.052>\n",
      "<LTCurve 363.317,280.760,364.761,282.118>\n",
      "<LTCurve 326.820,290.703,330.004,292.065>\n",
      "<LTCurve 328.705,288.116,331.162,291.656>\n",
      "<LTCurve 326.768,288.123,328.709,289.042>\n",
      "<LTCurve 326.290,288.411,328.750,290.947>\n",
      "<LTCurve 332.689,288.731,335.438,291.559>\n",
      "<LTCurve 335.578,288.990,337.401,290.289>\n",
      "<LTCurve 335.972,289.246,337.004,290.036>\n",
      "<LTCurve 337.567,288.990,339.390,290.289>\n",
      "<LTCurve 337.962,289.246,338.993,290.036>\n",
      "<LTCurve 339.553,287.916,341.298,290.553>\n",
      "<LTCurve 339.950,289.088,340.945,290.184>\n",
      "<LTRect 341.572,288.786,341.972,291.457>\n",
      "<LTCurve 342.164,288.731,343.848,290.549>\n",
      "<LTCurve 342.556,289.667,343.368,290.197>\n",
      "<LTCurve 344.657,288.731,346.922,291.307>\n",
      "<LTRect 347.197,288.786,347.513,291.253>\n",
      "<LTCurve 347.785,288.987,349.520,290.270>\n",
      "<LTCurve 348.101,289.185,349.203,290.072>\n",
      "<LTCurve 349.779,288.725,351.274,290.473>\n",
      "<LTCurve 351.545,288.731,353.238,291.253>\n",
      "<LTCurve 351.863,289.020,352.933,290.074>\n",
      "<LTCurve 358.858,287.499,360.499,289.307>\n",
      "<LTCurve 356.089,289.087,357.748,290.930>\n",
      "<LTCurve 356.661,290.711,358.310,292.524>\n",
      "<LTCurve 358.005,289.434,359.160,290.589>\n",
      "<LTCurve 358.381,289.810,358.785,290.214>\n",
      "<LTCurve 358.859,290.711,360.500,292.524>\n",
      "<LTCurve 356.661,287.499,358.310,289.343>\n",
      "<LTCurve 359.406,289.122,361.048,290.895>\n",
      "<LTCurve 375.532,289.083,377.348,290.958>\n",
      "<LTCurve 375.765,290.181,377.113,290.742>\n",
      "<LTCurve 371.791,289.083,373.569,290.958>\n",
      "<LTCurve 372.007,289.299,373.353,290.745>\n",
      "<LTCurve 369.934,289.077,370.934,291.740>\n",
      "<LTCurve 371.253,289.082,371.474,291.745>\n",
      "<LTCurve 361.801,289.082,363.116,290.957>\n",
      "<LTCurve 363.451,289.083,364.983,290.954>\n",
      "<LTCurve 365.278,289.083,367.056,290.958>\n",
      "<LTCurve 365.494,289.299,366.840,290.745>\n",
      "<LTCurve 367.222,289.083,369.684,290.953>\n",
      "<LTCurve 373.948,289.083,375.413,291.746>\n",
      "<LTLine 88.950,337.567,245.368,337.567>\n",
      "<LTLine 47.397,414.573,56.748,414.573>\n",
      "<LTLine 445.747,414.573,455.098,414.573>\n",
      "<LTLine 47.397,-79.157,56.748,-79.157>\n",
      "<LTLine 445.747,-79.157,455.098,-79.157>\n",
      "<LTLine 60.488,418.313,60.488,427.664>\n",
      "<LTLine 60.488,-92.248,60.488,-82.897>\n",
      "<LTLine 442.007,418.313,442.007,427.664>\n",
      "<LTLine 442.007,-92.248,442.007,-82.897>\n",
      "<LTLine 47.397,414.573,56.748,414.573>\n",
      "<LTLine 445.747,414.573,455.098,414.573>\n",
      "<LTLine 47.397,-79.157,56.748,-79.157>\n",
      "<LTLine 445.747,-79.157,455.098,-79.157>\n",
      "<LTLine 60.488,418.313,60.488,427.664>\n",
      "<LTLine 60.488,-92.248,60.488,-82.897>\n",
      "<LTLine 442.007,418.313,442.007,427.664>\n",
      "<LTLine 442.007,-92.248,442.007,-82.897>\n",
      "<LTRect 0.000,0.000,504.000,19.947>\n",
      "<LTCurve -25.149,465.782,141.447,632.378>\n",
      "<LTRect 68.178,517.208,245.338,555.988>\n",
      "<LTRect 68.179,563.029,301.765,601.809>\n",
      "<LTLine 101.435,346.936,101.435,423.358>\n",
      "<LTCurve 96.546,415.323,106.326,425.103>\n",
      "<LTCurve 96.546,390.884,106.326,400.664>\n",
      "<LTCurve 96.546,358.259,106.326,368.039>\n",
      "<LTCurve 96.546,342.443,106.326,352.223>\n",
      "<LTCurve 296.981,344.690,336.237,344.690>\n",
      "<LTLine 171.585,463.069,171.585,499.579>\n",
      "<LTRect 436.932,601.809,504.000,636.022>\n",
      "<LTCurve 473.849,621.146,476.354,626.570>\n",
      "<LTCurve 472.279,619.962,477.758,629.984>\n",
      "<LTCurve 480.525,621.064,482.783,624.065>\n",
      "<LTCurve 479.038,619.962,484.820,627.755>\n",
      "<LTCurve 485.177,620.072,488.894,629.668>\n",
      "<LTCurve 491.000,621.064,493.258,624.065>\n",
      "<LTCurve 489.513,619.962,495.295,627.755>\n",
      "<LTRect 472.637,608.092,474.124,615.499>\n",
      "<LTRect 472.637,616.545,474.124,618.032>\n",
      "<LTCurve 475.762,608.092,481.529,617.922>\n",
      "<LTCurve 482.273,607.900,487.463,615.499>\n",
      "<LTCurve 449.634,612.353,462.630,624.928>\n",
      "<LTRect 456.912,615.797,462.585,616.676>\n",
      "<LTCurve 445.143,607.645,467.737,630.239>\n",
      "<LTCurve 459.937,623.700,460.831,624.594>\n",
      "<LTTextBoxHorizontal(0) 79.014,458.759,432.012,510.837 'Introducing MLOps\\nHow to Scale Machine Learning in the Enterprise\\n'>\n",
      "<LTTextBoxHorizontal(1) 201.264,273.933,432.000,289.933 'Mark Treveil and the Dataiku Team\\n'>\n",
      "<LTTextBoxHorizontal(2) 144.000,54.790,170.208,67.041 'Beijing\\nBeijing\\n'>\n",
      "<LTTextBoxHorizontal(3) 183.277,54.790,209.387,67.041 'Boston\\nBoston\\n'>\n",
      "<LTTextBoxHorizontal(4) 221.084,54.790,308.242,67.041 'Farnham Sebastopol\\nFarnham Sebastopol\\n'>\n",
      "<LTTextBoxHorizontal(5) 320.817,54.790,343.117,67.041 'Tokyo\\nTokyo\\n'>\n",
      "<LTLine 72.000,517.375,432.000,517.375>\n",
      "<LTCurve 175.942,60.486,177.867,62.761>\n",
      "<LTCurve 213.922,60.486,215.847,62.761>\n",
      "<LTCurve 259.515,60.486,261.440,62.761>\n",
      "<LTCurve 314.035,60.486,315.960,62.761>\n",
      "<LTRect 355.340,52.872,432.087,73.788>\n",
      "<LTCurve 386.144,57.948,393.670,68.799>\n",
      "<LTCurve 399.271,57.948,406.272,68.799>\n",
      "<LTCurve 407.759,57.948,414.760,68.799>\n",
      "<LTRect 395.508,57.948,397.345,68.799>\n",
      "<LTCurve 362.428,59.523,370.129,67.224>\n",
      "<LTCurve 360.678,57.773,371.880,68.974>\n",
      "<LTCurve 377.568,64.336,382.556,67.049>\n",
      "<LTCurve 375.730,58.036,384.919,68.887>\n",
      "<LTCurve 371.792,66.349,374.417,68.974>\n",
      "<LTCurve 412.835,57.948,422.811,68.799>\n",
      "<LTCurve 423.774,65.387,426.924,68.537>\n",
      "<LTCurve 423.511,65.124,427.187,68.799>\n",
      "<LTCurve 424.911,67.049,425.874,67.574>\n",
      "<LTCurve 424.649,65.999,426.224,67.924>\n",
      "<LTCurve 175.942,60.486,177.867,62.761>\n",
      "<LTCurve 213.922,60.486,215.847,62.761>\n",
      "<LTCurve 259.515,60.486,261.440,62.761>\n",
      "<LTCurve 314.035,60.486,315.960,62.761>\n",
      "<LTRect 355.340,52.872,432.087,73.788>\n",
      "<LTCurve 386.144,57.948,393.670,68.799>\n",
      "<LTCurve 399.271,57.948,406.272,68.799>\n",
      "<LTCurve 407.759,57.948,414.760,68.799>\n",
      "<LTRect 395.508,57.948,397.345,68.799>\n",
      "<LTCurve 362.428,59.523,370.129,67.224>\n",
      "<LTCurve 360.678,57.773,371.880,68.974>\n",
      "<LTCurve 377.568,64.336,382.556,67.049>\n",
      "<LTCurve 375.730,58.036,384.919,68.887>\n",
      "<LTCurve 371.792,66.349,374.417,68.974>\n",
      "<LTCurve 412.835,57.948,422.811,68.799>\n",
      "<LTCurve 423.774,65.387,426.924,68.537>\n",
      "<LTCurve 423.511,65.124,427.187,68.799>\n",
      "<LTCurve 424.911,67.049,425.874,67.574>\n",
      "<LTCurve 424.649,65.999,426.224,67.924>\n",
      "<LTTextBoxHorizontal(0) 71.996,579.211,205.029,601.050 'Introducing MLOps\\nby Mark Treveil, and the Dataiku Team\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.004,563.011,230.222,571.511 'Copyright © 2020 Dataiku. All rights reserved.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.004,546.811,207.672,555.311 'Printed in the United States of America.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.004,530.611,384.115,539.111 'Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.003,494.010,432.004,522.911 'O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\n'>\n",
      "<LTTextBoxHorizontal(5) 259.497,443.728,375.199,486.000 'Indexer: Ellen Troutman-Zaig\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.999,432.908,186.437,486.000 'Acquisitions Editor: Rebecca Novack\\nDevelopment Editor: Angela Rufino\\nProduction Editor: Katherine Tozer\\nCopyeditor: Penelope Perkins\\nProofreader: Kim Wimpsett\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.997,409.708,127.298,418.208 'December 2020:\\n'>\n",
      "<LTTextBoxHorizontal(8) 144.003,409.708,188.951,418.208 ' First Edition\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,374.958,177.728,396.822 'Revision History for the First Edition\\n2020-11-30:  First Release\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.997,351.758,340.291,360.258 'See http://oreilly.com/catalog/errata.csp?isbn=9781492083290 for release details.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,318.358,431.997,337.058 'The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introducing MLOps, the cover image,\\nand related trade dress are trademarks of O’Reilly Media, Inc.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.996,230.758,432.004,310.658 'The  views  expressed  in  this  work  are  those  of  the  authors,  and  do  not  represent  the  publisher’s  views.\\nWhile  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\\nlicenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\\nthereof complies with such licenses and/or rights.\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.001,204.358,432.002,223.058 'This  work  is  part  of  a  collaboration  between  O’Reilly  and  Dataiku.  See  our  statement  of  editorial\\nindependence.\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.000,93.711,137.144,102.211 '978-1-492-08330-6\\n'>\n",
      "<LTTextBoxHorizontal(15) 72.000,77.511,89.366,86.011 '[LSI]\\n'>\n",
      "<LTTextBoxHorizontal(0) 287.631,561.061,431.990,586.280 'Table of Contents\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.000,418.491,432.000,430.491 'Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   ix\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.004,380.041,217.170,394.041 'Part I.  MLOps: What and Why\\n'>\n",
      "<LTTextBoxHorizontal(3) 77.946,265.377,432.005,366.641 '1. Why Now and Challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nDefining MLOps and Its Challenges                                                                             4\\nMLOps to Mitigate Risk                                                                                                  7\\nRisk Assessment                                                                                                            8\\nRisk Mitigation                                                                                                              9\\nMLOps for Responsible AI                                                                                          9\\nMLOps for Scale                                                                                                             10\\nClosing Thoughts                                                                                                           11\\n'>\n",
      "<LTTextBoxHorizontal(4) 77.957,139.377,432.005,253.241 '2. People of MLOps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13\\nSubject Matter Experts                                                                                                   15\\nData Scientists                                                                                                                 17\\nData Engineers                                                                                                                19\\nSoftware Engineers                                                                                                         20\\nDevOps                                                                                                                            20\\nModel Risk Manager/Auditor                                                                                       21\\nMachine Learning Architect                                                                                         21\\nClosing Thoughts                                                                                                           22\\n'>\n",
      "<LTTextBoxHorizontal(5) 77.957,76.377,432.005,127.241 '3. Key MLOps Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   23\\nA Primer on Machine Learning                                                                                   23\\nModel Development                                                                                                       24\\nEstablishing Business Objectives                                                                              24\\n'>\n",
      "<LTTextBoxHorizontal(6) 426.461,40.500,431.996,49.500 'iii\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,589.375,432.000,589.375>\n",
      "<LTLine 72.000,399.166,432.000,399.166>\n",
      "<LTTextBoxHorizontal(0) 91.435,368.236,432.003,605.537 'Data Sources and Exploratory Data Analysis                                                         24\\nFeature Engineering and Selection                                                                           25\\nTraining and Evaluation                                                                                            26\\nReproducibility                                                                                                            26\\nResponsible AI                                                                                                            26\\nProductionalization and Deployment                                                                         27\\nModel Deployment Types and Contents                                                                 28\\nModel Deployment Requirements                                                                           29\\nMonitoring                                                                                                                      29\\nDevOps Concerns                                                                                                       30\\nData Scientist Concerns                                                                                             30\\nBusiness Concerns                                                                                                      31\\nIteration and Life Cycle                                                                                                 32\\nIteration                                                                                                                        32\\nThe Feedback Loop                                                                                                    33\\nGovernance                                                                                                                     34\\nData Governance                                                                                                        36\\nProcess Governance                                                                                                   37\\nClosing Thoughts                                                                                                           38\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.993,330.850,173.689,344.850 'Part II.  MLOps: How\\n'>\n",
      "<LTTextBoxHorizontal(2) 77.953,102.786,432.001,317.450 '4. Developing Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41\\nWhat Is a Machine Learning Model?                                                                           42\\nIn Theory                                                                                                                     42\\nIn Practice                                                                                                                    43\\nRequired Components                                                                                               44\\nDifferent ML Algorithms, Different MLOps Challenges                                      45\\nData Exploration                                                                                                            46\\nFeature Engineering and Selection                                                                              47\\nFeature Engineering Techniques                                                                              47\\nHow Feature Selection Impacts MLOps Strategy                                                  48\\nExperimentation                                                                                                             49\\nEvaluating and Comparing Models                                                                             51\\nChoosing Evaluation Metrics                                                                                    51\\nCross-Checking Model Behavior                                                                             53\\nImpact of Responsible AI on Modeling                                                                   53\\nVersion Management and Reproducibility                                                                 56\\nClosing Thoughts                                                                                                           58\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.001,40.500,78.634,49.500 'iv \\n'>\n",
      "<LTTextBoxHorizontal(4) 86.212,40.500,89.524,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(5) 97.102,40.500,148.618,49.500 'Table of Contents\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,349.975,432.000,349.975>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 77.952,366.436,432.000,606.300 '5. Preparing for Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   59\\nRuntime Environments                                                                                                 60\\nAdaptation from Development to Production Environments                             60\\nData Access Before Validation and Launch to Production                                   62\\nFinal Thoughts on Runtime Environments                                                            62\\nModel Risk Evaluation                                                                                                   63\\nThe Purpose of Model Validation                                                                            63\\nThe Origins of ML Model Risk                                                                                 64\\nQuality Assurance for Machine Learning                                                                   64\\nKey Testing Considerations                                                                                          65\\nReproducibility and Auditability                                                                                 66\\nMachine Learning Security                                                                                           67\\nAdversarial Attacks                                                                                                     68\\nOther Vulnerabilities                                                                                                  68\\nModel Risk Mitigation                                                                                                   69\\nChanging Environments                                                                                            70\\nInteractions Between Models                                                                                    70\\nModel Misbehavior                                                                                                     71\\nClosing Thoughts                                                                                                           72\\n'>\n",
      "<LTTextBoxHorizontal(1) 77.952,342.300,415.968,354.300 '6. Deploying to Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n'>\n",
      "<LTTextBoxHorizontal(2) 91.443,190.036,432.000,354.300 ' 73\\nCI/CD Pipelines                                                                                                              73\\nBuilding ML Artifacts                                                                                                    75\\nWhat’s in an ML Artifact?                                                                                          75\\nThe Testing Pipeline                                                                                                   75\\nDeployment Strategies                                                                                                   77\\nCategories of Model Deployment                                                                             77\\nConsiderations When Sending Models to Production                                         78\\nMaintenance in Production                                                                                      79\\nContainerization                                                                                                             79\\nScaling Deployments                                                                                                     81\\nRequirements and Challenges                                                                                      83\\nClosing Thoughts                                                                                                           84\\n'>\n",
      "<LTTextBoxHorizontal(3) 77.952,76.636,432.000,177.900 '7. Monitoring and Feedback Loop. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85\\nHow Often Should Models Be Retrained?                                                                  86\\nUnderstanding Model Degradation                                                                             89\\nGround Truth Evaluation                                                                                          89\\nInput Drift Detection                                                                                                 91\\nDrift Detection in Practice                                                                                            92\\nExample Causes of Data Drift                                                                                   93\\nInput Drift Detection Techniques                                                                            93\\n'>\n",
      "<LTTextBoxHorizontal(4) 357.228,40.500,410.166,49.500 'Table of Contents \\n'>\n",
      "<LTTextBoxHorizontal(5) 417.744,40.500,421.056,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(6) 428.634,40.500,432.000,49.500 'v\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 91.436,544.636,432.003,605.537 'The Feedback Loop                                                                                                        95\\nLogging                                                                                                                         96\\nModel Evaluation                                                                                                        97\\nOnline Evaluation                                                                                                       99\\nClosing Thoughts                                                                                                         103\\n'>\n",
      "<LTTextBoxHorizontal(1) 77.955,520.500,411.423,532.500 '8. Model Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n'>\n",
      "<LTTextBoxHorizontal(2) 91.436,217.036,432.003,532.500 ' 105\\nWho Decides What Governance the Organization Needs?                                   105\\nMatching Governance with Risk Level                                                                     107\\nCurrent Regulations Driving MLOps Governance                                                 108\\nPharmaceutical Regulation in the US: GxP                                                          109\\nFinancial Model Risk Management Regulation                                                   109\\nGDPR and CCPA Data Privacy Regulations                                                        110\\nThe New Wave of AI-Specific Regulations                                                               111\\nThe Emergence of Responsible AI                                                                             112\\nKey Elements of Responsible AI                                                                                113\\nElement 1: Data                                                                                                         113\\nElement 2: Bias                                                                                                          114\\nElement 3: Inclusiveness                                                                                          115\\nElement 4: Model Management at Scale                                                                116\\nElement 5: Governance                                                                                            116\\nA Template for MLOps Governance                                                                         117\\nStep 1: Understand and Classify the Analytics Use Cases                                  118\\nStep 2: Establish an Ethical Position                                                                      118\\nStep 3: Establish Responsibilities                                                                            119\\nStep 4: Determine Governance Policies                                                                120\\nStep 5: Integrate Policies into the MLOps Process                                               121\\nStep 6: Select the Tools for Centralized Governance Management                   122\\nStep 7: Engage and Educate                                                                                     123\\nStep 8: Monitor and Refine                                                                                     124\\nClosing Thoughts                                                                                                         125\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.993,179.650,253.727,193.650 'Part III.  MLOps: Real-World Examples\\n'>\n",
      "<LTTextBoxHorizontal(4) 77.951,77.586,431.999,166.250 '9. MLOps in Practice: Consumer Credit Risk Management. . . . . . . . . . . . . . . . . . . . . . . . .  129\\nBackground: The Business Use Case                                                                         129\\nModel Development                                                                                                    130\\nModel Bias Considerations                                                                                         131\\nPrepare for Production                                                                                                131\\nDeploy to Production                                                                                                  132\\nClosing Thoughts                                                                                                         133\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.999,40.500,78.632,49.500 'vi \\n'>\n",
      "<LTTextBoxHorizontal(6) 86.210,40.500,89.522,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 97.100,40.500,148.616,49.500 'Table of Contents\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,198.775,432.000,198.775>\n",
      "<LTTextBoxHorizontal(0) 73.056,353.836,432.000,606.300 '10. MLOps in Practice: Marketing Recommendation Engines. . . . . . . . . . . . . . . . . . . . . . .   135\\nThe Rise of Recommendation Engines                                                                     135\\nThe Role of Machine Learning                                                                               136\\nPush or Pull?                                                                                                              136\\nData Preparation                                                                                                          137\\nDesign and Manage Experiments                                                                              138\\nModel Training and Deployment                                                                              138\\nScalability and Customizability                                                                              140\\nMonitoring and Retraining Strategy                                                                      140\\nReal-Time Scoring                                                                                                    140\\nAbility to Turn Recommendations On and Off                                                   141\\nPipeline Structure and Deployment Strategy                                                           141\\nMonitoring and Feedback                                                                                           142\\nRetraining Models                                                                                                    142\\nUpdating Models                                                                                                      143\\nRuns Overnight, Sleeps During Daytime                                                              143\\nOption to Manually Control Models                                                                     143\\nOption to Automatically Control Models                                                             144\\nMonitoring Performance                                                                                         144\\nClosing Thoughts                                                                                                         145\\n'>\n",
      "<LTTextBoxHorizontal(1) 73.056,215.236,432.000,341.700 '11. MLOps in Practice: Consumption Forecast. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   147\\nPower Systems                                                                                                              147\\nData Collection                                                                                                             149\\nProblem Definition: Machine Learning, or Not Machine Learning?                   151\\nSpatial and Temporal Resolution                                                                               151\\nImplementation                                                                                                            153\\nModeling                                                                                                                        153\\nDeployment                                                                                                                   155\\nMonitoring                                                                                                                    156\\nClosing Thoughts                                                                                                         157\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.000,191.100,432.000,203.100 'Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   159\\n'>\n",
      "<LTTextBoxHorizontal(3) 353.538,40.500,406.476,49.500 'Table of Contents \\n'>\n",
      "<LTTextBoxHorizontal(4) 414.054,40.500,417.366,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(5) 424.944,40.500,432.000,49.500 'vii\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 369.924,561.061,431.990,586.280 'Preface\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,383.627,432.005,444.527 'We’ve reached a turning point in the story of machine learning where the technology\\nhas moved from the realm of theory and academics and into the “real world”—that is,\\nbusinesses  providing  all  kinds  of  services  and  products  to  people  across  the  globe.\\nWhile  this  shift  is  exciting,  it’s  also  challenging,  as  it  combines  the  complexities  of\\nmachine learning models with the complexities of the modern organization.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,289.427,432.004,375.527 'One difficulty, as organizations move from experimenting with machine learning to\\nscaling it in production environments, is maintenance. How can companies go from\\nmanaging just one model to managing tens, hundreds, or even thousands? This is not\\nonly where MLOps comes into play, but it’s also where the aforementioned complexi‐\\nties, both on the technical and business sides, appear. This book will introduce read‐\\ners to the challenges at hand, while also offering practical insights and solutions for\\ndeveloping MLOps capabilities.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,189.027,432.003,277.087 'Who This Book Is For\\nWe wrote this book specifically for analytics and IT operations team managers, that\\nis, the people directly facing the task of scaling machine learning (ML) in production.\\nGiven  that  MLOps  is  a  new  field,  we  developed  this  book  as  a  guide  for  creating  a\\nsuccessful  MLOps  environment,  from  the  organizational  to  the  technical  challenges\\ninvolved.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,113.827,432.004,176.687 'How This Book Is Organized\\nThis  book  is  divided  into  three  parts.  The  first  is  an  introduction  to  the  topic  of\\nMLOps, diving into how (and why) it has developed as a discipline, who needs to be\\ninvolved to execute MLOps successfully, and what components are required.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,70.027,432.001,105.727 'The second part roughly follows the machine learning model life cycle, with chapters\\non developing models, preparing for production, deploying to production, monitor‐\\ning,  and  governance.  These  chapters  cover  not  only  general  considerations,  but\\n'>\n",
      "<LTTextBoxHorizontal(6) 426.829,40.500,432.004,49.500 'ix\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,589.375,432.000,589.375>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'MLOps  considerations  at  each  stage  of  the  life  cycle,  providing  more  detail  on  the\\ntopics touched on in Chapter 3.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,526.036,432.004,574.337 'The final part provides tangible examples of how MLOps looks in companies today,\\nso  that  readers  can  understand  the  setup  and  implications  in  practice.  Though  the\\ncompany names are fictitious, the stories are based on real-life companies’ experience\\nwith MLOps and model management at scale.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.004,476.036,336.363,513.697 'Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.004,456.436,92.637,466.936 'Italic\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.001,443.836,403.552,454.336 'Indicates new terms, URLs, email addresses, filenames, and file extensions.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.003,425.727,141.828,435.702 'Constant width\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.999,385.843,432.005,421.543 'Used for program listings, as well as within paragraphs to refer to program ele‐\\nments  such  as  variable  or  function  names,  databases,  data  types,  environment\\nvariables, statements, and keywords.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.995,367.734,166.758,377.709 'Constant width bold\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.003,353.050,392.991,363.550 'Shows commands or other text that should be typed literally by the user.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.995,334.941,176.733,344.916 'Constant width italic\\n'>\n",
      "<LTTextBoxHorizontal(10) 89.997,307.656,432.003,330.756 'Shows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.004,276.396,220.791,295.316 'O’Reilly Online Learning\\n'>\n",
      "<LTTextBoxHorizontal(12) 168.997,224.206,432.005,259.906 'For more than 40 years, O’Reilly Media has provided technol‐\\nogy  and  business  training,  knowledge,  and  insight  to  help\\ncompanies succeed.\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.996,146.206,432.005,207.106 'Our unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.998,40.500,76.750,49.500 'x \\n'>\n",
      "<LTTextBoxHorizontal(15) 84.328,40.500,117.367,49.500 '|  Preface\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTFigure(I1) 72.000,231.705,162.000,261.869 matrix=[90.00,0.00,0.00,30.16, (72.00,231.70)]>\n",
      "<LTTextBoxHorizontal(0) 72.000,570.312,402.197,607.973 'How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\n'>\n",
      "<LTTextBoxHorizontal(1) 90.001,484.712,285.773,558.212 'O’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.001,449.512,432.004,472.612 'We have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at https://oreil.ly/intro-mlops.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.998,418.312,432.001,441.412 'Email  bookquestions@oreilly.com  to  comment  or  ask  technical  questions  about  this\\nbook.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.998,399.712,402.212,410.212 'For news and information about our books and courses, visit http://oreilly.com.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.998,381.112,273.619,391.612 'Find us on Facebook: http://facebook.com/oreilly\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,362.512,290.093,373.012 'Follow us on Twitter: http://twitter.com/oreillymedia\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.998,343.912,323.809,354.412 'Watch us on YouTube: http://www.youtube.com/oreillymedia\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.995,256.112,432.001,331.573 'Acknowledgments\\nWe would like to thank the entire Dataiku team for their support in developing this\\nbook,  from  conception  to  completion.  It’s  been  a  true  team  effort  and,  like  most\\nthings we do at Dataiku, rooted in fundamental collaboration between countless peo‐\\nple and teams.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.997,187.112,432.004,248.012 'Thanks to those who supported our vision from the beginning of writing this book\\nwith  O’Reilly.  Thanks  to  those  who  stepped  in  to  help  with  writing  and  editing.\\nThanks  to  those  who  provided  honest  feedback  (even  when  it  meant  more  writing\\nand rewriting and re-rewriting). Thanks to those who were internal cheerleaders and,\\nof course, those who helped us promote the finished product to the world.\\n'>\n",
      "<LTTextBoxHorizontal(10) 384.783,40.500,408.354,49.500 'Preface \\n'>\n",
      "<LTTextBoxHorizontal(11) 415.932,40.500,419.244,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 426.822,40.500,431.997,49.500 'xi\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 222.461,527.418,431.991,581.880 'PART I\\nMLOps: What and Why\\n'>\n",
      "<LTLine 72.000,558.814,432.000,558.814>\n",
      "<LTTextBoxHorizontal(0) 222.975,533.502,431.998,582.331 'CHAPTER 1\\nWhy Now and Challenges\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,318.268,432.005,416.969 'Machine  learning  operations  (MLOps)  is  quickly  becoming  a  critical  component  of\\nsuccessful data science project deployment in the enterprise (Figure 1-1). It’s a process\\nthat  helps  organizations  and  business  leaders  generate  long-term  value  and  reduce\\nrisk associated with data science, machine learning, and AI initiatives. Yet it’s a rela‐\\ntively new concept; so why has it seemingly skyrocketed into the data science lexicon\\novernight?  This  introductory  chapter  delves  into  what  MLOps  is  at  a  high  level,  its\\nchallenges,  why  it  has  become  essential  to  a  successful  data  science  strategy  in  the\\nenterprise, and, critically, why it is coming to the forefront now.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.247,194.762,423.755,298.257 'MLOps Versus ModelOps Versus AIOps\\nMLOps  (or  ModelOps)  is  a  relatively  new  discipline,  emerging  under  these  names\\nparticularly in late 2018 and 2019. The two—MLOps and ModelOps—are, at the time\\nthis book is being written, largely being used interchangeably. However, some argue\\nthat ModelOps is more general than MLOps, as it’s not only about machine learning\\nmodels but any kind of model (e.g., rule-based models). For the purpose of this book,\\nwe’ll be specifically discussing the machine learning model life cycle and will thus use\\nthe term “MLOps.”\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.247,116.762,423.747,186.762 'AIOps, though sometimes confused with MLOps, is another topic entirely and refers\\nto  the  process  of  solving  operational  challenges  through  the  use  of  artificial  intelli‐\\ngence (i.e., AI for DevOps). An example would be a form of predictive maintenance\\nfor  network  failures,  alerting  DevOps  teams  to  possible  problems  before  they  arise.\\nWhile important and interesting in its own right, AIOps is outside the scope of this\\nbook.\\n'>\n",
      "<LTTextBoxHorizontal(4) 428.324,40.500,431.996,49.500 '3\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTLine 72.000,308.007,432.000,308.007>\n",
      "<LTLine 431.875,105.382,431.875,308.132>\n",
      "<LTLine 72.000,105.507,432.000,105.507>\n",
      "<LTLine 72.125,105.382,72.125,308.132>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,416.960,427.268,440.060 'Figure 1-1. Representation of the exponential growth of MLOps (not the parallel growth\\nof the term “ModelOps”)\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,316.559,432.005,404.620 'Defining MLOps and Its Challenges\\nAt its core, MLOps is the standardization and streamlining of machine learning life\\ncycle management (Figure 1-2). But taking a step back, why does the machine learn‐\\ning life cycle need to be streamlined? On the surface, just looking at the steps to go\\nfrom  business  problem  to  a  machine  learning  model  at  a  very  high  level,  it  seems\\nstraightforward.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,209.759,432.004,308.459 'For  most  traditional  organizations,  the  development  of  multiple  machine  learning\\nmodels and their deployment in a production environment are relatively new. Until\\nrecently, the number of models may have been manageable at a small scale, or there\\nwas  simply  less  interest  in  understanding  these  models  and  their  dependencies  at  a\\ncompany-wide  level.  With  decision  automation  (that  is,  an  increasing  prevalence  of\\ndecision  making  that  happens  without  human  intervention),  models  become  more\\ncritical,  and,  in  parallel,  managing  model  risks  becomes  more  important  at  the  top\\nlevel.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.001,178.559,432.004,201.659 'The  reality  of  the  machine  learning  life  cycle  in  an  enterprise  setting  is  much  more\\ncomplex, in terms of needs and tooling (Figure 1-3).\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.998,40.500,77.092,49.500 '4 \\n'>\n",
      "<LTTextBoxHorizontal(5) 84.670,40.500,87.982,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(6) 95.560,40.500,201.949,49.500 'Chapter 1: Why Now and Challenges\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,445.623,432.375,607.500>\n",
      "<LTLine 72.000,445.748,432.500,445.748>\n",
      "<LTLine 72.125,445.623,72.125,607.500>\n",
      "<LTFigure(I1) 126.250,451.603,378.250,602.250 matrix=[252.00,0.00,0.00,150.65, (126.25,451.60)]>\n",
      "<LTTextBoxHorizontal(0) 72.000,374.406,428.905,397.506 'Figure 1-2. A simple representation of the machine learning model life cycle, which often\\nunderplays the need for MLOps, compared to Figure 1-3\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,337.206,432.000,360.306 'There  are  three  key  reasons  that  managing  machine  learning  life  cycles  at  scale  is\\nchallenging:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.652,264.206,432.002,325.106 '• There are many dependencies. Not only is data constantly changing, but business\\nneeds shift as well. Results need to be continually relayed back to the business to\\nensure that the reality of the model in production and on production data aligns\\nwith  expectations  and,  critically,  addresses  the  original  problem  or  meets  the\\noriginal goal.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.652,209.806,432.002,258.106 '• Not everyone speaks the same language. Even though the machine learning life\\ncycle involves people from the business, data science, and IT teams, none of these\\ngroups  are  using  the  same  tools  or  even,  in  many  cases,  share  the  same  funda‐\\nmental skills to serve as a baseline of communication.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.658,92.406,432.005,203.706 '• Data scientists are not software engineers. Most are specialized in model building\\nand  assessment,  and  they  are  not  necessarily  experts  in  writing  applications.\\nThough this may start to shift over time as some data scientists become special‐\\nists  more  on  the  deployment  or  operational  side,  for  now  many  data  scientists\\nfind themselves having to juggle many roles, making it challenging to do any of\\nthem  thoroughly.  Data  scientists  being  stretched  too  thin  becomes  especially\\nproblematic at scale with increasingly more models to manage. The complexity\\nbecomes exponential when considering the turnover of staff on data teams and,\\nsuddenly, data scientists have to manage models they did not create.\\n'>\n",
      "<LTTextBoxHorizontal(5) 306.756,40.500,409.860,49.500 'Defining MLOps and Its Challenges \\n'>\n",
      "<LTTextBoxHorizontal(6) 417.438,40.500,420.750,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 428.328,40.500,432.000,49.500 '5\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,403.069,432.375,607.500>\n",
      "<LTLine 72.000,403.194,432.500,403.194>\n",
      "<LTLine 72.125,403.069,72.125,607.500>\n",
      "<LTFigure(I1) 154.570,409.049,349.930,602.250 matrix=[195.36,0.00,0.00,193.20, (154.57,409.05)]>\n",
      "<LTTextBoxHorizontal(0) 72.000,183.245,425.902,218.945 'Figure 1-3. The realistic picture of a machine learning model life cycle inside an average\\norganization today, which involves many different people with completely different skill\\nsets and who are often using entirely different tools.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,120.845,432.004,169.145 'If  the  definition  (or  even  the  name  MLOps)  sounds  familiar,  that’s  because  it  pulls\\nheavily  from  the  concept  of  DevOps,  which  streamlines  the  practice  of  software\\nchanges and updates. Indeed, the two have quite a bit in common. For example, they\\nboth center around:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.657,98.245,275.637,108.745 '• Robust automation and trust between teams\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,81.645,389.541,92.145 '• The idea of collaboration and increased communication between teams\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.003,40.500,77.097,49.500 '6 \\n'>\n",
      "<LTTextBoxHorizontal(5) 84.675,40.500,87.987,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(6) 95.565,40.500,201.954,49.500 'Chapter 1: Why Now and Challenges\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,224.509,432.375,607.500>\n",
      "<LTLine 72.000,224.634,432.500,224.634>\n",
      "<LTLine 72.125,224.509,72.125,607.500>\n",
      "<LTFigure(I1) 81.369,230.489,423.131,602.250 matrix=[341.76,0.00,0.00,371.76, (81.37,230.49)]>\n",
      "<LTTextBoxHorizontal(0) 80.655,595.037,310.823,605.537 '• The end-to-end service life cycle (build, test, release)\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,578.437,295.461,588.937 '• Prioritizing continuous delivery and high quality\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,442.436,432.005,566.337 'Yet there is one critical difference between MLOps and DevOps that makes the latter\\nnot immediately transferable to data science teams: deploying software code into pro‐\\nduction is fundamentally different than deploying machine learning models into pro‐\\nduction.  While  software  code  is  relatively  static  (“relatively”  because  many  modern\\nsoftware-as-a-service [SaaS] companies do have DevOps teams that can iterate quite\\nquickly  and  deploy  in  production  multiple  times  per  day),  data  is  always  changing,\\nwhich means machine learning models are constantly learning and adapting—or not,\\nas the case may be—to new inputs. The complexity of this environment, including the\\nfact that machine learning models are made up of both code and data, is what makes\\nMLOps a new and unique discipline.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.249,318.930,423.749,422.425 'What About DataOps?\\nTo  add  to  the  complexity  of  MLOps  versus  DevOps,  there  is  also  DataOps,  a  term\\nintroduced  in  2014  by  IBM.  DataOps  seeks  to  provide  business-ready  data  that  is\\nquickly  available  for  use,  with  a  large  focus  on  data  quality  and  metadata  manage‐\\nment. For example, if there’s a sudden change in data that a model relies on, a Data‐\\nOps  system  would  alert  the  business  team  to  deal  more  carefully  with  the  latest\\ninsights,  and  the  data  team  would  be  notified  to  investigate  the  change  or  revert  a\\nlibrary upgrade and rebuild the related partition.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.249,276.930,423.749,310.930 'The rise of MLOps, therefore, intersects with DataOps at some level, though MLOps\\ngoes a step further and brings even more robustness through additional key features\\n(discussed in more detail in Chapter 3).\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,167.486,432.002,253.586 'As was the case with DevOps and later DataOps, until recently teams have been able\\nto get by without defined and centralized processes mostly because—at an enterprise\\nlevel—they  weren’t  deploying  machine  learning  models  into  production  at  a  large\\nenough scale. Now, the tables are turning and teams are increasingly looking for ways\\nto  formalize  a  multi-stage,  multi-discipline,  multi-phase  process  with  a  heterogene‐\\nous environment and a framework for MLOps best practices, which is no small task.\\nPart II of this book, “MLOps: How,” will provide this guidance.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,92.286,432.004,155.147 'MLOps to Mitigate Risk\\nMLOps  is  important  to  any  team  that  has  even  one  model  in  production  because,\\ndepending on the model, continuous performance monitoring and adjusting is essen‐\\ntial.  By  allowing  safe  and  reliable  operations,  MLOps  is  key  in  mitigating  the  risks\\n'>\n",
      "<LTTextBoxHorizontal(7) 340.837,40.500,409.858,49.500 'MLOps to Mitigate Risk \\n'>\n",
      "<LTTextBoxHorizontal(8) 417.436,40.500,420.748,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 428.326,40.500,431.998,49.500 '7\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,432.175,432.000,432.175>\n",
      "<LTLine 431.875,265.550,431.875,432.300>\n",
      "<LTLine 72.000,265.675,432.000,265.675>\n",
      "<LTLine 72.125,265.550,72.125,432.300>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'induced by the use of ML models. However, MLOps practices do come at a cost, so a\\nproper cost-benefit evaluation should be performed for each use case.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,486.627,432.003,570.867 'Risk Assessment\\nWhen  it  comes  to  machine  learning  models,  risks  vary  widely.  For  example,  the\\nstakes  are  much  lower  for  a  recommendation  engine  used  once  a  month  to  decide\\nwhich  marketing  offer  to  send  a  customer  than  for  a  travel  site  whose  pricing  and\\nrevenue depend on a machine learning model. Therefore, when looking at MLOps as\\na way to mitigate risk, an analysis should cover:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,464.028,356.518,474.528 '• The risk that the model is unavailable for a given period of time\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,447.427,370.977,457.927 '• The risk that the model returns a bad prediction for a given sample\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,430.827,358.125,441.327 '• The risk that the model accuracy or fairness decreases over time\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,414.227,431.997,424.727 '• The risk that the skills necessary to maintain the model (i.e., data science talent)\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,401.627,120.420,412.127 'are lost\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,316.027,432.004,389.527 'Risks are usually larger for models that are deployed widely and used outside of the\\norganization. As shown in Figure 1-4, risk assessment is generally based on two met‐\\nrics:  the  probability  and  the  impact  of  the  adverse  event.  Mitigation  measures  are\\ngenerally  based  on  the  combination  of  the  two,  i.e.,  the  model’s  severity.  Risk\\nassessment should be performed at the beginning of each project and reassessed peri‐\\nodically, as models may be used in ways that were not foreseen initially.\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.003,98.037,382.204,108.537 'Figure 1-4. A table that helps decision makers with quantitative risk analysis\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.001,40.500,77.095,49.500 '8 \\n'>\n",
      "<LTTextBoxHorizontal(10) 84.673,40.500,87.985,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 95.563,40.500,201.952,49.500 'Chapter 1: Why Now and Challenges\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,309.766,432.500,309.766>\n",
      "<LTLine 432.375,114.100,432.375,309.891>\n",
      "<LTLine 72.000,114.225,432.500,114.225>\n",
      "<LTLine 72.125,114.100,72.125,309.891>\n",
      "<LTFigure(I1) 126.250,120.080,378.250,304.641 matrix=[252.00,0.00,0.00,184.56, (126.25,120.08)]>\n",
      "<LTTextBoxHorizontal(0) 71.996,511.055,432.005,607.894 'Risk Mitigation\\nMLOps  really  tips  the  scales  as  critical  for  risk  mitigation  when  a  centralized  team\\n(with unique reporting of its activities, meaning that there can be multiple such teams\\nat any given enterprise) has more than a handful of operational models. At this point,\\nit  becomes  difficult  to  have  a  global  view  of  the  states  of  these  models  without  the\\nstandardization that allows the appropriate mitigation measures to be taken for each\\nof them (see “Matching Governance with Risk Level” on page 107).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,416.855,432.005,502.955 'Pushing machine learning models into production without MLOps infrastructure is\\nrisky  for  many  reasons,  but  first  and  foremost  because  fully  assessing  the  perfor‐\\nmance of a machine learning model can often only be done in the production envi‐\\nronment.  Why?  Because  prediction  models  are  only  as  good  as  the  data  they  are\\ntrained  on,  which  means  the  training  data  must  be  a  good  reflection  of  the  data\\nencountered in the production environment. If the production environment changes,\\nthen the model performance is likely to decrease rapidly (see Chapter 5 for details).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,310.055,432.005,408.755 'Another major risk factor is that machine learning model performance is often very\\nsensitive  to  the  production  environment  it  is  running  in,  including  the  versions  of\\nsoftware and operating systems in use. They tend not to be buggy in the classic soft‐\\nware  sense,  because  most  weren’t  written  by  hand,  but  rather  were  machine-\\ngenerated. Instead, the problem is that they are often built on a pile of open source\\nsoftware  (e.g.,  libraries,  like  scikit-learn,  Python,  or  Linux),  and  having  versions  of\\nthis software in production that match those that the model was verified on is criti‐\\ncally important.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,215.855,432.004,301.955 'Ultimately, pushing models into production is not the final step of the machine learn‐\\ning life cycle—far from it. It’s often just the beginning of monitoring its performance\\nand ensuring that it behaves as expected. As more data scientists start pushing more\\nmachine learning models into production, MLOps becomes critical in mitigating the\\npotential risks, which (depending on the model) can be devastating for the business if\\nthings  go  wrong.  Monitoring  is  also  essential  so  that  the  organization  has  a  precise\\nknowledge of how broadly each model is used.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.993,157.846,431.995,204.285 'MLOps for Responsible AI\\nA  responsible  use  of  machine  learning  (more  commonly  referred  to  as  Responsible\\nAI) covers two main dimensions:\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.003,138.246,127.527,148.746 'Intentionality\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.998,87.846,432.004,136.146 'Ensuring  that  models  are  designed  and  behave  in  ways  aligned  with  their  pur‐\\npose. This includes assurance that data used for AI projects comes from compli‐\\nant  and  unbiased  sources  plus  a  collaborative  approach  to  AI  projects  that\\nensures multiple checks and balances on potential model bias. Intentionality also\\n'>\n",
      "<LTTextBoxHorizontal(7) 340.842,40.500,409.863,49.500 'MLOps to Mitigate Risk \\n'>\n",
      "<LTTextBoxHorizontal(8) 417.441,40.500,420.753,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 428.331,40.500,432.003,49.500 '9\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 89.997,582.437,431.997,605.537 'includes explainability, meaning the results of AI systems should be explainable\\nby humans (ideally, not just the humans who created the system).\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.005,562.837,131.403,573.337 'Accountability\\n'>\n",
      "<LTTextBoxHorizontal(2) 89.996,474.636,432.005,560.736 'Centrally  controlling,  managing,  and  auditing  the  enterprise  AI  effort—no\\nshadow  IT!  Accountability  is  about  having  an  overall  view  of  which  teams  are\\nusing what data, how, and in which models. It also includes the need for trust that\\ndata  is  reliable  and  being  collected  in  accordance  with  regulations  as  well  as  a\\ncentralized understanding of which models are used for what business processes.\\nThis  is  closely  tied  to  traceability:  if  something  goes  wrong,  is  it  easy  to  find\\nwhere in the pipeline it happened?\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,405.636,432.003,466.536 'These principles may seem obvious, but it’s important to consider that machine learn‐\\ning models lack the transparency of traditional imperative code. In other words, it is\\nmuch harder to understand what features are used to determine a prediction, which\\nin turn can make it much harder to demonstrate that models comply with the neces‐\\nsary regulatory or internal governance requirements.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,298.836,432.005,397.536 'The  reality  is  that  introducing  automation  vis-à-vis  machine  learning  models  shifts\\nthe fundamental onus of accountability from the bottom of the hierarchy to the top.\\nThat is, decisions that were perhaps previously made by individual contributors who\\noperated within a margin of guidelines (for example, what the price of a given prod‐\\nuct  should  be  or  whether  or  not  a  person  should  be  accepted  for  a  loan)  are  now\\nbeing made by a model. The person responsible for the automated decisions of said\\nmodel is likely a data team manager or even executive, and that brings the concept of\\nResponsible AI even more to the forefront.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,217.236,432.005,290.736 'Given the previously discussed risks as well as these particular challenges and princi‐\\nples,  it’s  easy  to  see  the  interplay  between  MLOps  and  Responsible  AI.  Teams  must\\nhave good MLOps principles to practice Responsible AI, and Responsible AI necessi‐\\ntates MLOps strategies. Given the gravity of this topic, we’ll come back to it multiple\\ntimes throughout this book, examining how it should be addressed at each stage of\\nthe ML model life cycle.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,116.836,432.005,204.896 'MLOps for Scale\\nMLOps  isn’t  just  important  because  it  helps  mitigate  the  risk  of  machine  learning\\nmodels  in  production;  it  is  also  an  essential  component  to  massively  deploying\\nmachine learning efforts (and in turn benefiting from the corresponding economies\\nof scale). Going from one or a handful of models in production to tens, hundreds, or\\nthousands that have a positive business impact requires MLOps discipline.\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,40.500,80.766,49.500 '10 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.234,40.500,205.623,49.500 'Chapter 1: Why Now and Challenges\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,595.037,299.692,605.537 'Good MLOps practices will help teams at a minimum:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,572.437,395.788,582.937 '• Keep track of versioning, especially with experiments in the design phase\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,555.837,431.997,566.337 '• Understand whether retrained models are better than the previous versions (and\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,543.236,344.396,553.736 'promoting models to production that are performing better)\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,526.637,431.997,537.137 '• Ensure (at defined periods—daily, monthly, etc.) that model performance is not\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,514.037,193.196,524.537 'degrading in production\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.994,388.436,432.004,501.697 'Closing Thoughts\\nKey features will be discussed at length in Chapter 3, but the point here is that these\\nare not optional practices. They are essential tasks for not only efficiently scaling data\\nscience and machine learning at the enterprise level, but also doing it in a way that\\ndoesn’t  put  the  business  at  risk.  Teams  that  attempt  to  deploy  data  science  without\\nproper MLOps practices in place will face issues with model quality and continuity—\\nor, worse, they will introduce models that have a real, negative impact on the business\\n(e.g., a model that makes biased predictions that reflect poorly on the company).\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,294.236,432.003,380.336 'MLOps is also, at a higher level, a critical part of transparent strategies for machine\\nlearning. Upper management and the C-suite should be able to understand as well as\\ndata  scientists  what  machine  learning  models  are  deployed  in  production  and  what\\neffect  they’re  having  on  the  business.  Beyond  that,  they  should  arguably  be  able  to\\ndrill down to understand the whole data pipeline (i.e., the steps taken to go from raw\\ndata to final output) behind those machine learning models. MLOps, as described in\\nthis book, can provide this level of transparency and accountability.\\n'>\n",
      "<LTTextBoxHorizontal(8) 354.041,40.500,406.187,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(9) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 424.655,40.500,431.999,49.500 '11\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 295.003,533.502,431.998,582.331 'CHAPTER 2\\nPeople of MLOps\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,368.668,432.001,416.969 'Even though machine learning models are primarily built by data scientists, it’s a mis‐\\nconception  that  only  data  scientists  can  benefit  from  robust  MLOps  processes  and\\nsystems.  In  fact,  MLOps  is  an  essential  piece  of  enterprise  AI  strategy  and  affects\\neveryone working on, or benefiting from, the machine learning model life cycle.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,312.268,432.003,360.568 'This chapter covers the roles each of these people plays in the machine learning life\\ncycle, who they should ideally be connected and working together with under a top-\\nnotch  MLOps  program  to  achieve  the  best  possible  results  from  machine  learning\\nefforts, and what MLOps requirements they may have.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.999,268.468,432.005,304.168 'It’s important to note that this field is constantly evolving, bringing with it many new\\njob titles that may not be listed here and presenting new challenges (or overlaps) in\\nMLOps responsibilities.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,237.268,431.998,260.368 'Before  we  dive  into  the  details,  let’s  look  at  the  following  table,  which  provides  an\\noverview:\\n'>\n",
      "<LTTextBoxHorizontal(5) 424.658,40.500,432.002,49.500 '13\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTTextBoxHorizontal(0) 75.600,561.200,95.994,604.600 'Role\\nSubject\\nmatter\\nexperts\\n'>\n",
      "<LTTextBoxHorizontal(1) 75.600,511.450,100.935,531.250 'Data\\nscientists\\n'>\n",
      "<LTTextBoxHorizontal(2) 75.600,367.900,102.771,387.700 'Data\\nengineers\\n'>\n",
      "<LTTextBoxHorizontal(3) 75.600,307.350,102.771,327.150 'Software\\nengineers\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.600,267.500,96.201,276.500 'DevOps\\n'>\n",
      "<LTTextBoxHorizontal(5) 75.600,195.250,105.597,225.850 'Model risk\\nmanagers/\\nauditors\\n'>\n",
      "<LTTextBoxHorizontal(6) 75.600,134.700,102.303,165.300 'Machine\\nlearning\\narchitects\\n'>\n",
      "<LTTextBoxHorizontal(7) 120.297,595.600,242.742,604.600 'Role in machine learning model life cycle\\n'>\n",
      "<LTTextBoxHorizontal(8) 283.671,595.600,345.231,604.600 'MLOps requirements\\n'>\n",
      "<LTTextBoxHorizontal(9) 121.484,577.800,264.909,586.800 '• Provide business questions, goals, or KPIs around\\n'>\n",
      "<LTTextBoxHorizontal(10) 284.858,577.800,404.559,586.800 '• Easy way to understand deployed model\\n'>\n",
      "<LTTextBoxHorizontal(11) 128.298,567.450,229.503,576.450 'which ML models should be framed.\\n'>\n",
      "<LTTextBoxHorizontal(12) 291.672,567.450,378.684,576.450 'performance in business terms.\\n'>\n",
      "<LTTextBoxHorizontal(13) 121.484,556.100,250.347,565.100 '• Continually evaluate and ensure that model\\n'>\n",
      "<LTTextBoxHorizontal(14) 284.858,556.100,425.007,565.100 '• Mechanism or feedback loop for flagging model\\n'>\n",
      "<LTTextBoxHorizontal(15) 128.298,545.750,273.333,554.750 'performance aligns with or resolves the initial need.\\n'>\n",
      "<LTTextBoxHorizontal(16) 291.672,535.850,394.740,554.750 'results that don’t align with business\\nexpectations.\\n'>\n",
      "<LTTextBoxHorizontal(17) 121.484,517.250,269.967,526.250 '• Build models that address the business question or\\n'>\n",
      "<LTTextBoxHorizontal(18) 128.298,506.900,243.075,515.900 'needs brought by subject matter experts.\\n'>\n",
      "<LTTextBoxHorizontal(19) 121.484,475.300,274.314,504.550 '• Deliver operationalizable models so that they can be\\nproperly used in the production environment and\\nwith production data.\\n'>\n",
      "<LTTextBoxHorizontal(20) 121.484,443.700,270.291,472.950 '• Assess model quality (of both original and tests) in\\ntandem with subject matter experts to ensure they\\nanswer initial business questions or needs.\\n'>\n",
      "<LTTextBoxHorizontal(21) 284.858,497.000,417.834,526.250 '• Automated model packaging and delivery for\\nquick and easy (yet safe) deployment to\\nproduction.\\n'>\n",
      "<LTTextBoxHorizontal(22) 284.858,465.400,425.394,494.650 '• Ability to develop tests to determine the quality\\nof deployed models and to make continual\\nimprovements.\\n'>\n",
      "<LTTextBoxHorizontal(23) 284.858,433.800,419.040,463.050 '• Visibility into the performance of all deployed\\nmodels (including side-by-side for tests) from\\none central location.\\n'>\n",
      "<LTTextBoxHorizontal(24) 284.858,392.300,419.625,431.450 '• Ability to investigate data pipelines of each\\nmodel to make quick assessments and\\nadjustments regardless of who originally built\\nthe model.\\n'>\n",
      "<LTTextBoxHorizontal(25) 121.484,373.700,270.624,382.700 '• Optimize the retrieval and use of data to power ML\\n'>\n",
      "<LTTextBoxHorizontal(26) 284.858,373.700,408.123,382.700 '• Visibility into performance of all deployed\\n'>\n",
      "<LTTextBoxHorizontal(27) 128.298,363.350,150.249,372.350 'models.\\n'>\n",
      "<LTTextBoxHorizontal(28) 291.672,363.350,313.623,372.350 'models.\\n'>\n",
      "<LTTextBoxHorizontal(29) 284.858,331.750,421.569,361.000 '• Ability to see the full details of individual data\\npipelines to address underlying data plumbing\\nissues.\\n'>\n",
      "<LTTextBoxHorizontal(30) 121.484,313.150,270.471,322.150 '• Integrate ML models in the company’s applications\\n'>\n",
      "<LTTextBoxHorizontal(31) 128.298,302.800,164.325,311.800 'and systems.\\n'>\n",
      "<LTTextBoxHorizontal(32) 284.858,301.350,409.608,322.150 '• Versioning and automatic tests.\\n• The ability to work in parallel on the same\\n'>\n",
      "<LTTextBoxHorizontal(33) 121.484,291.450,270.282,300.450 '• Ensure that ML models work seamlessly with other\\n'>\n",
      "<LTTextBoxHorizontal(34) 291.672,291.000,324.288,300.000 'application.\\n'>\n",
      "<LTTextBoxHorizontal(35) 128.298,281.100,246.639,290.100 'non-machine-learning-based applications.\\n'>\n",
      "<LTTextBoxHorizontal(36) 121.484,262.500,270.183,271.500 '• Conduct and build operational systems and test for\\n'>\n",
      "<LTTextBoxHorizontal(37) 284.858,262.500,419.274,271.500 '• Seamless integration of MLOps into the larger\\n'>\n",
      "<LTTextBoxHorizontal(38) 128.298,252.150,224.094,261.150 'security, performance, availability.\\n'>\n",
      "<LTTextBoxHorizontal(39) 291.672,252.150,386.298,261.150 'DevOps strategy of the enterprise.\\n'>\n",
      "<LTTextBoxHorizontal(40) 121.484,240.800,271.596,249.800 '• Continuous Integration/Continuous Delivery (CI/CD)\\n'>\n",
      "<LTTextBoxHorizontal(41) 284.858,240.800,378.315,249.800 '• Seamless deployment pipeline.\\n'>\n",
      "<LTTextBoxHorizontal(42) 128.298,230.450,191.568,239.450 'pipeline management.\\n'>\n",
      "<LTTextBoxHorizontal(43) 121.484,211.850,268.509,220.850 '• Minimize overall risk to the company as a result of\\n'>\n",
      "<LTTextBoxHorizontal(44) 284.859,211.850,423.180,220.850 '• Robust, likely automated, reporting tools on all\\n'>\n",
      "<LTTextBoxHorizontal(45) 128.298,201.500,199.380,210.500 'ML models in production.\\n'>\n",
      "<LTTextBoxHorizontal(46) 121.484,169.900,254.694,199.150 '• Ensure compliance with internal and external\\nrequirements before pushing ML models to\\nproduction.\\n'>\n",
      "<LTTextBoxHorizontal(47) 121.484,131.050,267.627,160.300 '• Ensure a scalable and flexible environment for ML\\nmodel pipelines, from design to development and\\nmonitoring.\\n'>\n",
      "<LTTextBoxHorizontal(48) 121.484,109.350,270.354,128.700 '• Introduce new technologies when appropriate that\\nimprove ML model performance in production.\\n'>\n",
      "<LTTextBoxHorizontal(49) 291.672,191.600,404.622,210.500 'models (currently or ever in production),\\nincluding data lineage.\\n'>\n",
      "<LTTextBoxHorizontal(50) 284.858,151.300,403.425,160.300 '• High-level overview of models and their\\n'>\n",
      "<LTTextBoxHorizontal(51) 291.672,140.950,349.506,149.950 'resources consumed.\\n'>\n",
      "<LTTextBoxHorizontal(52) 284.858,129.600,426.924,138.600 '• Ability to drill down into data pipelines to assess\\n'>\n",
      "<LTTextBoxHorizontal(53) 291.672,119.250,380.142,128.250 'and adjust infrastructure needs.\\n'>\n",
      "<LTTextBoxHorizontal(54) 72.000,40.500,80.766,49.500 '14 \\n'>\n",
      "<LTTextBoxHorizontal(55) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(56) 99.234,40.500,179.919,49.500 'Chapter 2: People of MLOps\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,593.700,116.697,607.500>\n",
      "<LTRect 116.697,593.700,280.071,607.500>\n",
      "<LTRect 280.071,593.700,432.000,607.500>\n",
      "<LTLine 72.000,533.275,116.797,533.275>\n",
      "<LTLine 116.597,533.275,280.171,533.275>\n",
      "<LTLine 279.971,533.275,432.000,533.275>\n",
      "<LTLine 72.000,389.725,116.797,389.725>\n",
      "<LTLine 116.597,389.725,280.171,389.725>\n",
      "<LTLine 279.971,389.725,432.000,389.725>\n",
      "<LTLine 72.000,329.175,116.797,329.175>\n",
      "<LTLine 116.597,329.175,280.171,329.175>\n",
      "<LTLine 279.971,329.175,432.000,329.175>\n",
      "<LTLine 72.000,278.525,116.797,278.525>\n",
      "<LTLine 116.597,278.525,280.171,278.525>\n",
      "<LTLine 279.971,278.525,432.000,278.525>\n",
      "<LTLine 72.000,227.875,116.797,227.875>\n",
      "<LTLine 116.597,227.875,280.171,227.875>\n",
      "<LTLine 279.971,227.875,432.000,227.875>\n",
      "<LTLine 72.000,167.325,116.797,167.325>\n",
      "<LTLine 116.597,167.325,280.171,167.325>\n",
      "<LTLine 279.971,167.325,432.000,167.325>\n",
      "<LTLine 72.000,106.775,116.797,106.775>\n",
      "<LTLine 116.597,106.775,280.171,106.775>\n",
      "<LTLine 279.971,106.775,432.000,106.775>\n",
      "<LTTextBoxHorizontal(0) 71.997,519.912,432.004,607.973 'Subject Matter Experts\\nThe  first  profile  to  consider  as  part  of  MLOps  efforts  is  the  subject  matter  experts\\n(SMEs); after all, the ML model life cycle starts and ends with them. While the data-\\noriented profiles (data scientist, engineer, architect, etc.) have expertise across many\\nareas,  they  tend  to  lack  a  deep  understanding  of  the  business  and  the  problems  or\\nquestions that need to be addressed using machine learning.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,413.112,432.005,511.812 'Subject matter experts usually come to the table—or, at least, they should come to the\\ntable—with clearly defined goals, business questions, and/or key performance indica‐\\ntors  (KPIs)  that  they  want  to  achieve  or  address.  In  some  cases,  they  might  be\\nextremely well defined (e.g., “To hit our numbers for the quarter, we need to reduce\\ncustomer churn by 10%” or “We’re losing $N per quarter due to unscheduled mainte‐\\nnance;  how  can  we  better  predict  downtime?”).  In  other  cases,  the  goals  and  ques‐\\ntions may be less well defined (e.g., “Our service staff needs to better understand our\\ncustomers to upsell them” or “How can we get people to buy more widgets?”).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,331.512,432.005,405.012 'In  organizations  with  healthy  processes,  starting  the  machine  learning  model  life\\ncycle with a more defined business question isn’t necessarily always an imperative, or\\neven  an  ideal,  scenario.  Working  with  a  less  defined  business  goal  can  be  a  good\\nopportunity for subject matter experts to work directly with data scientists up front to\\nbetter  frame  the  problem  and  brainstorm  possible  solutions  before  even  beginning\\nany data exploration or model experimentation.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,249.912,432.003,323.412 'Without this critical starting point from subject matter experts, other data professio‐\\nnals (particularly data scientists) risk starting the machine learning life cycle process\\ntrying  to  solve  problems  or  provide  solutions  that  don’t  serve  the  larger  business.\\nUltimately,  this  is  detrimental  not  only  to  the  subject  matter  experts  who  need  to\\npartner with data scientists and other data experts to build solutions, but to data sci‐\\nentists themselves who might struggle to provide larger value.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.999,193.512,432.005,241.812 'Another negative outcome when SMEs are not involved in the ML life cycle is that,\\nwithout real business outcomes, data teams subsequently struggle to gain traction and\\nadditional  budget  or  support  to  continue  advanced  analytics  initiatives.  Ultimately,\\nthis is bad for data teams, for SMEs, and for the business as a whole.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,149.712,431.999,185.412 'To add more structure around SME involvement, business decision modeling meth‐\\nodologies can be applied to formalize the business problems to be solved and frame\\nthe role of machine learning in the solution.\\n'>\n",
      "<LTTextBoxHorizontal(6) 337.841,40.500,406.187,49.500 'Subject Matter Experts \\n'>\n",
      "<LTTextBoxHorizontal(7) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 424.655,40.500,431.999,49.500 '15\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.246,506.130,423.746,597.625 'Business Decision Modeling\\nDecision  modeling  creates  a  business  blueprint  of  the  decision-making  process,\\nallowing  subject  matter  experts  to  directly  structure  and  describe  their  needs.  Deci‐\\nsion models can be helpful because they put machine learning in context for subject\\nmatter experts. This allows the models to be integrated with the business rules, as well\\nas helps the SMEs to fully understand decision contexts and the potential impact of\\nmodel changes.\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.246,452.130,423.746,498.130 'MLOps strategies that include a component of business decision modeling for subject\\nmatter experts can be an effective tool for ensuring that real-world machine learning\\nmodel results are properly contextualized for those who don’t have deep knowledge of\\nhow the underlying models themselves work.1\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,367.886,432.005,428.787 'Subject matter experts have a role to play not only at the beginning of the ML model\\nlife  cycle,  but  at  the  end  (post-production)  as  well.  Oftentimes,  to  understand  if  an\\nML  model  is  performing  well  or  as  expected,  data  scientists  need  subject  matter\\nexperts  to  close  the  feedback  loop  because  traditional  metrics  (accuracy,  precision,\\nrecall, etc.) are not enough.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,273.686,432.003,359.786 'For  example,  data  scientists  could  build  a  simple  churn  prediction  model  that  has\\nvery high accuracy in a production environment; however, marketing does not man‐\\nage  to  prevent  anyone  from  churning.  From  a  business  perspective,  that  means  the\\nmodel didn’t work, and that’s important information that needs to make its way back\\nto those building the ML model so that they can find another possible solution, such\\nas introducing uplift modeling that helps marketing better target potential churners\\nwho might be receptive to marketing messaging.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,179.486,432.005,265.586 'Given the role of SMEs in the ML model life cycle, it’s critical when building MLOps\\nprocesses to have an easy way for them to understand deployed model performance\\nin  business  terms.  That  is,  they  need  to  understand  not  just  model  accuracy,  preci‐\\nsion, and recall, but the results or impact of the model on the business process identi‐\\nfied up front. In addition, when there are unexpected shifts in performance, subject\\nmatter experts need a scalable way, through MLOps processes, to flag model results\\nthat don’t align with business expectations.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,135.686,432.005,171.386 'On  top  of  these  explicit  feedback  mechanisms,  more  generally,  MLOps  should  be\\nbuilt  in  a  way  that  increases  transparency  for  subject  matter  experts.  That  is,  they\\nshould be able to use MLOps processes as a jumping-off point for exploring the data\\n'>\n",
      "<LTTextBoxHorizontal(6) 73.140,91.954,423.880,99.954 '1 Decision requirements models are based on Decision Model and Notation, a framework for improving pro‐\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.000,71.954,431.800,89.954 'cesses, effectively managing business rules projects, framing predictive analytics efforts, and ensuring decision\\nsupport systems and dashboards are action-oriented.\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.002,40.500,80.768,49.500 '16 \\n'>\n",
      "<LTTextBoxHorizontal(9) 88.346,40.500,91.658,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 99.236,40.500,179.921,49.500 'Chapter 2: People of MLOps\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.000,607.375>\n",
      "<LTLine 431.875,440.750,431.875,607.500>\n",
      "<LTLine 72.000,440.875,432.000,440.875>\n",
      "<LTLine 72.125,440.750,72.125,607.500>\n",
      "<LTLine 72.000,107.900,162.000,107.900>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'pipelines  behind  the  models,  understanding  what  data  is  being  used,  how  it’s  being\\ntransformed and enhanced, and what kind of machine learning techniques are being\\napplied.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.005,561.736 'For subject matter experts who are also concerned with compliance of machine learn‐\\ning models with internal or external regulations, MLOps serves as an additional way\\nto bring transparency and understanding to these processes. This includes being able\\nto dig into individual decisions made by a model to understand why the model came\\nto  that  decision.  This  should  be  complementary  to  statistical  and  aggregated\\nfeedback.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,419.236,432.005,480.136 'Ultimately, MLOps is most relevant for subject matter experts as a feedback mecha‐\\nnism  and  a  platform  for  communication  with  data  scientists  about  the  models  they\\nare  building.  However,  there  are  other  MLOps  needs  as  well—specifically  around\\ntransparency,  which  ties  into  Responsible  AI—that  are  relevant  for  subject  matter\\nexperts and make them an important part of the MLOps picture.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,331.436,432.004,406.897 'Data Scientists\\nThe needs of data scientists are the most critical ones to consider when building an\\nMLOps strategy. To be sure, they have a lot to gain; data scientists at most organiza‐\\ntions  today  often  deal  with  siloed  data,  processes,  and  tools,  making  it  difficult  to\\neffectively scale their efforts. MLOps is well positioned to change this.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,262.436,432.003,323.336 'Though most see data scientists’ role in the ML model life cycle as strictly the model\\nbuilding portion, it is—or at least, it should be—much wider. From the very begin‐\\nning, data scientists need to be involved with subject matter experts, understanding\\nand  helping  to  frame  business  problems  in  such  a  way  that  they  can  build  a  viable\\nmachine learning solution.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,168.236,432.004,254.336 'The reality is that this very first, critical step in the ML model life cycle is often the\\nhardest.  It’s  challenging  particularly  for  data  scientists  because  it’s  not  where  their\\ntraining  lies.  Both  formal  and  informal  data  science  programs  in  universities  and\\nonline heavily emphasize technical skills and not necessarily skills for communicating\\neffectively with subject matter experts from the business side of the house, who usu‐\\nally are not intimately familiar with machine learning techniques. Once again, busi‐\\nness decision modeling techniques can help here.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,74.036,432.004,160.136 'It’s also a challenge because it can take time. For data scientists who want to dive in\\nand get their hands dirty, spending weeks framing and outlining the problem before\\ngetting  started  on  solving  it  can  be  torture.  To  top  it  off,  data  scientists  are  often\\nsiloed (physically, culturally, or both) from the core of the business and from subject\\nmatter  experts,  so  they  simply  don’t  have  access  to  an  organizational  infrastructure\\nthat facilitates easy collaboration between these profiles. Robust MLOps systems can\\nhelp address some of these challenges.\\n'>\n",
      "<LTTextBoxHorizontal(7) 361.347,40.500,406.185,49.500 'Data Scientists \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.763,40.500,417.075,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.653,40.500,431.997,49.500 '17\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'After overcoming the first hurdle, depending on the organization, the project might\\nget handed off to either data engineers or analysts to do some of the initial data gath‐\\nering, preparation, and exploration. In some cases, data scientists themselves manage\\nthese  parts  of  the  ML  model  life  cycle.  But  in  any  case,  data  scientists  step  back  in\\nwhen it comes time to build, test, robustify, and then deploy the model.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,425.236,432.005,536.536 'Following deployment, data scientists’ roles include constantly assessing model qual‐\\nity to ensure the way it’s working in production answers initial business questions or\\nneeds. The underlying question in many organizations is often whether data scientists\\nmonitor  only  the  models  they  have  had  a  hand  in  building  or  whether  one  person\\nhandles  all  monitoring.  In  the  former  scenario,  what  happens  when  there  is  staff\\nturnover? In the latter scenario, building good MLOps practices is critical, as the per‐\\nson monitoring also needs to be able to quickly jump in and take action should the\\nmodel drift and start negatively affecting the business. If they weren’t the ones who\\nbuilt it, how can MLOps make this process seamless?\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.248,301.730,423.748,405.225 'Operationalization and MLOps\\nThroughout  2018  and  the  beginning  of  2019,  operationalization  was  the  key  buzz‐\\nword when it came to ML model life cycles and AI in the enterprise. Put simply, oper‐\\nationalization  of  data  science  is  the  process  of  pushing  models  to  production  and\\nmeasuring their performance against business goals. So how does operationalization\\nfit into the MLOps story? MLOps takes operationalization one step further, encom‐\\npassing  not  just  the  push  to  production  but  the  maintenance  of  those  models—and\\nthe entire data pipeline—in production.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.248,247.730,423.748,293.730 'Though  they  are  distinct,  MLOps  might  be  considered  the  new  operationalization.\\nThat is, where many of the major hurdles for businesses to operationalize have disap‐\\npeared, MLOps is the next frontier and presents the next big challenge for machine\\nlearning efforts in the enterprise.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,138.286,432.003,224.386 'All  of  the  questions  in  the  previous  section  lead  directly  here:  data  scientists’  needs\\nwhen  it  comes  to  MLOps.  Starting  from  the  end  of  the  process  and  working  back‐\\nward, MLOps must provide data scientists with visibility into the performance of all\\ndeployed models as well as any models being A/B tested. But taking that one step fur‐\\nther, it’s not just about monitoring—it’s also about action. Top-notch MLOps should\\nallow  data  scientists  the  flexibility  to  select  winning  models  from  tests  and  easily\\ndeploy them.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,69.286,432.004,130.186 'Transparency is an overarching theme in MLOps, so it’s no surprise that it’s also a key\\nneed for data scientists. The ability to drill down into data pipelines and make quick\\nassessments and adjustments (regardless of who originally built the model) is critical. \\nAutomated model packaging and delivery for quick and easy (yet safe) deployment to\\nproduction is another important point for transparency, and it’s a crucial component\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.003,40.500,80.769,49.500 '18 \\n'>\n",
      "<LTTextBoxHorizontal(7) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.237,40.500,179.922,49.500 'Chapter 2: People of MLOps\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,414.975,432.000,414.975>\n",
      "<LTLine 431.875,236.350,431.875,415.100>\n",
      "<LTLine 72.000,236.475,432.000,236.475>\n",
      "<LTLine 72.125,236.350,72.125,415.100>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'of MLOps, especially to bring data scientists together to a place of trust with software\\nengineers and DevOps teams.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.005,574.337 'In addition to transparency, another theme for mastering MLOps—especially when it\\ncomes to meeting the needs of data scientists—is pure efficiency. In an enterprise set‐\\nting, agility and speed matter. It’s true for DevOps, and the story for MLOps is no dif‐\\nferent.  Of  course,  data  scientists  can  deploy,  test,  and  monitor  models  in  an  ad  hoc\\nfashion.  But  they  will  spend  enormous  amounts  of  time  reinventing  the  wheel  with\\nevery single ML model, and that will never add up to scalable ML processes for the\\norganization.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.998,413.036,432.005,475.897 'Data Engineers\\nData pipelines are at the core of the ML model life cycle, and data engineers are, in\\nturn,  at  the  core  of  data  pipelines.  Because  data  pipelines  can  be  abstract  and  com‐\\nplex, data engineers have a lot of efficiencies to gain from MLOps.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,331.436,432.003,404.936 'In  large  organizations,  managing  the  flow  of  data,  outside  of  the  application  of  ML\\nmodels, is a full-time job.  Depending on the technical stack and organizational struc‐\\nture of the enterprise, data engineers might, therefore, be more focused on the data‐\\nbases  themselves  than  on  pipelines  (especially  if  the  company  is  leveraging  data\\nscience and ML platforms that facilitate the visual building of pipelines by other data\\npractitioners, like business analysts).\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,237.236,432.005,323.336 'Ultimately,  despite  these  slight  variations  in  the  role  by  an  organization,  the  role  of\\ndata engineers in the life cycle is to optimize the retrieval and use of data to eventually\\npower ML models. Generally, this means working closely with business teams, partic‐\\nularly  subject  matter  experts,  to  identify  the  right  data  for  the  project  at  hand  and\\npossibly also prepare it for use. On the other end, they work closely with data scien‐\\ntists to resolve any data plumbing issues that might cause a model to behave undesira‐\\nbly in production.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.998,168.236,432.004,229.136 'Given data engineers’ central role in the ML model life cycle, underpinning both the\\nbuilding and monitoring portions, MLOps can bring significant efficiency gains. Data\\nengineers require not only visibility into the performance of all models deployed in\\nproduction,  but  the  ability  to  take  it  one  step  further  and  directly  drill  down  into\\nindividual data pipelines to address any underlying issues.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.999,124.436,432.005,160.136 'Ideally, for maximum efficiency for the data engineer profile (and for others as well,\\nincluding  data  scientists),  MLOps  must  not  consist  of  simple  monitoring,  but  be  a\\nbridge to underlying systems for investigating and tweaking ML models.\\n'>\n",
      "<LTTextBoxHorizontal(7) 360.228,40.500,406.191,49.500 'Data Engineers \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.659,40.500,432.003,49.500 '19\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,545.112,432.003,607.973 'Software Engineers\\nIt would be easy to exclude classical software engineers from MLOps consideration,\\nbut  it  is  crucial  from  a  wider  organizational  perspective  to  consider  their  needs  to\\nbuild a cohesive enterprise-wide strategy for machine learning.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,450.912,432.005,537.012 'Software  engineers  don’t  usually  build  ML  models,  but,  on  the  other  hand,  most\\norganizations  are  not  only  producing  ML  models,  but  classic  software  and  applica‐\\ntions as well. It’s important that software engineers and data scientists work together\\nto ensure the functioning of the larger system. After all, ML models aren’t just stand‐\\nalone experiments; the machine learning code, training, testing, and deployment have\\nto  fit  into  the  Continuous  Integration/Continuous  Delivery  (CI/CD)  pipelines  that\\nthe rest of the software is using.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,369.312,432.005,442.812 'For example, consider a retail company that has built an ML-based recommendation\\nengine  for  their  website.  The  ML  model  was  built  by  the  data  scientist,  but  to  inte‐\\ngrate it into the larger functioning of the site, software engineers will necessarily need\\nto  be  involved.  Similarly,  software  engineers  are  responsible  for  the  maintenance  of\\nthe  website  as  a  whole,  and  a  large  part  of  that  includes  the  functioning  of  the  ML\\nmodels in production.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,300.312,432.003,361.212 'Given  this  interplay,  software  engineers  need  MLOps  to  provide  them  with  model\\nperformance  details  as  part  of  a  larger  picture  of  software  application  performance\\nfor the enterprise. MLOps is a way for data scientists and software engineers to speak\\nthe same language and have the same baseline understanding of how different models\\ndeployed across the silos of the enterprise are working together in production.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,243.912,432.005,292.212 'Other  important  features  for  software  engineers  include  versioning,  to  be  sure  of\\nwhat  they  are  currently  dealing  with;  automatic  tests,  to  be  as  sure  as  possible  that\\nwhat they are currently dealing with is working; and the ability to work in parallel on\\nthe same application (thanks to a system that allows branches and merges like Git). \\n'>\n",
      "<LTTextBoxHorizontal(5) 71.999,181.312,432.004,231.572 'DevOps\\nMLOps was born out of DevOps principles, but that doesn’t mean they can be run in\\nparallel as completely separate and siloed systems.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,99.712,432.002,173.212 'DevOps teams have two primary roles in the ML model life cycle. First, they are the\\npeople conducting and building operational systems as well as tests to ensure security,\\nperformance, and availability of ML models. Second, they are responsible for CI/CD\\npipeline management. Both of these roles require tight collaboration with data scien‐\\ntists, data engineers, and data architects. Tight collaboration is, of course, easier said\\nthan done, but that is where MLOps can add value.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.998,40.500,80.764,49.500 '20 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.342,40.500,91.654,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.232,40.500,179.917,49.500 'Chapter 2: People of MLOps\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'For DevOps teams, MLOps needs to be integrated into the larger DevOps strategy of\\nthe  enterprise,  bridging  the  gap  between  traditional  CI/CD  and  modern  ML.  That\\nmeans systems that are fundamentally complementary and that allow DevOps teams\\nto automate tests for ML just as they can automate tests for traditional software.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,444.236,432.005,544.897 'Model Risk Manager/Auditor\\nIn certain industries (particularly the financial services sector), the model risk man‐\\nagement (MRM) function is crucial for regulatory compliance. But it’s not only highly\\nregulated industries that should be concerned or that should have a similar function;\\nMRM  can  protect  companies  in  any  industry  from  catastrophic  loss  introduced  by\\npoorly  performing  ML  models.  What’s  more,  audits  play  a  role  in  many  industries\\nand can be labor intensive, which is where MLOps comes into the picture.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,375.236,432.005,436.136 'When it comes to the ML model life cycle, model risk managers play the critical role\\nof analyzing not just model outcomes, but the initial goal and business questions ML\\nmodels  seek  to  resolve  to  minimize  overall  risk  to  the  company.  They  should  be\\ninvolved  along  with  subject  matter  experts  at  the  very  beginning  of  the  life  cycle  to\\nensure that an automated, ML-based approach in and of itself doesn’t present risk.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,306.236,432.003,367.136 'And, of course, they have a role to play in monitoring—their more traditional place\\nin the model life cycle—to ensure that risks are kept at bay once models are in pro‐\\nduction.  In  between  conception  and  monitoring,  MRM  also  is  a  factor  post-model\\ndevelopment and preproduction, ensuring initial compliance with internal and exter‐\\nnal requirements.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,249.836,432.005,298.136 'MRM professionals and teams have a lot to gain from MLOps, because their work is\\noften painstakingly manual. As MRM and the teams with which they work often use\\ndifferent tools, standardization can offer a huge leg up in the speed at which auditing\\nand risk management can occur.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,180.836,432.003,241.736 'When  it  comes  to  specific  MLOps  needs,  robust  reporting  tools  on  all  models\\n(whether they are currently in production or have been in production in the past) is\\nthe primary one. This reporting should include not just performance details, but the\\nability to see data lineage. Automated reporting adds an extra layer of efficiency for\\nMRM and audit teams in MLOps systems and processes.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,93.035,432.004,168.496 'Machine Learning Architect\\nTraditional  data  architects  are  responsible  for  understanding  the  overall  enterprise\\narchitecture and ensuring that it meets the requirements for data needs from across\\nthe  business.  They  generally  play  a  role  in  defining  how  data  will  be  stored  and\\nconsumed.\\n'>\n",
      "<LTTextBoxHorizontal(7) 320.079,40.500,406.191,49.500 'Model Risk Manager/Auditor \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.659,40.500,432.003,49.500 '21\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Today, demands on architects are much greater, and they often have to be knowledge‐\\nable not only on the ins and outs of data storage and consumption, but on how ML\\nmodels work in tandem. This adds a lot of complexity to the role and increases their\\nresponsibility  in  the  MLOps  life  cycle,  and  it’s  why  in  this  section,  we  have  called\\nthem machine learning architects instead of the more traditional “data architect” title.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.036,432.003,536.536 'Machine learning architects play a critical role in the ML model life cycle, ensuring a\\nscalable and flexible environment for model pipelines. In addition, data teams need\\ntheir  expertise  to  introduce  new  technologies  (when  appropriate)  that  improve  ML\\nmodel performance in production. It is for this reason that the data architect title isn’t\\nenough; they need to have an intimate understanding of machine learning, not just\\nenterprise architecture, to play this key role in the ML model life cycle.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,406.636,432.005,454.936 'This role requires collaboration across the enterprise, from data scientists and engi‐\\nneers  to  DevOps  and  software  engineers.  Without  a  complete  understanding  of  the\\nneeds of each of these people and teams, machine learning architects cannot properly\\nallocate resources to ensure optimal performance of ML models in production.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,325.036,432.005,398.536 'When  it  comes  to  MLOps,  the  machine  learning  architects’  role  is  about  having  a\\ncentralized  view  of  resource  allocation.  As  they  have  a  strategic,  tactical  role,  they\\nneed an overview of the situation to identify bottlenecks and use that information to\\nfind long-term improvements. Their role is one of pinpointing possible new technol‐\\nogy or infrastructure for investment, not necessarily operational quick fixes that don’t\\naddress the heart of the scalability of the system.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,186.836,432.004,312.696 'Closing Thoughts\\nMLOps isn’t just for data scientists; a diverse group of experts across the organization\\nhas a role to play not only in the ML model life cycle, but the MLOps strategy as well.\\nIn fact, each person—from the subject matter expert on the business side to the most\\ntechnical machine learning architect—plays a critical part in the maintenance of ML\\nmodels in production. This is ultimately important not only to ensure the best possi‐\\nble results from ML models (good results generally lead to more trust in ML-based\\nsystems as well as increased budget to build more), but, perhaps more pointedly, to\\nprotect the business from the risks outlined in Chapter 1.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,40.500,80.762,49.500 '22 \\n'>\n",
      "<LTTextBoxHorizontal(6) 88.340,40.500,91.652,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 99.230,40.500,179.915,49.500 'Chapter 2: People of MLOps\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 266.908,533.502,431.998,582.331 'CHAPTER 3\\nKey MLOps Features\\n'>\n",
      "<LTTextBoxHorizontal(1) 364.515,436.501,431.998,449.501 'Mark Treveil\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,347.468,432.003,408.369 'MLOps affects many different roles across the organization and, in turn, many parts\\nof the machine learning life cycle. This chapter introduces the five key components of\\nMLOps (development, deployment, monitoring, iteration, and governance) at a high\\nlevel as a foundation for Chapters 4 through 8, which delve into the more technical\\ndetails and requirements of these components.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.993,259.668,432.004,335.129 'A Primer on Machine Learning\\nTo  understand  the  key  features  of  MLOps,  it’s  essential  first  to  understand  how\\nmachine learning works and be intimately familiar with its specificities. Though often\\noverlooked  in  its  role  as  a  part  of  MLOps,  ultimately  algorithm  selection  (or  how\\nmachine learning models are built) can have a direct impact on MLOps processes.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,203.268,432.004,251.568 'At its core, machine learning is the science of computer algorithms that automatically\\nlearn  and  improve  from  experience  rather  than  being  explicitly  programmed.  The\\nalgorithms  analyze  sample  data,  known  as  training  data,  to  build  a  software  model\\nthat can make predictions.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.998,134.268,432.005,195.168 'For example, an image recognition model might be able to identify the type of elec‐\\ntricity meter from a photograph by searching for key patterns in the image that dis‐\\ntinguish each type of meter. Another example is an insurance recommender model,\\nwhich might suggest additional insurance products that a specific existing customer\\nis most likely to buy based on the previous behavior of similar customers.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,90.468,431.999,126.168 'When faced with unseen data, be it a photo or a customer, the ML model uses what it\\nhas  learned  from  previous  data  to  make  the  best  prediction  it  can  based  on  the\\nassumption that the unseen data is somehow related to the previous data.\\n'>\n",
      "<LTTextBoxHorizontal(7) 424.655,40.500,431.999,49.500 '23\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.004,605.537 'ML algorithms use a wide range of mathematical techniques, and the models can take\\nmany different forms, from simple decision trees to logistic regression algorithms to\\nmuch  more  complex  deep  learning  models  (see  “What  Is  a  Machine  Learning\\nModel?” on page 42 for details).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.998,482.036,432.004,544.897 'Model Development\\nLet’s  take  a  deeper  look  into  ML  model  development  as  a  whole  for  an  even  more\\ncomplete  understanding  of  its  components,  all  of  which  can  have  an  impact  on\\nMLOps after deployment.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,361.027,432.005,470.466 'Establishing Business Objectives\\nThe process of developing a machine learning model typically starts with a business\\nobjective,  which  can  be  as  simple  as  reducing  fraudulent  transactions  to  <  0.1%  or\\nhaving  the  ability  to  identify  people’s  faces  on  their  social  media  photos.  Business\\nobjectives naturally come with performance targets, technical infrastructure require‐\\nments, and cost constraints; all of these factors can be captured as key performance\\nindicators, or KPIs, which will ultimately enable the business performance of models\\nin production to be monitored.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,266.827,432.005,352.927 'It’s important to recognize that ML projects don’t happen in a vacuum. They are gen‐\\nerally part of a larger project that in turn impacts technologies, processes, and people.\\nThat means part of establishing objectives also includes change management, which\\nmay even provide some guidance for how the ML model should be built. For exam‐\\nple,  the  required  degree  of  transparency  will  strongly  influence  the  choice  of  algo‐\\nrithms and may drive the need to provide explanations together with predictions so\\nthat predictions are turned into valuable decisions at the business level.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,183.618,432.003,255.257 'Data Sources and Exploratory Data Analysis\\nWith  clear  business  objectives  defined,  it  is  time  to  bring  together  subject  matter\\nexperts  and  data  scientists  to  begin  the  journey  of  developing  the  ML  model.  This\\nstarts with the search for suitable input data. Finding data sounds simple, but in prac‐\\ntice, it can be the most arduous part of the journey.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.003,165.018,320.171,175.518 'Key questions for finding data to build ML models include:\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.658,142.418,243.150,152.918 '• What relevant datasets are available?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,125.818,276.949,136.318 '• Is this data sufficiently accurate and reliable?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,109.218,278.535,119.718 '• How can stakeholders get access to this data?\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,92.618,431.997,103.118 '• What  data  properties  (known  as  features)  can  be  made  available  by  combining\\n'>\n",
      "<LTTextBoxHorizontal(10) 90.002,80.018,194.067,90.518 'multiple sources of data?\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.000,40.500,80.766,49.500 '24 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.234,40.500,189.945,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.655,595.037,251.277,605.537 '• Will this data be available in real time?\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,553.236,432.002,588.937 '• Is there a need to label some of the data with the “ground truth” that is to be pre‐\\ndicted, or does unsupervised learning make sense? If so, how much will this cost\\nin terms of time and resources?\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.654,536.637,221.478,547.137 '• What platform should be used?\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,520.036,317.784,530.537 '• How will data be updated once the model is deployed?\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,503.437,394.182,513.937 '• Will the use of the model itself reduce the representativeness of the data?\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,486.837,431.997,497.337 '• How will the KPIs, which were established along with the business objectives, be\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,474.237,134.984,484.737 'measured?\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.005,451.637,381.041,462.137 'The constraints of data governance bring even more questions, including:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,429.037,301.089,439.537 '• Can the selected datasets be used for this purpose?\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,412.436,201.917,422.936 '• What are the terms of use?\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.655,395.836,432.003,406.336 '• Is there personally identifiable information (PII) that must be redacted or anony‐\\n'>\n",
      "<LTTextBoxHorizontal(11) 89.997,383.236,119.806,393.736 'mized?\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.655,366.636,431.997,377.136 '• Are  there  features,  such  as  gender,  that  legally  cannot  be  used  in  this  business\\n'>\n",
      "<LTTextBoxHorizontal(13) 90.002,354.036,125.145,364.536 'context?\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.660,337.436,431.997,347.936 '• Are minority populations sufficiently well represented that the model has equiva‐\\n'>\n",
      "<LTTextBoxHorizontal(15) 90.001,324.836,232.770,335.336 'lent performances on each group?\\n'>\n",
      "<LTTextBoxHorizontal(16) 71.995,239.236,432.004,312.736 'Since data is the essential ingredient to power ML algorithms, it always helps to build\\nan understanding of the patterns in data before attempting to train models. Explora‐\\ntory data analysis (EDA) techniques can help build hypotheses about the data, iden‐\\ntify  data  cleaning  requirements,  and  inform  the  process  of  selecting  potentially\\nsignificant features. EDA can be carried out visually for intuitive insight and statisti‐\\ncally if more rigor is required.\\n'>\n",
      "<LTTextBoxHorizontal(17) 71.993,118.227,432.004,227.667 'Feature Engineering and Selection\\nEDA leads naturally into feature engineering and feature selection. Feature engineer‐\\ning  is  the  process  of  taking  raw  data  from  the  selected  datasets  and  transforming  it\\ninto “features” that better represent the underlying problem to be solved. “Features”\\nare arrays of numbers of fixed size, as it is the only object that ML algorithms under‐\\nstand.  Feature  engineering  includes  data  cleansing,  which  can  represent  the  largest\\npart  of  an  ML  project  in  terms  of  time  spent.  For  details,  see  “Feature  Engineering\\nand Selection” on page 47.\\n'>\n",
      "<LTTextBoxHorizontal(18) 344.816,40.500,406.187,49.500 'Model Development \\n'>\n",
      "<LTTextBoxHorizontal(19) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(20) 424.655,40.500,431.999,49.500 '25\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,511.055,432.004,607.894 'Training and Evaluation\\nAfter  data  preparation  by  way  of  feature  engineering  and  selection,  the  next  step  is\\ntraining. The process of training and optimizing a new ML model is iterative; several\\nalgorithms may be tested, features can be automatically generated, feature selections\\nmay be adapted, and algorithm hyperparameters tuned. In addition to—or in many\\ncases  because  of—its  iterative  nature,  training  is  also  the  most  intensive  step  of  the\\nML model life cycle when it comes to computing power.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,416.855,432.005,502.955 'Keeping  track  of  the  results  of  each  experiment  when  iterating  becomes  complex\\nquickly. Nothing is more frustrating to data scientists than not being able to re-create\\nthe best results because they cannot remember the precise configuration. An experi‐\\nment tracking tool can greatly simplify the process of remembering the data, the fea‐\\ntures  selection,  and  model  parameters  alongside  the  performance  metrics.  These\\nenable  experiments  to  be  compared  side-by-side,  highlighting  the  differences  in\\nperformance.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,373.055,431.998,408.755 'Deciding  what  is  the  best  solution  requires  both  quantitative  criteria,  such  as  accu‐\\nracy or average error, and qualitative criteria regarding the explainability of the algo‐\\nrithm or its ease of deployment.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,277.246,432.004,361.485 'Reproducibility\\nWhile many experiments may be short-lived, significant versions of a model need to\\nbe  saved  for  possible  later  use.  The  challenge  here  is  reproducibility,  which  is  an\\nimportant  concept  in  experimental  science  in  general.  The  aim  in  ML  is  to  save\\nenough information about the environment the model was developed in so that the\\nmodel can be reproduced with the same results from scratch.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,183.046,432.004,269.146 'Without reproducibility, data scientists have little chance of being able to confidently\\niterate on models, and worse, they are unlikely to be able to hand over the model to\\nDevOps to see if what was created in the lab can be faithfully reproduced in produc‐\\ntion.  True  reproducibility  requires  version  control  of  all  the  assets  and  parameters\\ninvolved, including the data used to train and evaluate the model, as well as a record\\nof  the  software  environment  (see  “Version  Management  and  Reproducibility”  on\\npage 56 for details).\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,74.637,432.005,171.476 'Responsible AI\\nBeing  able  to  reproduce  the  model  is  only  part  of  the  operationalization  challenge;\\nthe DevOps team also needs to understand how to verify the model (i.e., what does\\nthe model do, how should it be tested, and what are the expected results?). Those in\\nhighly regulated industries are likely required to document even more detail, includ‐\\ning how the model was built and how it was tuned. In critical cases, the model may be\\nindependently recoded and rebuilt.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,40.500,80.764,49.500 '26 \\n'>\n",
      "<LTTextBoxHorizontal(7) 88.342,40.500,91.654,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.232,40.500,189.943,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Documentation is still the standard  solution to this communication challenge. Auto‐\\nmated model document generation, whereby a tool automatically creates documenta‐\\ntion associated with any trained model, can make the task less onerous. But in almost\\nall cases, some documentation will need to be written by hand to explain the choices\\nmade.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,437.836,432.005,536.536 'It is a fundamental consequence of their statistical nature that ML models are chal‐\\nlenging  to  understand.  While  model  algorithms  come  with  standard  performance\\nmeasures  to  assess  their  efficacy,  these  don’t  explain  how  the  predictions  are  made.\\nThe  “how”  is  important  as  a  way  to  sanity-check  the  model  or  help  better  engineer\\nfeatures,  and  it  may  be  necessary  to  ensure  that  fairness  requirements  (e.g.,  around\\nfeatures like sex, age, or race) have been met. This is the field of explainability, which\\nis connected to Responsible AI as discussed in Chapter 1 and which will be discussed\\nin further detail in Chapter 4.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,381.436,432.004,429.736 'Explainability  techniques  are  becoming  increasingly  important  as  global  concerns\\ngrow about the impact of unbridled AI. They offer a way to mitigate uncertainty and\\nhelp prevent unintended consequences. The techniques most commonly used today\\ninclude:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.650,358.836,431.997,369.336 '• Partial  dependence  plots,  which  look  at  the  marginal  impact  of  features  on  the\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.002,346.236,171.629,356.736 'predicted outcome \\n'>\n",
      "<LTTextBoxHorizontal(5) 80.660,329.636,431.997,340.136 '• Subpopulation analyses, which look at how the model treats specific subpopula‐\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,317.036,310.386,327.536 'tions and that are the basis of many fairness analyses\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,300.436,431.997,310.936 '• Individual  model  predictions,  such  as  Shapley  values,  which  explain  how  the\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.001,287.836,323.994,298.336 'value of each feature contributes to a specific prediction\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.659,271.236,431.998,281.736 '• What-if analysis, which helps the ML model user to understand the sensitivity of\\n'>\n",
      "<LTTextBoxHorizontal(10) 90.003,258.636,201.502,269.136 'the prediction to its inputs\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.995,198.236,432.003,246.536 'As we’ve seen in this section, even though model development happens very early on,\\nit’s still an important place to incorporate MLOps practices. Any MLOps work done\\nup front during the model development stage will make the models easier to manage\\ndown the line (especially when pushing to production).\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.991,85.236,432.003,185.896 'Productionalization and Deployment\\nProductionalizing and deploying models is a key component of MLOps that presents\\nan  entirely  different  set  of  technical  challenges  than  developing  the  model.  It  is  the\\ndomain of the software engineer and the DevOps team, and the organizational chal‐\\nlenges  in  managing  the  information  exchange  between  the  data  scientists  and  these\\nteams must not be underestimated. As touched on in Chapter 1, without effective col‐\\nlaboration between the teams, delays or failures to deploy are inevitable.\\n'>\n",
      "<LTTextBoxHorizontal(13) 295.793,40.500,406.187,49.500 'Productionalization and Deployment \\n'>\n",
      "<LTTextBoxHorizontal(14) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 424.655,40.500,431.999,49.500 '27\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,548.855,432.002,607.894 'Model Deployment Types and Contents\\nTo understand what happens in these phases, it’s helpful to take a step back and ask:\\nwhat exactly is going into production, and what does a model consist of? There are\\ncommonly two types of model deployment:\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,529.255,239.460,539.755 'Model-as-a-service, or live-scoring model\\n'>\n",
      "<LTTextBoxHorizontal(2) 89.997,491.455,432.003,527.155 'Typically the model is deployed into a simple framework to provide a REST API\\nendpoint  (the  means  from  which  the  API  can  access  the  resources  it  needs  to\\nperform the task) that responds to requests in real time.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.005,471.855,142.460,482.355 'Embedded model\\n'>\n",
      "<LTTextBoxHorizontal(4) 89.996,446.655,432.002,469.755 'Here the model is packaged into an application, which is then published. A com‐\\nmon example is an application that provides batch-scoring of requests.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,377.655,432.003,438.555 'What  to-be-deployed  models  consist  of  depends,  of  course,  on  the  technology\\nchosen, but typically they comprise a set of code (commonly Python, R, or Java) and\\ndata artifacts. Any of these can have version dependencies on runtimes and packages\\nthat need to match in the production environment because the use of different ver‐\\nsions may cause model predictions to differ.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,283.455,432.005,369.555 'One approach to reducing dependencies on the production environment is to export\\nthe model to a portable format such as PMML, PFA, ONNX, or POJO. These aim to\\nimprove model portability between systems and simplify deployment. However, they\\ncome  at  a  cost:  each  format  supports  a  limited  range  of  algorithms,  and  sometimes\\nthe portable models behave in subtly different ways than the original. Whether or not\\nto use a portable format is a choice to be made based on a thorough understanding of\\nthe technological and business context.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.246,183.948,423.746,263.443 'Containerization\\nContainerization  is  an  increasingly  popular  solution  to  the  headaches  of  dependen‐\\ncies  when  deploying  ML  models.  Container  technologies  such  as  Docker  are  light‐\\nweight  alternatives  to  virtual  machines,  allowing  applications  to  be  deployed  in\\nindependent, self-contained environments, matching the exact requirements of each\\nmodel.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.246,129.948,423.749,175.948 'They also enable new models to be seamlessly deployed using the blue-green deploy‐\\nment technique.1 Compute resources for models can be scaled elastically using multi‐\\nple containers, too. Orchestrating many containers is the role of technologies such as\\nKubernetes and can be used both in the cloud and on-premise.\\n'>\n",
      "<LTTextBoxHorizontal(9) 73.140,81.954,427.136,89.954 '1 Describing the blue-green deployment technique will require more space than we have here. For more infor‐\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.000,71.954,185.936,79.954 'mation, see Martin Fowler’s blog.\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.001,40.500,80.767,49.500 '28 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.345,40.500,91.657,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.235,40.500,189.946,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,273.193,432.000,273.193>\n",
      "<LTLine 431.875,118.568,431.875,273.318>\n",
      "<LTLine 72.000,118.693,432.000,118.693>\n",
      "<LTLine 72.125,118.568,72.125,273.318>\n",
      "<LTLine 72.000,97.900,162.000,97.900>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.998,536.255,432.005,607.894 'Model Deployment Requirements\\nSo  what  about  the  productionalization  process  between  completing  model  develop‐\\nment  and  physically  deploying  into  production—what  needs  to  be  addressed?  One\\nthing is for sure: rapid, automated deployment is always preferred to labor-intensive\\nprocesses.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,429.455,432.004,528.155 'For  short-lifetime,  self-service  applications,  there  often  isn’t  much  need  to  worry\\nabout testing and validation. If the maximum resource demands of the model can be\\nsecurely capped by technologies such as Linux cgroups, then a fully automated single-\\nstep push-to-production may be entirely adequate. It is even possible to handle sim‐\\nple  user  interfaces  with  frameworks  like  Flask  when  using  this  lightweight\\ndeployment  mode.  Along  with  integrated  data  science  and  machine  learning  plat‐\\nforms, some business rule management systems may also allow some sort of autono‐\\nmous deployment of basic ML models.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.000,398.255,432.003,421.355 'In  customer-facing,  mission-critical  use  cases,  a  more  robust  CI/CD  pipeline  is\\nrequired. This typically involves:\\n'>\n",
      "<LTTextBoxHorizontal(3) 77.316,375.655,400.335,386.155 '1. Ensuring all coding, documentation and sign-off standards have been met\\n'>\n",
      "<LTTextBoxHorizontal(4) 77.316,359.055,418.605,369.555 '2. Re-creating the model in something approaching the production environment\\n'>\n",
      "<LTTextBoxHorizontal(5) 77.316,342.455,226.423,352.955 '3. Revalidating the model accuracy\\n'>\n",
      "<LTTextBoxHorizontal(6) 77.316,325.855,228.135,336.355 '4. Performing explainability checks\\n'>\n",
      "<LTTextBoxHorizontal(7) 77.316,309.255,312.965,319.755 '5. Ensuring all governance requirements have been met\\n'>\n",
      "<LTTextBoxHorizontal(8) 77.316,292.655,261.682,303.155 '6. Checking the quality of any data artifacts\\n'>\n",
      "<LTTextBoxHorizontal(9) 77.316,276.055,232.713,286.555 '7. Testing resource usage under load\\n'>\n",
      "<LTTextBoxHorizontal(10) 77.316,259.455,392.019,269.955 '8. Embedding into a more complex application, including integration tests\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,186.455,432.005,247.355 'In  heavily  regulated  industries  (e.g.,  finance  and  pharmaceuticals),  governance  and\\nregulatory checks will be extensive and are likely to involve manual intervention. The\\ndesire in MLOps, just as in DevOps, is to automate the CI/CD pipeline as far as possi‐\\nble.  This  not  only  speeds  up  the  deployment  process,  but  it  enables  more  extensive\\nregression testing and reduces the likelihood of errors in the deployment. \\n'>\n",
      "<LTTextBoxHorizontal(12) 71.998,111.254,432.004,174.115 'Monitoring\\nOnce a model is deployed to production, it is crucial that it continue to perform well\\nover time. But good performance means different things to different people, in par‐\\nticular to the DevOps team, to data scientists, and to the business.\\n'>\n",
      "<LTTextBoxHorizontal(13) 371.446,40.500,406.186,49.500 'Monitoring \\n'>\n",
      "<LTTextBoxHorizontal(14) 413.764,40.500,417.076,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 424.654,40.500,431.998,49.500 '29\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,574.055,401.249,607.894 'DevOps Concerns\\nThe concerns of the DevOps team are very familiar and include questions like:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.656,551.455,299.682,561.955 '• Is the model getting the job done quickly enough?\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,534.855,348.916,545.355 '• Is it using a sensible amount of memory and processing time?\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,487.055,432.000,522.755 'This is traditional IT performance monitoring, and DevOps teams know how to do\\nthis well already. The resource demands of ML models are not so different from tra‐\\nditional software in this respect.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.999,418.055,432.005,478.955 'Scalability  of  compute  resources  can  be  an  important  consideration,  for  example,  if\\nyou are retraining models in production. Deep learning models have greater resource\\ndemands  than  much  simpler  decision  trees.  But  overall,  the  existing  expertise  in\\nDevOps teams for monitoring and managing resources can be readily applied to ML\\nmodels.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,297.046,432.005,406.485 'Data Scientist Concerns\\nThe data scientist is interested in monitoring ML models for a new, more challenging\\nreason:  they  can  degrade  over  time,  since  ML  models  are  effectively  models  of  the\\ndata they were trained on. This is not a problem faced by traditional software, but it is\\ninherent to machine learning. ML mathematics builds a concise representation of the\\nimportant patterns in the training data with the hope that this is a good reflection of\\nthe real world. If the training data reflects the real world well, then the model should\\nbe accurate and, thus, useful.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,190.246,432.005,288.946 'But the real world doesn’t stand still. The training data used to build a fraud detection\\nmodel six months ago won’t reflect a new type of fraud that has started to occur in the\\nlast  three  months.  If  a  given  website  starts  to  attract  an  increasingly  younger  user\\nbase, then a model that generates advertisements is likely to produce less and less rel‐\\nevant adverts. At some point, the performance will become unacceptable, and model\\nretraining  becomes  necessary.  How  soon  models  need  to  be  retrained  depends  on\\nhow fast the real world is changing and how accurate the model needs to be, but also,\\nimportantly, on how easy it is to build and deploy a better model.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.998,146.446,432.001,182.146 'But  first,  how  can  data  scientists  tell  a  model’s  performance  is  degrading?  It’s  not\\nalways easy. There are two common approaches, one based on ground truth and the\\nother on input drift.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,122.078,121.600,133.638 'Ground truth\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.995,91.554,431.998,114.654 'The  ground  truth,  put  simply,  is  the  correct  answer  to  the  question  that  the  model\\nwas asked to solve—for example, “Is this credit card transaction actually fraudulent?”\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.004,40.500,80.770,49.500 '30 \\n'>\n",
      "<LTTextBoxHorizontal(11) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 99.238,40.500,189.949,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'In knowing the ground truth for all the predictions a model has made, one can judge\\nhow well that model is performing.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.036,432.004,574.337 'Sometimes ground truth is obtained rapidly after a prediction—for example, in mod‐\\nels that decide which advertisements to display to a user on a web page. The user is\\nlikely to click on the advertisements within seconds, or not at all. However, in many\\nuse cases, obtaining the ground truth is much slower. If a model predicts that a trans‐\\naction is fraudulent, how can this be confirmed? In some cases, verification may only\\ntake a few minutes, such as a phone call placed to the cardholder. But what about the\\ntransactions the model thought were OK but actually weren’t? The best hope is that\\nthey will be reported by the cardholder when they review their monthly transactions,\\nbut this could happen up to a month after the event (or not at all).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.999,419.236,432.005,454.936 'In the fraud example, ground truth isn’t going to enable data science teams to moni‐\\ntor performance accurately on a daily basis. If the situation requires rapid feedback,\\nthen input drift may be a better approach.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.999,394.869,111.373,406.429 'Input drift\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,301.344,432.004,387.444 'Input drift is based on the principle that a model is only going to predict accurately if\\nthe data it was trained on is an accurate reflection of the real world. So if a compari‐\\nson  of  recent  requests  to  a  deployed  model  against  the  training  data  shows  distinct\\ndifferences, then there is a strong likelihood that the model performance is compro‐\\nmised. This is the basis of input drift monitoring. The beauty of this approach is that\\nall the data required for this test already exists, so there is no need to wait for ground\\ntruth or any other information.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,244.944,432.003,293.244 'Identifying  drift  is  one  of  the  most  important  components  of  an  adaptable  MLOps\\nstrategy, and one that can bring agility to the organization’s enterprise AI efforts over‐\\nall. Chapter 7 will go into more technical depth about data scientists’ concerns when\\nit comes to model monitoring.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,186.935,432.003,233.375 'Business Concerns\\nThe business has a holistic outlook on monitoring, and some of its concerns might\\ninclude questions like:\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,164.335,286.011,174.835 '• Is the model delivering value to the enterprise?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,147.735,431.997,158.235 '• Do the benefits of the model outweigh the cost of developing and deploying it?\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,135.135,227.247,145.635 '(And how can we measure this?)\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.998,87.335,432.004,123.035 'The  KPIs  identified  for  the  original  business  objective  are  one  part  of  this  process.\\nWhere  possible,  these  should  be  monitored  automatically,  but  this  is  rarely  trivial.\\nThe  objective  of  reducing  fraud  to  less  than  0.1%  of  transactions  in  our  previous\\n'>\n",
      "<LTTextBoxHorizontal(11) 371.446,40.500,406.186,49.500 'Monitoring \\n'>\n",
      "<LTTextBoxHorizontal(12) 413.764,40.500,417.076,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 424.654,40.500,431.998,49.500 '31\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'example is reliant on establishing the ground truth. But even monitoring this doesn’t\\nanswer the question: what is the net gain to the business in dollars?\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.036,432.005,574.337 'This  is  an  age-old  challenge  for  software,  and  with  ever-increasing  expenditure  on\\nML, the pressure for data scientists to demonstrate value is only going to grow. In the\\nabsence  of  a  “dollar-o-meter,”  effectively  monitoring  the  business  KPIs  is  the  best\\noption available (see “Design and Manage Experiments” on page 138). The choice of\\nthe baseline is important here and should ideally allow for differentiation of the value\\nof the ML subproject specifically, rather than that of the global project. For example,\\nthe  ML  performance  can  be  assessed  with  respect  to  a  rule-based  decision  model\\nbased on subject matter expertise to set apart the contribution of decision automation\\nand of ML per se.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,350.036,432.004,450.697 'Iteration and Life Cycle\\nDeveloping  and  deploying  improved  versions  of  a  model  is  an  essential  part  of  the\\nMLOps  life  cycle,  and  one  of  the  more  challenging.  There  are  various  reasons  to\\ndevelop a new model version, one of which is model performance degradation due to\\nmodel  drift,  as  discussed  in  the  prior  section.  Sometimes  there  is  a  need  to  reflect\\nrefined business objectives and KPIs, and other times, it’s just that the data scientists\\nhave come up with a better way to design the model.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.998,279.427,432.005,338.466 'Iteration\\nIn  some  fast-moving  business  environments,  new  training  data  becomes  available\\nevery  day.  Daily  retraining  and  redeployment  of  the  model  are  often  automated  to\\nensure that the model reflects recent experience as closely as possible.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,235.627,432.002,271.327 'Retraining an existing model with the latest training data is the simplest scenario for\\niterating a new model version. But while there are no changes to feature selection or\\nalgorithm, there are still plenty of pitfalls. In particular:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.651,213.027,431.997,223.527 '• Does  the  new  training  data  look  as  expected?  Automated  validation  of  the  new\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,200.427,323.847,210.927 'data through predefined metrics and checks is essential.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,183.827,240.987,194.327 '• Is the data complete and consistent?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,167.227,431.997,177.727 '• Are the distributions of features broadly similar to those in the previous training\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.001,154.627,400.581,165.127 'set? Remember that the goal is to refine the model, not radically change it.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,81.627,432.004,142.527 'With a new model version built, the next step is to compare the metrics with the cur‐\\nrent live model version. Doing so requires evaluating both models on the same devel‐\\nopment  dataset,  whether  it  be  the  previous  or  latest  version.  If  metrics  and  checks\\nsuggest a wide variation between the models, automated scripts should not be rede‐\\nployed, and manual intervention should be sought.\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.003,40.500,80.769,49.500 '32 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.237,40.500,189.948,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Even in the “simple” automated retraining scenario with new training data, there is a\\nneed  for  multiple  development  datasets  based  on  scoring  data  reconciliation  (with\\nground truth when it becomes available), data cleaning and validation, the previous\\nmodel version, and a set of carefully considered checks. Retraining in other scenarios\\nis likely to be even more complicated, rendering automated redeployment unlikely.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,475.636,432.001,536.536 'As  an  example,  consider  retraining  motivated  by  the  detection  of  significant  input\\ndrift. How can the model be improved? If new training data is available, then retrain‐\\ning with this data is the action with the highest cost-benefit ratio, and it may suffice.\\nHowever,  in  environments  where  it’s  slow  to  obtain  the  ground  truth,  there  may  be\\nlittle new labeled data.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,368.836,432.004,467.536 'This case requires direct invention from data scientists who need to understand the\\ncause  of  the  drift  and  work  out  how  the  existing  training  data  could  be  adjusted  to\\nmore  accurately  reflect  the  latest  input  data.  Evaluating  a  model  generated  by  such\\nchanges is difficult. The data scientist has to spend time assessing the situation—time\\nthat  increases  with  the  amount  of  modeling  debt—as  well  as  estimate  the  potential\\nimpact  on  performance  and  design  custom  mitigation  measures.  For  example,\\nremoving a specific feature or sampling the existing rows of training data may lead to\\na better-tuned model.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,285.627,432.004,357.267 'The Feedback Loop\\nIn large enterprises, DevOps best practices typically dictate that the live model scor‐\\ning environment and the model retraining environment are distinct. As a result, the\\nevaluation of a new model version on the retraining environment is likely to be com‐\\npromised.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,191.427,432.004,277.527 'One approach to mitigating this uncertainty is shadow testing, where the new model\\nversion  is  deployed  into  the  live  environment  alongside  the  existing  model.  All  live\\nscoring  is  handled  by  the  incumbent  model  version,  but  each  new  request  is  then\\nscored again by the new model version and the results logged, but not returned to the\\nrequestor. Once sufficient requests have been scored by both versions, the results can\\nbe  compared  statistically.  Shadow  scoring  also  gives  more  visibility  to  the  SMEs  on\\nthe future versions of the model and may thus allow for a smoother transition.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,135.027,432.001,183.327 'In the advertisement generation model previously discussed, it is impossible to tell if\\nthe  ads  selected  by  the  model  are  good  or  bad  without  allowing  the  end  user  the\\nchance to click on them. In this use case, shadow testing has limited benefits, and A/B\\ntesting is more common.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,91.227,432.003,126.927 'In  A/B  testing,  both  models  are  deployed  into  the  live  environment,  but  input\\nrequests  are  split  between  the  two  models.  Each  request  is  processed  by  one  or  the\\nother model, not both. Results from the two models are logged for analysis (but never\\n'>\n",
      "<LTTextBoxHorizontal(7) 336.960,40.500,406.188,49.500 'Iteration and Life Cycle \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.766,40.500,417.078,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.656,40.500,432.000,49.500 '33\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'for the same request). Drawing statistically meaningful conclusions from an A/B test\\nrequires careful planning of the test.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.004,574.337 'Chapter 7 will cover the how-to of A/B testing in more detail, but as a preview, the\\nsimplest form of A/B testing is often referred to as a fixed-horizon test. That’s because\\nin the search for a statistically meaningful conclusion, one has to wait until the care‐\\nfully  predetermined  number  of  samples  have  been  tested.  “Peeking”  at  the  result\\nbefore the test is finished is unreliable. However, if the test is running live in a com‐\\nmercial environment, every bad prediction is likely to cost money, so not being able\\nto stop a test early could be expensive.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,406.636,432.005,480.136 'Bayesian,  and  in  particular  multi-armed  bandit,  tests  are  an  increasingly  popular\\nalternative  to  the  “frequentist”  fixed-horizon  test,  with  the  aim  of  drawing  conclu‐\\nsions more quickly. Multi-armed bandit testing is adaptive: the algorithm that decides\\nthe split between models adapts according to live results and reduces the workload of\\nunderperforming models. While multi-armed bandit testing is more complex, it can\\nreduce the business cost of sending traffic to a poorly performing model.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.253,295.130,423.754,386.625 'Iterating on the Edge\\nIterating on an ML model deployed to millions of devices, such as smartphones, sen‐\\nsors, or cars, presents different challenges to iteration in a corporate IT environment.\\nOne  approach  is  to  relay  all  the  feedback  from  the  millions  of  model  instances  to  a\\ncentral point and perform training centrally. Tesla’s autopilot system, running in more\\nthan 500,000 cars, does exactly this. Full retraining of their 50 or so neural networks\\ntakes 70,000 GPU hours.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.253,205.130,423.753,287.130 'Google  has  taken  a  different  approach  with  its  smartphone  keyboard  software,\\nGBoard.  Instead  of  centralized  retraining,  every  smartphone  retrains  the  model\\nlocally and sends a summary of the improvements it has found to Google centrally.\\nThese improvements from every device are averaged and the shared model updated. \\nThis  federated  learning  approach  means  that  an  individual  user’s  personal  data\\ndoesn’t need to be collected centrally, the improved model on each phone can be used\\nimmediately, and the overall power consumption goes down. \\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,106.086,432.003,181.547 'Governance\\nGovernance is the set of controls placed on a business to ensure that it delivers on its\\nresponsibilities  to  all  stakeholders,  from  shareholders  and  employees  to  the  public\\nand national governments. These responsibilities include financial, legal, and ethical\\nobligations. Underpinning all three of these is the fundamental principle of fairness.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,74.886,432.004,97.986 'Legal obligations are the easiest to understand. Businesses were constrained by regu‐\\nlations long before the advent of machine learning. Many regulations target specific\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.004,40.500,80.770,49.500 '34 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.238,40.500,189.949,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,396.375,432.000,396.375>\n",
      "<LTLine 431.875,193.750,431.875,396.500>\n",
      "<LTLine 72.000,193.875,432.000,193.875>\n",
      "<LTLine 72.125,193.750,72.125,396.500>\n",
      "<LTTextBoxHorizontal(0) 71.995,544.636,432.003,605.537 'industries;  for  example,  financial  regulations  aim  to  protect  the  public  and  wider\\neconomy  from  finance  mismanagement  and  fraud,  while  pharmaceutical  industries\\nmust  comply  with  rules  to  safeguard  the  health  of  the  public.  Business  practice  is\\nimpacted by broader legislation to protect vulnerable sectors of society and to ensure\\na level playing field on criteria such as sex, race, age, or religion.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,450.436,432.004,536.536 'Recently, governments across the world have imposed regulations to protect the pub‐\\nlic from the impact of the use of personal data by businesses.  The 2016 EU General\\nData Protection Regulation (GDPR) and the 2018 California Consumer Privacy Act\\n(CCPA) typify this trend, and their impact on ML—with its total dependency on data\\n—has  been  immense.  For  example,  GDPR  attempts  to  protect  personal  data  from\\nindustrial  misuse  with  a  goal  of  limiting  the  potential  discrimination  against\\nindividuals.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.247,362.930,423.753,430.425 'GDPR Principles\\nThe GDPR sets out principles for the processing of personal data, and it’s worth not‐\\ning that the CCPA was built to closely mirror its principles, though it does have some\\nsignificant differences.2 Processing includes the collection, storage, alteration, and use\\nof personal data. These principles are:\\n'>\n",
      "<LTTextBoxHorizontal(3) 89.353,340.930,252.030,350.930 '• Lawfulness, fairness, and transparency\\n'>\n",
      "<LTTextBoxHorizontal(4) 89.350,324.930,173.060,334.930 '• Purpose limitation\\n'>\n",
      "<LTTextBoxHorizontal(5) 89.350,308.930,174.100,318.930 '• Data minimization\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.350,292.930,135.590,302.930 '• Accuracy\\n'>\n",
      "<LTTextBoxHorizontal(7) 89.350,276.930,170.070,286.930 '• Storage limitation\\n'>\n",
      "<LTTextBoxHorizontal(8) 89.350,260.930,251.530,270.930 '• Integrity and confidentiality (security)\\n'>\n",
      "<LTTextBoxHorizontal(9) 89.350,244.930,157.100,254.930 '• Accountability\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,137.486,432.005,223.586 'Governments are now starting to turn their regulatory eye to ML specifically, hoping\\nto  mitigate  the  negative  impact  of  its  use.  The  European  Union  is  leading  the  way\\nwith planned legislation to define the acceptable uses of various forms of AI. This is\\nnot necessarily about reducing use; for example, it may enable beneficial applications\\nof facial recognition technology that are currently restricted by data privacy regula‐\\ntions. But what is clear is that businesses will have to take heed of yet more regulation\\nwhen applying ML.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.996,106.286,432.003,129.386 'Do businesses care about moral responsibilities to society, beyond formal legislation?\\nIncreasingly, the answer is yes, as seen in the current development of environmental,\\n'>\n",
      "<LTTextBoxHorizontal(12) 73.140,71.954,250.648,79.954 '2 Delve into the differences between GDPR and CCPA.\\n'>\n",
      "<LTTextBoxHorizontal(13) 369.867,40.500,406.191,49.500 'Governance \\n'>\n",
      "<LTTextBoxHorizontal(14) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 424.659,40.500,432.003,49.500 '35\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,440.175,432.000,440.175>\n",
      "<LTLine 431.875,235.550,431.875,440.300>\n",
      "<LTLine 72.000,235.675,432.000,235.675>\n",
      "<LTLine 72.125,235.550,72.125,440.300>\n",
      "<LTLine 72.000,87.900,162.000,87.900>\n",
      "<LTTextBoxHorizontal(0) 71.995,544.636,432.004,605.537 'social,  and  governance  (ESG)  performance  indicators.  Trust  matters  to  consumers,\\nand a lack of trust is bad for business. With increasing public activism on the subject,\\nbusinesses  are  engaging  with  ideas  of  Responsible  AI,  the  ethical,  transparent,  and\\naccountable application of AI technology. Trust matters to shareholders, too, and full\\ndisclosure of ML risks is on its way.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,500.836,432.003,536.536 'Applying good governance to MLOPs is challenging. The processes are complex, the\\ntechnology is opaque, and the dependence on data is fundamental. Governance ini‐\\ntiatives in MLOps broadly fall into one of two categories:\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,481.236,141.223,491.736 'Data governance\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.004,468.636,374.113,479.136 'A framework for ensuring appropriate use and management of data\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,449.036,149.886,459.536 'Process governance\\n'>\n",
      "<LTTextBoxHorizontal(5) 89.998,411.236,432.004,446.936 'The  use  of  well-defined  processes  to  ensure  all  governance  considerations  have\\nbeen addressed at the correct point in the life cycle of the model and that a full\\nand accurate record has been kept\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,353.227,432.002,399.667 'Data Governance\\nData  governance  concerns  itself  with  the  data  being  used,  especially  that  for  model\\ntraining, and it can address questions like:\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.654,330.627,218.076,341.127 '• What is the data’s provenance?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,314.027,365.139,324.527 '• How was the original data collected and under what terms of use?\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,297.427,237.091,307.927 '• Is the data accurate and up to date?\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.655,280.827,374.295,291.327 '• Is there PII or other forms of sensitive data that should not be used?\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.996,170.027,432.005,268.727 'ML projects usually involve significant pipelines, consisting of data cleaning, combi‐\\nnation,  and  transformation  steps.  Understanding  the  data  lineage  is  complex,  espe‐\\ncially  at  the  feature  level,  but  it  is  essential  for  compliance  with  GDPR-style\\nregulations. How can teams—and more broadly organizations, as it matters at the top\\nas well—be sure that no PII is used to train a given model? Anonymizing or pseudo-\\nanonymizing data is not always a sufficient solution to managing personal informa‐\\ntion.  If  these  processes  are  not  performed  correctly,  it  can  still  be  possible  to  single\\nout an individual and their data, contrary to the requirements of GDPR.3\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.997,126.227,432.003,161.927 'Inappropriate biases in models can arise quite accidentally despite the best intentions\\nof data scientists. An ML recruitment model famously discriminated against women\\nby identifying that certain schools—all-female schools—were less well represented in\\n'>\n",
      "<LTTextBoxHorizontal(13) 73.140,81.954,425.104,89.954 '3 For more on anonymization, pseudo-anonymization, and why they don’t solve all data privacy woes, we rec‐\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.000,71.954,303.088,79.954 'ommend Executing Data Privacy-Compliant Data Projects by Dataiku.\\n'>\n",
      "<LTTextBoxHorizontal(15) 71.997,40.500,80.763,49.500 '36 \\n'>\n",
      "<LTTextBoxHorizontal(16) 88.341,40.500,91.653,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(17) 99.231,40.500,189.942,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,97.900,162.000,97.900>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.003,605.537 'the company’s upper management, which reflected the historical dominance of men\\nin  the  organization.4  The  point  is  that  making  predictions  based  on  experience  is  a\\npowerful  technique,  but  sometimes  the  consequences  are  not  only  counter-\\nproductive, but illegal.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,526.036,431.997,549.136 'Data  governance  tools  that  can  address  these  problems  are  in  their  infancy.  Most\\nfocus on answering these two questions about data lineage:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.659,503.437,431.997,513.937 '• Where did the information in this dataset come from, and what does this tell me\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,490.837,186.413,501.337 'about how I can use it?\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,474.237,431.997,484.737 '• How is this dataset used, and if I change it in some way, what are the implications\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,461.637,146.439,472.137 'downstream?\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,401.236,432.004,449.536 'Neither question is easy to answer fully and accurately in real-world data preparation\\npipelines. For example, if a data scientist writes a Python function to in-memory pro‐\\ncess several input datasets and output a single dataset, how can one be sure from what\\ninformation each cell of the new dataset was derived?\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,330.627,432.005,389.667 'Process Governance\\nProcess governance focuses on formalizing the steps in the MLOps process and asso‐\\nciating actions with them. Typically these actions are reviews, sign-offs, and the cap‐\\nture of supporting materials, such as documentation. The aim is twofold:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.657,282.827,431.997,318.527 '• To ensure every governance-related consideration is made at the correct time and\\ncorrectly acted upon. For example, models should not be deployed to production\\nuntil all validation checks have been passed.\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.660,241.027,432.002,276.727 '• To  enable  oversight  from  outside  of  the  strict  MLOps  process.  Auditors,  risk\\nmanagers, compliance officers, and the business as a whole all have an interest in\\nbeing able to track progress and review decisions at a later stage.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.999,218.427,306.926,228.927 'Effective implementation of process governance is hard:\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.654,158.027,432.001,206.327 '• Formal  processes  for  the  ML  life  cycle  are  rarely  easy  to  define  accurately.  The\\nunderstanding of the complete process is usually spread across the many teams\\ninvolved,  often  with  no  one  person  having  a  detailed  understanding  of  it  as  a\\nwhole.\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.658,141.427,431.997,151.927 '• For the process to be applied successfully, every team must be willing to adopt it\\n'>\n",
      "<LTTextBoxHorizontal(13) 90.001,128.827,156.162,139.327 'wholeheartedly.\\n'>\n",
      "<LTTextBoxHorizontal(14) 73.140,71.954,375.544,79.954 '4 In 2018, Amazon famously scrapped an AI recruiting tool because of its bias against women.\\n'>\n",
      "<LTTextBoxHorizontal(15) 369.867,40.500,406.191,49.500 'Governance \\n'>\n",
      "<LTTextBoxHorizontal(16) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(17) 424.659,40.500,432.003,49.500 '37\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,87.900,162.000,87.900>\n",
      "<LTTextBoxHorizontal(0) 80.655,595.037,431.997,605.537 '• If  the  process  is  just  too  heavy-weight  for  some  use-cases,  teams  will  certainly\\n'>\n",
      "<LTTextBoxHorizontal(1) 90.002,582.437,284.409,592.937 'subvert it, and much of the benefit will be lost.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,509.436,432.005,570.337 'Today,  process  governance  is  most  commonly  found  in  organizations  with  a  tradi‐\\ntionally  heavy  burden  of  regulation  and  compliance,  such  as  finance.  Outside  of\\nthese,  it  is  rare.  With  ML  creeping  into  all  spheres  of  commercial  activity  and  with\\nrising  concern  about  Responsible  AI,  we  will  need  new  and  innovative  solutions  to\\nthis problem that can work for all businesses. \\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,409.036,432.004,497.097 'Closing Thoughts\\nGiven  this  overview  of  features  required  for  and  processes  affected  by  MLOps,  it’s\\nclearly not something data teams—or even the data-driven organization at large—can\\nignore.  Nor  is  it  an  item  to  check  off  of  a  list  (“yes,  we  do  MLOps!”),  but  rather  a\\ncomplex  interplay  between  technologies,  processes,  and  people  that  requires  disci‐\\npline and time to do correctly.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,365.236,431.998,400.936 'The following chapters go deeper into each of the ML model life cycle components at\\nplay in MLOps, providing a look at how each should be done to get closer to the ideal\\nMLOps implementation.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.004,40.500,80.770,49.500 '38 \\n'>\n",
      "<LTTextBoxHorizontal(6) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 99.238,40.500,189.949,49.500 'Chapter 3: Key MLOps Features\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 316.995,527.418,431.991,581.880 'PART II\\nMLOps: How\\n'>\n",
      "<LTLine 72.000,558.814,432.000,558.814>\n",
      "<LTTextBoxHorizontal(0) 273.970,533.502,431.998,582.331 'CHAPTER 4\\nDeveloping Models\\n'>\n",
      "<LTTextBoxHorizontal(1) 336.513,436.501,431.998,449.501 'Adrien Lavoillotte\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,347.468,432.003,408.369 'Anyone who wants to be serious about MLOps needs to have at least a cursory under‐\\nstanding  of  the  model  development  process,  which  is  presented  in  Figure  4-1  as  an\\nelement  of  the  larger  ML  project  life  cycle.  Depending  on  the  situation,  the  model\\ndevelopment process can range from quite simple to extremely complex, and it dic‐\\ntates the constraints of subsequent usage, monitoring, and maintenance of models.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,183.838,416.733,206.938 'Figure 4-1. Model development highlighted in the larger context of the ML project life\\ncycle\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,134.038,432.004,169.738 'The implications of the data collection process on the rest of the model’s life is quite\\nstraightforward, and one easily sees how a model can become stale. For other parts of\\nthe model, the effects may be less obvious.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,77.638,432.003,125.938 'For  example,  take  feature  creation,  where  feeding  a  date  to  the  model  versus  a  flag\\nindicating whether the day is a public holiday may make a big difference in perfor‐\\nmance, but also comes with significantly different constraints on updating the model.\\nOr consider how the metrics used for evaluating and comparing models may enable\\n'>\n",
      "<LTTextBoxHorizontal(6) 424.659,40.500,432.003,49.500 '41\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTLine 72.000,341.207,432.500,341.207>\n",
      "<LTLine 432.375,212.502,432.375,341.332>\n",
      "<LTLine 72.000,212.627,432.500,212.627>\n",
      "<LTLine 72.125,212.502,72.125,341.332>\n",
      "<LTFigure(I1) 82.449,218.482,422.051,336.082 matrix=[339.60,0.00,0.00,117.60, (82.45,218.48)]>\n",
      "<LTTextBoxHorizontal(0) 71.996,582.437,431.999,605.537 'automatic switching to the best possible version down the line, should the situation\\nrequire it.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,538.636,432.003,574.337 'This  chapter  therefore  covers  the  basics  of  model  development,  specifically  in  the\\ncontext  of  MLOps,  that  is,  how  models  might  be  built  and  developed  in  ways  that\\nmake MLOps considerations easier to implement down the line.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.998,450.836,432.005,526.297 'What Is a Machine Learning Model?\\nMachine learning models are leveraged both in academia and in the real world (i.e.,\\nbusiness contexts), so it’s important to distinguish what they represent in theory ver‐\\nsus how they are implemented in practice. Let’s dive into both, building on what we’ve\\nalready seen in Chapter 3.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,342.427,432.004,439.266 'In Theory\\nA machine learning model is a projection of reality; that is, it’s a partial and approxi‐\\nmate  representation  of  some  aspect  (or  aspects)  of  a  real  thing  or  process.  Which\\naspects  are  represented  often  depends  on  what  is  available  and  useful.  A  machine\\nlearning model, once trained, boils down a mathematical formula that yields a result\\nwhen fed some inputs—say, a probability estimation of some event happening or the\\nestimated value of a raw number.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,260.827,432.004,334.327 'Machine learning models are based on statistical theory, and machine learning algo‐\\nrithms are the tools that build models from training data. Their goal is to find a syn‐\\nthetic representation of the data they are fed, and this data represents the world as it\\nwas  at  the  time  of  collection.  Therefore,  machine  learning  models  can  be  used  to\\nmake predictions when the future looks like the past, because their synthetic repre‐\\nsentation is still valid.\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.251,161.320,423.752,240.815 'Generalization Capacity\\nMachine  learning  models’  ability  to  accurately  predict  for  cases  that  are  not  exactly\\nlike the input data is called their generalization capacity. Even when they yield outputs\\nlike horses with zebra stripes1 that do not exist in training datasets, they do it by mod‐\\neling a probability distribution that allows them to have this kind of surprising gener‐\\nalization capacity.\\n'>\n",
      "<LTTextBoxHorizontal(6) 73.140,81.954,423.976,89.954 '1 CycleGAN is the implementation of recent research by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.000,71.954,107.888,79.954 'A. Efros.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.998,40.500,80.764,49.500 '42 \\n'>\n",
      "<LTTextBoxHorizontal(9) 88.342,40.500,91.654,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 99.232,40.500,187.423,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,250.566,432.000,250.566>\n",
      "<LTLine 431.875,149.941,431.875,250.691>\n",
      "<LTLine 72.000,150.066,432.000,150.066>\n",
      "<LTLine 72.125,149.941,72.125,250.691>\n",
      "<LTLine 72.000,97.900,162.000,97.900>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.996,494.236,432.005,605.537 'An often-used example for how machine learning models can predict and generalize\\nis the price of a house. Of course, the selling price of a house will depend on too many\\nfactors too complex to model precisely, but getting close enough to be useful is not so\\ndifficult. The input data for that model may be things inherent to the house like sur‐\\nface  area,  number  of  bedrooms  and  bathrooms,  year  of  construction,  location,  etc.,\\nbut also other more contextual information like the state of the housing market at the\\ntime of sale, whether the seller is in a hurry, and so on. With complete enough histor‐\\nical data, and provided the market conditions do not change too much, an algorithm\\ncan compute a formula that provides a reasonable estimate.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,437.836,432.005,486.136 'Another  frequent  example  is  a  health  diagnosis  or  prediction  that  someone  will\\ndevelop a certain disease within a given timeframe. This kind of classification model\\noften  outputs  the  probability  of  some  event,  sometimes  also  with  a  confidence\\ninterval.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,367.227,432.005,426.267 'In Practice\\nA model is the set of parameters necessary to rebuild and apply the formula. It is usu‐\\nally  stateless  and  deterministic  (i.e.,  the  same  inputs  always  give  the  same  outputs,\\nwith some exceptions; see “Online Learning” on page 88).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,285.627,432.003,359.127 'This  includes  the  parameters  of  the  end  formula  itself,  but  it  also  includes  all  the\\ntransformations  to  go  from  the  input  data  that  will  be  fed  to  the  model  to  the  end\\nformula that will yield a value plus the possible derived data (like a classification or a\\ndecision).  Given  this  description  in  practice,  it  usually  does  not  make  a  difference\\nwhether the model is ML-based or not: it is just a computable mathematical function\\napplied to the input data, one row at a time.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,191.427,432.005,277.527 'In the house price case, for instance, it may not be practical to gather enough pricing\\ndata for every zip code to get a model that’s accurate enough in all target locations.\\nInstead,  maybe  the  zip  codes  will  be  replaced  with  some  derived  inputs  that  are\\ndeemed  to  have  the  most  influence  on  price—say,  average  income,  population  den‐\\nsity,  or  proximity  to  some  amenities.  But  since  end  users  will  continue  to  input  the\\nzip code and not these derived inputs, for all intents and purposes, all of this transfor‐\\nmation is also part of the pricing model.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,84.627,432.005,183.327 'Outputs  can  also  be  richer  than  a  single  number.  A  system  that  detects  fraud,  for\\nexample, will often provide some kind of probability (and in some cases maybe also a\\nconfidence interval) rather than a binary answer. Depending on the acceptability of\\nfraud and the cost of subsequent verification or denial of the transaction, it may be set\\nup  to  only  classify  fraudulent  instances  where  the  probability  reaches  some  fine-\\ntuned threshold. Some models even include recommendations or decisions, such as\\nwhich product to show a visitor to maximize spending or which treatment provides\\nthe most probable recovery.\\n'>\n",
      "<LTTextBoxHorizontal(6) 300.299,40.500,406.184,49.500 'What Is a Machine Learning Model? \\n'>\n",
      "<LTTextBoxHorizontal(7) 413.762,40.500,417.074,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 424.652,40.500,431.996,49.500 '43\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.637,432.005,605.537 'All of these transformations and associated data are part of the model to some degree;\\nhowever, this does not mean they are always bundled in a monolithic package, as one\\nsingle  artifact  compiled  together.  This  could  quickly  get  unwieldy,  and,  in  addition,\\nsome parts of this information come with varying constraints (different refresh rates,\\nexternal sources, etc.).\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.002,486.628,432.005,533.067 'Required Components\\nBuilding  a  machine  learning  model  requires  many  components  as  outlined  in\\nTable 4-1.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.003,462.121,308.823,472.121 'Table 4-1. Required components of a machine learning model\\n'>\n",
      "<LTTextBoxHorizontal(3) 75.597,433.291,120.012,455.091 'ML component\\nTraining data\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.597,377.041,116.538,396.841 'A performance\\nmetric\\n'>\n",
      "<LTTextBoxHorizontal(5) 75.597,342.391,113.244,351.391 'ML algorithm\\n'>\n",
      "<LTTextBoxHorizontal(6) 75.597,296.941,123.585,305.941 'Hyperparameters\\n'>\n",
      "<LTTextBoxHorizontal(7) 138.144,232.141,426.774,455.091 'Description\\nTraining data is usually labeled for the prediction case with examples of what is being modeled\\n(supervised learning). It might sound obvious, but it’s important to have good training data. An\\nillustrative example of when this was not the case was data from damaged planes during World War II,\\nwhich suffered from survivor bias and therefore was not good training data.\\nA performance metric is what the model being developed seeks to optimize. It should be chosen\\ncarefully to avoid unintended consequences, like the cobra effect (named for a famous anecdote, where\\na reward for dead cobras led some to breed them). For example, if 95% of the data has class A,\\noptimizing for raw accuracy may produce a model that always predicts A and is 95% accurate.\\nThere are a variety of models that work in various ways and have different pros and cons. It is\\nimportant to note that some algorithms are more suited to certain tasks than others, but their selection\\nalso depends on what needs to be prioritized: performance, stability, interpretability, computation cost,\\netc.\\nHyperparameters are configurations for ML algorithms. The algorithm contains the basic formula, the\\nparameters it learns are the operations and operands that make up this formula for that particular\\nprediction task, and the hyperparameters are the ways that the algorithm may go to find these\\nparameters.\\nFor example, in a decision tree (where data continues to be split in two according to what looks to be\\nthe best predictor in the subset that reached this path), one hyperparameter is the depth of the tree\\n(i.e., the number of splits).\\n'>\n",
      "<LTTextBoxHorizontal(8) 75.600,219.091,421.414,228.091 'Evaluation dataset When using labeled data, an evaluation dataset that is different from the training set will be required\\n'>\n",
      "<LTTextBoxHorizontal(9) 138.148,208.291,377.989,217.291 'to evaluate how the model performs on unseen data (i.e., how well it can generalize).\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.997,155.977,432.000,191.677 'The sheer number and complexity of each individual component is part of what can\\nmake good MLOps a challenging undertaking. But the complexity doesn’t stop here,\\nas algorithm choice is yet another piece of the puzzle.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.998,40.500,80.764,49.500 '44 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.342,40.500,91.654,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.232,40.500,187.423,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,444.191,134.548,457.991>\n",
      "<LTRect 134.548,444.191,432.000,457.991>\n",
      "<LTLine 72.000,398.866,134.648,398.866>\n",
      "<LTLine 134.448,398.866,432.000,398.866>\n",
      "<LTLine 72.000,353.416,134.648,353.416>\n",
      "<LTLine 134.448,353.416,432.000,353.416>\n",
      "<LTLine 72.000,307.966,134.648,307.966>\n",
      "<LTLine 134.448,307.966,432.000,307.966>\n",
      "<LTLine 72.000,230.116,134.648,230.116>\n",
      "<LTLine 134.448,230.116,432.000,230.116>\n",
      "<LTLine 72.000,206.266,134.648,206.266>\n",
      "<LTLine 134.448,206.266,432.000,206.266>\n",
      "<LTTextBoxHorizontal(0) 71.997,536.255,432.004,607.894 'Different ML Algorithms, Different MLOps Challenges\\nWhat ML algorithms all have in common is that they model patterns in past data to\\nmake inferences, and the quality and relevance of this experience are the key factors\\nin  their  effectiveness.  Where  they  differ  is  that  each  style  of  algorithm  has  specific\\ncharacteristics and presents different challenges in MLOps (outlined in Table 4-2).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,511.749,270.646,521.749 'Table 4-2. MLOps considerations by algorithm type\\n'>\n",
      "<LTTextBoxHorizontal(2) 75.598,472.118,105.694,504.719 'Algorithm\\ntype\\nLinear\\n'>\n",
      "<LTTextBoxHorizontal(3) 75.600,424.418,106.389,433.418 'Tree-based\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.600,342.068,114.471,351.068 'Deep learning\\n'>\n",
      "<LTTextBoxHorizontal(5) 123.864,495.719,141.288,504.719 'Name\\n'>\n",
      "<LTTextBoxHorizontal(6) 174.490,495.719,239.065,504.719 'MLOps considerations\\n'>\n",
      "<LTTextBoxHorizontal(7) 123.865,424.418,159.631,481.118 'Linear\\nregression\\nLogistic\\nregression\\nDecision tree\\n'>\n",
      "<LTTextBoxHorizontal(8) 123.865,400.568,164.689,409.568 'Random forest\\n'>\n",
      "<LTTextBoxHorizontal(9) 123.865,331.268,149.632,374.918 'Gradient\\nboosting\\nNeural\\nnetworks\\n'>\n",
      "<LTTextBoxHorizontal(10) 174.491,472.118,270.521,481.118 'There is a tendency for overfitting.\\n'>\n",
      "<LTTextBoxHorizontal(11) 174.491,448.269,270.521,457.269 'There is a tendency for overfitting.\\n'>\n",
      "<LTTextBoxHorizontal(12) 174.491,309.668,427.526,433.418 'Can be unstable—small changes in data can lead to a large change in the structure of the\\noptimal decision tree.\\nPredictions can be difficult to understand, which is challenging from a Responsible AI\\nperspective. Random forest models can also be relatively slow to output predictions, which\\ncan present challenges for applications.\\nLike random forest, predictions can be difficult to understand. Also, a small change in the\\nfeature or training set can create radical changes in the model.\\nIn terms of interpretability, deep learning models are almost impossible to understand.\\nDeep learning algorithms, including neural networks, are also extremely slow to train and\\nrequire a lot of power (and data). Is it worth the resources, or would a simpler model work\\njust as well?\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.995,194.355,432.004,293.055 'Some ML algorithms can best support specific use cases, but governance considera‐\\ntions may also play a part in the choice of algorithm. In particular, highly regulated\\nenvironments where decisions must be explained (e.g., financial services) cannot use\\nopaque algorithms such as neural networks; rather, they have to favor simpler techni‐\\nques, such as decision trees. In many use cases, it’s not so much a trade-off on perfor‐\\nmance but rather a trade-off on cost. That is, simpler techniques usually require more\\ncostly  manual  feature  engineering  to  reach  the  same  level  of  performance  as  more\\ncomplex techniques.\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.248,106.848,423.748,174.343 'Computing Power\\nWhen talking about components of machine learning model development, it’s impos‐\\nsible to ignore computing power. Some say planes fly thanks to human ingenuity, but\\nit’s also thanks to a lot of fuel. This holds true with machine learning as well: its devel‐\\nopment is inversely proportional to the cost of computing power.\\n'>\n",
      "<LTTextBoxHorizontal(15) 80.248,76.848,423.748,98.848 'From hand-computed linear regression of the early twentieth century to today’s larg‐\\nest deep learning models, new algorithms arose when the required computing power\\n'>\n",
      "<LTTextBoxHorizontal(16) 300.304,40.500,406.189,49.500 'What Is a Machine Learning Model? \\n'>\n",
      "<LTTextBoxHorizontal(17) 413.767,40.500,417.079,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(18) 424.657,40.500,432.001,49.500 '45\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,483.018,120.265,507.618>\n",
      "<LTRect 120.265,483.018,170.891,507.618>\n",
      "<LTRect 170.891,483.018,432.000,507.618>\n",
      "<LTLine 72.000,459.293,120.365,459.293>\n",
      "<LTLine 120.165,459.293,170.991,459.293>\n",
      "<LTLine 170.791,459.293,432.000,459.293>\n",
      "<LTLine 72.000,435.443,120.365,435.443>\n",
      "<LTLine 120.165,435.443,170.991,435.443>\n",
      "<LTLine 170.791,435.443,432.000,435.443>\n",
      "<LTLine 72.000,353.093,120.265,353.093>\n",
      "<LTLine 120.265,411.593,170.991,411.593>\n",
      "<LTLine 170.791,411.593,432.000,411.593>\n",
      "<LTLine 120.265,376.943,170.991,376.943>\n",
      "<LTLine 170.791,376.943,432.000,376.943>\n",
      "<LTLine 120.165,353.093,170.991,353.093>\n",
      "<LTLine 170.791,353.093,432.000,353.093>\n",
      "<LTLine 72.000,307.643,120.365,307.643>\n",
      "<LTLine 120.165,307.643,170.991,307.643>\n",
      "<LTLine 170.791,307.643,432.000,307.643>\n",
      "<LTLine 72.000,184.093,432.000,184.093>\n",
      "<LTLine 431.875,68.650,431.875,184.218>\n",
      "<LTLine 72.125,68.650,72.125,184.218>\n",
      "<LTTextLineHorizontal 75.600,448.269,77.013,457.269 ' \\n'>\n",
      "<LTTextBoxHorizontal(0) 80.250,583.630,423.750,605.630 'became available. For example, mainstream algorithms like random forest and gradi‐\\nent boosting both require a computing power that was expensive 20 years ago.\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.250,505.630,423.750,575.630 'In exchange, they brought an ease of use that considerably lowered the cost of devel‐\\noping ML models, thus putting new use cases within the reach of the average organi‐\\nzation. The decrease in the cost of data also helped, but it was not the first driver: very\\nfew algorithms leverage big data technology in which both data and computation are\\ndistributed over a large number of computers; rather, most of them still operate with\\nall the training data in memory.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.993,393.986,432.003,482.047 'Data Exploration\\nWhen data scientists or analysts consider data sources to train a model, they need to\\nfirst  get  a  grasp  of  what  that  data  looks  like.  Even  a  model  trained  using  the  most\\neffective  algorithm  is  only  as  good  as  its  training  data.  At  this  stage,  a  number  of\\nissues can prevent all or part of the data from being useful, including incompleteness,\\ninaccuracy, inconsistency, etc.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.005,375.386,240.372,385.886 'Examples of such processes can include:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,352.786,431.997,363.286 '• Documenting  how  the  data  was  collected  and  what  assumptions  were  already\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.001,340.186,113.217,350.686 'made\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.659,298.386,432.002,334.086 '• Looking  at  summarizing  statistics  of  the  data:  What  is  the  domain  of  each  col‐\\numn? Are there some rows with missing values? Obvious mistakes? Strange out‐\\nliers? No outliers at all?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.654,281.786,298.632,292.286 '• Taking a closer look at the distribution of the data\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,265.186,343.488,275.686 '• Cleaning, filling, reshaping, filtering, clipping, sampling, etc.\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,248.586,431.997,259.086 '• Checking correlations between the different columns, running statistical tests on\\n'>\n",
      "<LTTextBoxHorizontal(10) 90.002,235.986,292.274,246.486 'some subpopulations, fitting distribution curves\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.660,219.386,431.997,229.886 '• Comparing that data to other data or models in the literature: Is there some usual\\n'>\n",
      "<LTTextBoxHorizontal(12) 90.002,206.786,401.747,217.286 'information that seems to be missing? Is this data comparably distributed?\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.996,121.186,432.005,194.686 'Of  course,  domain  knowledge  is  required  to  make  informed  decisions  during  this\\nexploration. Some oddities may be hard to spot without specific insight, and assump‐\\ntions made can have consequences that are not obvious to the untrained eye. Indus‐\\ntrial  sensor  data  is  a  good  example:  unless  the  data  scientist  is  also  a  mechanical\\nengineer or expert in the equipment, they might not know what constitutes normal\\nversus strange outliers for a particular machine.\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.004,40.500,80.770,49.500 '46 \\n'>\n",
      "<LTTextBoxHorizontal(15) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 99.238,40.500,187.429,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 431.875,494.250,431.875,607.500>\n",
      "<LTLine 72.000,494.375,432.000,494.375>\n",
      "<LTLine 72.125,494.250,72.125,607.500>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.995,545.112,432.003,607.973 'Feature Engineering and Selection\\nFeatures are how data is presented to a model, serving to inform that model on things\\nit  may  not  infer  by  itself.  This  table  provides  examples  of  how  features  may  be\\nengineered:\\n'>\n",
      "<LTTextBoxHorizontal(1) 75.599,447.026,111.752,529.576 'Feature\\nengineering\\ncategory\\nDerivatives\\nEnrichment\\nEncoding\\nCombination\\n'>\n",
      "<LTTextBoxHorizontal(2) 137.766,520.576,171.273,529.576 'Description\\n'>\n",
      "<LTTextBoxHorizontal(3) 137.767,436.226,421.645,495.176 'Infer new information from existing information—e.g., what day of the week is this date?\\nAdd new external information—e.g., is this day a public holiday?\\nPresent the same information differently—e.g., day of the week or weekday versus weekend.\\nLink features together—e.g., the size of the backlog might need to be weighted by the complexity of\\nthe different items in it.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,358.712,432.005,419.612 'For instance, in trying to estimate the potential duration of a business process given\\nthe current backlog, if one of the inputs is a date, it is pretty common to derive the\\ncorresponding day of the week or how far ahead the next public holiday is from that\\ndate.  If  the  business  serves  multiple  locations  that  observe  different  business  calen‐\\ndars, that information may also be important.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,302.312,432.003,350.612 'Another example, to follow up on the house pricing scenario from the previous sec‐\\ntion, would be using average income and population density, which ideally allows the\\nmodel to better generalize and train on more diverse data than trying to segment by\\narea (i.e., zip code).\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,231.703,432.002,290.743 'Feature Engineering Techniques\\nA whole market exists for such complementary data that extends far beyond the open\\ndata  that  public  institutions  and  companies  share.  Some  services  provide  direct\\nenrichment that can save a lot of time and effort.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,162.703,432.002,223.603 'There are, however, many cases when information that data scientists need for their\\nmodels is not available. In this case, there are techniques like impact coding, whereby\\ndata scientists replace a modality by the average value of the target for that modality,\\nthus allowing the model to benefit from data in a similar range (at the cost of some\\ninformation loss).\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,118.903,432.004,154.603 'Ultimately, most ML algorithms require a table of numbers as input, each row repre‐\\nsenting a sample, and all samples coming from the same dataset. When the input data\\nis not tabular, data scientists can use other tricks to transform it.\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.001,87.703,432.004,110.803 'The most common one is one-hot encoding. For example, a feature that can take three\\nvalues (e.g., Raspberry, Blueberry, and Strawberry) is transformed into three features\\n'>\n",
      "<LTTextBoxHorizontal(10) 303.115,40.500,406.192,49.500 'Feature Engineering and Selection \\n'>\n",
      "<LTTextBoxHorizontal(11) 413.770,40.500,417.082,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 424.660,40.500,432.004,49.500 '47\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,497.076,134.167,532.476>\n",
      "<LTRect 134.167,497.076,432.000,532.476>\n",
      "<LTLine 72.000,484.151,134.267,484.151>\n",
      "<LTLine 134.067,484.151,432.000,484.151>\n",
      "<LTLine 72.000,471.101,134.267,471.101>\n",
      "<LTLine 134.067,471.101,432.000,471.101>\n",
      "<LTLine 72.000,458.051,134.267,458.051>\n",
      "<LTLine 134.067,458.051,432.000,458.051>\n",
      "<LTLine 72.000,434.201,134.267,434.201>\n",
      "<LTLine 134.067,434.201,432.000,434.201>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'that  can  take  only  two  values—yes  or  no  (e.g.,  Raspberry  yes/no,  Blueberry  yes/no,\\nStrawberry yes/no).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,513.436,432.005,574.337 'Text  or  image  inputs,  on  the  other  hand,  require  more  complex  engineering.  Deep\\nlearning  has  recently  revolutionized  this  field  by  providing  models  that  transform\\nimages and text into tables of numbers that are usable by ML algorithms. These tables\\nare  called  embeddings,  and  they  allow  data  scientists  to  perform  transfer  learning\\nbecause they can be used in domains on which they were not trained.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.247,425.930,423.747,493.425 'Transfer Learning\\nTransfer  learning  is  the  technique  of  using  information  gained  from  solving  one\\nproblem in solving a different problem. Transfer learning can be used to significantly\\naccelerate learning of second or subsequent tasks, and it is very popular in deep learn‐\\ning, where the resources needed to train models can be enormous.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.247,371.930,423.747,417.930 'For example, even if a particular deep learning model was trained on images that did\\nnot contain any forks, it may give a useful embedding to be used by a model that is\\ntrained  to  detect  them,  because  a  fork  is  an  object,  and  that  model  was  trained  to\\ndetect similar human-made objects.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.994,264.877,432.005,349.117 'How Feature Selection Impacts MLOps Strategy\\nWhen it comes to feature creation and selection, the question of how much and when\\nto  stop  comes  up  regularly.  Adding  more  features  may  produce  a  more  accurate\\nmodel, achieve more fairness when splitting into more precise groups, or compensate\\nfor some other useful missing information. However, it also comes with downsides,\\nall of which can have a significant impact on MLOps strategies down the line:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.657,242.277,351.594,252.777 '• The model can become more and more expensive to compute.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,225.677,396.492,236.177 '• More features require more inputs and more maintenance down the line.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,209.077,271.857,219.577 '• More features mean a loss of some stability.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,192.477,328.442,202.977 '• The sheer number of features can raise privacy concerns.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.997,119.477,432.005,180.377 'Automated  feature  selection  can  help  by  using  heuristics  to  estimate  how  critical\\nsome features will be for the predictive performance of the model. For instance, one\\ncan look at the correlation with the target variable or quickly train a simple model on\\na representative subset of the data and then look at which features are the strongest\\npredictors.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,75.677,432.003,111.377 'Which inputs to use, how to encode them, how they interact or interfere with each\\nother—such  decisions  require  a  certain  understanding  of  the  inner  workings  of  the\\nML algorithm. The good news is that some of this can be partly automated, e.g., by\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.003,40.500,80.769,49.500 '48 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.237,40.500,187.428,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,503.175,432.000,503.175>\n",
      "<LTLine 431.875,360.550,431.875,503.300>\n",
      "<LTLine 72.000,360.675,432.000,360.675>\n",
      "<LTLine 72.125,360.550,72.125,503.300>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'using tools such as Auto-sklearn or AutoML applications that cross-reference features\\nagainst  a  given  target  to  estimate  which  features,  derivatives,  or  combinations  are\\nlikely  to  yield  the  best  results,  leaving  out  all  the  features  that  would  probably  not\\nmake that much of a difference.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,450.436,432.005,549.136 'Other  choices  still  require  human  intervention,  such  as  deciding  whether  to  try  to\\ncollect additional information that might improve the model. Spending time to build\\nbusiness-friendly  features  will  often  improve  the  final  performance  and  ease  the\\nadoption  by  end  users,  as  model  explanations  are  likely  to  be  simpler.  It  can  also\\nreduce  modeling  debt,  allowing  data  scientists  to  understand  the  main  prediction\\ndrivers  and  ensure  that  they  are  robust.  Of  course,  there  are  trade-offs  to  consider\\nbetween  the  cost  of  time  spent  to  understand  the  model  and  the  expected  value,  as\\nwell as risks associated with the model’s use.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.248,350.930,423.748,430.425 'Feature Stores\\nFeature  factories,  or  feature  stores,  are  repositories  of  different  features  associated\\nwith business entities that are created and stored in a central location for easier reuse.\\nThey usually combine an offline part (slower, but potentially more powerful) and an\\nonline part (quicker and more useful for real-time needs), making sure they remain\\nconsistent with each other.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.248,284.930,423.748,342.930 'Given  how  time-consuming  feature  engineering  is  for  data  scientists,  feature  stores\\nhave huge potential to free up their time for even more valuable tasks. Machine learn‐\\ning  is  still  often  the  “high-interest  credit  card  of  technical  debt”.  Reversing  this  will\\nrequire huge efficiency gains in the data-to-model-to-production process and in the\\nMLOps process, and feature stores are one way to get there.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,225.886,431.998,261.586 'The bottom line is that when building models, the process of engineering and select‐\\ning  features,  like  many  other  ML  model  components,  is  a  delicate  balance  between\\nconsidering MLOps components and performance.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.991,125.486,432.005,213.547 'Experimentation\\nExperimentation takes place throughout the entire model development process, and\\nusually every important decision or assumption comes with at least some experiment\\nor previous research to justify it. Experimentation can take many shapes, from build‐\\ning full-fledged predictive ML models to doing statistical tests or charting data. Goals\\nof experimentation include:\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.659,77.686,432.003,113.386 '• Assessing  how  useful  or  how  good  of  a  model  can  be  built  given  the  elements\\noutlined in Table 4-1. (The next section will cover model evaluation and compar‐\\nison in more detail.)\\n'>\n",
      "<LTTextBoxHorizontal(7) 354.893,40.500,406.184,49.500 'Experimentation \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.762,40.500,417.074,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.652,40.500,431.996,49.500 '49\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,440.175,432.000,440.175>\n",
      "<LTLine 431.875,273.550,431.875,440.300>\n",
      "<LTLine 72.000,273.675,432.000,273.675>\n",
      "<LTLine 72.125,273.550,72.125,440.300>\n",
      "<LTTextBoxHorizontal(0) 80.655,595.037,431.997,605.537 '• Finding the best modeling parameters (algorithms, hyperparameters, feature pre‐\\n'>\n",
      "<LTTextBoxHorizontal(1) 90.002,582.437,160.142,592.937 'processing, etc.).\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.660,565.837,431.804,576.337 '• Tuning the bias/variance trade-off for a given training cost to fit that definition of\\n'>\n",
      "<LTTextBoxHorizontal(3) 89.997,553.236,116.184,563.736 '“best.”\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,524.037,431.997,547.137 '• Finding  a  balance  between  model  improvement  and  improved  computation\\ncosts. (Since there’s always room for improvement, how good is good enough?)\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.246,438.530,423.747,494.025 'Bias and Variance\\nA high bias model (also known as underfitting) fails to discover some of the rules that\\ncould  have  been  learned  from  the  training  data,  possibly  because  of  reductive\\nassumptions making the model overly simplistic.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.246,396.530,423.747,430.530 'A  high  variance  model  (or  overfitting)  sees  patterns  in  noise  and  seeks  to  predict\\nevery  single  variation,  resulting  in  a  complex  model  that  does  not  generalize  well\\nbeyond its training data.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.997,312.286,432.004,373.186 'When experimenting, data scientists need to be able to quickly iterate through all the\\npossibilities for each of the model building blocks outlined in Table 4-1. Fortunately,\\nthere are tools to do all of this semiautomatically, where you only need to define what\\nshould  be  tested  (the  space  of  possibilities)  depending  on  prior  knowledge  (what\\nmakes sense) and the constraints (e.g., computation, budget).\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,243.286,432.003,304.186 'Some tools allow for even more automation, for instance by offering stratified model\\ntraining. For example, say the business wants to predict customer demand for prod‐\\nucts to optimize inventory, but behavior varies a lot from one store to the next. Strati‐\\nfied modeling consists of training one model per store that can be better optimized\\nfor each store rather than a model that tries to predict in all stores.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.997,186.886,432.000,235.186 'Trying  all  combinations  of  every  possible  hyperparameter,  feature  handling,  etc.,\\nquickly becomes untraceable. Therefore, it is useful to define a time and/or computa‐\\ntion budget for experiments as well as an acceptability threshold for usefulness of the\\nmodel (more on that notion in the next section).\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.995,105.286,432.004,178.786 'Notably, all or part of this process may need to be repeated every time anything in the\\nsituation  changes  (including  whenever  the  data  and/or  problem  constraints  change\\nsignificantly; see “Drift Detection in Practice” on page 92). Ultimately, this means that\\nall experiments that informed the final decisions data scientists made to arrive at the\\nmodel as well as all the assumptions and conclusions along the way may need to be\\nrerun and reexamined.\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.000,74.086,432.003,97.186 'Fortunately,  more  and  more  data  science  and  machine  learning  platforms  allow  for\\nautomating these workflows not only on the first run, but also to preserve all the pro‐\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.000,40.500,80.766,49.500 '50 \\n'>\n",
      "<LTTextBoxHorizontal(13) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(14) 99.234,40.500,187.425,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,503.775,432.000,503.775>\n",
      "<LTLine 431.875,385.150,431.875,503.900>\n",
      "<LTLine 72.000,385.275,432.000,385.275>\n",
      "<LTLine 72.125,385.150,72.125,503.900>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'cessing operations for repeatability. Some also allow for the use of version control and\\nexperimental branch spin-off to test theories and then merge, discard, or keep them\\n(see “Version Management and Reproducibility” on page 56).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.998,456.836,432.005,557.497 'Evaluating and Comparing Models\\nGeorge E. P. Box, a twentieth century British statistician, once said that all models are\\nwrong, but some are useful. In other words, a model should not aim to be perfect, but\\nit  should  pass  the  bar  of  “good  enough  to  be  useful”  while  keeping  an  eye  on  the\\nuncanny valley—typically a model that looks like it’s doing a good job but does a bad\\n(or  catastrophic)  job  for  a  specific  subset  of  cases  (say,  an  underrepresented\\npopulation).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,400.436,432.004,448.736 'With this in mind, it’s important to evaluate a model in context and have some ability\\nto compare it to what existed before the model—whether a previous model or a rules-\\nbased process—to get an idea of what the outcome would be if the current model or\\ndecision process were replaced by the new one.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.998,344.036,432.005,392.336 'A  model  with  an  absolute  performance  that  could  technically  be  considered  disap‐\\npointing  can  still  possibly  enhance  an  existing  situation.  For  instance,  having  a\\nslightly more accurate forecast of demand for a certain product or service may result\\nin huge cost-savings.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,262.436,432.004,335.936 'Conversely,  a  model  that  gets  a  perfect  score  is  suspicious,  as  most  problems  have\\nnoise in the data that’s at least somewhat hard to predict. A perfect or nearly-perfect\\nscore may be a sign that there is a leak in the data (i.e., that the target to be predicted\\nis also in the input data or that an input feature is very correlated to the target but,\\npractically,  available  only  once  the  target  is  known)  or  that  the  model  overfits  the\\ntraining data and will not generalize well.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,128.827,432.005,250.866 'Choosing Evaluation Metrics\\nChoosing the proper metric by which to evaluate and compare different models for a\\ngiven problem can lead to very different models (think of the cobra effect mentioned\\nin Table 4-1). A simple example: accuracy is often used for automated classification\\nproblems but is rarely the best fit when the classes are unbalanced (i.e., when one of\\nthe outcomes is very unlikely compared to the other). In a binary classification prob‐\\nlem where the positive class (i.e., the one that is interesting to predict because its pre‐\\ndiction  triggers  an  action)  is  rare,  say  5%  of  occurrences,  a  model  that  constantly\\npredicts the negative class is therefore 95% accurate, while also utterly useless.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,72.427,432.002,120.727 'Unfortunately, there is no one-size-fits-all metric. You need to pick one that matches\\nthe  problem  at  hand,  which  means  understanding  the  limits  and  trade-offs  of  the\\nmetric (the mathematics side) and their impact on the optimization of the model (the\\nbusiness side).\\n'>\n",
      "<LTTextBoxHorizontal(7) 303.679,40.500,406.189,49.500 'Evaluating and Comparing Models \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.767,40.500,417.079,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.657,40.500,432.001,49.500 '51\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.996,506.837,432.005,605.537 'To get an idea of how well a model will generalize, that metric should be evaluated on\\na  part  of  the  data  that  was  not  used  for  the  model’s  training  (a  holdout  dataset),  a\\nmethod called cross-testing. There can be multiple steps where some data is held for\\nevaluation and the rest is used for training or optimizing, such as metric evaluation or\\nhyperparameter  optimization.  There  are  different  strategies  as  well,  not  necessarily\\njust  a  simple  split.  In  k-fold  cross-validation,  for  example,  data  scientists  rotate  the\\nparts that they hold out to evaluate and train multiple times. This multiplies the train‐\\ning time but gives an idea of the stability of the metric.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,425.236,432.005,498.737 'With a simple split, the holdout dataset can consist of the most recent records instead\\nof randomly chosen ones. Indeed, as models are usually used for future predictions, it\\nis  likely  that  assessing  them  as  if  they  were  used  for  prediction  on  the  most  recent\\ndata leads to more realistic estimations. In addition, it allows one to assess whether\\nthe data drifted between the training and the holdout dataset (see “Drift Detection in\\nPractice” on page 92 for more details).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,343.636,432.003,417.136 'As  an  example,  Figure  4-2  shows  a  scheme  in  which  a  test  dataset  is  a  holdout  (in\\ngray) in order to perform the evaluation. The remaining data is split into three parts\\nto find the best hyperparameter combination by training the model three times with a\\ngiven  combination  on  each  of  the  blue  datasets,  and  validating  its  performance  on\\ntheir respective green datasets. The gray dataset is used only once with the best hyper‐\\nparameter combination, while the other datasets are used with all of them.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,225.006,315.768,235.506 'Figure 4-2. An example of dataset split for model evaluation\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,124.806,432.005,210.906 'Oftentimes,  data  scientists  want  to  periodically  retrain  models  with  the  same  algo‐\\nrithms, hyperparameters, features, etc., but on more recent data. Naturally, the next\\nstep  is  to  compare  the  two  models  and  see  how  the  new  version  fares.  But  it’s  also\\nimportant  to  make  sure  all  previous  assumptions  still  hold:  that  the  problem  hasn’t\\nfundamentally  shifted,  that  the  modeling  choices  made  previously  still  fit  the  data,\\netc.  This  is  more  specifically  part  of  performance  and  drift  monitoring  (find  more\\ndetails on this in Chapter 7).\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.003,40.500,80.769,49.500 '52 \\n'>\n",
      "<LTTextBoxHorizontal(6) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 99.237,40.500,187.428,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,337.375,432.500,337.375>\n",
      "<LTLine 432.375,241.070,432.375,337.500>\n",
      "<LTLine 72.000,241.195,432.500,241.195>\n",
      "<LTLine 72.125,241.070,72.125,337.500>\n",
      "<LTFigure(I1) 114.010,247.050,390.490,332.250 matrix=[276.48,0.00,0.00,85.20, (114.01,247.05)]>\n",
      "<LTTextBoxHorizontal(0) 71.995,498.455,432.004,607.894 'Cross-Checking Model Behavior\\nBeyond  the  raw  metrics,  when  evaluating  a  model,  it’s  critical  to  understand  how  it\\nwill behave. Depending on the impact of the model’s predictions, decisions, or classi‐\\nfications, a more or less deep understanding may be required. For example, data sci‐\\nentists  should  take  reasonable  steps  (with  respect  to  that  impact)  to  ensure  that  the\\nmodel is not actively harmful: a model that would predict that all patients need to be\\nchecked by a doctor may score high in terms of raw prevention, but not so much on\\nrealistic resource allocation.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.998,479.855,253.858,490.355 'Examples of these reasonable steps include:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.653,457.255,431.997,467.755 '• Cross-checking different metrics (and not only the ones on which the model was\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.001,444.655,170.852,455.155 'initially optimized)\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.659,402.855,432.002,438.555 '• Checking how the model reacts to different inputs—e.g., plot the average predic‐\\ntion (or probability for classification models) for different values of some inputs\\nand see whether there are oddities or extreme variability\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.654,361.055,432.002,396.755 '• Splitting  one  particular  dimension  and  checking  the  difference  in  behavior  and\\nmetrics across different subpopulations—e.g., is the error rate the same for males\\nand females?\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,262.855,432.005,348.955 'These kinds of global analyses should not be understood as causality, just as correla‐\\ntion. They do not necessarily imply a specific causal relationship between some vari‐\\nables  and  an  outcome;  they  merely  show  how  the  model  sees  that  relationship.  In\\nother words, the model should be used with care for what-if analysis. If one feature\\nvalue is changed, the model prediction is likely to be wrong if the new feature value\\nhas never been seen in the training dataset or if it has never been seen in combination\\nwith the values of the other features in this dataset.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,181.255,432.003,254.755 'When comparing models, those different aspects should be accessible to data scien‐\\ntists, who need to be able to go deeper than a single metric. That means the full envi‐\\nronment  (interactive  tooling,  data,  etc.)  needs  to  be  available  for  all  models,  ideally\\nallowing for comparison from all angles and between all components. For example,\\nfor  drift,  the  comparison  might  use  the  same  settings  but  different  data,  while  for\\nmodeling performance, it might use the same data but different settings.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.997,72.846,432.004,169.685 'Impact of Responsible AI on Modeling\\nDepending on the situation (and sometimes depending on the industry or sector of\\nthe  business),  on  top  of  a  general  understanding  of  model  behavior,  data  scientists\\nmay also need models’ individual predictions to be explainable, including having an\\nidea of what specific features pushed the prediction one way or the other. Sometimes\\npredictions may be very different for a specific record than on average. Popular meth‐\\nods to compute individual prediction explanations include Shapley value (the average\\n'>\n",
      "<LTTextBoxHorizontal(9) 303.678,40.500,406.188,49.500 'Evaluating and Comparing Models \\n'>\n",
      "<LTTextBoxHorizontal(10) 413.766,40.500,417.078,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 424.656,40.500,432.000,49.500 '53\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'marginal contribution of a feature value across all possible coalitions) and individual\\nconditional  expectation  (ICE)  computations,  which  show  the  dependence  between\\nthe target functions and features of interest.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,488.236,432.004,561.736 'For example, the measured level of a specific hormone could generally push a model\\nto predict someone has a health issue, but for a pregnant woman, that level makes the\\nmodel  infer  she  is  at  no  such  risk.  Some  legal  frameworks  mandate  some  kind  of\\nexplainability for decisions made by a model that have consequences on humans, like\\nrecommending a loan to be denied. “Element 2: Bias” on page 114 discusses this topic\\nin detail.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,368.836,432.005,480.136 'Note  that  the  notion  of  explainability  has  several  dimensions.  In  particular,  deep\\nlearning networks are sometimes called black-box models because of their complexity\\n(though when reading the model coefficients, a model is fully specified, and it is usu‐\\nally a conceptually remarkably simple formula, but a very large formula that becomes\\nimpossible to intuitively apprehend). Conversely, global and local explanation tools—\\nsuch as partial dependence plots or Shapley value computations—give some insights\\nbut  arguably  do  not  make  the  model  intuitive.  To  actually  communicate  a  rigorous\\nand  intuitive  understanding  of  what  exactly  the  model  is  doing,  limiting  the  model\\ncomplexity is required.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,287.236,432.003,360.736 'Fairness  requirements  can  also  have  dimensioning  constraints  on  model  develop‐\\nment. Consider this theoretical example to better understand what is at stake when it\\ncomes to bias: a US-based organization regularly hires people who do the same types\\nof  jobs.  Data  scientists  could  train  a  model  to  predict  the  workers’  performance\\naccording  to  various  characteristics,  and  people  would  then  be  hired  based  on  the\\nprobability that they would be high-performing workers.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,230.836,432.005,279.136 'Though this seems like a simple problem, unfortunately, it’s fraught with pitfalls. To\\nmake  this  problem  completely  hypothetical  and  to  detach  it  from  the  complexities\\nand problems of the real world, let’s say everyone in the working population belongs\\nto one of two groups: Weequay or Togruta.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,174.436,432.003,222.736 'For  this  hypothetical  example,  let’s  claim  that  a  far  larger  population  of  Weequay\\nattend  university.  Off  the  bat,  there  would  be  an  initial  bias  in  favor  of  Weequay\\n(amplified by the fact they would have been able to develop their skills through years\\nof experience).\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,130.636,432.005,166.336 'As a result, there would not only be more Weequay than Togruta in the pool of appli‐\\ncants, but Weequay applicants would tend to be more qualified. The employer has to\\nhire 10 people during the month to come. What should it do?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.657,70.236,432.001,118.536 '• As an equal opportunity employer, it should ensure the fairness of its recruitment\\nprocess  as  it  controls  it.  That  means  in  mathematical  terms,  for  each  applicant\\nand all things being equal, being hired (or not) should not depend on their group\\naffiliation (in this case, Weequay or Togruta). However, this results in bias in and\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.000,40.500,80.766,49.500 '54 \\n'>\n",
      "<LTTextBoxHorizontal(9) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 99.234,40.500,187.425,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 89.996,569.837,432.002,605.537 'of itself, as Weequay are more qualified. Note that “all things being equal” can be\\ninterpreted in various ways, but the usual interpretation is that the organization\\nis likely not considered accountable for processes it does not control.\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.654,465.036,432.005,563.737 '• The  employer  may  also  have  to  avoid  disparate  impact,  that  is,  practices  in\\nemployment that adversely affect one group of people of a protected characteris‐\\ntic  more  than  another.  Disparate  impact  is  assessed  on  subpopulations  and  not\\non individuals; practically, it assesses whether proportionally speaking, the com‐\\npany has hired as many Weequay as Togruta. Once again, the target proportions\\nmay be those of the applicants or those of the general population, though the for‐\\nmer  is  more  likely,  as  again,  the  organization  can’t  be  accountable  for  biases  in\\nprocesses out of its control.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,404.636,432.005,452.936 'The two objectives are mutually exclusive. In this scenario, equal opportunity would\\nlead to hiring 60% (or more) Weequay and 40% (or fewer) Togruta. As a result, the\\nprocess  has  a  disparate  impact  on  the  two  populations,  because  the  hiring  rates  are\\ndifferent.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,348.236,432.002,396.536 'Conversely,  if  the  process  is  corrected  so  that  40%  of  people  hired  are  Togruta  to\\navoid  disparate  impact,  it  means  that  some  rejected  Weequay  applicants  will  have\\nbeen predicted as more qualified than some accepted Togruta applicants (contradict‐\\ning the equal opportunity assertion).\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,291.836,432.003,340.136 'There needs to be a trade-off—the law sometimes referred to as the 80% rule. In this\\nexample, it would mean that the hiring rate of Togruta should be equal to or larger\\nthan 80% of the hiring rate of Weequay. In this example, it means that it would be OK\\nto hire up to 65% Weequay.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.999,248.036,432.005,283.736 'The  point  here  is  that  defining  these  objectives  cannot  be  a  decision  made  by  data\\nscientists  alone.  But  even  once  the  objectives  are  defined,  the  implementation  itself\\nmay also be problematic:\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.654,124.636,432.005,235.936 '• Without any indications, data scientists naturally try to build equal opportunity\\nmodels because they correspond to models of the world as it is. Most of the tools\\ndata scientists employ also try to achieve this because it is the most mathemati‐\\ncally  sound  option.  Yet  some  ways  to  achieve  this  goal  may  be  unlawful.  For\\nexample,  the  data  scientist  may  choose  to  implement  two  independent  models:\\none for Weequay and one for Togruta. This could be a reasonable way to address\\nthe biases induced by a training dataset in which Weequay are overrepresented,\\nbut  it  would  induce  a  disparate  treatment  of  the  two  that  could  be  considered\\ndiscriminatory.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,70.236,432.002,118.536 '• To let data scientists use their tools in the way they were designed (i.e., to model\\nthe world as it is), they may decide to post-process the predictions so that they fit\\nwith  the  organization’s  vision  of  the  world  as  it  should  be.  The  simplest  way  of\\ndoing this is to choose a higher threshold for Weequay than for Togruta. The gap\\n'>\n",
      "<LTTextBoxHorizontal(8) 303.678,40.500,406.188,49.500 'Evaluating and Comparing Models \\n'>\n",
      "<LTTextBoxHorizontal(9) 413.766,40.500,417.078,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 424.656,40.500,432.000,49.500 '55\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 89.996,569.837,432.002,605.537 'between  them  will  adjust  the  trade-off  between  “equal  opportunity”  and  “equal\\nimpact”; however, it may still be considered discriminatory because of the dispa‐\\nrate treatment.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,496.836,432.002,557.736 'Data  scientists  are  unlikely  to  be  able  to  sort  this  problem  out  alone  (see  “Key  Ele‐\\nments of Responsible AI” on page 113 for a broader view on the subject). This simple\\nexample illustrates the complexity of the subject, which may be even more complex\\ngiven that there may be many protected attributes, and the fact that bias is as much a\\nbusiness question as a technical question.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,440.436,432.004,488.736 'Consequently, the solution heavily depends on the context. For instance, this example\\nof Weequay and Togruta is representative of processes that give access to privileges.\\nThe situation is different if the process has negative impacts on the user (like fraud\\nprediction that leads to transaction rejection) or is neutral (like disease prediction).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.994,340.036,432.004,428.097 'Version Management and Reproducibility\\nDiscussing the evaluation and comparison of models (for fairness as discussed imme‐\\ndiately before, but also a host of other factors) necessarily brings up the idea of ver‐\\nsion control and the reproducibility of different model versions. With data scientists\\nbuilding, testing, and iterating on several versions of models, they need to be able to\\nkeep all the versions straight.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.003,321.436,365.058,331.936 'Version management and reproducibility address two different needs:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.658,235.836,432.005,309.336 '• During  the  experimentation  phase,  data  scientists  may  find  themselves  going\\nback  and  forth  on  different  decisions,  trying  out  different  combinations,  and\\nreverting  when  they  don’t  produce  the  desired  results.  That  means  having  the\\nability  to  go  back  to  different  “branches”  of  the  experiments—for  example,\\nrestoring a previous state of a project when the experimentation process led to a\\ndead end.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.657,194.036,432.001,229.736 '• Data scientists or others (auditors, managers, etc.) may need to be able to replay\\nthe computations that led to model deployment for an audit team several years\\nafter the experimentation was first done.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,108.436,432.003,181.936 'Versioning has arguably been somewhat solved when everything is code-based, with\\nsource version control technology. Modern data processing platforms typically offer\\nsimilar capabilities for data transformation pipelines, model configuration, etc. Merg‐\\ning several parts is, of course, less straightforward than merging code that diverged,\\nbut the basic need is to be able to go back to some specific experiment, if only to be\\nable to copy its settings to replicate them in another branch.\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.002,77.236,432.005,100.336 'Another very important property of a model is reproducibility. After a lot of experi‐\\nments and tweaking, data scientists may arrive at a model that fits the bill. But after\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.002,40.500,80.768,49.500 '56 \\n'>\n",
      "<LTTextBoxHorizontal(10) 88.346,40.500,91.658,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 99.236,40.500,187.427,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'that, operationalization necessitates model reproduction not only in another environ‐\\nment,  but  also  possibly  from  a  different  starting  point.  Repeatability  also  makes\\ndebugging much easier (sometimes even simply possible). To this end, all facets of the\\nmodel need to be documented and reusable, including:\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.002,537.636,124.281,548.136 'Assumptions\\n'>\n",
      "<LTTextBoxHorizontal(2) 89.997,499.836,432.003,535.536 'When  a  data  scientist  makes  decisions  and  assumptions  about  the  problem  at\\nhand, its scope, the data, etc., they should all be explicit and logged so that they\\ncan be checked against any new information down the line.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,480.236,123.555,490.736 'Randomness\\n'>\n",
      "<LTTextBoxHorizontal(4) 89.996,417.236,432.002,478.136 'A  lot  of  ML  algorithms  and  processes,  such  as  sampling,  make  use  of  pseudo-\\nrandom numbers. Being able to precisely reproduce an experiment, such as for\\ndebugging, means to have control over that pseudo-randomness, most often by\\ncontrolling  the  “seed”  of  the  generator  (i.e.,  the  same  generator  initialized  with\\nthe same seed would yield the same sequence of pseudo-random numbers).\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,397.636,92.682,408.136 'Data\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.997,347.236,432.005,395.536 'To  get  repeatability,  the  same  data  must  be  available.  This  can  sometimes  be\\ntricky  because  the  storage  capacity  required  to  version  data  can  be  prohibitive\\ndepending on the rate of update and quantity. Also, branching on data does not\\nyet have as rich an ecosystem of tools as branching on code.\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,327.636,103.616,338.136 'Settings\\n'>\n",
      "<LTTextBoxHorizontal(8) 89.997,302.436,431.997,325.536 'This one is a given: all processing that has been done must be reproducible with\\nthe same settings.\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.005,282.836,100.586,293.336 'Results\\n'>\n",
      "<LTTextBoxHorizontal(10) 89.996,232.436,432.002,280.736 'While developers use merging tools to compare and merge different text file ver‐\\nsions,  data  scientists  need  to  be  able  to  compare  in-depth  analysis  of  models\\n(from confusion matrices to partial dependence plots) to obtain models that sat‐\\nisfy the requirements.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,212.836,137.118,223.336 'Implementation\\n'>\n",
      "<LTTextBoxHorizontal(12) 89.996,112.036,432.005,210.736 'Ever-so-slightly  different  implementations  of  the  same  model  can  actually  yield\\ndifferent models, enough to change the predictions on some close calls. And the\\nmore  sophisticated  the  model,  the  higher  the  chances  that  these  discrepancies\\nhappen. On the other hand, scoring a dataset in bulk with a model comes with\\ndifferent  constraints  than  scoring  a  single  record  live  in  an  API,  so  different\\nimplementations  may  sometimes  be  warranted  for  the  same  model.  But  when\\ndebugging and comparing, data scientists need to keep the possible differences in\\nmind.\\n'>\n",
      "<LTTextBoxHorizontal(13) 282.005,40.500,406.187,49.500 'Version Management and Reproducibility \\n'>\n",
      "<LTTextBoxHorizontal(14) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 424.655,40.500,431.999,49.500 '57\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,595.037,125.434,605.537 'Environment\\n'>\n",
      "<LTTextBoxHorizontal(1) 89.996,481.636,432.005,592.937 'Given  all  the  steps  covered  in  this  chapter,  it’s  clear  that  a  model  is  not  just  its\\nalgorithm and parameters. From the data preparation to the scoring implementa‐\\ntion, including feature selection, feature encoding, enrichment, etc., the environ‐\\nment in which several of those steps run may be more or less implicitly tied to the\\nresults. For instance, a slightly different version of a Python package involved in\\none step may change the results in ways that can be hard to predict. Preferably,\\ndata scientists should make sure that the runtime environment is also repeatable.\\nGiven the pace at which ML is evolving, this might require techniques that freeze\\nthe computation environments.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,425.236,432.003,473.536 'Fortunately,  part  of  the  underlying  documentation  tasks  associated  with  versioning\\nand  reproducibility  can  be  automated,  and  the  use  of  an  integrated  platform  for\\ndesign  and  deployment  can  greatly  decrease  the  reproducibility  costs  by  ensuring\\nstructured information transfer.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,381.436,432.005,417.136 'Clearly, while maybe not the sexiest part of model development, version management\\nand  reproducibility  are  critical  to  building  machine  learning  efforts  in  real-world\\norganizational settings where governance—including audits—matters.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,281.036,432.004,369.097 'Closing Thoughts\\nModel  development  is  one  of  the  most  critical  and  consequential  steps  of  MLOps.\\nThe  many  technical  questions  that  are  necessarily  answered  during  this  phase  have\\nbig repercussions on all aspects of the MLOps process throughout the life of the mod‐\\nels.  Therefore,  exposure,  transparency,  and  collaboration  are  crucial  to  long-term\\nsuccess.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,224.636,432.001,272.936 'The model development stage is also the one that has been practiced the most by pro‐\\nfiles like data scientists and, in the pre-MLOps world, often represents the whole ML\\neffort,  yielding  a  model  that  will  then  be  used  as  is  (with  all  its  consequences  and\\nlimitations).\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,40.500,80.763,49.500 '58 \\n'>\n",
      "<LTTextBoxHorizontal(7) 88.341,40.500,91.653,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.231,40.500,187.422,49.500 'Chapter 4: Developing Models\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 226.960,533.502,431.998,582.331 'CHAPTER 5\\nPreparing for Production\\n'>\n",
      "<LTTextBoxHorizontal(1) 349.708,436.501,431.998,449.501 'Joachim Zentici\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,334.868,432.005,408.369 'Confirming that something works in the laboratory has never been a sure sign it will\\nwork well in the real world, and machine learning models are no different. Not only\\nis  the  production  environment  typically  very  different  from  the  development  envi‐\\nronment,  but  the  commercial  risks  associated  with  models  in  production  are  much\\ngreater.  It  is  important  that  the  complexities  of  the  transition  to  production  are\\nunderstood and tested and that the potential risks have been adequately mitigated.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.999,291.068,432.002,326.768 'This chapter explores the steps required to prepare for production (highlighted in the\\ncontext of the entire life cycle in Figure 5-1). The goal is to illustrate, by extension, the\\nelements that must be considered for robust MLOps systems.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.998,123.358,423.539,146.458 'Figure 5-1. Preparing for production highlighted in the larger context of the ML project\\nlife cycle\\n'>\n",
      "<LTTextBoxHorizontal(5) 424.660,40.500,432.004,49.500 '59\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTLine 72.000,284.807,432.500,284.807>\n",
      "<LTLine 432.375,152.022,432.375,284.932>\n",
      "<LTLine 72.000,152.147,432.500,152.147>\n",
      "<LTLine 72.125,152.022,72.125,284.932>\n",
      "<LTFigure(I1) 81.969,158.002,422.531,279.682 matrix=[340.56,0.00,0.00,121.68, (81.97,158.00)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,532.512,432.005,607.973 'Runtime Environments\\nThe first step in sending a model to production is making sure it’s technically possi‐\\nble. As discussed in Chapter 3, ideal MLOps systems favor rapid, automated deploy‐\\nment over labor-intensive processes, and runtime environments can have a big effect\\non which approach prevails.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.512,432.004,524.412 'Production  environments  take  a  wide  variety  of  forms:  custom-built  services,  data\\nscience  platforms,  dedicated  services  like  TensorFlow  Serving,  low-level  infrastruc‐\\nture like Kubernetes clusters, JVMs on embedded systems, etc. To make things even\\nmore complex, consider that in some organizations, multiple heterogeneous produc‐\\ntion environments coexist.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,394.512,432.002,455.412 'Ideally, models running in the development environment would be validated and sent\\nas is to production; this minimizes the amount of adaptation work and improves the\\nchances that the model in production will behave as it did in development. Unfortu‐\\nnately,  this  ideal  scenario  is  not  always  possible,  and  it’s  not  unheard  of  that  teams\\nfinish a long-term project only to realize it can’t be put in production.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,298.703,432.004,382.943 'Adaptation from Development to Production Environments\\nIn terms of adaptation work, on one end of the spectrum, the development and pro‐\\nduction platforms are from the same vendor or are otherwise interoperable, and the\\ndev model can run without any modification in production. In this case, the technical\\nsteps required to push the model into production are reduced to a few clicks or com‐\\nmands, and all efforts can be focused on validation.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,191.903,432.005,290.603 'On the other end of the spectrum, there are cases where the model needs to be reim‐\\nplemented  from  scratch—possibly  by  another  team,  and  possibly  in  another  pro‐\\ngramming language. Given the resources and time required, there are few cases today\\nwhere this approach makes sense. However, it’s still the reality in many organizations\\nand is often a consequence of the lack of appropriate tooling and processes. The real‐\\nity is that handing over a model for another team to reimplement and adapt for the\\nproduction  environment  means  that  model  won’t  reach  production  for  months\\n(maybe years), if at all.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,122.903,432.003,183.803 'Between  these  two  extreme  cases,  there  can  be  a  number  of  transformations  per‐\\nformed on the model or the interactions with its environment to make it compatible\\nwith  production.  In  all  cases,  it  is  crucial  to  perform  validation  in  an  environment\\nthat  mimics  production  as  closely  as  possible,  rather  than  in  the  development\\nenvironment.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.002,40.500,80.768,49.500 '60 \\n'>\n",
      "<LTTextBoxHorizontal(7) 88.346,40.500,91.658,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.236,40.500,204.203,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,596.229,157.879,607.789 'Tooling considerations\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,527.904,432.005,588.805 'The format required to send to production should be considered early, as it may have\\na large impact on the model itself and the quantity of work required to productional‐\\nize it. For example, when a model is developed using scikit-learn (Python) and pro‐\\nduction  is  a  Java-based  environment  that  expects  PMML  or  ONNX  as  input,\\nconversion is obviously required.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,458.905,432.003,519.804 'In this case, teams should set up tooling while developing the model, ideally before\\nthe first version of the model is finished or even started. Failure to create this pipeline\\nup  front  would  block  the  validation  process  (and,  of  course,  final  validation  should\\nnot  be  performed  on  the  scikit-learn  model,  as  it’s  not  the  one  that  will  be  put  into\\nproduction).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,434.537,178.232,446.097 'Performance considerations\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,366.212,432.005,427.112 'Another common reason conversion may be required is for performance. For exam‐\\nple, a Python model will typically have higher latency for scoring a single record than\\nits  equivalent  converted  to  C++.  The  resulting  model  will  likely  be  dozens  of  times\\nfaster  (although  obviously  it  depends  on  many  factors,  and  the  result  can  also  be  a\\nmodel that is dozens of times slower).\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,297.212,432.003,358.112 'Performance  also  comes  into  play  when  the  production  model  must  run  on  a  low-\\npower device. In the specific case of deep neural networks, for example, trained mod‐\\nels  can  become  extremely  large  with  billions  or  hundreds  of  billions  of  parameters.\\nRunning them on small devices is simply impossible, and running them on standard\\nservers can be slow and expensive.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,253.412,432.000,289.112 'For these models, an optimized runtime is not enough. To obtain better performance,\\nthe  model  definition  must  be  optimized.  One  solution  is  to  use  compression\\ntechniques:\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,205.612,432.002,241.312 '• With quantization, the model can be trained using 32-bit floating-point numbers\\nand used for inference at a lower precision so that the model requires less mem‐\\nory and is faster while accuracy is mostly preserved.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.658,163.812,432.002,199.512 '• With pruning, one simply removes weights (or even entire layers) from the neu‐\\nral  network.  This  is  a  rather  radical  approach,  but  some  methods  allow  for  the\\npreservation of accuracy.\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.654,122.012,432.002,157.712 '• With distillation, a smaller “student” network is trained to mimic a bigger, more\\npowerful  network.  Done  appropriately,  this  can  lead  to  better  models  (as  com‐\\npared to trying to train the smaller network directly from the data).\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,74.212,432.003,109.912 'These methods are efficient if the initial model is trained in a way that reduces infor‐\\nmation loss while performing them, so these operations are not simply conversions of\\nthe  trained  model  post  hoc,  but  rather  orient  the  way  the  model  is  trained.  These\\n'>\n",
      "<LTTextBoxHorizontal(11) 336.531,40.500,406.191,49.500 'Runtime Environments \\n'>\n",
      "<LTTextBoxHorizontal(12) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 424.659,40.500,432.003,49.500 '61\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,582.437,431.860,605.537 'methods are still very recent and quite advanced but already commonly used in natu‐\\nral language processing (NLP) pretrained models.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,486.627,432.005,570.867 'Data Access Before Validation and Launch to Production\\nAnother technical aspect that needs to be addressed before validation and launch to\\nproduction is data access. For example, a model evaluating apartment prices may use\\nthe average market price in a zip code area; however, the user or the system request‐\\ning the scoring will probably not provide this average and would most likely provide\\nsimply the zip code, meaning a lookup is necessary to fetch the value of the average.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,405.027,432.005,478.528 'In some cases, data can be frozen and bundled with the model. But when this is not\\npossible (e.g., if the dataset is too large or the enrichment data needs to always be up\\nto  date),  the  production  environment  should  access  a  database  and  thus  have  the\\nappropriate network connectivity, libraries, or drivers required to communicate with\\nthe data storage installed, and authentication credentials stored in some form of pro‐\\nduction configuration.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,336.027,432.004,396.927 'Managing this setup and configuration can be quite complex in practice since, again,\\nit requires appropriate tooling and collaboration (in particular to scale to more than a\\nfew dozen models). When using external data access, model validation in situations\\nthat closely match production is even more critical as technical connectivity is a com‐\\nmon source of production malfunction.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,227.618,432.005,324.458 'Final Thoughts on Runtime Environments\\nTraining a model is usually the most impressive computation, requiring a high level\\nof software sophistication, massive data volumes, and high-end machines with pow‐\\nerful GPUs. But in the whole life cycle of a model, there is a good chance that most of\\nthe compute is spent at inference time (even if this computation is orders of magni‐\\ntude simpler and faster). This is because a model is trained once and can be used bil‐\\nlions of times for inference.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,171.218,432.003,219.518 'Scaling  inference  on  complex  models  can  be  expensive  and  have  significant  energy\\nand  environmental  impact.  Lowering  the  complexity  of  models  or  compressing\\nextremely  complex  models  can  lower  the  infrastructure  cost  of  operating  machine\\nlearning models.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,89.618,432.005,163.118 'It’s important to remember that not all applications require deep learning, and in fact,\\nnot  all  applications  require  machine  learning  at  all.  A  valuable  practice  to  control\\ncomplexity in production is to develop complex models only to provide a baseline for\\nwhat  seems  achievable.  What  goes  into  production  can  then  be  a  much  simpler\\nmodel, with the advantages of lowering the operating risk, increasing computational\\nperformance, and lowering power consumption. If the simple model is close enough\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.003,40.500,80.769,49.500 '62 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.237,40.500,204.204,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'to  the  high  complexity  baseline,  then  it  can  be  a  much  more  desirable  solution  for\\nproduction.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,469.436,432.004,570.097 'Model Risk Evaluation\\nBefore  exploring  how  validation  should  be  done  in  an  ideal  MLOps  system,  it’s\\nimportant to consider the purpose of validation. As discussed in Chapter 4, models\\nattempt to mimic reality, but they are imperfect; their implementation can have bugs,\\nas  can  the  environment  they  are  executing  in.  The  indirect,  real-world  impact  a\\nmodel in production can have is never certain, and the malfunctioning of a seemingly\\ninsignificant cog can have tremendous consequences in a complex system.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,361.027,432.004,457.866 'The Purpose of Model Validation\\nIt is, to some extent, possible (not to mention absolutely necessary) to anticipate the\\nrisks of models in production and thus design and validate so as to minimize these\\nrisks. As organizations become more and more complex, it is essential to understand\\nthat involuntary malfunctions or malicious attacks are potentially threatening in most\\nuses  of  machine  learning  in  the  enterprise,  not  only  in  financial  or  safety-related\\napplications.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,329.827,432.003,352.927 'Before putting a model in production (and in fact constantly from the beginning of\\nthe machine learning project), teams should ask the uncomfortable questions:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,307.227,309.079,317.727 '• What if the model acts in the worst imaginable way?\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,290.627,431.997,301.127 '• What  if  a  user  manages  to  extract  the  training  data  or  the  internal  logic  of  the\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,278.027,120.599,288.527 'model?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,261.427,376.227,271.927 '• What are the financial, business, legal, safety, and reputational risks?\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.997,201.027,432.005,249.327 'For  high-risk  applications,  it  is  essential  that  the  whole  team  (and  in  particular  the\\nengineers in charge of validation) be fully aware of these risks so that they can design\\nthe  validation  process  appropriately  and  apply  the  strictness  and  complexity  appro‐\\npriate for the magnitude of the risks.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,132.027,432.003,192.927 'In  many  ways,  machine  learning  risk  management  covers  model  risk  management\\npractices that are well established in many industries, such as banking and insurance.\\nHowever, machine learning introduces new types of risks and liabilities, and as data\\nscience gets democratized, it involves many new organizations or teams that have no\\nexperience with more traditional model risk management.\\n'>\n",
      "<LTTextBoxHorizontal(10) 339.675,40.500,406.185,49.500 'Model Risk Evaluation \\n'>\n",
      "<LTTextBoxHorizontal(11) 413.763,40.500,417.075,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 424.653,40.500,431.997,49.500 '63\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.995,523.655,432.003,607.894 'The Origins of ML Model Risk\\nThe magnitude of risk ML models can bring is hard to model for mathematical rea‐\\nsons,  but  also  because  the  materialization  of  risks  arises  through  real-world  conse‐\\nquences. The ML metrics, and in particular the cost matrix, allow teams to evaluate\\nthe  average  cost  of  operating  a  model  in  its  “nominal”  case,  meaning  on  its  cross-\\nvalidation data, compared to operating a perfect magical model.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,467.255,432.004,515.555 'But while computing this expected cost can be very important, a wide range of things\\ncan  go  wrong  well  beyond  expected  cost.  In  some  applications,  the  risk  can  be  a\\nfinancially unbounded liability, a safety issue for individuals, or an existential threat\\nfor the organization. ML model risk originates essentially from:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.653,444.655,429.399,455.155 '• Bugs, errors in designing, training, or evaluating the model (including data prep)\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,428.055,431.997,438.555 '• Bugs in the runtime framework, bugs in the model post-processing/conversion,\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.002,415.455,353.279,425.955 'or hidden incompatibilities between the model and its runtime\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.660,398.855,206.736,409.355 '• Low quality of training data\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,382.255,337.555,392.755 '• High difference between production data and training data\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,365.655,431.997,376.155 '• Expected  error  rates,  but  with  failures  that  have  higher  consequences  than\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.002,353.055,127.035,363.555 'expected\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.660,336.455,320.955,346.955 '• Misuse of the model or misinterpretation of its outputs\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.655,319.855,169.493,330.355 '• Adversarial attacks\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.655,303.255,431.997,313.755 '• Legal  risk  originating  in  particular  from  copyright  infringement  or  liability  for\\n'>\n",
      "<LTTextBoxHorizontal(12) 90.002,290.655,163.071,301.155 'the model output\\n'>\n",
      "<LTTextBoxHorizontal(13) 80.660,274.055,376.279,284.555 '• Reputational risk due to bias, unethical use of machine learning, etc.\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.000,251.455,423.844,261.955 'The probability of materialization of the risk and its magnitude can be amplified by:\\n'>\n",
      "<LTTextBoxHorizontal(15) 80.655,228.855,187.353,239.355 '• Broad use of the model\\n'>\n",
      "<LTTextBoxHorizontal(16) 80.655,212.255,227.274,222.755 '• A rapidly changing environment\\n'>\n",
      "<LTTextBoxHorizontal(17) 80.655,195.655,250.731,206.155 '• Complex interactions between models\\n'>\n",
      "<LTTextBoxHorizontal(18) 71.997,147.855,432.000,183.555 'The  following  sections  provide  more  details  on  these  threats  and  how  to  mitigate\\nthem,  which  should  ultimately  be  the  goal  of  any  MLOps  system  the  organization\\nputs in place.\\n'>\n",
      "<LTTextBoxHorizontal(19) 71.996,72.654,431.999,135.515 'Quality Assurance for Machine Learning\\nSoftware engineering has developed a mature set of tools and methodologies for qual‐\\nity assurance (QA), but the equivalent for data and models is still in its infancy, which\\nmakes it challenging to incorporate into MLOps processes. The statistical methods as\\n'>\n",
      "<LTTextBoxHorizontal(20) 71.996,40.500,80.762,49.500 '64 \\n'>\n",
      "<LTTextBoxHorizontal(21) 88.340,40.500,91.652,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(22) 99.230,40.500,204.197,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'well as documentation best practices are well known, but implementing them at scale\\nis not common.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.436,432.005,574.337 'Though it’s being covered as a part of this chapter on preparing for production, to be\\nclear,  QA  for  machine  learning  does  not  occur  only  at  the  final  validation  stage;\\nrather, it should accompany all stages of model development. Its purpose is to ensure\\ncompliance  with  processes  as  well  as  ML  and  computational  performance  require‐\\nments, with a level of detail that is proportionate to the level of risk.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,419.236,432.005,505.336 'In the case where the people in charge of validation are not the ones who developed\\nthe  model,  it  is  essential  that  they  have  enough  training  in  machine  learning  and\\nunderstand the risks so that they can design appropriate validation or detect breaches\\nin  the  validation  proposed  by  the  development  team.  It  is  also  essential  that  the\\norganization’s  structure  and  culture  give  them  the  authority  to  appropriately  report\\nissues and contribute to continuous improvement or block passage to production if\\nthe level of risk justifies it.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,299.836,432.005,411.136 'Robust MLOps practices dictate that performing QA before sending to production is\\nnot  only  about  technical  validation.  It  is  also  the  occasion  to  create  documentation\\nand validate the model against organizational guidelines. In particular, this means the\\norigin  of  all  input  datasets,  pretrained  models,  or  other  assets  should  be  known,  as\\nthey could be subject to regulations or copyrights. For this reason (and for computer\\nsecurity  reasons  in  particular),  some  organizations  choose  to  allow  only  whitelisted\\ndependencies.  While  this  can  significantly  impact  the  ability  of  data  scientists  to\\ninnovate quickly, though the list of dependencies can be reported and checked partly\\nautomatically, it can also provide additional safety.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,212.036,432.004,287.496 'Key Testing Considerations\\nObviously, model testing will consist of applying the model to carefully curated data\\nand validating measurements against requirements. How the data is selected or gen‐\\nerated as well as how much data is required is crucial, but it will depend on the prob‐\\nlem tackled by the model.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,155.636,432.002,203.936 'There are some scenarios in which the test data should not always match “real-world”\\ndata. For example, it can be a good idea to prepare a certain number of scenarios, and\\nwhile some of them should match realistic situations, other data should be specifically\\ngenerated in ways that could be problematic (e.g., extreme values, missing values).\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,86.636,432.004,147.536 'Metrics must be collected on both statistical (accuracy, precision, recall, etc.) as well\\nas computational (average latency, 95th latency percentile, etc.) aspects, and the test\\nscenarios should fail if some assumptions on them are not verified. For example, the\\ntest  should  fail  if  the  accuracy  of  the  model  falls  below  90%,  the  average  inference\\ntime goes above 100 milliseconds, or more than 5% of inferences take more than 200\\n'>\n",
      "<LTTextBoxHorizontal(7) 325.435,40.500,406.192,49.500 'Key Testing Considerations \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.770,40.500,417.082,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.660,40.500,432.004,49.500 '65\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'milliseconds. These assumptions can also be called expectations, checks, or assertions,\\nas in traditional software engineering.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.436,432.005,574.337 'Statistical tests on results can also be performed but are typically used for subpopula‐\\ntions. It is also important to be able to compare the model with its previous version. It\\ncan  allow  putting  in  place  a  champion/challenger  approach  (described  in  detail  in\\n“Champion/Challenger”  on  page  100)  or  checking  that  a  metric  does  not  suddenly\\ndrop.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.247,437.930,423.747,493.425 'Subpopulation Analysis and Model Fairness\\nIt can be useful to design test scenarios by splitting data into subpopulations based on\\na “sensitive” variable (that may or may not be used as a feature of the model). This is\\nhow fairness (typically between genders) is evaluated.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.247,371.930,423.747,429.930 'Virtually all models that apply to people should be analyzed for fairness. Increasingly,\\nfailure to assess model fairness will have business, regulatory, and reputational impli‐\\ncations  for  organizations.  For  details  about  biases  and  fairness,  refer  to  “Impact  of\\nResponsible AI on Modeling” on page 53 and “Key Elements of Responsible AI” on\\npage 113.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,249.886,432.005,348.586 'In addition to validating the ML and computational performance metrics, model sta‐\\nbility  is  an  important  testing  property  to  consider.  When  changing  one  feature\\nslightly, one expects small changes in the outcome. While this cannot be always true,\\nit is generally a desirable model property. A very unstable model introduces a lot of\\ncomplexity  and  loopholes  in  addition  to  delivering  a  frustrating  experience,  as  the\\nmodel can feel unreliable even if it has decent performance. There is no single answer\\nto  model  stability,  but  generally  speaking,  simpler  models  or  more  regularized  ones\\nshow better stability.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,149.486,432.005,237.547 'Reproducibility and Auditability\\nReproducibility  in  MLOps  does  not  have  the  same  meaning  as  in  academia.  In  the\\nacademic world, reproducibility essentially means that the findings of an experiment\\nare  described  well  enough  that  another  competent  person  can  replicate  the  experi‐\\nment using the explanations alone, and if the person doesn’t make any mistakes, they\\nwill arrive at the same conclusion.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,93.086,432.005,141.386 'In general, reproducibility in MLOps also involves the ability to easily rerun the exact\\nsame experiment. It implies that the model comes with detailed documentation, the\\ndata used for training and testing, and with an artifact that bundles the implementa‐\\ntion  of  the  model  plus  the  full  specification  of  the  environment  it  was  run  in  (see\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,40.500,80.762,49.500 '66 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.340,40.500,91.652,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.230,40.500,204.197,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,503.175,432.000,503.175>\n",
      "<LTLine 431.875,360.550,431.875,503.300>\n",
      "<LTLine 72.000,360.675,432.000,360.675>\n",
      "<LTLine 72.125,360.550,72.125,503.300>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 '“Version Management and Reproducibility” on page 56). Reproducibility is essential\\nto prove model findings, but also to debug or build on a previous experiment.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,526.036,432.005,574.337 'Auditability is related to reproducibility, but it adds some requirements. For a model\\nto be auditable, it must be possible to access the full history of the ML pipeline from a\\ncentral  and  reliable  storage  and  to  easily  fetch  metadata  on  all  model  versions\\nincluding:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.654,503.437,189.558,513.937 '• The full documentation\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,486.837,405.616,497.337 '• An artifact that allows running the model with its exact initial environment\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,470.237,352.119,480.737 '• Test results, including model explanations and fairness reports\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,453.637,283.753,464.137 '• Detailed model logs and monitoring metadata\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,355.436,432.005,441.536 'Auditability  can  be  an  obligation  in  some  highly  regulated  applications,  but  it  has\\nbenefits  for  all  organizations  because  it  can  facilitate  model  debugging,  continuous\\nimprovement, and keeping track of actions and responsibilities (which is an essential\\npart of governance for responsible applications of ML, as discussed at length in Chap‐\\nter  8).  A  full  QA  toolchain  for  machine  learning—and,  thus,  MLOps  processes—\\nshould provide a clear view of model performance with regard to requirements while\\nalso facilitating auditability. \\n'>\n",
      "<LTTextBoxHorizontal(7) 71.997,311.636,432.003,347.336 'Even when MLOps frameworks allow data scientists (or others) to find a model with\\nall its metadata, understanding the model itself can still be challenging (see “Impact\\nof Responsible AI on Modeling” on page 53 for a detailed discussion).\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,217.436,432.005,303.536 'To have a strong practical impact, auditability must allow for intuitive human under‐\\nstanding of all the parts of the system and their version histories. This doesn’t change\\nthe fact that understanding a machine learning model (even a relatively simple one)\\nrequires  appropriate  training,  but  depending  on  the  criticality  of  the  application,  a\\nwider  audience  may  need  to  be  able  to  understand  the  details  of  the  model.  As  a\\nresult, full auditability comes at a cost that should be balanced with the criticality of\\nthe model itself.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,129.636,432.003,205.096 'Machine Learning Security\\nAs  a  piece  of  software,  a  deployed  model  running  in  its  serving  framework  can\\npresent multiple security issues that range from low-level glitches to social engineer‐\\ning. Machine learning introduces a new range of potential threats where an attacker\\nprovides malicious data designed to cause the model to make a mistake.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.997,73.236,432.005,121.536 'There are numerous cases of potential attacks. For example, spam filters were an early\\napplication of machine learning essentially based on scoring words that were in a dic‐\\ntionary.  One  way  for  spam  creators  to  avoid  detection  was  to  avoid  writing  these\\nexact  words  while  still  making  their  message  easily  understandable  by  a  human\\n'>\n",
      "<LTTextBoxHorizontal(11) 326.600,40.500,406.187,49.500 'Machine Learning Security \\n'>\n",
      "<LTTextBoxHorizontal(12) 413.765,40.500,417.077,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 424.655,40.500,431.999,49.500 '67\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'reader (e.g., using exotic Unicode characters, voluntarily introducing typos, or using\\nimages).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,461.427,432.004,570.867 'Adversarial Attacks\\nA more modern but quite analogous example of a machine learning model security\\nissue is an adversarial attack for deep neural networks in which an image modifica‐\\ntion that can seem minor or even impossible for a human eye to notice can cause the\\nmodel to drastically change its prediction. The core idea is mathematically relatively\\nsimple:  since  deep  learning  inference  is  essentially  matrix  multiplication,  carefully\\nchosen  small  perturbations  to  coefficients  can  cause  a  large  change  in  the  output\\nnumbers.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,392.427,432.005,453.327 'One example of this is that small stickers glued to road signs can confuse an autono‐\\nmous car’s computer vision system, rendering signs invisible or incorrectly classified\\nby  the  system,  while  remaining  fully  visible  and  understandable  to  a  human  being.\\nThe more the attacker knows about the system, the more likely they are to find exam‐\\nples that will confuse it.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,285.627,432.005,384.327 'A  human  can  use  reason  to  find  these  examples  (in  particular  for  simple  models).\\nHowever,  for  more  complex  models  like  deep  learning,  the  attacker  will  probably\\nneed  to  perform  many  queries  and  either  use  brute  force  to  test  as  many  combina‐\\ntions as possible or use a model to search for problematic examples. The difficulty of\\ncountermeasures  is  increasing  with  the  complexity  of  models  and  their  availability.\\nSimple  models  such  as  logistic  regressions  are  essentially  immune,  while  an  open\\nsource pretrained deep neural network will basically always be vulnerable, even with\\nadvanced, built-in attack detectors.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,241.827,432.003,277.527 'Adversarial attacks don’t necessarily happen at inference time. If an attacker can get\\naccess to the training data, even partially, then they get control over the system. This\\nkind of attack is traditionally known as a poisoning attack in computer security.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,135.027,432.005,233.727 'One famous example is the Twitter chatbot released by Microsoft in 2016. Just a few\\nhours after launch, the bot started to generate very offensive tweets. This was caused\\nby  the  bot  adapting  to  its  input;  when  realizing  that  some  users  submitted  a  large\\namount of offensive content, the bot started to replicate. In theory, a poisoning attack\\ncan  occur  as  a  result  of  an  intrusion  or  even,  in  a  more  sophisticated  way,  through\\npretrained models. But in practice, one should mostly care about data collected from\\neasily  manipulated  data  sources.  Tweets  sent  to  a  specific  account  are  a  particularly\\nclear example.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,77.018,432.003,123.457 'Other Vulnerabilities\\nSome patterns do not exploit machine learning vulnerabilities per se, but they do use\\nthe machine learning model in ways that lead to undesirable situations. One example\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.001,40.500,80.767,49.500 '68 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.345,40.500,91.657,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.235,40.500,204.202,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,532.036,432.004,605.537 'is in credit scoring: for a given amount of money, borrowers with less flexibility tend\\nto choose a longer period to lower the payments, while borrowers who are not con‐\\ncerned about their ability to pay may choose a shorter period to lower the total cost of\\ncredit. Salespeople may advise those who do not have a good enough score to shorten\\ntheir  payments.  This  increases  the  risk  for  the  borrower  and  the  bank  and  is  not  a\\nmeaningful course of action. Correlation is not causality!\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,425.236,432.005,523.936 'Models can also leak data in many ways. Since the machine learning models can fun‐\\ndamentally be considered a summary of the data they have been trained on, they can\\nleak more or less precise information on the training data, up to the full training set\\nin  some  cases.  Imagine,  for  example,  that  a  model  predicts  how  much  someone  is\\npaid using the nearest neighbor algorithm. If one knows the zip code, age, and pro‐\\nfession of a certain person registered on the service, it’s pretty easy to obtain that per‐\\nson’s  exact  income.  There  are  a  wide  range  of  attacks  that  can  extract  information\\nfrom models in this way.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,331.036,432.005,417.136 'In addition to technical hardening and audit, governance plays a critical role in secu‐\\nrity. Responsibilities must be assigned clearly and in a way that ensures an appropri‐\\nate balance between security and capacity of execution. It is also important to put in\\nplace feedback mechanisms, and employees and users should have an easy channel to\\ncommunicate  breaches  (including,  potentially,  “bug  bounty  programs”  that  reward\\nreporting  vulnerabilities).  It  is  also  possible,  and  necessary,  to  build  safety  nets\\naround the system to mitigate the risks.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,236.836,432.005,322.936 'Machine learning security shares many common traits with general computer system\\nsecurity,  one  of  the  main  ideas  being  that  security  is  not  an  additional  independent\\nfeature  of  the  system;  that  is,  generally  you  cannot  secure  a  system  that  is  not\\ndesigned  to  be  secure,  and  the  organization  processes  must  take  into  account  the\\nnature of the threat from the beginning. Strong MLOps processes, including all of the\\nsteps  in  preparing  for  production  described  in  this  chapter,  can  help  make  this\\napproach a reality.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.992,111.236,432.003,224.496 'Model Risk Mitigation\\nGenerally speaking, as discussed in detail in Chapter 1, the broader the model deploy‐\\nment, the greater the risk. When risk impact is high enough, it is essential to control\\nthe deployment of new versions, which is where tightly controlled MLOps processes\\ncome  into  play  in  particular.  Progressive  or  canary  rollouts  should  be  a  common\\npractice, with new versions of models being served to a small proportion of the orga‐\\nnization or customer base first and slowly increasing that proportion, while monitor‐\\ning behavior and getting human feedback if appropriate.\\n'>\n",
      "<LTTextBoxHorizontal(5) 340.061,40.500,406.184,49.500 'Model Risk Mitigation \\n'>\n",
      "<LTTextBoxHorizontal(6) 413.762,40.500,417.074,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 424.652,40.500,431.996,49.500 '69\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.995,460.655,432.004,607.894 'Changing Environments\\nRapidly changing environments also multiply risk, as mentioned earlier in this chap‐\\nter. Changes in inputs is a related and also well-identified risk, and Chapter 7 dives\\ninto these challenges and how to address them in more detail. But what’s important to\\nnote  is  that  the  speed  of  change  can  amplify  the  risk  depending  on  the  application.\\nChanges may be so fast that they have consequences even before the monitoring sys‐\\ntem sends alerts. That is to say, even with an efficient monitoring system and a proce‐\\ndure  to  retrain  models,  the  time  necessary  to  remediate  may  be  a  critical  threat,\\nespecially  if  simply  retraining  the  model  on  new  data  is  not  sufficient  and  a  new\\nmodel must be developed. During this time, the production systems misbehaving can\\ncause large losses for the organization.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,366.455,432.004,452.555 'To  control  this  risk,  monitoring  via  MLOps  should  be  reactive  enough  (typically,\\nalerting on distributions computed every week might not be enough), and the proce‐\\ndure should consider the period necessary for remediation. For example, in addition\\nto  retraining  or  rollout  strategies,  the  procedure  may  define  thresholds  that  would\\ntrigger  a  degraded  mode  for  the  system.  A  degraded  mode  may  simply  consist  of  a\\nwarning message displayed for end users, but could be as drastic as shutting down the\\ndysfunctional system to avoid harm until a stable solution can be deployed.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,284.855,432.004,358.355 'Less dramatic issues that are frequent enough can also do harm that quickly becomes\\ndifficult  to  control.  If  the  environment  changes  often,  even  if  remediation  never\\nseems urgent, a model can always be slightly off, never operating within its nominal\\ncase, and the operating cost can be challenging to evaluate. This can only be detected\\nthrough dedicated MLOps, including relatively long-term monitoring and reevaluat‐\\ning the cost of operating the model.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,215.855,432.003,276.755 'In  many  cases,  retraining  the  model  on  more  data  will  increasingly  improve  the\\nmodel, and this problem will eventually disappear, but this can take time. Before this\\nconvergence, a solution might be to use a less complex model that may have a lower\\nevaluated  performance  and  may  be  more  consistent  in  a  frequently  changing\\nenvironment.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,94.846,432.005,204.285 'Interactions Between Models\\nComplex  interactions  between  models  is  probably  the  most  challenging  source  of\\nrisk. This class of issue will be a growing concern as ML models become pervasive,\\nand it’s an important potential area of focus for MLOps systems. Obviously, adding\\nmodels  will  often  add  complexity  to  an  organization,  but  the  complexity  does  not\\nnecessarily grow linearly in proportion to the number of models; having two models\\nis more complicated to understand than the sum since there are potential interactions\\nbetween them.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.002,40.500,80.768,49.500 '70 \\n'>\n",
      "<LTTextBoxHorizontal(6) 88.346,40.500,91.658,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 99.236,40.500,204.203,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,519.436,432.005,605.537 'Moreover,  the  total  complexity  is  heavily  determined  by  how  the  interactions  with\\nmodels  are  designed  at  a  local  scale  and  governed  at  an  organizational  scale.  Using\\nmodels in chains (where a model uses inputs from another model) can create signifi‐\\ncant additional complexity as well as totally unexpected results, whereas using models\\nin independent parallel processing chains, which are each as short and explainable as\\npossible,  is  a  much  more  sustainable  way  to  design  large-scale  deployment  of\\nmachine learning.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,437.836,432.005,511.336 'First, the absence of obvious interactions between models makes the complexity grow\\ncloser  to  linearly  (though  note  that,  in  practice,  it  is  rarely  the  case,  as  there  can\\nalways be interactions in the real world even if models are not connected). Also, mod‐\\nels used in redundant chains of processing can avoid errors—that is, if a decision is\\nbased on several independent chains of processing with methods as different as possi‐\\nble, it can be more robust.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,368.836,432.003,429.736 'Finally, generally speaking, the more complex the model, the more complex its inter‐\\nactions with other systems may be, as it may have many edge cases, be less stability in\\nsome domains, overreact to the changes of an upstream model, or confuse a sensitive\\ndownstream model, etc. Here again, we see that model complexity has a cost, and a\\npotentially highly unpredictable one at that.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.994,260.427,432.003,357.267 'Model Misbehavior\\nA  number  of  measures  can  be  implemented  to  avoid  model  misbehavior,  including\\nexamining its inputs and outputs in real time. While training a model, it is possible to\\ncharacterize  its  domain  of  applicability  by  examining  the  intervals  on  which  the\\nmodel was trained and validated. If the value of a feature at inference time is out of\\nbounds,  the  system  can  trigger  appropriate  measures  (e.g.,  rejecting  the  sample  or\\ndispatching a warning message).\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,166.227,432.004,252.327 'Controlling feature-value intervals is a useful and simple technique, but it might be\\ninsufficient. For example, when training an algorithm to evaluate car prices, the data\\nmay  have  provided  examples  of  recent  light  cars  and  old  heavy  cars,  but  no  recent\\nheavy  cars.  The  performance  of  a  complex  model  for  these  is  unpredictable.  When\\nthe  number  of  features  is  large,  this  issue  becomes  unavoidable  due  to  the  curse  of\\ndimensionality—i.e., the number of combinations is exponential relative to the num‐\\nber of features.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,72.027,432.005,158.127 'In  these  situations,  more  sophisticated  methods  can  be  used,  including  anomaly\\ndetection  to  identify  records  where  the  model  is  used  outside  of  its  application\\ndomain. After scoring, the outputs of the model can be examined before confirming\\nthe inference. In the case of classification, many algorithms provide certainty scores\\nin  addition  to  their  prediction,  and  a  threshold  can  be  fixed  to  accept  an  inference\\noutput.  Note  that  these  certainty  scores  do  not  typically  translate  into  probabilities,\\neven if they are named this way in the model.\\n'>\n",
      "<LTTextBoxHorizontal(6) 340.062,40.500,406.185,49.500 'Model Risk Mitigation \\n'>\n",
      "<LTTextBoxHorizontal(7) 413.763,40.500,417.075,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 424.653,40.500,431.997,49.500 '71\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,605.537 'Conformal prediction is a set of techniques that helps calibrate these scores to obtain\\nan accurate estimation of the probability of correctness. For regression, the value can\\nbe checked against a predetermined interval. For example, if the model predicts a car\\ncosts $50 or $500,000, you may not want to commit any business on this prediction.\\nThe  complexity  of  the  implemented  techniques  should  be  relevant  for  the  level  of\\nrisk: a highly complex, highly critical model will require more thorough safeguards.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.993,431.636,432.003,519.697 'Closing Thoughts\\nIn practice, preparing models for production starts from the beginning at the devel‐\\nopment  phase;  that  is  to  say,  the  requirements  of  production  deployments,  security\\nimplications, and risk mitigation aspects should be considered when developing the\\nmodels. MLOps includes having a clear validation step before sending models to pro‐\\nduction, and the key ideas to successfully prepare models for productions are:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.652,409.036,354.607,419.536 '• Clearly identifying the nature of the risks and their magnitudes\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,367.236,432.002,402.936 '• Understanding  model  complexity  and  its  impact  at  multiple  levels,  including\\nincreased  latency,  increased  memory  and  power  consumption,  lower  ability  to\\ninterpret inference in production, and a harder-to-control risk\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.654,325.436,432.001,361.136 '• Providing a simple but clear standard of quality, making sure the team is appro‐\\npriately trained and the organization structure allows for fast and reliable valida‐\\ntion processes\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.654,308.836,431.997,319.336 '• Automating all the validation that can be automated to ensure it is properly and\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,296.236,385.283,306.736 'consistently performed while maintaining the ability to deploy quickly\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.002,40.500,80.768,49.500 '72 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.346,40.500,91.658,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.236,40.500,204.203,49.500 'Chapter 5: Preparing for Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 230.995,533.502,431.998,582.331 'CHAPTER 6\\nDeploying to Production\\n'>\n",
      "<LTTextBoxHorizontal(1) 349.708,436.501,431.998,449.501 'Joachim Zentici\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,322.268,432.005,408.369 'Business leaders view the rapid deployment of new systems into production as key to\\nmaximizing business value. But this is only true if deployment can be done smoothly\\nand  at  low  risk  (software  deployment  processes  have  become  more  automated  and\\nrigorous in recent years to address this inherent conflict). This chapter dives into the\\nconcepts and considerations when deploying machine learning models to production\\nthat  impact—and  indeed,  drive—the  way  MLOps  deployment  processes  are  built\\n(Figure 6-1 presents this phase in the context of the larger life cycle).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,171.838,429.804,194.938 'Figure 6-1. Deployment to production highlighted in the larger context of the ML project\\nlife cycle\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,84.038,432.002,159.498 'CI/CD Pipelines\\nCI/CD is a common acronym for continuous integration and continuous delivery (or\\nput more simply, deployment). The two form a modern philosophy of agile software\\ndevelopment and a set of practices and tools to release applications more often and\\nfaster, while also better controlling quality and risk.\\n'>\n",
      "<LTTextBoxHorizontal(5) 424.656,40.500,432.000,49.500 '73\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTLine 72.000,316.007,432.500,316.007>\n",
      "<LTLine 432.375,200.502,432.375,316.132>\n",
      "<LTLine 72.000,200.627,432.500,200.627>\n",
      "<LTLine 72.125,200.502,72.125,316.132>\n",
      "<LTFigure(I1) 81.969,206.482,422.531,310.882 matrix=[340.56,0.00,0.00,104.40, (81.97,206.48)]>\n",
      "<LTTextBoxHorizontal(0) 71.996,519.436,432.004,605.537 'While  these  ideas  are  decades  old  and  already  used  to  various  extents  by  software\\nengineers, different people and organizations use certain terms in very different ways.\\nBefore digging into how CI/CD applies to machine learning workflows, it is essential\\nto keep in mind that these concepts should be tools to serve the purpose of delivering\\nquality  fast,  and  the  first  step  is  always  to  identify  the  specific  risks  present  at  the\\norganization.  In  other  words,  as  always,  CI/CD  methodology  should  be  adapted\\nbased on the needs of the team and the nature of the business.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,450.436,432.005,511.336 'CI/CD concepts apply to traditional software engineering, but they apply just as well\\nto machine learning systems and are a critical part of MLOps strategy. After success‐\\nfully developing a model, a data scientist should push the code, metadata, and docu‐\\nmentation to a central repository and trigger a CI/CD pipeline. An example of such\\npipeline could be:\\n'>\n",
      "<LTTextBoxHorizontal(2) 77.318,427.836,157.417,438.336 '1. Build the model\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.746,411.236,206.536,421.736 'a. Build the model artifacts\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.327,394.636,264.066,405.136 'b. Send the artifacts to long-term storage\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.914,378.036,291.450,388.536 'c. Run basic checks (smoke tests/sanity checks)\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.812,361.436,286.074,371.936 'd. Generate fairness and explainability reports\\n'>\n",
      "<LTTextBoxHorizontal(7) 77.316,344.836,212.374,355.336 '2. Deploy to a test environment\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.746,328.236,383.808,338.736 'a. Run tests to validate ML performance, computational performance\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.327,311.636,178.491,322.136 'b. Validate manually\\n'>\n",
      "<LTTextBoxHorizontal(10) 77.316,295.036,238.089,305.536 '3. Deploy to production environment\\n'>\n",
      "<LTTextBoxHorizontal(11) 90.746,278.436,219.819,288.936 'a. Deploy the model as canary\\n'>\n",
      "<LTTextBoxHorizontal(12) 90.327,261.836,199.428,272.336 'b. Fully deploy the model\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.997,188.836,432.005,249.736 'Many scenarios are possible and depend on the application, the risks from which the\\nsystem should be protected, and the way the organization chooses to operate. Gener‐\\nally  speaking,  an  incremental  approach  to  building  a  CI/CD  pipeline  is  preferred:  a\\nsimple or even naïve workflow on which a team can iterate is often much better than\\nstarting with complex infrastructure from scratch.\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.996,107.236,432.003,180.736 'A starting project does not have the infrastructure requirements of a tech giant, and it\\ncan be hard to know up front which challenges deployments will present. There are\\ncommon tools and best practices, but there is no one-size-fits-all CI/CD methodol‐\\nogy. This means the best path forward is starting from a simple (but fully functional)\\nCI/CD  workflow  and  introducing  additional  or  more  sophisticated  steps  along  the\\nway as quality or scaling challenges appear.\\n'>\n",
      "<LTTextBoxHorizontal(15) 72.003,40.500,80.769,49.500 '74 \\n'>\n",
      "<LTTextBoxHorizontal(16) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(17) 99.237,40.500,202.764,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.995,519.912,432.002,607.973 'Building ML Artifacts\\nThe goal of a continuous integration pipeline is to avoid unnecessary effort in merg‐\\ning the work from several contributors as well as to detect bugs or development con‐\\nflicts  as  soon  as  possible.  The  very  first  step  is  using  centralized  version  control\\nsystems  (unfortunately,  working  for  weeks  on  code  stored  only  on  a  laptop  is  still\\nquite common).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,438.312,432.004,511.812 'The  most  common  version  control  system  is  Git,  an  open  source  software  initially\\ndeveloped to manage the source code for the Linux kernel. The majority of software\\nengineers across the world already use Git, and it is increasingly being adopted in sci‐\\nentific  computing  and  data  science.  It  allows  for  maintaining  a  clear  history  of\\nchanges, safe rollback to a previous version of the code, multiple contributors to work\\non their own branches of the project before merging to the main branch, etc.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,381.912,432.003,430.212 'While Git is appropriate for code, it was not designed to store other types of assets\\ncommon  in  data  science  workflows,  such  as  large  binary  files  (for  example,  trained\\nmodel weights), or to version the data itself. Data versioning is a more complex topic\\nwith numerous solutions, including Git extensions, file formats, databases, etc.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,286.103,432.005,370.343 'What’s in an ML Artifact?\\nOnce the code and data is in a centralized repository, a testable and deployable bun‐\\ndle of the project must be built. These bundles are usually called artifacts in the con‐\\ntext  of  CI/CD.  Each  of  the  following  elements  needs  to  be  bundled  into  an  artifact\\nthat  goes  through  a  testing  pipeline  and  is  made  available  for  deployment  to\\nproduction:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.658,263.503,263.352,274.003 '• Code for the model and its preprocessing\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,246.903,241.501,257.403 '• Hyperparameters and configuration\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,230.303,208.962,240.803 '• Training and validation data\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,213.703,238.614,224.203 '• Trained model in its runnable form\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,197.103,431.997,207.603 '• An  environment  including  libraries  with  specific  versions,  environment  vari‐\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,184.503,130.196,195.003 'ables, etc.\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.660,167.903,156.420,178.403 '• Documentation\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.655,151.303,237.501,161.803 '• Code and data for testing scenarios\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.995,93.294,431.999,139.734 'The Testing Pipeline\\nAs touched on in Chapter 5, the testing pipeline can validate a wide variety of proper‐\\nties of the model contained in the artifact. One of the important operational aspects\\n'>\n",
      "<LTTextBoxHorizontal(13) 343.022,40.500,406.184,49.500 'Building ML Artifacts \\n'>\n",
      "<LTTextBoxHorizontal(14) 413.762,40.500,417.074,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 424.652,40.500,431.996,49.500 '75\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'of  testing  is  that,  in  addition  to  verifying  compliance  with  requirements,  good  tests\\nshould make it as easy as possible to diagnose the source issue when they fail.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,551.236,431.997,574.337 'For that purpose, naming the tests is extremely important, and carefully choosing a\\nnumber of datasets to validate the model against can be valuable. For example:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.660,478.237,432.002,539.137 '• A test on a fixed (not automatically updated) dataset with simple data and not-\\ntoo-restrictive  performance  thresholds  can  be  executed  first  and  called  “base\\ncase.” If the test reports show that this test failed, there is a strong possibility that\\nthe model is way off, and the cause may be a programming error or a misuse of\\nthe model, for example.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.652,360.836,432.005,472.137 '• Then,  a  number  of  datasets  that  each  have  one  specific  oddity  (missing  values,\\nextreme values, etc.) could be used with tests appropriately named so that the test\\nreport immediately shows the kind of data that is likely to make the model fail.\\nThese datasets can represent realistic yet remarkable cases, but it may also be use‐\\nful to generate synthetic data that is not expected in production. This could pos‐\\nsibly  protect  the  model  from  new  situations  not  yet  encountered,  but  most\\nimportantly,  this  could  protect  the  model  from  malfunctions  in  the  system\\nquerying or from adversarial examples (as discussed in “Machine Learning Secu‐\\nrity” on page 67).\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,293.836,432.003,354.736 '• Then, an essential part of model validation is testing on recent production data.\\nOne or several datasets should be used, extracted from several time windows and\\nnamed appropriately. This category of tests should be performed and automati‐\\ncally analyzed when the model is already deployed to production. Chapter 7 pro‐\\nvides more specific details on how to do that.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,220.836,432.004,281.736 'Automating these tests as much as possible is essential and, indeed, is a key compo‐\\nnent  of  efficient  MLOps.  A  lack  of  automation  or  speed  wastes  time,  but,  more\\nimportantly, it discourages the development team from testing and deploying often,\\nwhich  can  delay  the  discovery  of  bugs  or  design  choices  that  make  it  impossible  to\\ndeploy to production.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,139.236,432.003,212.736 'In  extreme  cases,  a  development  team  can  hand  over  a  monthslong  project  to  a\\ndeployment team that will simply reject it because it does not satisfy requirements for\\nthe  production  infrastructure.  Also,  less  frequent  deployments  imply  larger  incre‐\\nments that are harder to manage; when many changes are deployed at once and the\\nsystem is not behaving in the desired way, isolating the origin of an issue is more time\\nconsuming.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,82.836,432.005,131.136 'The most widespread tool for software engineering continuous integration is Jenkins,\\na very flexible build system that allows for the building of CI/CD pipelines regardless\\nof the programming language, testing framework, etc. Jenkins can be used in data sci‐\\nence to orchestrate CI/CD pipelines, although there are many other options.\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.001,40.500,80.767,49.500 '76 \\n'>\n",
      "<LTTextBoxHorizontal(9) 88.345,40.500,91.657,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 99.235,40.500,202.762,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,557.712,432.004,607.973 'Deployment Strategies\\nTo  understand  the  details  of  a  deployment  pipeline,  it  is  important  to  distinguish\\namong concepts often used inconsistently or interchangeably.\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.001,538.112,117.088,548.612 'Integration\\n'>\n",
      "<LTTextBoxHorizontal(2) 89.997,500.312,432.003,536.012 'The process of merging a contribution to a central repository (typically merging\\na Git feature branch to the main branch) and performing more or less complex\\ntests.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,480.712,106.272,491.212 'Delivery\\n'>\n",
      "<LTTextBoxHorizontal(4) 89.997,442.912,432.004,478.612 'As used in the continuous delivery (CD) part of CI/CD, the process of building a\\nfully  packaged  and  validated  version  of  the  model  ready  to  be  deployed  to\\nproduction.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.005,423.312,121.786,433.812 'Deployment\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.996,347.712,432.005,421.212 'The  process  of  running  a  new  model  version  on  a  target  infrastructure.  Fully\\nautomated deployment is not always practical or desirable and is a business deci‐\\nsion as much as a technical decision, whereas continuous delivery is a tool for the\\ndevelopment  team  to  improve  productivity  and  quality  as  well  as  measure  pro‐\\ngress more reliably. Continuous delivery is required for continuous deployment,\\nbut it also provides enormous value without.\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.001,328.112,101.915,338.612 'Release\\n'>\n",
      "<LTTextBoxHorizontal(8) 89.996,277.712,432.002,326.012 'In principle, release is yet another step, as deploying a model version (even to the\\nproduction infrastructure) does not necessarily mean that the production work‐\\nload is directed to the new version. As we will see, multiple versions of a model\\ncan run at the same time on the production infrastructure.\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.001,233.912,432.004,269.612 'Getting everyone in the MLOps process on the same page about what these concepts\\nmean and how they apply will allow for smoother processes on both the technical and\\nbusiness sides.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.999,175.903,432.001,222.343 'Categories of Model Deployment\\nIn addition to different deployment strategies, there are two ways to approach model\\ndeployment:\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.654,153.303,431.997,163.803 '• Batch scoring, where whole datasets are processed using a model, such as in daily\\n'>\n",
      "<LTTextBoxHorizontal(12) 90.002,140.703,154.157,151.203 'scheduled jobs.\\n'>\n",
      "<LTTextBoxHorizontal(13) 80.660,98.903,432.002,134.603 '• Real-time  scoring,  where  one  or  a  small  number  of  records  are  scored,  such  as\\nwhen an ad is displayed on a website and a user session is scored by models to\\ndecide what to display.\\n'>\n",
      "<LTTextBoxHorizontal(14) 336.820,40.500,406.192,49.500 'Deployment Strategies \\n'>\n",
      "<LTTextBoxHorizontal(15) 413.770,40.500,417.073,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 424.651,40.500,431.995,49.500 '77\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.004,605.537 'There  is  a  continuum  between  these  two  approaches,  and  in  fact,  in  some  systems,\\nscoring  on  one  record  is  technically  identical  to  requesting  a  batch  of  one.  In  both\\ncases,  multiple  instances  of  the  model  can  be  deployed  to  increase  throughput  and\\npotentially lower latency.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.036,432.684,549.136 'Deploying many real-time scoring systems is conceptually simpler since the records\\nto be scored can be dispatched between several machines (e.g., using a load balancer). \\nBatch scoring can also be parallelized, for example by using a parallel processing run‐\\ntime  like  Apache  Spark,  but  also  by  splitting  datasets  (which  is  usually  called  parti‐\\ntioning  or  sharding)  and  scoring  the  partitions  independently.  Note  that  these  two\\nconcepts of splitting the data and computation can be combined, as they can address\\ndifferent problems.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.998,329.427,432.005,451.467 'Considerations When Sending Models to Production\\nWhen sending a new model version to production, the first consideration is often to\\navoid downtime, in particular for real-time scoring. The basic idea is that rather than\\nshutting down the system, upgrading it, and then putting it back online, a new system\\ncan  be  set  up  next  to  the  stable  one,  and  when  it’s  functional,  the  workload  can  be\\ndirected to the newly deployed version (and if it remains healthy, the old one is shut\\ndown).  This  deployment  strategy  is  called  blue-green—or  sometimes  red-black—\\ndeployment. There are many variations and frameworks (like Kubernetes) to handle\\nthis natively.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,260.427,432.004,321.327 'Another more advanced solution to mitigate the risk is to have canary releases (also\\ncalled canary deployments). The idea is that the stable version of the model is kept in\\nproduction, but a certain percentage of the workload is redirected to the new model,\\nand results are monitored. This strategy is usually implemented for real-time scoring,\\nbut a version of it could also be considered for batch.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,204.027,432.002,252.327 'A  number  of  computational  performance  and  statistical  tests  can  be  performed  to\\ndecide whether to fully switch to the new model, potentially in several workload per‐\\ncentage increments. This way, a malfunction would likely impact only a small portion\\nof the workload.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,109.827,432.004,195.927 'Canary releases apply to production systems, so any malfunction is an incident, but\\nthe idea here is to limit the blast radius. Note that scoring queries that are handled by\\nthe canary model should be carefully picked, because some issues may go unnoticed\\notherwise. For example, if the canary model is serving a small percentage of a region\\nor  country  before  the  model  is  fully  released  globally,  it  could  be  the  case  that  (for\\nmachine learning or infrastructure reasons) the model does not perform as expected\\nin other regions.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.999,40.500,80.765,49.500 '78 \\n'>\n",
      "<LTTextBoxHorizontal(7) 88.343,40.500,91.655,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 99.233,40.500,202.760,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'A more robust approach is to pick the portion of users served by the new model at\\nrandom,  but  then  it  is  often  desirable  for  user  experience  to  implement  an  affinity\\nmechanism so that the same user always uses the same version of the model.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.005,561.736 'Canary testing can be used to carry out A/B testing, which is a process to compare\\ntwo  versions  of  an  application  in  terms  of  a  business  performance  metric.  The  two\\nconcepts  are  related  but  not  the  same,  as  they  don’t  operate  at  the  same  level  of\\nabstraction. A/B testing can be made possible through a canary release, but it could\\nalso  be  implemented  as  logic  directly  coded  into  a  single  version  of  an  application.\\nChapter 7 provides more details on the statistical aspects of setting up A/B testing.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,444.436,432.000,480.136 'Overall,  canary  releases  are  a  powerful  tool,  but  they  require  somewhat  advanced\\ntooling to manage the deployment, gather the metrics, specify and run computations\\non them, display the results, and dispatch and process alerts.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,386.427,431.999,432.867 'Maintenance in Production\\nOnce a model is released, it must be maintained. At a high level, there are three main‐\\ntenance measures:\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,366.827,156.038,377.327 'Resource monitoring\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.004,341.627,431.684,364.727 'Just as for any application running on a server, collecting IT metrics such as CPU,\\nmemory, disk, or network usage can be useful to detect and troubleshoot issues.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,322.027,124.129,332.527 'Health check\\n'>\n",
      "<LTTextBoxHorizontal(7) 89.998,284.227,432.004,319.927 'To check if the model is indeed online and to analyze its latency, it is common to\\nimplement  a  health  check  mechanism  that  simply  queries  the  model  at  a  fixed\\ninterval (on the order of one minute) and logs the results.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.995,264.627,166.967,275.127 'ML metrics monitoring\\n'>\n",
      "<LTTextBoxHorizontal(9) 89.996,201.627,432.005,262.527 'This  is  about  analyzing  the  accuracy  of  the  model  and  comparing  it  to  another\\nversion or detecting when it is going stale. Since it may require heavy computa‐\\ntion, this is typically lower frequency, but as always, will depend on the applica‐\\ntion;  it  is  typically  done  once  a  week.  Chapter  7  details  how  to  implement  this\\nfeedback loop.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.999,157.827,432.002,193.527 'Finally, when a malfunction is detected, a rollback to a previous version may be nec‐\\nessary. It is critical to have the rollback procedure ready and as automated as possible;\\ntesting it regularly can make sure it is indeed functional.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.991,70.027,432.001,145.487 'Containerization\\nAs described earlier, managing the versions of a model is much more than just saving\\nits code into a version control system. In particular, it is necessary to provide an exact\\ndescription of the environment (including, for example, all the Python libraries used\\nas well as their versions, the system dependencies that need to be installed, etc.).\\n'>\n",
      "<LTTextBoxHorizontal(12) 355.386,40.500,406.191,49.500 'Containerization \\n'>\n",
      "<LTTextBoxHorizontal(13) 413.769,40.500,417.081,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(14) 424.659,40.500,432.003,49.500 '79\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,605.537 'But storing this metadata is not enough. Deploying to production should automati‐\\ncally  and  reliably  rebuild  this  environment  on  the  target  machine.  In  addition,  the\\ntarget  machine  will  typically  run  multiple  models  simultaneously,  and  two  models\\nmay have incompatible dependency versions. Finally, several models running on the\\nsame machine could compete for resources, and one misbehaving model could hurt\\nthe performance of multiple cohosted models.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,450.436,432.003,523.936 'Containerization  technology  is  increasingly  used  to  tackle  these  challenges.  These\\ntools bundle an application together with all of its related configuration files, libraries,\\nand dependencies that are required for it to run across different operating environ‐\\nments.  Unlike  virtual  machines  (VMs),  containers  do  not  duplicate  the  complete\\noperating  system;  multiple  containers  share  a  common  operating  system  and  are\\ntherefore far more resource efficient. \\n'>\n",
      "<LTTextBoxHorizontal(2) 71.999,394.036,432.005,442.336 'The  most  well-known  containerization  technology  is  the  open  source  platform\\nDocker. Released in 2014, it has become the de facto standard. It allows an application\\nto be packaged, sent to a server (the Docker host), and run with all its dependencies\\nin isolation from other applications.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,350.236,431.999,385.936 'Building  the  basis  of  a  model-serving  environment  that  can  accommodate  many\\nmodels, each of which may run multiple copies, may require multiple Docker hosts.\\nWhen deploying a model, the framework should solve a number of issues:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.658,327.636,308.491,338.136 '• Which Docker host(s) should receive the container?\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,311.036,426.627,321.536 '• When a model is deployed in several copies, how can the workload be balanced?\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,294.436,431.997,304.936 '• What happens if the model becomes unresponsive, for example, if the machine\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.002,281.836,394.523,292.336 'hosting it fails? How can that be detected and a container reprovisioned?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,240.036,432.002,275.736 '• How  can  a  model  running  on  multiple  machines  be  upgraded,  with  assurances\\nthat old and new versions are switched on and off, and that the load balancer is\\nupdated with a correct sequence?\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,129.236,432.005,227.936 'Kubernetes, an open source platform that has gained a lot of traction in the past few\\nyears  and  is  becoming  the  standard  for  container  orchestration,  greatly  simplifies\\nthese issues and many others. It provides a powerful declarative API to run applica‐\\ntions  in  a  group  of  Docker  hosts,  called  a  Kubernetes  cluster.  The  word  declarative\\nmeans that rather than trying to express in code the steps to set up, monitor, upgrade,\\nstop, and connect the container (which can be complex and error prone), users spec‐\\nify in a configuration file the desired state, and Kubernetes makes it happen and then\\nmaintains it.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,72.836,432.003,121.136 'For example, users need only specify to Kubernetes “make sure four instances of this\\ncontainer run at all times,” and Kubernetes will allocate the hosts, start the containers,\\nmonitor them, and start a new instance if one of them fails. Finally, the major cloud\\nproviders all provide managed Kubernetes services; users do not even have to install\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.000,40.500,80.766,49.500 '80 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.234,40.500,202.761,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'and maintain Kubernetes itself. If an application or a model is packaged as a Docker\\ncontainer,  users  can  directly  submit  it,  and  the  service  will  provision  the  required\\nmachines to run one or several instances of the container inside Kubernetes.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.005,561.736 'Docker  with  Kubernetes  can  provide  a  powerful  infrastructure  to  host  applications,\\nincluding ML models. Leveraging these products greatly simplifies the implementa‐\\ntion of the deployment strategies—like blue-green deployments or canary releases—\\nalthough they are not aware of the nature of the deployed applications and thus can’t\\nnatively manage the ML performance analysis. Another major advantage of this type\\nof infrastructure is the ability to easily scale the model’s deployment.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,438.236,383.440,475.897 'Scaling Deployments\\nAs ML adoption grows, organizations face two types of growth challenges:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.654,415.636,345.577,426.136 '• The ability to use a model in production with high-scale data\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,399.036,324.031,409.536 '• The ability to train larger and larger numbers of models\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,300.836,432.005,386.936 'Handling more data for real-time scoring is made much easier by frameworks such as\\nKubernetes. Since most of the time trained models are essentially formulas, they can\\nbe replicated in the cluster in as many copies as necessary. With the auto-scaling fea‐\\ntures  in  Kubernetes,  both  provisioning  new  machines  and  load  balancing  are  fully\\nhandled by the framework, and setting up a system with huge scaling capabilities is\\nnow relatively simple. The major difficulty can then be to process the large amount of\\nmonitoring data; Chapter 7 provides some details on this challenge.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.247,177.330,423.749,280.825 'Scalable and Elastic Systems\\nA computational system is said to be horizontally scalable (or just scalable) if it is pos‐\\nsible to incrementally add more computers to expand its processing power. For exam‐\\nple,  a  Kubernetes  cluster  can  be  expanded  to  hundreds  of  machines.  However,  if  a\\nsystem includes only one machine, it may be challenging to incrementally upgrade it\\nsignificantly,  and  at  some  point,  a  migration  to  a  bigger  machine  or  a  horizontally\\nscalable system will be required (and may be very expensive and require interruption\\nof service).\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.249,87.330,423.749,169.330 'An elastic system allows, in addition to being scalable, easy addition and removal of\\nresources to match the compute requirements. For example, a Kubernetes cluster in\\nthe cloud can have an auto-scaling capability that automatically adds machines when\\nthe cluster usage metrics are high and removes them when they are low. In principle,\\nelastic  systems  can  optimize  the  usage  of  resources;  they  automatically  adapt  to  an\\nincrease in usage without the need to permanently provision resources that are rarely\\nrequired.\\n'>\n",
      "<LTTextBoxHorizontal(8) 342.900,40.500,406.188,49.500 'Scaling Deployments \\n'>\n",
      "<LTTextBoxHorizontal(9) 413.766,40.500,417.078,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 424.656,40.500,432.000,49.500 '81\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,290.575,432.000,290.575>\n",
      "<LTLine 431.875,75.950,431.875,290.700>\n",
      "<LTLine 72.000,76.075,432.000,76.075>\n",
      "<LTLine 72.125,75.950,72.125,290.700>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'For  batch  scoring,  the  situation  can  be  more  complex.  When  the  volume  of  data\\nbecomes  too  large,  there  are  essentially  two  types  of  strategies  to  distribute  the\\ncomputation:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.660,383.436,432.004,557.737 '• Using  a  framework  that  handles  distributed  computation  natively,  in  particular\\nSpark. Spark is an open source distributed computation framework. It is useful to\\nunderstand that Spark and Kubernetes do not play similar roles and can be com‐\\nbined.  Kubernetes  orchestrates  containers,  but  Kubernetes  is  not  aware  of  what\\nthe containers are actually doing; as far as Kubernetes is concerned, they are just\\ncontainers that run an application on one specific host. (In particular, Kubernetes\\nhas no concept of data processing, as it can be used to run any kind of applica‐\\ntion.) Spark is a computation framework that can split the data and the computa‐\\ntion among its nodes. A modern way to use Spark is through Kubernetes. To run\\na  Spark  job,  the  desired  number  of  Spark  containers  are  started  by  Kubernetes;\\nonce they are started, they can communicate to complete the computation, after\\nwhich  the  containers  are  destroyed  and  the  resources  are  available  for  other\\napplications, including other Spark jobs that may have different Spark versions or\\ndependencies.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.659,316.436,432.001,377.336 '• Another  way  to  distribute  batch  processing  is  to  partition  the  data.  There  are\\nmany ways to achieve this, but the general idea is that scoring is typically a row-\\nby-row  operation  (each  row  is  scored  one  by  one),  and  the  data  can  be  split  in\\nsome way so that several machines can each read a subset of the data and score a\\nsubset of the rows.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,256.036,432.005,304.336 'In terms of computation, scaling the number of models is somewhat simpler. The key\\nis to add more computing power and to make sure the monitoring infrastructure can\\nhandle the workload. But in terms of governance and processes, this is the most chal‐\\nlenging situation.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,199.636,432.003,247.936 'In particular, scaling the number of models means that the CI/CD pipeline must be\\nable  to  handle  large  numbers  of  deployments.  As  the  number  of  models  grows,  the\\nneed for automation and governance grows, as human verification cannot necessarily\\nbe systematic or consistent.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,105.436,432.005,191.536 'In some applications, it is possible to rely on fully automated continuous deployment\\nif the risks are well controlled by automated validation, canary releases, and automa‐\\nted canary analysis. There can be numerous infrastructure challenges since training,\\nbuilding  models,  validating  on  test  data,  etc.,  all  need  to  be  performed  on  clusters\\nrather than on a single machine. Also, with a higher number of models, the CI/CD\\npipeline of each model can vary widely, and if nothing is done, each team will have to\\ndevelop its own CI/CD pipeline for each model.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.000,74.236,432.003,97.336 'This is suboptimal from efficiency and governance perspectives. While some models\\nmay need highly specific validation pipelines, most projects can probably use a small\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,40.500,80.766,49.500 '82 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.344,40.500,91.656,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.234,40.500,202.761,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.996,519.436,432.004,605.537 'number of common patterns. In addition, maintenance is made much more complex\\nas  it  may  become  impractical  to  implement  a  new  systematic  validation  step,  for\\nexample,  since  the  pipelines  would  not  necessarily  share  a  common  structure  and\\nwould then be impossible to update safely, even programmatically. Sharing practices\\nand  standardized  pipelines  can  help  limit  complexity.  A  dedicated  tool  to  manage\\nlarge numbers of pipelines can also be used; for example, Netflix released Spinnaker,\\nan open source continuous deployment and infrastructure management platform.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.991,469.436,331.163,507.097 'Requirements and Challenges\\nWhen deploying a model, there are several possible scenarios:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.657,446.836,237.323,457.336 '• One model deployed on one server\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,430.236,260.885,440.736 '• One model deployed on multiple servers\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,413.636,310.214,424.136 '• Multiple versions of a model deployed on one server\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,397.036,333.775,407.536 '• Multiple versions of a model deployed on multiple servers\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,380.436,368.289,390.936 '• Multiple versions of multiple models deployed on multiple servers\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.997,332.636,432.000,368.336 'An effective logging system should be able to generate centralized datasets that can be\\nexploited by the model designer or the ML engineer, usually outside of the produc‐\\ntion environment. More specifically, it should cover all of the following situations:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,310.036,431.997,320.536 '• The system can access and retrieve scoring logs from multiple servers, either in a\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.001,297.436,324.634,307.936 'real-time scoring use case or in a batch scoring use case.\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.660,280.836,431.997,291.336 '• When a model is deployed on multiple servers, the system can handle the map‐\\n'>\n",
      "<LTTextBoxHorizontal(11) 90.002,268.236,363.453,278.736 'ping and aggregation of all information per model across servers.\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.660,226.436,432.002,262.136 '• When  different  versions  of  a  model  are  deployed,  the  system  can  handle  the\\nmapping  and  aggregation  of  all  information  per  version  of  the  model  across\\nservers.\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.996,128.236,432.005,214.336 'In terms of challenges, for large-scale machine learning applications, the number of\\nraw event logs generated can be an issue if there are no preprocessing steps in place to\\nfilter  and  aggregate  data.  For  real-time  scoring  use  cases,  logging  streaming  data\\nrequires  setting  up  a  whole  new  set  of  tooling  that  entails  a  significant  engineering\\neffort to maintain. However, in both cases, because the goal of monitoring is usually\\nto  estimate  aggregate  metrics,  saving  only  a  subset  of  the  predictions  may  be\\nacceptable.\\n'>\n",
      "<LTTextBoxHorizontal(14) 316.586,40.500,406.190,49.500 'Requirements and Challenges \\n'>\n",
      "<LTTextBoxHorizontal(15) 413.768,40.500,417.080,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 424.658,40.500,432.002,49.500 '83\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.995,507.312,432.004,607.973 'Closing Thoughts\\nDeploying  to  production  is  a  key  component  of  MLOps,  and  as  dissected  in  this\\nchapter,  having  the  right  processes  and  tools  in  place  can  ensure  that  it  happens\\nquickly. The good news is that many of the elements of success, particularly CI/CD\\nbest  practices,  are  not  new.  Once  teams  understand  how  they  can  be  applied  to\\nmachine learning models, the organization will have a good foundation on which to\\nexpand as MLOps scales with the business.\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.004,40.500,80.770,49.500 '84 \\n'>\n",
      "<LTTextBoxHorizontal(2) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(3) 99.238,40.500,202.765,49.500 'Chapter 6: Deploying to Production\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 176.167,533.502,431.998,582.331 'CHAPTER 7\\nMonitoring and Feedback Loop\\n'>\n",
      "<LTTextBoxHorizontal(1) 384.769,436.501,431.998,449.501 'Du Phan\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,347.468,432.003,408.369 'When a machine learning model is deployed in production, it can start degrading in\\nquality fast—and without warning—until it’s too late (i.e., it’s had a potentially nega‐\\ntive impact on the business). That’s why model monitoring is a crucial step in the ML\\nmodel life cycle and a critical piece of MLOps (illustrated in Figure 7-1 as a part of\\nthe overall life cycle).\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.004,157.678,414.294,180.778 'Figure 7-1. Monitoring and feedback loop highlighted in the larger context of the ML\\nproject life cycle\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.004,133.078,327.868,143.578 'Machine learning models need to be monitored at two levels:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.660,72.678,432.002,120.978 '• At  the  resource  level,  including  ensuring  the  model  is  running  correctly  in  the\\nproduction  environment.  Key  questions  include:  Is  the  system  alive?  Are  the\\nCPU, RAM, network usage, and disk space as expected? Are requests being pro‐\\ncessed at the expected rate?\\n'>\n",
      "<LTTextBoxHorizontal(6) 424.655,40.500,431.999,49.500 '85\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTLine 72.000,341.207,432.500,341.207>\n",
      "<LTLine 432.375,186.341,432.375,341.332>\n",
      "<LTLine 72.000,186.466,432.500,186.466>\n",
      "<LTLine 72.125,186.341,72.125,341.332>\n",
      "<LTFigure(I1) 103.329,192.321,401.171,336.082 matrix=[297.84,0.00,0.00,143.76, (103.33,192.32)]>\n",
      "<LTTextBoxHorizontal(0) 80.655,557.236,432.002,605.537 '• At the performance level, meaning monitoring the pertinence of the model over\\ntime. Key questions include: Is the model still an accurate representation of the\\npattern of new incoming data? Is it performing as well as it did during the design\\nphase?\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,433.836,432.005,545.136 'The first level is a traditional DevOps topic that has been extensively addressed in the\\nliterature (and has been covered in Chapter 6). However, the latter is more compli‐\\ncated.  Why?  Because  how  well  a  model  performs  is  a  reflection  of  the  data  used  to\\ntrain it; in particular, how representative that training data is of the live request data.\\nAs the world is constantly changing, a static model cannot catch up with new patterns\\nthat  are  emerging  and  evolving  without  a  constant  source  of  new  data.  While  it  is\\npossible to detect large deviations on single predictions (see Chapter 5), smaller but\\nstill significant deviations have to be detected statistically on datasets of scored rows,\\nwith or without ground truth.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,377.436,432.002,425.736 'Model performance monitoring attempts to track this degradation, and, at an appro‐\\npriate time, it will also trigger the retraining of the model with more representative\\ndata. This chapter delves into detail on how data teams should handle both monitor‐\\ning and subsequent retraining.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,302.236,432.002,365.097 'How Often Should Models Be Retrained?\\nOne  of  the  key  questions  teams  have  regarding  monitoring  and  retraining  is:  how\\noften  should  models  be  retrained?  Unfortunately,  there  is  no  easy  answer,  as  this\\nquestion depends on many factors, including:\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,282.636,121.052,293.136 'The domain\\n'>\n",
      "<LTTextBoxHorizontal(5) 89.997,207.036,432.004,280.536 'Models in areas like cybersecurity or real-time trading need to be updated regu‐\\nlarly to keep up with the constant changes inherent in these fields. Physical mod‐\\nels,  like  voice  recognition,  are  generally  more  stable,  because  the  patterns  don’t\\noften abruptly change. However, even more stable physical models need to adapt\\nto change: what happens to a voice recognition model if the person has a cough\\nand the tone of their voice changes?\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,187.436,104.913,197.936 'The cost\\n'>\n",
      "<LTTextBoxHorizontal(7) 89.997,149.636,432.002,185.336 'Organizations  need  to  consider  whether  the  cost  of  retraining  is  worth  the\\nimprovement in performance. For example, if it takes one week to run the whole\\ndata pipeline and retrain the model, is it worth a 1% improvement?\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.004,130.036,168.562,140.536 'The model performance\\n'>\n",
      "<LTTextBoxHorizontal(9) 89.995,92.236,432.001,127.936 'In some situations, the model performance is restrained by the limited number of\\ntraining examples, and thus the decision to retrain hinges on collecting enough\\nnew data.\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.004,40.500,80.770,49.500 '86 \\n'>\n",
      "<LTTextBoxHorizontal(11) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 99.238,40.500,222.331,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,519.436,432.004,605.537 'Whatever the domain, the delay to obtain the ground truth is key to defining a lower\\nbound to the retraining period. It is very risky to use a prediction model when there\\nis  a  possibility  that  it  drifts  faster  than  the  lag  between  prediction  time  and  ground\\ntruth obtention time. In this scenario, the model can start giving bad results without\\nany  recourse  other  than  to  withdraw  the  model  if  the  drift  is  too  significant.  What\\nthis means in practice is that it is unlikely a model with a lag of one year is retrained\\nmore than a few times a year.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,463.036,432.003,511.336 'For the same reason, it is unlikely that a model is trained on data collected during a\\nperiod  smaller  than  this  lag.  Retraining  will  not  be  performed  in  a  shorter  period,\\neither.  In  other  words,  if  the  model  retraining  occurs  way  more  often  than  the  lag,\\nthere will be almost no impact of the retraining on the performance of the model.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.002,431.836,432.005,454.936 'There  are  also  two  organizational  bounds  to  consider  when  it  comes  to  retraining\\nfrequency:\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.002,412.236,139.170,422.736 'An upper bound\\n'>\n",
      "<LTTextBoxHorizontal(4) 89.997,361.836,432.003,410.136 'It  is  better  to  perform  retraining  once  every  year  to  ensure  that  the  team  in\\ncharge has the skills to do it (despite potential turnover—i.e., the possibility that\\nthe people retraining the model were not the ones who built it) and that the com‐\\nputing toolchain is still up.\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.005,342.236,132.390,352.736 'A lower bound\\n'>\n",
      "<LTTextBoxHorizontal(6) 89.996,228.836,432.005,340.136 'Take, for example, a model with near-instantaneous feedback, such as a recom‐\\nmendation engine where the user clicks on the product offerings within seconds\\nafter the prediction. Advanced deployment schemes will involve shadow testing\\nor A/B testing to make sure that the model performs as anticipated. Because it is\\na statistical validation, it takes some time to gather the required information. This\\nnecessarily  sets  a  lower  bound  to  the  retraining  period.  Even  with  a  simple\\ndeployment,  the  process  will  probably  allow  for  some  human  validation  or  for\\nthe  possibility  of  manual  rollback,  which  means  it’s  unlikely  that  the  retraining\\nwill occur more than once a day.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,134.636,432.005,220.736 'Therefore, it is very likely that retraining will be done between once a day and once a\\nyear. The simplest solution that consists of retraining the model in the same way and\\nin the same environment it was trained in originally is acceptable. Some critical cases\\nmay require retraining in a production environment, even though the initial training\\nwas done in a design environment, but the retraining method is usually identical to\\nthe training method so that the overall complexity is limited. As always, there is an\\nexception to this rule: online learning.\\n'>\n",
      "<LTTextBoxHorizontal(8) 285.980,40.500,406.184,49.500 'How Often Should Models Be Retrained? \\n'>\n",
      "<LTTextBoxHorizontal(9) 413.762,40.500,417.074,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 424.652,40.500,431.996,49.500 '87\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.245,520.130,423.745,587.625 'Online Learning\\nSometimes, the use case requires teams to go further than the automation of the exist‐\\ning manual ML pipeline by using dedicated algorithms that can train themselves iter‐\\natively. (Standard algorithms, by contrast, are retrained from scratch most of the time,\\nwith the exception of deep learning algorithms.)\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.245,418.130,423.745,512.130 'While  conceptually  attractive,  these  algorithms  are  more  costly  to  set  up.  The\\ndesigner has to not only test the performance of the model on a test dataset, but also\\nqualify its behavior when data changes. (The latter is required because it’s difficult to\\nmitigate bad learning once the algorithm is deployed, and it’s hard to reproduce the\\nbehavior when each training recursively relies on the previous one because one needs\\nto replay all the steps to understand the bad behavior). In addition, these algorithms\\nare not stateless: running them twice on the same data will not give the same result\\nbecause they have learned from the first run.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.245,352.130,423.755,410.130 'There  is  no  standard  way—similar  to  cross-validation—to  do  this  process,  so  the\\ndesign costs will be higher. Online machine learning is a vivid branch of research with\\nsome  mature  technologies  like  state-space  models,  though  they  require  significant\\nskills  to  be  used  effectively.  Online  learning  is  typically  appealing  in  streaming  use\\ncases, though mini batches may be more than enough to handle it.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,280.486,432.004,328.787 'In any case, some level of model retraining is definitely necessary—it’s not a question\\nof  if,  but  of  when.  Deploying  ML  models  without  considering  retraining  would  be\\nlike launching an unmanned aircraft from Paris in the exact right direction and hop‐\\ning it will land safely in New York City without further control.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,173.686,432.004,272.386 'The good news is that if it was possible to gather enough data to train the model the\\nfirst  time,  then  most  of  the  solutions  for  retraining  are  already  available  (with  the\\npossible  exception  of  cross-trained  models  that  are  used  in  a  different  context—for\\nexample, trained with data from one country but used in another). It is therefore crit‐\\nical for organizations to have a clear idea of deployed models’ drift and accuracy by\\nsetting up a process that allows for easy monitoring and notifications. An ideal sce‐\\nnario would be a pipeline that automatically triggers checks for degradation of model\\nperformance.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,104.686,432.003,165.586 'It’s  important  to  note  that  the  goal  of  notifications  is  not  necessarily  to  kick  off  an\\nautomated process of retraining, validation, and deployment. Model performance can\\nchange  for  a  variety  of  reasons,  and  retraining  may  not  always  be  the  answer.  The\\npoint  is  to  alert  the  data  scientist  of  the  change;  that  person  can  then  diagnose  the\\nissue and evaluate the next course of action.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,73.486,432.004,96.586 'It is therefore critical that as part of MLOps and the ML model life cycle, data scien‐\\ntists  and  their  managers  and  the  organization  as  a  whole  (which  is  ultimately  the\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.004,40.500,80.770,49.500 '88 \\n'>\n",
      "<LTTextBoxHorizontal(8) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 99.238,40.500,222.331,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,597.375,432.000,597.375>\n",
      "<LTLine 431.875,340.750,431.875,597.500>\n",
      "<LTLine 72.000,340.875,432.000,340.875>\n",
      "<LTLine 72.125,340.750,72.125,597.500>\n",
      "<LTTextBoxHorizontal(0) 71.996,532.036,432.005,605.537 'entity that has to deal with the business consequences of degrading model perform‐\\nances and any subsequent changes) understand model degradation. Practically, every\\ndeployed  model  should  come  with  monitoring  metrics  and  corresponding  warning\\nthresholds  to  detect  meaningful  business  performance  drops  as  quickly  as  possible.\\nThe  following  sections  focus  on  understanding  these  metrics  to  be  able  to  define\\nthem for a particular model.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.992,444.236,432.003,519.697 'Understanding Model Degradation\\nOnce a machine learning model is trained and deployed in production, there are two\\napproaches  to  monitor  its  performance  degradation:  ground  truth  evaluation  and\\ninput  drift  detection.  Understanding  the  theory  behind  and  limitations  of  these\\napproaches is critical to determining the best strategy.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,361.027,432.003,432.666 'Ground Truth Evaluation\\nGround truth retraining requires waiting for the label event. For example, in a fraud\\ndetection model, the ground truth would be whether or not a specific transaction was\\nactually  fraudulent.  For  a  recommendation  engine,  it  would  be  whether  or  not  the\\ncustomer clicked on—or ultimately bought—one of the recommended products.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,304.627,432.005,352.927 'With the new ground truth collected, the next step is to compute the performance of\\nthe model based on ground truth and compare it with registered metrics in the train‐\\ning  phase.  When  the  difference  surpasses  a  threshold,  the  model  can  be  deemed  as\\noutdated, and it should be retrained.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.002,286.027,290.769,296.527 'The metrics to be monitored can be of two varieties:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.657,213.027,432.002,273.927 '• Statistical metrics like accuracy, ROC AUC, log loss, etc. As the model designer\\nhas probably already chosen one of these metrics to pick the best model, it is a\\nfirst-choice candidate for monitoring. For more complex models, where the aver‐\\nage performance is not enough, it may be necessary to look at metrics computed\\nby subpopulations.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.652,196.427,431.997,206.927 '• Business  metrics,  like  cost-benefit  assessment.  For  example,  the  credit  scoring\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.002,183.827,289.974,194.327 'business has developed its own specific metrics.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,123.427,432.005,171.727 'The  main  advantage  of  the  first  kind  of  metric  is  that  it  is  domain  agnostic,  so  the\\ndata  scientist  likely  feels  comfortable  setting  thresholds.  So  as  to  have  the  earliest\\nmeaningful warning, it is even possible to compute p-values to assess the probability\\nthat the observed drop is not due to random fluctuations.\\n'>\n",
      "<LTTextBoxHorizontal(9) 302.087,40.500,406.190,49.500 'Understanding Model Degradation \\n'>\n",
      "<LTTextBoxHorizontal(10) 413.768,40.500,417.080,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 424.658,40.500,432.002,49.500 '89\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 80.245,544.130,423.745,587.625 'A Stats Primer: From Null Hypothesis to p-Values\\nThe null hypothesis says that there is no relationship between the variables being com‐\\npared; any results are due to sheer chance.\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.245,502.130,423.745,536.130 'The alternative hypothesis says that the variables being compared are related, and the\\nresults  are  significant  in  supporting  the  theory  being  considered,  and  not  due  to\\nchance.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.245,460.130,423.745,494.130 'The  level  of  statistical  significance  is  often  expressed  as  a  p-value  between  0  and  1.\\nThe  smaller  the  p-value,  the  stronger  the  evidence  that  one  should  reject  the  null\\nhypothesis.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,375.886,432.002,436.787 'The  drawback  is  that  the  drop  may  be  statistically  significant  without  having  any\\nnoticeable  impact.  Or  worse,  the  cost  of  retraining  and  the  risk  associated  with  a\\nredeployment  may  be  higher  than  the  expected  benefits.  Business  metrics  are  far\\nmore  interesting  because  they  ordinarily  have  a  monetary  value,  enabling  subject\\nmatter experts to better handle the cost-benefit trade-off of the retraining decision.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,344.686,432.000,367.786 'When  available,  ground  truth  monitoring  is  the  best  solution.  However,  it  may  be\\nproblematic. There are three main challenges:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.652,259.086,432.005,332.586 '• Ground truth is not always immediately, or even imminently, available. For some\\ntypes of models, teams need to wait months (or longer) for ground truth labels to\\nbe available, which can mean significant economic loss if the model is degrading\\nquickly. As said before, deploying a model for which the drift is faster than the\\nlag  is  risky.  However,  by  definition,  drifts  are  not  forecastable,  so  models  with\\nlong lags need mitigation measures.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.657,166.886,432.005,252.986 '• Ground truth and prediction are decoupled. To compute the performance of the\\ndeployed model on new data, it’s necessary to be able to match ground truth with\\nthe corresponding observation. In many production environments, this is a chal‐\\nlenging task because these two pieces of information are generated and stored in\\ndifferent systems and at different timestamps. For low-cost or short-lived models,\\nit might not be worth automated ground truth collection. Note that this is rather\\nshort-sighted, because sooner or later, the model will need to be retrained.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.657,125.086,432.002,160.786 '• Ground truth is only partially available. In some situations, it is extremely expen‐\\nsive to retrieve the ground truth for all the observations, which means choosing\\nwhich samples to label and thus inadvertently introducing bias into the system.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.995,77.286,432.003,112.986 'For the last challenge, fraud detection presents a clear use case. Given that each trans‐\\naction needs to be examined manually and the process takes a long time, does it make\\nsense  to  establish  ground  truth  for  only  suspect  cases  (i.e.,  cases  where  the  model\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.003,40.500,80.769,49.500 '90 \\n'>\n",
      "<LTTextBoxHorizontal(10) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 99.237,40.500,222.330,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,597.375,432.000,597.375>\n",
      "<LTLine 431.875,448.750,431.875,597.500>\n",
      "<LTLine 72.000,448.875,432.000,448.875>\n",
      "<LTLine 72.125,448.750,72.125,597.500>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'gives  a  high  probability  of  fraud)?  At  first  glance,  the  approach  seems  reasonable;\\nhowever,  a  critical  mind  understands  that  this  creates  a  feedback  loop  that  will\\namplify the flaws of the model. Fraud patterns that were never captured by the model\\n(i.e.,  those  that  have  a  low  fraud  probability  according  to  the  model)  will  never  be\\ntaken into account in the retraining process.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,463.036,432.003,536.536 'One  solution  to  this  challenge  might  be  to  randomly  label,  establishing  a  ground\\ntruth  for  just  a  subsample  of  transactions  in  addition  to  those  that  were  flagged  as\\nsuspicious. Another solution might be to reweight the biased sample so that its char‐\\nacteristics  match  the  general  population  more  closely.  For  example,  if  the  system\\nawarded  little  credit  to  people  with  low  income,  the  model  should  reweight  them\\naccording to their importance in the applicant, or even in the general, population.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,406.636,432.005,454.936 'The  bottom  line  is  that  whatever  the  mitigation  measure,  the  labeled  sample  subset\\nmust cover all possible future predictions so that the trained model makes good pre‐\\ndictions whatever the sample; this will sometimes mean making suboptimal decisions\\nfor the sake of checking that the model continues to generalize well.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,350.236,432.003,398.536 'Once this problem is solved for retraining, the solution (reweighting, random sam‐\\npling) can be used for monitoring. Input drift detection complements this approach,\\nas it is needed to make sure that ground truth covering new, unexplored domains is\\nmade available to retrain the model.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,267.027,432.004,338.667 'Input Drift Detection\\nGiven the challenges and limitations of ground truth retraining presented in the pre‐\\nvious section, a more practical approach might be input drift detection. This section\\ntakes a brief but deep dive into the underlying logic behind drift and presents differ‐\\nent scenarios that can cause models and data to drift.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.998,223.227,432.001,258.927 'Say the goal is to predict the quality of Bordeaux wines using as training data the UCI\\nWine Quality dataset, which contains information about red and white variants of the\\nPortuguese wine vinho verde along with a quality score varying between 0 and 10.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,179.427,432.004,215.127 'The following features are provided for each wine: type, fixed acidity, volatile acidity,\\ncitric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density,\\npH, sulphates, and alcohol rate.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,135.627,431.998,171.327 'To simplify the modeling problem, say that a good wine is one with a quality score\\nequal to or greater than 7. The goal is thus to build a binary model that predicts this\\nlabel from the wine’s attributes.\\n'>\n",
      "<LTTextBoxHorizontal(8) 302.083,40.500,406.186,49.500 'Understanding Model Degradation \\n'>\n",
      "<LTTextBoxHorizontal(9) 413.764,40.500,417.076,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 424.654,40.500,431.998,49.500 '91\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.999,595.037,378.904,605.537 'To demonstrate data drift, we explicitly split the original dataset into two:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,572.437,431.997,582.937 '• wine_alcohol_above_11,  which  contains  all  wines  with  an  alcohol  rate  of  11%\\n'>\n",
      "<LTTextBoxHorizontal(2) 90.001,559.837,132.905,570.337 'and above\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.659,543.236,431.997,553.736 '• wine_alcohol_below_11,  which  contains  all  wines  with  an  alcohol  rate  below\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.001,530.636,108.019,541.136 '11%\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.998,482.836,432.005,518.536 'We split wine_alcohol_above_11 to train and score our model, and the second data‐\\nset, wine_alcohol_below_11, will be considered as new incoming data that needs to\\nbe scored once the model has been deployed.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,401.236,432.003,474.737 'We have artificially created a big problem: it is very unlikely that the quality of wine is\\nindependent from the alcohol level. Worse, the alcohol level is likely to be correlated\\ndifferently with the other features in the two datasets. As a result, what is learned on\\none dataset (“if the residual sugar is low and the pH is high, then the probability that\\nthe wine is good is high”) may be wrong on the other one because, for example, the\\nresidual sugar is not important anymore when the alcohol level is high.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.995,319.636,432.003,393.136 'Mathematically speaking, the samples of each dataset cannot be assumed to be drawn\\nfrom  the  same  distribution  (i.e.,  they  are  not  “identically  distributed”).  Another\\nmathematical  property  is  necessary  to  ensure  that  ML  algorithms  perform  as\\nexpected:  independence.  This  property  is  broken  if  samples  are  duplicated  in  the\\ndataset  or  if  it  is  possible  to  forecast  the  “next”  sample  given  the  previous  one,  for\\nexample.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.997,250.636,432.004,311.536 'Let’s  assume  that  despite  the  obvious  problems,  we  train  the  algorithm  on  the  first\\ndataset and then deploy it on the second one. The resulting distribution shift is called\\na drift. It will be called a feature drift if the alcohol level is one of the features used by\\nthe  ML  model  (or  if  the  alcohol  level  is  correlated  with  other  features  used  by  the\\nmodel) and a concept drift if it is not.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,175.436,432.004,238.296 'Drift Detection in Practice\\nAs  explained  previously,  to  be  able  to  react  in  a  timely  manner,  model  behavior\\nshould be monitored solely based on the feature values of the incoming data, without\\nwaiting for the ground truth to be available.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.995,119.036,432.003,167.336 'The logic is that if the data distribution (e.g., mean, standard deviation, correlations\\nbetween features) diverges between the training and testing phases1 on one side and\\nthe development phase on the other, it is a strong signal that the model’s performance\\nwon’t  be  the  same.  It  is  not  the  perfect  mitigation  measure,  as  retraining  on  the\\n'>\n",
      "<LTTextBoxHorizontal(11) 73.140,81.954,427.752,89.954 '1 It is also advisable to assess the drift between the training and the test dataset, especially when the test dataset\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.000,71.954,374.872,79.954 'is posterior to the training dataset. See “Choosing Evaluation Metrics” on page 51 for details.\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.003,40.500,80.769,49.500 '92 \\n'>\n",
      "<LTTextBoxHorizontal(14) 88.347,40.500,91.659,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 99.237,40.500,222.330,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,97.900,162.000,97.900>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'drifted dataset will not be an option, but it can be part of mitigation measures (e.g.,\\nreverting to a simpler model, reweighting).\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.000,537.027,271.903,570.867 'Example Causes of Data Drift\\nThere are two frequent root causes of data drift:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.659,388.427,432.005,524.928 '• Sample selection bias, where the training sample is not representative of the pop‐\\nulation.  For  instance,  building  a  model  to  assess  the  effectiveness  of  a  discount\\nprogram  will  be  biased  if  the  best  discounts  are  proposed  for  the  best  clients.\\n  Selection  bias  often  stems  from  the  data  collection  pipeline  itself.  In  the  wine\\nexample,  the  original  dataset  sample  with  alcohol  levels  above  11%  surely  does\\nnot represent the whole population of wines—this is sample selection at its best.\\nIt could have been mitigated if a few samples of wine with an alcohol level above\\n11% had been kept and reweighted according to the expected proportion in the\\npopulation of wines to be seen by the deployed model. Note that this task is eas‐\\nier said than done in real life, as the problematic features are often unknown or\\nmaybe even not available.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.656,283.627,432.005,382.327 '• Non-stationary environment, where training data collected from the source pop‐\\nulation  does  not  represent  the  target  population.  This  often  happens  for  time-\\ndependent tasks—such as forecasting use cases—with strong seasonality effects,\\nwhere learning a model over a given month won’t generalize to another month.\\nBack to the wine example: one can imagine a case where the original dataset sam‐\\nple only includes wines from a specific year, which might represent a particularly\\ngood (or bad) vintage. A model trained on this data may not generalize to other\\nyears.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,200.418,432.005,272.058 'Input Drift Detection Techniques\\nAfter understanding the possible situations that can cause different types of drift, the\\nnext  logical  question  is:  how  can  drift  be  detected?  This  section  presents  two  com‐\\nmon  approaches.  The  choice  between  them  depends  on  the  expected  level  of\\ninterpretability.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,131.418,432.004,192.318 'Organizations  that  need  proven  and  explainable  methods  should  prefer  univariate\\nstatistical tests. If complex drift involving several features simultaneously is expected,\\nor if the data scientists want to reuse what they already know and assuming the orga‐\\nnization doesn’t dread the black box effect, the domain classifier approach may be a\\ngood option, too.\\n'>\n",
      "<LTTextBoxHorizontal(6) 329.080,40.500,406.192,49.500 'Drift Detection in Practice \\n'>\n",
      "<LTTextBoxHorizontal(7) 413.770,40.500,417.082,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 424.660,40.500,432.004,49.500 '93\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,596.229,170.734,607.789 'Univariate statistical tests\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,553.105,432.002,588.805 'This method requires applying a statistical test on data from the source distribution\\nand the target distribution for each feature. A warning will be raised when the results\\nof those tests are significant.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,521.904,431.997,545.005 'The choice of hypothesis tests have been extensively studied in the literature, but the\\nbasic approaches rely on these two tests:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.659,474.105,432.002,509.805 '• For  continuous  features,  the  Kolmogorov-Smirnov  test  is  a  nonparametric\\nhypothesis  test  that  is  used  to  check  whether  two  samples  come  from  the  same\\ndistribution. It measures a distance between the empirical distribution functions.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.654,432.304,432.002,468.005 '• For  categorical  features,  the  Chi-squared  test  is  a  practical  choice  that  checks\\nwhether  the  observed  frequencies  for  a  categorical  feature  in  the  target  data\\nmatch the expected frequencies seen from the source data.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,321.504,432.005,420.204 'The  main  advantage  of  p-values  is  that  they  help  detect  drift  as  quickly  as  possible.\\nThe main drawback is that they detect an effect, but they do not quantify the level of\\nthe effect (i.e., on large datasets, they detect very small changes, which may be com‐\\npletely without impact). As a result, if development datasets are very large, it is neces‐\\nsary  to  complement  p-values  with  business-significant  metrics.  For  example,  on  a\\nsufficiently large dataset, the average age may have significantly drifted from a statisti‐\\ncal perspective, but if the drift is only a few months, this is probably an insignificant\\nvalue for many business use cases.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,297.137,136.616,308.697 'Domain classifier\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,228.812,432.004,289.712 'In this approach, data scientists train a model that tries to discriminate between the\\noriginal dataset (input features and, optionally, predicted target) and the development\\ndataset. In other words, they stack the two datasets and train a classifier that aims at\\npredicting the data’s origin. The performance of the model (its accuracy, for example)\\ncan then be considered as a metric for the drift level.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.995,159.812,432.005,220.712 'If this model is successful in its task, and thus has a high drift score, it implies that the\\ndata used at training time and the new data can be distinguished, so it’s fair to say that\\nthe new data has drifted. To gain more insights, in particular to identify the features\\nthat  are  responsible  for  the  drift,  one  can  use  the  feature  importance  of  the  trained\\nmodel.\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.004,135.445,163.501,147.005 'Interpretation of results\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.995,79.720,432.004,128.020 'Both domain classifier and univariate statistical tests point to the importance of fea‐\\ntures  or  of  the  target  to  explain  drift.  Drift  attributed  to  the  target  is  important  to\\nidentify because it often directly impacts the bottom line of the business. (Think, for\\nexample, of credit scores: if the scores are lower overall, the number of awarded loans\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.004,40.500,80.770,49.500 '94 \\n'>\n",
      "<LTTextBoxHorizontal(12) 88.348,40.500,91.660,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 99.238,40.500,222.331,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'is likely to be lower, and therefore revenue will be lower.) Drift attributed to features\\nis useful to mitigate the impact of drift, as it may hint at the need for:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.652,534.636,432.002,570.337 '• Reweighting according to this feature (e.g., if customers above 60 now represent\\n60% of users but were only 30% in the training set, then double their weight and\\nretrain the model)\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.654,518.036,334.332,528.537 '• Removing the feature and training a new model without it\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,457.637,432.005,505.937 'In all cases, it is very unlikely that automatic actions exist if drift is detected. It could\\nhappen  if  it  is  costly  to  deploy  retrained  models:  the  model  would  be  retrained  on\\nnew data only if performance based on ground truth had dropped or significant drift\\nwas detected. In this peculiar case, new data is indeed available to mitigate the drift.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.993,382.436,432.004,445.297 'The Feedback Loop\\nAll effective machine learning projects implement a form of data feedback loop; that\\nis, information from the production environment flows back to the model prototyp‐\\ning environment for further improvement.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,288.236,432.002,374.336 'One can see in Figure 7-2 that data collected in the monitoring and feedback loop is\\nsent  to  the  model  development  phase  (details  about  this  data  are  covered  in  Chap‐\\nter 6). From there, the system analyzes whether the model is working as expected. If it\\nis, no action is required. If the model’s performance is degrading, an update will be\\ntriggered, either automatically or manually by the data scientist. In practice, as seen at\\nthe beginning of this chapter, this usually means either retraining the model with new\\nlabeled data or developing a new model with additional features.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,89.686,366.448,100.186 'Figure 7-2. Continuous delivery for end-to-end machine learning process\\n'>\n",
      "<LTTextBoxHorizontal(7) 348.673,40.500,406.192,49.500 'The Feedback Loop \\n'>\n",
      "<LTTextBoxHorizontal(8) 413.770,40.500,417.082,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 424.660,40.500,432.004,49.500 '95\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,281.975,432.500,281.975>\n",
      "<LTLine 432.375,105.750,432.375,282.100>\n",
      "<LTLine 72.000,105.875,432.500,105.875>\n",
      "<LTLine 72.125,105.750,72.125,282.100>\n",
      "<LTFigure(I1) 79.450,111.730,425.050,276.850 matrix=[345.60,0.00,0.00,165.12, (79.45,111.73)]>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'In either case, the goal is to be able to capture the emerging patterns and make sure\\nthat the business is not negatively impacted. This infrastructure is comprised of three\\nmain components, which in addition to the concepts discussed in the first part of this\\nchapter, are critical to robust MLOps capabilities:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.657,534.637,370.840,545.137 '• A logging system that collects data from several production servers\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,518.036,431.997,528.537 '• A model evaluation store that does versioning and evaluation between different\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,505.437,153.842,515.937 'model versions\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,476.237,431.997,499.337 '• An  online  system  that  does  model  comparison  on  production  environments,\\neither with the shadow scoring (champion/challenger) setup or with A/B testing\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.002,441.036,432.005,464.137 'The following sections address each of these components individually, including their\\npurpose, key features, and challenges.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,357.827,432.005,429.467 'Logging\\nMonitoring a live system, with or without machine learning components, means col‐\\nlecting and aggregating data about its states. Nowadays, as production infrastructures\\nare  getting  more  and  more  complex,  with  several  models  deployed  simultaneously\\nacross several servers, an effective logging system is more important than ever.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,301.427,431.999,349.727 'Data from these environments needs to be centralized to be analyzed and monitored,\\neither automatically or manually. This will enable continuous improvement of the ML\\nsystem. An event log of a machine learning system is a record with a timestamp and\\nthe following information.\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.003,281.827,139.098,292.327 'Model metadata\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.000,269.227,271.976,279.727 'Identification of the model and the version.\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.003,249.627,125.480,260.127 'Model inputs\\n'>\n",
      "<LTTextBoxHorizontal(11) 89.999,211.827,432.005,247.527 'Feature  values  of  new  observations,  which  allow  for  verification  of  whether  the\\nnew incoming data is what the model was expecting and thus allowing for detec‐\\ntion of data drift (as explained in the previous section).\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.002,192.227,130.833,202.727 'Model outputs\\n'>\n",
      "<LTTextBoxHorizontal(13) 89.997,154.427,432.003,190.127 'Predictions made by the model that, along with the ground truth collected later\\non,  give  a  concrete  idea  about  the  model  performance  in  a  production\\nenvironment.\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.000,134.827,128.186,145.327 'System action\\n'>\n",
      "<LTTextBoxHorizontal(15) 89.996,84.427,432.002,132.727 'It’s rare that the model prediction is the end product of a machine learning appli‐\\ncation; the more common situation is that the system will take an action based on\\nthis prediction. For example, in a fraud detection use case, when the model gives\\nhigh probability, the system can either block the transaction or send a warning to\\n'>\n",
      "<LTTextBoxHorizontal(16) 71.996,40.500,80.762,49.500 '96 \\n'>\n",
      "<LTTextBoxHorizontal(17) 88.340,40.500,91.652,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(18) 99.230,40.500,222.323,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 89.997,582.437,431.997,605.537 'the bank. This type of information is important because it affects the user reac‐\\ntion and thus indirectly affects the feedback data.\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.005,562.837,148.634,573.337 'Model explanation\\n'>\n",
      "<LTTextBoxHorizontal(2) 89.996,499.836,432.005,560.736 'In  some  highly  regulated  domains  such  as  finance  or  healthcare,  predictions\\nmust come with an explanation (i.e., which features have the most influence on\\nthe  prediction).  This  kind  of  information  is  usually  computed  with  techniques\\nsuch  as  Shapley  value  computation  and  should  be  logged  to  identify  potential\\nissues with the model (e.g., bias, overfitting).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,416.627,432.003,488.267 'Model Evaluation\\nOnce the logging system is in place, it periodically fetches data from the production\\nenvironment for monitoring. Everything goes well until one day the data drift alert is\\ntriggered: the incoming data distribution is drifting away from the training data dis‐\\ntribution. It’s possible that the model performance is degrading.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,322.427,432.004,408.527 'After  review,  data  scientists  decide  to  improve  the  model  by  retraining  it,  using  the\\ntechniques  described  earlier  in  this  chapter.  With  several  trained  candidate  models,\\nthe  next  step  is  to  compare  them  with  the  deployed  model.  In  practice,  this  means\\nevaluating all the models (the candidates as well as the deployed model) on the same\\ndataset.  If  one  of  the  candidate  models  outperforms  the  deployed  model,  there  are\\ntwo  ways  to  proceed:  either  update  the  model  on  the  production  environment  or\\nmove to an online evaluation via a champion/challenger or A/B testing setup.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,291.227,431.999,314.327 'In a nutshell, this is the notion of model store. It is a structure that allows data scien‐\\ntists to:\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.651,268.627,431.997,279.127 '• Compare  multiple,  newly  trained  model  versions  against  existing  deployed\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.002,256.027,124.841,266.527 'versions\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,239.427,431.997,249.927 '• Compare  completely  new  models  against  versions  of  other  models  on  labeled\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.001,226.827,107.767,237.327 'data\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.659,210.227,241.638,220.727 '• Track model performance over time\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,149.827,432.005,198.127 'Formally,  the  model  evaluation  store  serves  as  a  structure  that  centralizes  the  data\\nrelated to model life cycle to allow comparisons (though note that comparing models\\nmakes sense only if they address the same problem). By definition, all these compari‐\\nsons are grouped under the umbrella of a logical model.\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.002,125.460,124.865,137.020 'Logical model\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.995,69.735,432.002,118.035 'Building  a  machine  learning  application  is  an  iterative  process,  from  deploying  to\\nproduction,  monitoring  performance,  retrieving  data,  and  looking  for  ways  to\\nimprove how the system addresses the target problem. There are many ways to iter‐\\nate, some of which have already been discussed in this chapter, including:\\n'>\n",
      "<LTTextBoxHorizontal(14) 348.673,40.500,406.192,49.500 'The Feedback Loop \\n'>\n",
      "<LTTextBoxHorizontal(15) 413.770,40.500,417.082,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 424.660,40.500,432.004,49.500 '97\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.655,591.037,256.695,601.537 '• Retraining the same model on new data\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,574.437,232.272,584.937 '• Adding new features to the model\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,557.837,206.158,568.337 '• Developing new algorithms\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,510.036,432.000,545.736 'For  those  reasons,  the  machine  learning  model  itself  is  not  a  static  object;  it  con‐\\nstantly changes with time. It is therefore helpful to have a higher abstraction level to\\nreason about machine learning applications, which is referred to as a logical model.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,390.636,432.005,501.936 'A  logical  model  is  a  collection  of  model  templates  and  their  versions  that  aims  to\\nsolve a business problem. A model version is obtained by training a model template\\non a given dataset. All versions of model templates of the same logical model can usu‐\\nally be evaluated on the same kinds of datasets (i.e., on datasets with the same feature\\ndefinition and/or schema); however, this may not be the case if the problem did not\\nchange  but  the  features  available  to  solve  it  did.  Model  versions  could  be  imple‐\\nmented  using  completely  different  technologies,  and  there  could  even  be  several\\nimplementations of the same model version (Python, SQL, Java, etc.); regardless, they\\nare supposed to give the same prediction if given the same input.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,296.436,432.003,382.536 'Let’s  get  back  to  the  wine  example  introduced  earlier  in  this  chapter.  Three  months\\nafter  deployment,  there  is  new  data  about  less  alcoholic  wine.  We  can  retrain  our\\nmodel  on  the  new  data,  thus  obtaining  a  new  model  version  using  the  same  model\\ntemplate. While investigating the result, we discover new patterns are emerging. We\\nmay  decide  to  create  new  features  that  capture  this  information  and  add  it  to  the\\nmodel, or we may decide to use another ML algorithm (like deep learning) instead of\\nXGBoost. This would result in a new model template.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.005,277.836,352.775,288.336 'As a result, our model has two model templates and three versions:\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,255.236,409.134,265.736 '• The first version is live in production, based on the original model template.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,238.636,417.292,249.136 '• The second version is based on the original template, but trained on new data.\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,222.036,431.997,232.536 '• The third version uses the deep learning–based template with additional features\\n'>\n",
      "<LTTextBoxHorizontal(10) 90.002,209.436,314.849,219.936 'and is trained on the same data as the second version.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.999,161.636,432.005,197.336 'The information about the evaluation of these versions on various datasets (both the\\ntest datasets used at training time and the development datasets that may be scored\\nafter training) is then stored in the model evaluation store.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.998,137.269,159.612,148.829 'Model evaluation store\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.997,94.144,432.000,129.844 'As a reminder, model evaluation stores are structures that centralize the data related\\nto model life cycles to allow comparisons. The two main tasks of a model evaluation\\nstore are:\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.998,40.500,80.764,49.500 '98 \\n'>\n",
      "<LTTextBoxHorizontal(15) 88.342,40.500,91.654,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 99.232,40.500,222.325,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.655,565.837,432.002,601.537 '• Versioning the evolution of a logical model through time. Each logged version of\\nthe  logical  model  must  come  with  all  the  essential  information  concerning  its\\ntraining phase, including:\\n'>\n",
      "<LTTextBoxHorizontal(1) 91.068,549.236,202.473,559.737 '— The list of features used\\n'>\n",
      "<LTTextBoxHorizontal(2) 91.069,532.637,359.910,543.137 '— The preprocessing techniques that are applied to each feature\\n'>\n",
      "<LTTextBoxHorizontal(3) 91.069,516.036,355.678,526.537 '— The algorithm used, along with the chosen hyperparameters\\n'>\n",
      "<LTTextBoxHorizontal(4) 91.069,499.437,187.332,509.937 '— The training dataset\\n'>\n",
      "<LTTextBoxHorizontal(5) 91.069,482.837,431.996,493.337 '— The test dataset used to evaluate the trained model (this is necessary for the\\n'>\n",
      "<LTTextBoxHorizontal(6) 103.000,470.237,216.389,480.737 'version comparison phase)\\n'>\n",
      "<LTTextBoxHorizontal(7) 91.073,453.637,181.630,464.137 '— Evaluation metrics\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,411.836,432.001,447.537 '• Comparing  the  performance  between  different  versions  of  a  logical  model.  To\\ndecide which version of a logical model to deploy, all of them (the candidates and\\nthe deployed one) must be evaluated on the same dataset.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,326.236,432.003,399.736 'The choice of dataset to evaluate is crucial. If there is enough new labeled data to give\\na reliable estimation of the model performance, this is the preferred choice because it\\nis closest to what we are expecting to receive in the production environment. Other‐\\nwise, we can use the original test dataset of the deployed model. Assuming that the\\ndata has not drifted, this gives us a concrete idea about the performance of the candi‐\\ndate models compared to the original model.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,257.236,432.005,318.136 'After identifying the best candidate model, the job is not yet done. In practice, there\\nis often a substantial discrepancy between the offline and online performance of the\\nmodels. Therefore, it’s critical to take the testing to the production environment. This\\nonline  evaluation  gives  the  most  truthful  feedback  about  the  behavior  of  the  candi‐\\ndate model when facing real data.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.996,186.627,431.999,245.667 'Online Evaluation\\nOnline evaluation of models in production is critical from a business perspective, but\\ncan  be  challenging  from  a  technical  perspective.  There  two  main  modes  of  online\\nevaluation:\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.659,164.027,431.997,174.527 '• Champion/challenger (otherwise known as shadow testing), where the candidate\\n'>\n",
      "<LTTextBoxHorizontal(13) 90.002,151.427,380.726,161.927 'model shadows the deployed model and scores the same live requests\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.660,134.827,431.997,145.327 '• A/B testing, where the candidate model scores a portion of the live requests and\\n'>\n",
      "<LTTextBoxHorizontal(15) 90.001,122.227,245.664,132.727 'the deployed model scores the others\\n'>\n",
      "<LTTextBoxHorizontal(16) 72.002,87.027,432.005,110.127 'Both  cases  require  ground  truth,  so  the  evaluation  will  necessarily  take  longer  than\\nthe  lag  between  prediction  and  ground  truth  obtention.  In  addition,  whenever\\n'>\n",
      "<LTTextBoxHorizontal(17) 348.671,40.500,406.190,49.500 'The Feedback Loop \\n'>\n",
      "<LTTextBoxHorizontal(18) 413.767,40.500,417.080,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(19) 424.658,40.500,432.002,49.500 '99\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'shadow testing is possible, it should be used over A/B testing because it is far simpler\\nto understand and set up, and it detects differences more quickly.\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.001,558.069,154.967,569.629 'Champion/Challenger\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,477.145,432.004,550.644 'Champion/challenger involves deploying one or several additional models (the chal‐\\nlengers)  to  the  production  environment.  These  models  receive  and  score  the  same\\nincoming requests as the active model (the champion). However, they do not return\\nany  response  or  prediction  to  the  system:  that’s  still  the  job  of  the  old  model.  The\\npredictions  are  simply  logged  for  further  analysis.  That’s  why  this  method  is  also\\ncalled “shadow testing” or “dark launch.”\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.003,458.544,208.230,469.044 'This setup allows for two things:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.658,385.544,432.002,446.444 '• Verification that the performance of the new models is better than, or at least as\\ngood  as,  the  old  model.  Because  the  two  models  are  scoring  on  the  same  data,\\nthere  is  a  direct  comparison  of  their  accuracy  in  the  production  environment.\\nNote that this could also be done offline by using the new models on the dataset\\nmade of new requests scored by the champion model.\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.652,318.544,432.001,379.444 '• Measurement  of  how  the  new  models  handle  realistic  load.  Because  the  new\\nmodels  can  have  new  features,  new  preprocessing  techniques,  or  even  a  new\\nalgorithm, the prediction time for a request won’t be the same as that of the origi‐\\nnal model, and it is important to have a concrete idea of this change. Of course,\\nthis is the main advantage of doing it online.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,245.544,432.005,306.444 'The other advantage of this deployment scheme is that the data scientist or the ML\\nengineer  is  giving  visibility  to  other  stakeholders  on  the  future  champion  model:\\ninstead of being locked in the data science environment, the challenger model results\\nare exposed to the business leaders, which decreases the perceived risk to switch to a\\nnew model.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,189.144,432.003,237.444 'To be able to compare the champion and the challenger models, the same informa‐\\ntion must be logged for both, including input data, output data, processing time, etc.\\nThis means updating the logging system so that it can differentiate between the two\\nsources of data.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,69.744,432.005,181.044 'How long should both models be deployed before it’s clear that one is better than the\\nother?  Long  enough  that  the  metric  fluctuations  due  to  randomness  are  dampened\\nbecause  enough  predictions  have  been  made.  This  can  be  assessed  graphically  by\\nchecking  that  the  metric  estimations  are  not  fluctuating  anymore  or  by  doing  a\\nproper statistical test (as most metrics are averages of row-wise scores, the most usual\\ntest is a paired sample T-test) that yields the probability that the observation that one\\nmetric  is  higher  than  the  other  is  due  to  these  random  fluctuations.  The  wider  the\\nmetric  difference,  the  fewer  predictions  necessary  to  ensure  that  the  difference  is\\nsignificant.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.998,40.500,84.436,49.500 '100 \\n'>\n",
      "<LTTextBoxHorizontal(10) 92.014,40.500,95.326,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 102.904,40.500,225.997,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Depending on the use case and the implementation of the champion/challenger sys‐\\ntem,  server  performance  can  be  a  concern.  If  two  memory-intensive  models  are\\ncalled synchronously, they can slow the system down. This will not only have a nega‐\\ntive impact on the user experience but also corrupt the data collected about the func‐\\ntioning of the models.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,450.436,432.005,536.536 'Another concern is communication with the external system. If the two models use\\nan external API to enrich their features, that doubles the number of requests to these\\nservices, thus doubling costs. If that API has a caching system in place, then the sec‐\\nond  request  will  be  processed  much  faster  than  the  first,  which  can  bias  the  result\\nwhen comparing the total prediction time of the two models. Note that the challenger\\nmay be used only for a random subset of the incoming requests, which will alleviate\\nthe load at the expense of increased time before a conclusion can be drawn.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.999,419.236,432.002,442.336 'Finally,  when  implementing  a  challenger  model,  it’s  important  to  ensure  it  doesn’t\\nhave any influence on the system’s actions. This implies two scenarios:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.654,371.436,432.001,407.136 '• When  the  challenger  model  encounters  an  unexpected  issue  and  fails,  the  pro‐\\nduction environment will not experience any discontinuation or degradation in\\nterms of response time.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.654,317.036,432.002,365.336 '• Actions  taken  by  the  system  depend  only  on  the  prediction  of  the  champion\\nmodel,  and  they  happen  only  once.  For  example,  in  a  fraud  detection  use  case,\\nimagine that by mistake the challenger model is plugged directly into the system,\\ncharging each transaction twice—a catastrophic scenario.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,269.236,432.003,304.936 'In general, some effort needs to be spent on the logging, monitoring, and serving sys‐\\ntem to ensure the production environment functions as usual and is not impacted by\\nany issues coming from the challenger model.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.003,244.869,114.509,256.429 'A/B testing\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,201.744,432.003,237.444 'A/B testing (a randomized experiment testing two variants, A and B) is a widely used\\ntechnique  in  website  optimization.  For  ML  models,  it  should  be  used  only  when\\nchampion/challenger is not possible. This might happen when:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,78.344,432.005,189.644 '• The ground truth cannot be evaluated for both models. For example, for a rec‐\\nommendation engine, the prediction gives a list of items on which a given cus‐\\ntomer is likely to click if they are presented. Therefore, it is impossible to know if\\nthe customer would have clicked if an item was not presented. In this case, some\\nkind of A/B testing will have to be done, in which some customers will be shown\\nthe recommendations of model A, and some the recommendations of model B.\\nSimilarly,  for  a  fraud  detection  model,  because  heavy  work  is  needed  to  obtain\\nthe ground truth, it may not be possible to do so for the positive predictions of\\ntwo models; it would increase the workload too much, because some frauds are\\n'>\n",
      "<LTTextBoxHorizontal(9) 344.999,40.500,402.518,49.500 'The Feedback Loop \\n'>\n",
      "<LTTextBoxHorizontal(10) 410.096,40.500,413.408,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 420.986,40.500,432.002,49.500 '101\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 89.997,582.437,431.997,605.537 'detected by only one model. As a result, randomly applying only the B model to a\\nsmall fraction of the requests will allow the workload to remain constant.\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.660,502.837,432.004,576.337 '• The objective to optimize is only indirectly related to the performance of the pre‐\\ndiction. Imagine an ad engine based on an ML model that predicts if a user will\\nclick on the ad. Now imagine that it is evaluated on the buy rate, i.e., whether the\\nuser  bought  the  product  or  service.  Once  again,  it  is  not  possible  to  record  the\\nreaction  of  the  user  for  two  different  models,  so  in  this  case,  A/B  testing  is  the\\nonly way.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,417.236,432.003,490.737 'Entire books are dedicated to A/B testing, so this section presents only its main idea\\nand  a  simple  walkthrough.  Unlike  the  champion/challenger  framework,  with  A/B\\ntesting, the candidate model returns predictions for certain requests, and the original\\nmodel handles the other requests. Once the test period is over, statistical tests com‐\\npare the performance of the two models, and teams can make a decision based on the\\nstatistical significance of those tests.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,386.036,431.996,409.136 'In an MLOps context, some considerations need to be made. A walkthrough of these\\nconsiderations is presented in Table 7-1.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.000,361.530,271.440,371.530 'Table 7-1. Considerations for A/B testing in MLOps\\n'>\n",
      "<LTTextBoxHorizontal(5) 75.600,321.900,104.526,354.500 'Stage\\nBefore the\\nA/B test\\n'>\n",
      "<LTTextBoxHorizontal(6) 113.817,221.250,427.728,354.500 'MLOps consideration\\nDefine a clear goal: A quantitative business metric that needs to be optimized, such as click-through rate.\\nDefine a precise population: Carefully choose a segment for the test along with a splitting strategy that assures\\nno bias between groups. (This is the so-called experimental design or randomized control trial that’s been\\npopularized by drug studies.) This may be a random split, or it may be more complex. For example, the situation\\nmight dictate that all the requests of a particular customer are handled by the same model.\\nDefine the statistical protocol: The resulting metrics are compared using statistical tests, and the null hypothesis\\nis either rejected or retained. To make the conclusion robust, teams need to define beforehand the sample size\\nfor the desired minimum effect size, which is the minimum difference between the two models’ performance\\nmetrics. Teams must also fix a test duration (or alternatively have a method to handle multiple tests). Note that\\nwith similar sample sizes, the power to detect meaningful differences will be lower than with champion/\\nchallenger because unpaired sample tests have to be used. (It is usually impossible to match each request scored\\nwith model B to a request scored with model A, whereas with champion/challenger, this is trivial.)\\n'>\n",
      "<LTTextBoxHorizontal(7) 75.600,196.850,105.228,216.650 'During the\\nA/B test\\n'>\n",
      "<LTTextBoxHorizontal(8) 75.600,162.200,100.278,182.000 'After the\\nA/B test\\n'>\n",
      "<LTTextBoxHorizontal(9) 113.817,151.400,426.693,216.650 'It is important not to stop the experiment before the test duration is over, even if the statistical test starts to\\nreturn a significant metric difference. This practice (also called p-hacking) produces unreliable and biased results\\ndue to cherry-picking the desired outcome.\\nOnce the test duration is over, check the collected data to make sure the quality is good. From there, run the\\nstatistical tests; if the metric difference is statistically significant in favor of the candidate model, the original\\nmodel can be replaced with the new version.\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.000,40.500,84.438,49.500 '102 \\n'>\n",
      "<LTTextBoxHorizontal(11) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 102.906,40.500,225.999,49.500 'Chapter 7: Monitoring and Feedback Loop\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,343.600,110.217,357.400>\n",
      "<LTRect 110.217,343.600,432.000,357.400>\n",
      "<LTLine 72.000,218.675,110.317,218.675>\n",
      "<LTLine 110.117,218.675,432.000,218.675>\n",
      "<LTLine 72.000,184.025,110.317,184.025>\n",
      "<LTLine 110.117,184.025,432.000,184.025>\n",
      "<LTLine 72.000,149.375,110.317,149.375>\n",
      "<LTLine 110.117,149.375,432.000,149.375>\n",
      "<LTTextBoxHorizontal(0) 71.995,519.912,432.003,607.973 'Closing Thoughts\\nOrdinary software is built to satisfy specifications. Once an application is deployed,\\nits ability to fulfill its objective does not degrade. ML models, by contrast, have objec‐\\ntives  statistically  defined  by  their  performance  on  a  given  dataset.  As  a  result,  their\\nperformance changes, usually for the worse, when the statistical properties of the data\\nchange.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,400.512,432.004,511.812 'In  addition  to  ordinary  software  maintenance  needs  (bug  correction,  release\\nupgrades,  etc.),  this  performance  drift  has  to  be  carefully  monitored.  We  have  seen\\nthat  performance  monitoring  based  on  the  ground  truth  is  the  cornerstone,  while\\ndrift monitoring can provide early warning signals. Among possible drift mitigation\\nmeasures, the workhorse is definitely retraining on new data, while model modifica‐\\ntion remains an option. Once a new model is ready to be deployed, its improved per‐\\nformance  can  be  validated  thanks  to  shadow  scoring  or,  as  a  second  choice,  A/B\\ntesting.  This  enables  proving  that  the  new  model  is  better  in  order  to  improve  the\\nperformance of the system. \\n'>\n",
      "<LTTextBoxHorizontal(2) 350.368,40.500,402.514,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(3) 410.092,40.500,413.404,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(4) 420.982,40.500,431.998,49.500 '103\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 278.283,533.502,431.998,582.331 'CHAPTER 8\\nModel Governance\\n'>\n",
      "<LTTextBoxHorizontal(1) 364.515,436.501,431.998,449.501 'Mark Treveil\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,347.468,432.003,408.369 'We explored the idea of governance as a set of controls placed on a business in Chap‐\\nter 3. These goals aim to ensure that the business delivers on its responsibilities to all\\nstakeholders,  from  shareholders  and  employees  to  the  public  and  national  govern‐\\nments. The responsibilities include financial, legal, and ethical, and are all underpin‐\\nned by the desire for fairness.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.001,316.268,432.004,339.368 'This chapter goes even more in depth on these topics, shifting from why they matter\\nto how organizations can incorporate them as a part of their MLOps strategy.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,215.868,432.004,303.929 'Who Decides What Governance the Organization Needs?\\nNational regulations are a key part of a society’s framework for safeguarding fairness. \\nBut  these  take  considerable  time  to  be  agreed  upon  and  implemented;  they  always\\nreflect a slightly historical understanding of fairness and the challenges to it. Just as\\nwith  ML  models,  the  past  cannot  always  anticipate  the  evolving  problems  of  the\\nfuture.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,134.268,432.005,207.768 'What most businesses want from governance is to safeguard shareholder investment\\nand to help ensure a suitable ROI, both now and in the future. That means the busi‐\\nness has to perform effectively, profitability, and sustainably. The shareholders need\\nclear visibility that customers, employees, and regulatory bodies are happy, and they\\nwant reassurances that appropriate measures are in place to detect and manage any\\ndifficulties that could occur in the future.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,77.868,432.003,126.168 'None of this is news, of course, nor specific to MLOps. What is different with ML is\\nthat it is a new and often opaque technology that carries many risks, but it is rapidly\\nbeing  embedded  in  decision-making  systems  that  impact  every  aspect  of  our  lives.\\nML  systems  invent  their  own  statistically  driven  decision-making  processes,  often\\n'>\n",
      "<LTTextBoxHorizontal(7) 420.987,40.500,432.003,49.500 '105\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'extremely difficult to understand, based on large volumes of data that is thought to\\nrepresent the real world. It’s not hard to see what could go wrong!\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.436,432.005,574.337 'Perhaps  the  most  surprising  influence  on  the  direction  of  ML  governance  is  public\\nopinion, which evolves much faster than formal regulation. It follows no formal pro‐\\ncess or etiquette. It doesn’t have to be based on fact or reason. Public opinion deter‐\\nmines what products people buy, where they invest their money, and what rules and\\nregulations governments make. Public opinion decides what is fair and what is not.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,419.236,432.005,505.336 'For  example,  the  agricultural  biotechnology  companies  that  developed  genetically\\nmodified  crops  felt  the  power  of  public  opinion  painfully  in  the  1990s.  While  the\\narguments rage back and forth about whether there was, or was not, a risk to health,\\npublic opinion in Europe swung against genetic modification, and these crops were\\nbanned in many European countries. The parallels with ML are clear: ML offers ben‐\\nefits  to  all  and  yet  brings  risks  that  need  to  be  managed  if  the  public  is  to  trust  it.\\nWithout public trust, the benefits will not fully materialize.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,337.636,432.003,411.136 'The general public needs to be reassured that ML is fair. What is considered “fair” is\\nnot defined in a rule book, and it is not fixed; it will fluctuate based on events, and it\\nwill not always be the same across the world. Right now, opinion on ML is in the bal‐\\nance. Most people prefer getting sensibly targeted ads, they like their cars being able\\nto  read  speed-limit  signs,  and  improving  fraud  detection  ultimately  saves  them\\nmoney.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,243.436,432.005,329.536 'But  there  have  also  been  well-publicized  scandals  that  have  rocked  the  public’s\\nacceptance  of  this  technology.  The  Facebook-Cambridge  Analytica  affair,  where  the\\ncompanies  used  the  power  of  ML  to  manipulate  public  opinion  on  social  media,\\nshocked the world. This looked like ML with explicitly malicious intent. Equally wor‐\\nrying have been instances of entirely unintentional harm, where ML black box judg‐\\nments  proved  to  be  unacceptably  and  illegally  biased  on  criteria  such  as  race  or\\ngender, for example in criminal assessment systems and in recruitment tools.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,149.236,432.004,235.336 'If  businesses  and  governments  want  to  reap  the  benefits  of  ML,  they  have  to  safe‐\\nguard the public trust in it as well as proactively address the risks. For businesses, this\\nmeans developing strong governance of their MLOps process. They must assess the\\nrisks,  determine  their  own  set  of  fairness  values,  and  then  implement  the  necessary\\nprocess  to  manage  them.  Much  of  this  is  simply  about  good  housekeeping  with  an\\nadded  focus  on  mitigating  the  inherent  risks  of  ML,  addressing  topics  such  as  data\\nprovenance, transparency, bias, performance management, and reproducibility.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.000,40.500,84.438,49.500 '106 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.906,40.500,189.558,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,570.312,350.515,607.973 'Matching Governance with Risk Level\\nGovernance is not a free lunch; it takes effort, discipline, and time.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,501.312,432.005,562.212 'From  the  business  stakeholders’  perspective,  governance  is  likely  to  slow  down  the\\ndelivery of new models, which may cost the business money. For data scientists, it can\\nlook like a lot of bureaucracy that erodes their ability to get things done. In contrast,\\nthose  responsible  for  managing  risk  and  the  DevOps  team  managing  deployment\\nwould argue that strict governance across the board should be mandatory.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,432.312,432.003,493.212 'Those  responsible  for  MLOps  must  manage  the  inherent  tension  between  different\\nuser profiles, striking a balance between getting the job done efficiently and protect‐\\ning  against  all  possible  threats.  This  balance  can  be  found  by  assessing  the  specific\\nrisk of each project and matching the governance process to that risk level. There are\\nseveral dimensions to consider when assessing risk, including:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,409.712,205.780,420.212 '• The audience for the model\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,393.112,270.313,403.612 '• The lifetime of the model and its outcomes\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,376.512,207.629,387.012 '• The impact of the outcomes\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,341.312,432.000,364.412 'This  assessment  should  not  only  determine  the  governance  measures  applied,  but\\nalso drive the complete MLOps development and deployment tool chain.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,234.512,432.005,333.212 'For example, a self-service analytics (SSA) project (one consumed by a small internal-\\nonly audience and often built by business analysts) calls for relatively lightweight gov‐\\nernance.  Conversely,  a  model  deployed  to  a  public-facing  website  making  decisions\\nthat impact people’s lives or company finances requires a very thorough process. This\\nprocess would consider the type of KPIs chosen by the business, the type of model-\\nbuilding algorithm used for the required level of explainability, the coding tools used,\\nthe  level  of  documentation  and  reproducibility,  the  level  of  automated  testing,  the\\nresilience of the hardware platform, and the type of monitoring implemented.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,165.512,432.005,226.412 'But the business risk is not always so clear cut. An SSA project that makes a decision\\nthat has a long-term impact can also be high risk and can justify stronger governance\\nmeasures.  That’s  why  across  the  board,  teams  need  well  thought  out,  regularly\\nreviewed  strategies  for  MLOps  risk  assessment  (see  Figure  8-1  for  a  breakdown  of\\nproject criticality and operationalization approaches).\\n'>\n",
      "<LTTextBoxHorizontal(9) 291.111,40.500,402.513,49.500 'Matching Governance with Risk Level \\n'>\n",
      "<LTTextBoxHorizontal(10) 410.091,40.500,413.403,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 420.981,40.500,431.997,49.500 '107\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,380.556,414.468,403.656 'Figure 8-1. Choosing the right kind of operationalization model and MLOps features\\ndepending on the project’s criticality\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,305.356,431.998,368.216 'Current Regulations Driving MLOps Governance\\nThere  is  little  regulation  around  the  world  today  specifically  aimed  at  ML  and  AI.\\nMany existing regulations do, however, have a significant impact on ML governance.\\nThese take two forms:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.658,282.756,431.997,293.256 '• Industry-specific  regulation.  This  is  particularly  significant  in  the  finance  and\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,270.156,188.870,280.656 'pharmaceutical sectors.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,253.556,360.435,264.056 '• Broad-spectrum regulation, particularly addressing data privacy.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,193.156,432.005,241.456 'A few of the most pertinent regulations are outlined in the following sections. Their\\nrelevance  to  the  challenges  of  MLOps  governance  is  striking,  and  these  regulations\\ngive  a  good  indication  of  what  governance  measures  will  be  needed  broadly  across\\nthe industry to establish and maintain trust in ML.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,136.756,432.002,185.056 'Even for those working in industries that don’t have specific regulations, the follow‐\\ning  sections  can  give  a  brief  idea  of  what  organizations  worldwide,  regardless  of\\nindustry,  might  face  in  the  future  in  terms  of  the  level  of  specificity  of  control  with\\nregards to machine learning.\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.003,40.500,84.441,49.500 '108 \\n'>\n",
      "<LTTextBoxHorizontal(8) 92.019,40.500,95.331,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 102.909,40.500,189.561,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,409.220,432.375,607.500>\n",
      "<LTLine 72.000,409.345,432.500,409.345>\n",
      "<LTLine 72.125,409.220,72.125,607.500>\n",
      "<LTFigure(I1) 79.450,415.200,425.050,602.250 matrix=[345.60,0.00,0.00,187.05, (79.45,415.20)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,548.855,432.000,607.894 'Pharmaceutical Regulation in the US: GxP\\nGxP is a collection of quality guidelines (such as the Good Clinical Practice, or GCP,\\nguidelines)  and  regulations  established  by  the  U.S.  Food  and  Drug  Administration\\n(FDA), which aim to ensure that bio and pharmaceutical products are safe.\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.005,530.255,181.394,540.755 'GxP’s guidelines focus on:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.660,507.655,431.997,518.155 '• Traceability, or the ability to re-create the development history of a drug or medi‐\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,495.055,133.073,505.555 'cal device.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,478.455,431.997,488.955 '• Accountability, meaning who has contributed what to the development of a drug\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,465.855,133.682,476.355 'and when.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,411.455,432.002,459.755 '• Data  Integrity  (DI),  or  the  reliability  of  data  used  in  development  and  testing.\\nThis  is  based  on  the  ALCOA  principle:  attributable,  legible,  contemporaneous,\\noriginal,  and  accurate,  and  considerations  include  identifying  risks  and  mitiga‐\\ntion strategies.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,340.846,431.999,399.885 'Financial Model Risk Management Regulation\\nIn finance, model risk is the risk of incurring losses when the models used for making\\ndecisions  about  tradable  assets  prove  to  be  inaccurate.  These  models,  such  as  the\\nBlack–Scholes model, existed long before the arrival of ML.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.998,271.846,432.004,332.746 'Model risk management (MRM) regulation has been driven by the experience of the\\nimpact of extraordinary events, such as financial crashes, and the resulting harm to\\nthe public and the wider economy if severe losses are incurred. Since the financial cri‐\\nsis  of  2007–2008,  a  large  amount  of  additional  regulation  has  been  introduced  to\\nforce good MRM practices (see Figure 8-2).\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.998,240.646,432.001,263.746 'The  UK  Prudential  Regulation  Authority’s  (PRA)  regulation,  for  example,  defines\\nfour principles for good MRM:\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.998,221.046,140.290,231.546 'Model definition\\n'>\n",
      "<LTTextBoxHorizontal(11) 89.995,208.446,312.983,218.946 'Define a model and record such models in inventory.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.998,188.846,137.927,199.346 'Risk governance\\n'>\n",
      "<LTTextBoxHorizontal(13) 89.995,176.246,422.824,186.746 'Establish model risk governance framework, policies, procedures, and controls.\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.998,156.646,164.450,167.146 'Life cycle management\\n'>\n",
      "<LTTextBoxHorizontal(15) 89.995,144.046,395.923,154.546 'Create robust model development, implementation, and usage processes.\\n'>\n",
      "<LTTextBoxHorizontal(16) 71.998,124.446,146.632,134.946 'Effective challenge\\n'>\n",
      "<LTTextBoxHorizontal(17) 89.995,111.846,364.517,122.346 'Undertake appropriate model validation and independent review.\\n'>\n",
      "<LTTextBoxHorizontal(18) 261.684,40.500,402.516,49.500 'Current Regulations Driving MLOps Governance \\n'>\n",
      "<LTTextBoxHorizontal(19) 410.094,40.500,413.406,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(20) 420.984,40.500,432.000,49.500 '109\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,350.046,353.232,360.546 'Figure 8-2. The history of model risk management (MRM) regulation\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,216.264,432.005,338.303 'GDPR and CCPA Data Privacy Regulations\\nThe EU General Data Protection Regulation (GDPR) was first implemented in 2018,\\nsetting  guidelines  for  the  collection  and  processing  of  personal  information  from\\nindividuals  who  live  in  the  European  Union.  However,  it  was  developed  with  the\\ninternet age in mind, so it actually applies for EU visitors to any website, regardless of\\nwhere  that  website  is  based.  Since  few  websites  want  to  exclude  EU  visitors,  sites\\nacross the world have been forced to meet the requirements, making GDPR a de facto\\nstandard for data protection. The regulations aim to give people control of their per‐\\nsonal data that IT systems have collected, including the rights to:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.660,193.664,284.352,204.164 '• Be informed about data collected or processed\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,177.064,306.097,187.564 '• Access collected data and understand its processing\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,160.464,188.361,170.964 '• Correct inaccurate data\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,143.864,258.963,154.364 '• Be forgotten (i.e., to have data removed)\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,127.264,239.065,137.764 '• Restrict processing of personal data\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,110.664,274.051,121.164 '• Obtain collected data and reuse it elsewhere\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,94.064,249.093,104.564 '• Object to automated decision-making\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.000,40.500,84.438,49.500 '110 \\n'>\n",
      "<LTTextBoxHorizontal(10) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 102.906,40.500,189.558,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,366.109,432.375,607.500>\n",
      "<LTLine 72.000,366.234,432.500,366.234>\n",
      "<LTLine 72.125,366.109,72.125,607.500>\n",
      "<LTFigure(I1) 79.569,372.089,424.931,602.250 matrix=[345.36,0.00,0.00,230.16, (79.57,372.09)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'The California Consumer Privacy Act (CCPA) is quite similar to GDPR in terms of\\nwho and what is protected, although the scope, territorial reach, and financial penal‐\\nties are all more limited.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.998,494.636,432.004,557.497 'The New Wave of AI-Specific Regulations\\nAround the world, a new wave of regulations and guidelines specifically targeting AI\\napplications (and thus all ML applications) is emerging. The European Union is lead‐\\ning the way with an attempt to establish a framework for trustworthy AI.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,413.036,432.003,486.536 'In a white paper on artificial intelligence, the EU emphasizes the potential benefits of\\nAI for all walks of life. Equally, it highlights that scandals surrounding the misuse of\\nAI  and  warnings  of  the  dangers  of  potential  advances  in  the  power  of  AI  have  not\\ngone  unnoticed.  The  EU  considers  that  regulatory  framework  based  on  its  funda‐\\nmental values “will enable it to become a global leader in innovation in the data econ‐\\nomy and its applications.”\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.002,381.836,432.004,404.936 'The  EU  identifies  seven  key  requirements  that  AI  applications  should  respect  to  be\\nconsidered trustworthy:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.657,359.236,212.962,369.736 '• Human agency and oversight\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,342.636,222.454,353.136 '• Technical robustness and safety\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,326.036,210.653,336.536 '• Privacy and data governance\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,309.436,147.180,319.936 '• Transparency\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,292.836,270.471,303.336 '• Diversity, non-discrimination, and fairness\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,276.236,251.245,286.736 '• Societal and environmental well-being\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.655,259.636,151.790,270.136 '• Accountability\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,211.836,432.000,247.536 'The  EU  approach  is  not  one-size-fits-all:  it  will  primarily  impact  specific  high-risk\\nsectors,  including  healthcare,  transportation,  energy,  and  parts  of  the  public  sector.\\nThe regulations are expected to be optional for other sectors.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.996,142.836,432.005,203.736 'As  with  GDPR,  the  EU  approach  is  likely  to  have  a  worldwide  influence.  It  is  also\\nprobable that many large organizations will decide to opt in considering the impor‐\\ntance to their businesses of public trust in the use of AI. Even for those not opting in,\\nthe framework is likely to establish a way of thinking about governance in AI and will\\ninfluence their approach.\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.997,99.036,432.003,134.736 'Table 8-1 outlines some of the statuses of AI governance initiatives across the world.\\nAll are following an unmistakably similar route, even if the level of prescriptiveness\\nreflects their traditionally distinct approaches to regulation.\\n'>\n",
      "<LTTextBoxHorizontal(14) 281.001,40.500,402.519,49.500 'The New Wave of AI-Specific Regulations \\n'>\n",
      "<LTTextBoxHorizontal(15) 410.097,40.500,413.409,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 420.987,40.500,432.003,49.500 '111\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,595.630,309.720,605.630 'Table 8-1. Status of AI governance initiatives across the world\\n'>\n",
      "<LTTextBoxHorizontal(1) 128.326,579.600,145.245,588.600 'Stage\\n'>\n",
      "<LTTextBoxHorizontal(2) 189.830,579.600,206.300,588.600 'Focus\\n'>\n",
      "<LTTextBoxHorizontal(3) 371.732,579.600,409.415,588.600 'Coming next\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.603,556.000,116.247,588.600 'Regions &\\norganizations\\nOECD\\n'>\n",
      "<LTTextBoxHorizontal(5) 128.322,556.000,153.981,565.000 'Guidance\\n'>\n",
      "<LTTextBoxHorizontal(6) 191.014,541.200,358.937,561.000 '• 42 signatories\\n• 5 principles for responsible stewardship of trustworthy AI:\\n'>\n",
      "<LTTextBoxHorizontal(7) 197.828,508.800,332.468,539.400 'inclusive growth, human-centered and fairness,\\ntransparency and explainability, robustness, and\\naccountability\\n'>\n",
      "<LTTextBoxHorizontal(8) 191.014,498.000,306.008,507.000 '• Recommendations for national policies\\n'>\n",
      "<LTTextBoxHorizontal(9) 191.014,481.200,357.569,490.200 '• Binding for high-risk activities (Sector X impact), optional\\n'>\n",
      "<LTTextBoxHorizontal(10) 197.828,470.400,293.534,479.400 'with possibility for label for others\\n'>\n",
      "<LTTextBoxHorizontal(11) 191.014,427.200,358.388,468.600 '• Specifically targeting model fairness, robustness, and\\nauditability, mixing policies and controls, integrating\\nstrong ethical considerations on environmental and social\\nimpacts\\n'>\n",
      "<LTTextBoxHorizontal(12) 191.014,410.400,343.934,419.400 '• Positive, nonsanctioned-based approach focusing on\\n'>\n",
      "<LTTextBoxHorizontal(13) 197.828,388.800,348.722,408.600 'practical steps to implementation AI governance at an\\norganization level\\n'>\n",
      "<LTTextBoxHorizontal(14) 191.014,378.000,350.324,387.000 '• Best practice center, supporting AI governance work at\\n'>\n",
      "<LTTextBoxHorizontal(15) 197.828,367.200,258.695,376.200 'Economic Forum level\\n'>\n",
      "<LTTextBoxHorizontal(16) 191.014,350.400,356.390,359.400 '• Federal guidelines issued to prepare ground for industry-\\n'>\n",
      "<LTTextBoxHorizontal(17) 197.828,339.600,286.244,348.600 'specific guidelines or regulation\\n'>\n",
      "<LTTextBoxHorizontal(18) 191.014,328.800,342.440,337.800 '• Focus on public trust and fairness; no broader ethics\\n'>\n",
      "<LTTextBoxHorizontal(19) 197.828,318.000,237.905,327.000 'considerations\\n'>\n",
      "<LTTextBoxHorizontal(20) 189.827,281.600,361.709,314.200 'High-level guidelines only; nonbinding and broad in coverage\\nDetailed guidelines issued, integrating ethical and a strong\\nfocus on end-consumer protection\\n'>\n",
      "<LTTextBoxHorizontal(21) 372.916,438.000,425.297,490.200 '• Directive by end\\n2020/early 2021\\n• To be translated\\ninto national\\nregime\\n'>\n",
      "<LTTextBoxHorizontal(22) 372.916,388.800,421.940,419.400 '• Regulation by\\nend 2020/early\\n2021\\n'>\n",
      "<LTTextBoxHorizontal(23) 75.600,485.200,82.899,494.200 'EU\\n'>\n",
      "<LTTextBoxHorizontal(24) 128.322,452.800,173.313,494.200 'Guidance,\\ncommunication,\\ndirection, and\\nregulation\\n'>\n",
      "<LTTextBoxHorizontal(25) 75.600,414.400,103.554,423.400 'Singapore\\n'>\n",
      "<LTTextBoxHorizontal(26) 128.322,414.400,153.981,423.400 'Guidance\\n'>\n",
      "<LTTextBoxHorizontal(27) 75.600,354.400,82.962,363.400 'US\\n'>\n",
      "<LTTextBoxHorizontal(28) 128.322,332.800,173.313,363.400 'Guidance,\\ncommunication,\\nand regulation\\n'>\n",
      "<LTTextBoxHorizontal(29) 75.600,292.400,100.152,314.200 'UK\\nAustralia\\n'>\n",
      "<LTTextBoxHorizontal(30) 128.322,292.400,153.981,314.200 'Guidance\\nGuidance\\n'>\n",
      "<LTTextBoxHorizontal(31) 71.997,166.586,432.005,267.247 'The Emergence of Responsible AI\\nAs the adoption of data science, machine learning, and AI has accelerated worldwide,\\na  loose  consensus  among  AI  thinkers  has  emerged.  The  most  common  banner  for\\nthis  consensus  is  Responsible  AI:  the  idea  of  developing  machine  learning  systems\\nthat  are  accountable,  sustainable,  and  governable.  In  essence,  AI  systems  should  do\\nwhat they are supposed to, remain reliable over time, and be well controlled as well as\\nauditable. \\n'>\n",
      "<LTTextBoxHorizontal(32) 71.996,97.586,432.003,158.486 'There is no strict definition of Responsible AI or the terms used to frame it, but there\\nis agreement about the overarching considerations and largely about what is needed\\nto  deliver  it  (see  Table  8-2).  Despite  the  lack  of  any  single  body  driving  the  move‐\\nment, Responsible AI has already had a significant influence on collective thinking,\\nand especially on the EU’s trustworthy AI regulators.\\n'>\n",
      "<LTTextBoxHorizontal(33) 71.997,40.500,84.435,49.500 '112 \\n'>\n",
      "<LTTextBoxHorizontal(34) 92.013,40.500,95.325,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(35) 102.903,40.500,189.555,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,566.900,124.722,591.500>\n",
      "<LTRect 124.722,566.900,186.227,591.500>\n",
      "<LTRect 186.227,566.900,368.129,591.500>\n",
      "<LTRect 368.129,566.900,432.000,591.500>\n",
      "<LTLine 72.000,279.575,124.822,279.575>\n",
      "<LTLine 124.623,279.575,186.327,279.575>\n",
      "<LTLine 186.127,279.575,368.229,279.575>\n",
      "<LTLine 368.029,279.575,432.000,279.575>\n",
      "<LTTextLineHorizontal 371.729,556.000,373.142,565.000 ' \\n'>\n",
      "<LTTextLineHorizontal 371.729,354.400,373.142,363.400 ' \\n'>\n",
      "<LTTextLineHorizontal 371.729,305.200,373.142,314.200 ' \\n'>\n",
      "<LTTextLineHorizontal 371.729,292.400,373.142,301.400 ' \\n'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,595.630,382.830,605.630 'Table 8-2. Components of Responsible AI, an increasingly critical part of MLOps\\n'>\n",
      "<LTTextBoxHorizontal(1) 75.597,566.800,116.781,588.600 'Intentionality\\nMust have:\\n'>\n",
      "<LTTextBoxHorizontal(2) 242.921,566.800,285.482,588.600 'Accountability\\nMust have:\\n'>\n",
      "<LTTextBoxHorizontal(3) 76.784,552.000,225.108,561.000 '• Assurance that models are designed and behave in\\n'>\n",
      "<LTTextBoxHorizontal(4) 244.111,552.000,411.215,561.000 '• Central control, management, and the ability to audit the\\n'>\n",
      "<LTTextBoxHorizontal(5) 83.601,541.200,173.376,550.200 'ways aligned with their purpose\\n'>\n",
      "<LTTextBoxHorizontal(6) 250.925,541.200,348.665,550.200 'enterprise AI effort (no shadow IT!)\\n'>\n",
      "<LTTextBoxHorizontal(7) 76.787,498.000,228.906,539.400 '• Assurance that data used for AI projects comes from\\ncompliant and unbiased sources plus a collaborative\\napproach to AI projects that ensures multiple checks\\nand balances on potential model bias\\n'>\n",
      "<LTTextBoxHorizontal(8) 76.787,454.800,232.713,496.200 '• Intentionality also includes explainability, meaning\\nthe result of AI systems should be explainable by\\nhumans (ideally not just the humans that created the\\nsystem)\\n'>\n",
      "<LTTextBoxHorizontal(9) 244.111,530.400,413.087,539.400 '• An overall view of which teams are using what data, how,\\n'>\n",
      "<LTTextBoxHorizontal(10) 250.925,519.600,308.840,528.600 'and in which models\\n'>\n",
      "<LTTextBoxHorizontal(11) 244.111,465.600,416.363,517.800 '• Trust that data is reliable and being collected in accordance\\nwith regulation as well as a centralized understanding of\\nwhich models are being used for which business process.\\nThis is closely tied to traceability—if something goes\\nwrong, is it easy to find where in the pipeline it happened?\\n'>\n",
      "<LTTextBoxHorizontal(12) 75.600,428.200,353.979,450.000 'Human-centered approach\\nProviding people with the tools and training to be aware of and then execute on both components  \\n'>\n",
      "<LTTextBoxHorizontal(13) 71.995,338.386,432.004,413.847 'Key Elements of Responsible AI\\nResponsible  AI  is  about  the  responsibility  of  data  practitioners,  not  about  AI  itself\\nbeing responsible: this is a very important distinction.  Another important distinction\\nis that, according to Kurt Muemel of Dataiku, “It is not necessarily about intentional\\nharm, but accidental harm.”\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.995,294.586,432.003,330.286 'This section presents five key elements that figure in Responsible AI thinking—data,\\nbias, inclusiveness, model management at scale, and governance—as well as MLOps\\nconsiderations for each element.\\n'>\n",
      "<LTTextBoxHorizontal(15) 71.995,223.977,432.000,283.016 'Element 1: Data\\nThe dependence on data is a fundamental differentiator between ML and traditional\\nsoftware development. The quality of the data used will make the biggest impact on\\nthe accuracy of the model. Some real-world considerations are as follows:\\n'>\n",
      "<LTTextBoxHorizontal(16) 80.660,201.377,432.002,211.877 '• Provenance is king. Understand how the data was collected and its journey to the\\n'>\n",
      "<LTTextBoxHorizontal(17) 89.996,188.777,141.929,199.277 'point of use.\\n'>\n",
      "<LTTextBoxHorizontal(18) 80.654,172.177,431.997,182.677 '• Get the data off of desktops. Data must be manageable, securable, and traceable.\\n'>\n",
      "<LTTextBoxHorizontal(19) 90.002,159.577,256.427,170.077 'Personal data must be strictly managed.\\n'>\n",
      "<LTTextBoxHorizontal(20) 80.660,142.977,394.875,153.477 '• The quality of data over time: consistency, completeness, and ownership.\\n'>\n",
      "<LTTextBoxHorizontal(21) 80.655,126.377,389.604,136.877 '• Bias in, bias out. Biased input data can occur easily and unintentionally.\\n'>\n",
      "<LTTextBoxHorizontal(22) 309.432,40.500,402.519,49.500 'Key Elements of Responsible AI \\n'>\n",
      "<LTTextBoxHorizontal(23) 410.097,40.500,413.409,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(24) 420.987,40.500,432.003,49.500 '113\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,577.700,239.324,591.500>\n",
      "<LTRect 239.324,577.700,423.369,591.500>\n",
      "<LTLine 72.000,426.175,423.469,426.175>\n",
      "<LTLine 423.269,426.175,432.000,426.175>\n",
      "<LTTextLineHorizontal 426.969,441.000,428.400,450.000 ' \\n'>\n",
      "<LTTextLineHorizontal 426.969,428.200,428.382,437.200 ' \\n'>\n",
      "<LTTextBoxHorizontal(0) 71.995,523.655,432.003,607.894 'Element 2: Bias\\nML predictive modeling is about building a system to recognize and exploit tenden‐\\ncies in the real world. Certain types of cars, driven by certain types of people, in cer‐\\ntain places are more likely to be costlier to insurance companies than others. But is\\nmatching  a  pattern  always  considered  ethical?  When  is  such  pattern-matching  pro‐\\nportionate, and when is it an unfair bias?\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,379.055,432.004,515.555 'Establishing what is fair is not clear-cut. Even using a churn model to give rebates to\\nthe customers who are more likely to leave might be considered as unfair against dor‐\\nmant customers who will pay more for the same product. Regulations are a place to\\nstart looking, but as already discussed, opinion is not universal and is not fixed. Even\\nwith a clear understanding of the fairness constraints to work toward, achieving them\\nis not simple. When the developers of the recruitment system that was biased against\\nwomen’s  schools  adapted  the  model  to  ignore  the  words  like  “women’s,”  they  found\\nthat even the tone of the language in a resume reflected the gender of the author and\\ncreated unwanted bias against women. Addressing these biases has deep implications\\non the ML model to be built (see “Impact of Responsible AI on Modeling” on page 53\\nfor a detailed example).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.998,310.055,432.005,370.955 'Taking a step back, these bias problems are not new; for example, hiring discrimina‐\\ntion has always been an issue. What is new is that, thanks to the IT revolution, data to\\nassess biases is more available. On top of that, thanks to the automation of decision\\nmaking with machine learning, it is possible to change the behavior without having to\\ngo through the filter of individuals making subjective decisions.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,266.255,432.002,301.955 'The bottom line is that biases are not only statistical. Bias checks should be integrated\\nin governance frameworks so that issues are identified as early as possible, since they\\ndo have the potential to derail data science and machine learning projects.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,235.055,431.996,258.155 'It’s  not  all  bad  news:  there  are  many  potential  sources  of  statistical  bias  (i.e.,  of  the\\nworld as it was) that can be addressed by data scientists:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.659,212.455,431.997,222.955 '• Is bias encoded into the training data? Is the raw material biased? Has data prepa‐\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,199.855,281.543,210.355 'ration, sampling, or splitting introduced bias?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,183.255,226.098,193.755 '• Is the problem framed properly?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,166.655,431.997,177.155 '• Do we have the right target for all subpopulations? Beware that many variables\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,154.055,195.926,164.555 'may be highly correlated.\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.660,137.455,431.997,147.955 '• Is feedback-loop data biased through factors such as the order in which choices\\n'>\n",
      "<LTTextBoxHorizontal(11) 90.002,124.855,190.550,135.355 'are presented in the UI?\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.002,89.655,432.005,112.755 'It  is  so  complex  to  prevent  the  problems  caused  by  bias  that  much  of  the  current\\nfocus  is  on  detecting  bias  before  it  causes  harm.  ML  interpretability  is  the  current\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.002,40.500,84.440,49.500 '114 \\n'>\n",
      "<LTTextBoxHorizontal(14) 92.018,40.500,95.330,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 102.908,40.500,189.560,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'mainstay  of  bias  detection,  bringing  understanding  to  ML  models  through  a  set  of\\ntechnical tools to analyze models including:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.652,559.837,391.862,570.337 '• Prediction understanding: Why did a model make a specific prediction?\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,543.236,345.031,553.736 '• Subpopulation analysis: Is there bias among subpopulations?\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,526.637,428.947,537.137 '• Dependency understanding: What contributions are individual features making?\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,478.837,432.000,514.537 'A  very  different,  but  complementary,  approach  to  addressing  bias  is  to  leverage  as\\nbroad a range of human expertise as possible in the development process. This is one\\naspect of the idea of inclusiveness in Responsible AI.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.993,383.027,432.003,467.267 'Element 3: Inclusiveness\\nThe human-in-the-loop (HITL) approach aims to combine the best of human intelli‐\\ngence with the best of machine intelligence. Machines are great at making smart deci‐\\nsions  from  vast  datasets,  whereas  people  are  much  better  at  making  decisions  with\\nless  information.  Human  judgment  is  particularly  effective  for  making  ethical  and\\nharm-related judgments.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,326.627,432.004,374.927 'This concept can be applied to the way models are used in production, but it can be\\nequally important in the way models are built. Formalizing human responsibility in\\nthe  MLOps  loop,  for  example  through  sign-off  processes,  can  be  simple  to  do,  but\\nhighly effective.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.995,270.227,432.001,318.527 'The  principle  of  inclusiveness  takes  the  idea  of  human-AI  collaboration  further:\\nbringing as diverse a set of human expertise to the ML life cycle as possible reduces\\nthe  risk  of  serious  blind  spots  and  omissions.  The  less  inclusive  the  group  building\\nthe ML, the greater the risk.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,176.027,432.005,262.127 'The perspectives of the business analyst, the subject matter expert, the data scientist,\\nthe data engineer, the risk manager, and the technical architect are all different. All of\\nthese perspectives together bring far greater clarity to managing model development\\nand deployment than relying on any single user profile, and enabling these user pro‐\\nfiles to collaborate effectively is a key factor in reducing risk and increasing the per‐\\nformance  of  MLOps  in  any  organization.  Refer  to  Chapter  2  for  clear  examples  of\\ncollaboration among different profiles for better MLOps performance.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,119.627,432.003,167.927 'Full  inclusiveness  may  even  bring  the  consumer  into  the  process,  perhaps  through\\nfocus group testing. The objective of inclusiveness is to bring the appropriate human\\nexpertise  into  the  process,  regardless  of  source.  Leaving  ML  to  data  scientists  is  not\\nthe answer to managing risk.\\n'>\n",
      "<LTTextBoxHorizontal(10) 309.429,40.500,402.516,49.500 'Key Elements of Responsible AI \\n'>\n",
      "<LTTextBoxHorizontal(11) 410.094,40.500,413.406,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 420.984,40.500,432.000,49.500 '115\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.995,536.255,432.003,607.894 'Element 4: Model Management at Scale\\nManaging the risk associated with ML when there are a handful of models in produc‐\\ntion  can  afford  to  be  largely  manual.  But  as  the  volume  of  deployments  grows,  the\\nchallenges  multiply  rapidly.  Here  are  some  key  considerations  for  managing  ML  at\\nscale:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,513.655,422.668,524.155 '• A scalable model life cycle needs to be largely automated as well as streamlined.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.655,497.055,430.187,507.555 '• Errors, for example in a subset of a dataset, will propagate out rapidly and widely.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,480.455,356.518,490.955 '• Existing software engineering techniques can assist ML at scale.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,463.855,323.349,474.355 '• Decisions must be explainable, auditable, and traceable.\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,447.255,431.997,457.755 '• Reproducibility  is  key  to  understanding  what  went  wrong,  who  or  what  was\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,434.655,302.427,445.155 'responsible, and who should ensure it is corrected.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.660,418.055,431.997,428.555 '• Model  performance  will  degrade  over  time:  monitoring,  drift  management,\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.001,405.455,332.257,415.955 'retraining, and remodeling must be built into the process.\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.659,388.855,431.997,399.355 '• Technology  is  evolving  rapidly;  an  approach  to  integrating  new  technologies  is\\n'>\n",
      "<LTTextBoxHorizontal(10) 90.002,376.255,128.411,386.755 'required.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.997,318.246,432.000,364.685 'Element 5: Governance\\nResponsible AI sees strong governance as the key to achieving fairness and trustwor‐\\nthiness. The approach builds on traditional governance techniques:\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.652,295.646,311.043,306.146 '• Determine intentions at the beginning of the process\\n'>\n",
      "<LTTextBoxHorizontal(13) 80.655,279.046,254.731,289.546 '• Formalize bringing humans in the loop\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.655,262.446,273.831,272.946 '• Clearly identify responsibilities (Figure 8-3)\\n'>\n",
      "<LTTextBoxHorizontal(15) 80.655,245.846,306.937,256.346 '• Integrate goals that define and structure the process\\n'>\n",
      "<LTTextBoxHorizontal(16) 80.655,229.246,287.849,239.746 '• Establish and communicate a process and rules\\n'>\n",
      "<LTTextBoxHorizontal(17) 80.655,212.646,314.361,223.146 '• Define measurable metrics and monitor for deviation\\n'>\n",
      "<LTTextBoxHorizontal(18) 80.655,196.046,395.673,206.546 '• Build multiple checks into the MLOps pipeline aligned with overall goals\\n'>\n",
      "<LTTextBoxHorizontal(19) 80.655,179.446,241.564,189.946 '• Empower people through education\\n'>\n",
      "<LTTextBoxHorizontal(20) 80.655,162.846,353.442,173.346 '• Teach builders as well as decision makers how to prevent harm\\n'>\n",
      "<LTTextBoxHorizontal(21) 71.997,115.046,432.000,150.746 'Governance  is,  therefore,  both  the  foundation  and  the  glue  of  MLOps  initiatives.\\nHowever,  it’s  important  to  recognize  that  it  goes  beyond  the  borders  of  traditional\\ndata governance.\\n'>\n",
      "<LTTextBoxHorizontal(22) 72.000,40.500,84.438,49.500 '116 \\n'>\n",
      "<LTTextBoxHorizontal(23) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(24) 102.906,40.500,189.558,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,393.606,423.078,416.706 'Figure 8-3. A representation of who is responsible at different levels of the organization\\nfor different parts of the Responsible AI process\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.992,318.405,432.002,381.266 'A Template for MLOps Governance\\nHaving  explored  the  key  themes  to  be  addressed  by  an  MLOps  governance,  both\\nthrough regulatory measures and the Responsible AI movement, it is time to map out\\nhow to implement a robust governance framework of MLOps.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,274.605,432.003,310.305 'There is no one-size-fits-all solution across businesses, and different use cases within\\na business justify different levels of management, but the step-by-step approach out‐\\nlined can be applied in any organization to guide the implementation process.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,256.005,187.584,266.505 'The process has eight steps:\\n'>\n",
      "<LTTextBoxHorizontal(4) 77.317,233.405,287.932,243.905 '1. Understand and classify the analytics use cases.\\n'>\n",
      "<LTTextBoxHorizontal(5) 77.316,216.805,210.222,227.305 '2. Establish an ethical position.\\n'>\n",
      "<LTTextBoxHorizontal(6) 77.316,200.205,196.099,210.705 '3. Establish responsibilities.\\n'>\n",
      "<LTTextBoxHorizontal(7) 77.316,183.605,222.906,194.105 '4. Determine governance policies.\\n'>\n",
      "<LTTextBoxHorizontal(8) 77.316,167.005,267.636,177.505 '5. Integrate policies into the MLOps process.\\n'>\n",
      "<LTTextBoxHorizontal(9) 77.316,150.405,326.226,160.905 '6. Select the tools for centralized governance management.\\n'>\n",
      "<LTTextBoxHorizontal(10) 77.316,133.805,175.761,144.305 '7. Engage and educate.\\n'>\n",
      "<LTTextBoxHorizontal(11) 77.316,117.205,172.989,127.705 '8. Monitor and refine.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.997,82.005,432.000,105.105 'This section will go through each of the steps in detail, including a simple definition\\nand the “how” of actually implementing the step.\\n'>\n",
      "<LTTextBoxHorizontal(13) 300.137,40.500,402.512,49.500 'A Template for MLOps Governance \\n'>\n",
      "<LTTextBoxHorizontal(14) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 420.980,40.500,431.996,49.500 '117\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,422.269,432.375,607.500>\n",
      "<LTLine 72.000,422.394,432.500,422.394>\n",
      "<LTLine 72.125,422.269,72.125,607.500>\n",
      "<LTFigure(I1) 126.010,428.249,378.490,602.250 matrix=[252.48,0.00,0.00,174.00, (126.01,428.25)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,561.455,431.997,607.894 'Step 1: Understand and Classify the Analytics Use Cases\\nThis step entails defining what the different classes of analytics use cases are and, sub‐\\nsequently, what the governance needs are for each.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,505.055,432.005,553.355 'Consider the answers to the following questions for a representative cross-section of\\nanalytics use cases. Identify the key distinguishing features of the different use cases\\nand categorize these features. Conflate categories where appropriate. Typically, it will\\nbe necessary to associate several categories to each use case to fully describe it.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.651,482.455,431.997,492.955 '• What  regulations  is  each  use  case  subject  to,  and  what  are  the  implications?\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,469.855,261.582,480.355 'Sector-specific regulations, regional, PII?\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,453.255,431.997,463.755 '• Who  consumes  the  results  of  the  model?  The  public?  One  of  many  internal\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,440.655,115.622,451.155 'users?\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,424.055,431.997,434.555 '• What  are  the  availability  requirements  for  the  deployed  model?  24/7  real-time\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.001,411.455,381.922,421.955 'scoring, scheduled batch scoring, ad-hoc runs (self-service analytics)?\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,394.855,431.997,405.355 '• What is the impact of any errors and deficiencies? Legal, financial, personal, pub‐\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,382.255,126.006,392.755 'lic trust?\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.660,365.655,278.188,376.155 '• What is the cadence and urgency of releases?\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.655,349.055,418.910,359.555 '• What is the lifetime of the model and the lifetime of the impact of its decision?\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.655,332.455,282.798,342.955 '• What is the likely rate of model quality decay?\\n'>\n",
      "<LTTextBoxHorizontal(13) 80.655,315.855,314.256,326.355 '• What is the need for explainability and transparency?\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.997,207.446,432.005,304.285 'Step 2: Establish an Ethical Position\\nWe established that fairness and ethical considerations are important motivating fac‐\\ntors  for  effective  governance,  that  businesses  have  a  choice  on  their  ethical  stance,\\nand that this impacts public perception and trust. The position a business takes is a\\ntrade-off between the cost to implement the position and public perception. Respon‐\\nsible stances rarely come at zero short-term financial cost even if the long-term ROI\\nmay be positive.\\n'>\n",
      "<LTTextBoxHorizontal(15) 71.996,125.846,432.003,199.346 'Any MLOps governance framework needs to reflect the ethical position of the com‐\\npany. While the position typically impacts what a model does and how it does it, the\\nMLOps governance process needs to ensure that deployed models match the chosen\\nethical stance. This stance is likely to influence the governance process more widely,\\nincluding the selection and verification of new models and the acceptable likelihood\\nof accidental harm.\\n'>\n",
      "<LTTextBoxHorizontal(16) 71.999,40.500,84.437,49.500 '118 \\n'>\n",
      "<LTTextBoxHorizontal(17) 92.015,40.500,95.327,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(18) 102.905,40.500,189.557,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,595.037,243.664,605.537 'Consider the following ethical questions:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,572.437,431.997,582.937 '• What aspects of well-being in society matter? E.g., equality, privacy, human rights\\n'>\n",
      "<LTTextBoxHorizontal(2) 90.001,559.837,266.716,570.337 'and dignity, employment, democracy, bias\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.660,543.236,431.997,553.736 '• Is  the  potential  impact  on  human  psychology  to  be  considered?  E.g.,  human-\\n'>\n",
      "<LTTextBoxHorizontal(4) 90.002,530.636,398.376,541.136 'human or human-AI relationships, deception, manipulation, exploitation\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.660,514.036,386.895,524.537 '• Is a stance on the financial impact required? E.g., market manipulation\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,497.437,295.702,507.937 '• How transparent should the decision making be?\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,480.837,431.997,491.337 '• What  level  of  accountability  for  AI-driven  mistakes  does  the  business  want  to\\n'>\n",
      "<LTTextBoxHorizontal(8) 90.002,468.237,120.641,478.737 'accept?\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.002,410.227,432.005,456.667 'Step 3: Establish Responsibilities\\nIdentify the groups of people responsible for overseeing MLOps governance as well\\nas their roles.\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.657,387.627,431.997,398.127 '• Engage  the  whole  organization,  across  departments,  from  top  to  bottom  of  the\\n'>\n",
      "<LTTextBoxHorizontal(11) 90.002,375.027,188.670,385.527 'management hierarchy.\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.660,358.427,431.997,368.927 '• Peter  Drucker’s  famous  line  “Culture  eats  strategy  for  breakfast”  highlights  the\\n'>\n",
      "<LTTextBoxHorizontal(13) 90.002,345.827,287.780,356.327 'power of broad engagement and shared beliefs.\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.660,329.227,431.997,339.727 '• Avoid  creating  all-new  governance  structures.  Look  at  what  structures  exist\\n'>\n",
      "<LTTextBoxHorizontal(15) 90.002,316.627,347.294,327.127 'already and try to incorporate MLOps governance into them.\\n'>\n",
      "<LTTextBoxHorizontal(16) 80.660,300.027,361.936,310.527 '• Get senior management sponsorship for the governance process.\\n'>\n",
      "<LTTextBoxHorizontal(17) 80.655,283.427,299.220,293.927 '• Think in terms of separate levels of responsibility:\\n'>\n",
      "<LTTextBoxHorizontal(18) 91.069,266.827,215.608,277.327 '— Strategic: set out the vision\\n'>\n",
      "<LTTextBoxHorizontal(19) 91.069,250.227,281.317,260.727 '— Tactical: implement and enforce the vision\\n'>\n",
      "<LTTextBoxHorizontal(20) 91.069,233.627,255.981,244.127 '— Operational: execute on a daily basis\\n'>\n",
      "<LTTextBoxHorizontal(21) 80.655,166.627,432.001,227.527 '• Consider  building  a  RACI  matrix  for  the  complete  MLOps  process  (see\\nFigure 8-4). RACI stands for responsible, accountable, consulted, informed, and it\\nhighlights the roles of different stakeholders in the overall MLOps process. It is\\nquite likely that any matrix you create at this stage will need to be refined later on\\nin the process.\\n'>\n",
      "<LTTextBoxHorizontal(22) 300.137,40.500,402.512,49.500 'A Template for MLOps Governance \\n'>\n",
      "<LTTextBoxHorizontal(23) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(24) 420.980,40.500,431.996,49.500 '119\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 72.000,316.585,258.869,327.085 'Figure 8-4. A typical RACI matrix for MLOps\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.994,220.603,432.004,304.843 'Step 4: Determine Governance Policies\\nWith an understanding of the scope and objectives for governance now established,\\nand the engagement of the responsible governance leaders, it is time to consider the\\ncore  policies  for  the  MLOps  process.  This  is  no  small  task,  and  it  is  unlikely  to  be\\nachieved in one iteration. Focus on establishing the broad areas of policy and accept\\nthat experience will help to evolve the details.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.998,189.403,431.998,212.503 'Consider the classification of initiatives from Step 1. What governance measures do\\nthe team or organization need in each case?\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,82.603,432.005,181.303 'In  initiatives  where  there  is  less  concern  about  the  risk  or  regulatory  compliance,\\nlighter-weight, cheaper measures may be appropriate. For example, “what if ” calcula‐\\ntions to determine the number of in-flight meals of different types has relatively little\\nimpact—after  all,  the  mix  was  never  right  even  before  the  introduction  of  machine\\nlearning. Even such a seemingly insignificant use case may have ethical implications\\nas meal choices are likely to be correlated to religion or gender, which are protected\\nattributes in many countries. On the other hand, the implications of calculations to\\ndetermine the level of fueling of planes carry substantially greater risk.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.004,40.500,84.442,49.500 '120 \\n'>\n",
      "<LTTextBoxHorizontal(5) 92.020,40.500,95.332,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(6) 102.910,40.500,189.562,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,332.648,432.375,607.500>\n",
      "<LTLine 72.000,332.773,432.500,332.773>\n",
      "<LTLine 72.125,332.648,72.125,607.500>\n",
      "<LTFigure(I1) 79.450,338.628,425.050,602.250 matrix=[345.60,0.00,0.00,263.62, (79.45,338.63)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'Governance considerations can be broadly grouped under the headings in Table 8-3.\\nFor each heading, there are a range of measures to consider for each class.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.999,557.930,246.309,567.930 'Table 8-3. MLOps governance considerations\\n'>\n",
      "<LTTextBoxHorizontal(2) 75.596,435.950,130.091,550.900 'Governance\\nconsideration\\nReproducibility and\\ntraceability\\nAudit and\\ndocumentation\\nHuman-in-the-loop\\nsign-off\\nPreproduction\\nverification\\n'>\n",
      "<LTTextBoxHorizontal(3) 75.600,388.250,136.611,421.100 'Transparency and\\nexplainability\\nBias and harm testing\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.600,340.550,141.030,373.400 'Production deployment\\nmodes\\nProduction monitoring\\n'>\n",
      "<LTTextBoxHorizontal(5) 75.600,295.100,121.626,314.900 'Data quality and\\ncompliance\\n'>\n",
      "<LTTextBoxHorizontal(6) 150.395,541.900,205.925,550.900 'Example measures\\n'>\n",
      "<LTTextBoxHorizontal(7) 150.395,470.600,428.108,527.300 'Full VM and data snapshot for precise and rapid model re-instantiation, or ability to re-create the\\nenvironment and retrain with a data sample, or only record metrics of models deployed?\\nFull log of all changes during development including experiments run and reasons for choices made\\nor automated documentation of deployed model only or no documentation at all\\nMultiple sign-offs for every environment move (development, QA, preproduction, production)\\n'>\n",
      "<LTTextBoxHorizontal(8) 150.398,284.300,427.202,455.750 'Verify model documentation by hand-coding the model and comparing results or full automated\\ntest pipeline re-creating in production-like environment with extensive unit and end-to-end test\\ncases or automated checks on database, software version, and naming standards only\\nUse manually-coded decision tree for maximum explainability or use regression algorithms’\\nexplainability tools such as Shapely values or accept opaque algorithms such as neural networks\\n“Red team” adversarial manual testing using multiple tools and attack vectors or automated bias\\nchecking on specific subpopulations\\nContainerized deployment to elastic scalable high-availability, multi-node configuration with\\nautomated stress/load testing prior to deployment or a single production server\\nReal-time alerting of errors, dynamic multi-armed bandit model balancing, automated nightly\\nretraining, model evaluation, and redeployment or weekly input drift monitoring and manual\\nretraining or basic infrastructure alerts, no monitoring, no feedback-based retraining\\nPII considerations including anonymization and documented and reviewed column-level lineage to\\nunderstand the source, quality, and appropriateness of the data and automated data quality checks\\nfor anomalies\\n'>\n",
      "<LTTextBoxHorizontal(9) 72.003,257.186,280.438,267.686 'The finalized governance policies should provide:\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.658,234.586,431.997,245.086 '• A process for determining the classification of any analytics initiative. This could\\n'>\n",
      "<LTTextBoxHorizontal(11) 90.002,221.986,351.830,232.486 'be implemented as a checklist or a risk assessment application.\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.660,205.386,431.997,215.886 '• A matrix of initiative classification against governance consideration, where each\\n'>\n",
      "<LTTextBoxHorizontal(13) 90.002,192.786,243.365,203.286 'cell identifies the measures required.\\n'>\n",
      "<LTTextBoxHorizontal(14) 71.997,122.177,432.000,181.217 'Step 5: Integrate Policies into the MLOps Process\\nOnce the governance policies for the different classes of initiatives have been identi‐\\nfied, measures to implement them need to be incorporated into the MLOps process\\nand responsibilities for actioning the measures assigned.\\n'>\n",
      "<LTTextBoxHorizontal(15) 71.998,78.377,432.004,114.077 'While most businesses will have an existing MLOps process, it is quite likely that this\\nhas  not  been  defined  explicitly,  but  rather  has  evolved  in  response  to  individual\\nneeds.  Now  is  the  time  to  revisit,  enhance,  and  document  the  process.  Successful\\n'>\n",
      "<LTTextBoxHorizontal(16) 300.139,40.500,402.514,49.500 'A Template for MLOps Governance \\n'>\n",
      "<LTTextBoxHorizontal(17) 410.092,40.500,413.404,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(18) 420.982,40.500,431.998,49.500 '121\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,529.200,146.798,553.800>\n",
      "<LTRect 146.798,529.200,432.000,553.800>\n",
      "<LTLine 72.000,505.475,146.898,505.475>\n",
      "<LTLine 146.698,505.475,432.000,505.475>\n",
      "<LTLine 72.000,481.625,146.898,481.625>\n",
      "<LTLine 146.698,481.625,432.000,481.625>\n",
      "<LTLine 72.000,457.775,146.898,457.775>\n",
      "<LTLine 146.698,457.775,432.000,457.775>\n",
      "<LTLine 72.000,423.125,146.898,423.125>\n",
      "<LTLine 146.698,423.125,432.000,423.125>\n",
      "<LTLine 72.000,399.275,146.898,399.275>\n",
      "<LTLine 146.698,399.275,432.000,399.275>\n",
      "<LTLine 72.000,375.425,146.898,375.425>\n",
      "<LTLine 146.698,375.425,432.000,375.425>\n",
      "<LTLine 72.000,351.575,146.898,351.575>\n",
      "<LTLine 146.698,351.575,432.000,351.575>\n",
      "<LTLine 72.000,316.925,146.898,316.925>\n",
      "<LTLine 146.698,316.925,432.000,316.925>\n",
      "<LTLine 72.000,282.275,146.898,282.275>\n",
      "<LTLine 146.698,282.275,432.000,282.275>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'adoption  of  the  governance  process  can  only  happen  if  it  is  communicated  clearly\\nand buy-in is sought from each stakeholder group.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,538.636,432.005,574.337 'Understand all of the steps in the existing process by interviewing those responsible.\\nWhere there is no existing formal process, this is often harder than it sounds because\\nthe process steps are often not explicitly defined, and ownership is unclear.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,494.836,432.002,530.536 'Attempting to map the policy-driven governance measures into the understanding of\\nthe process will identify issues in the process very quickly. Within one business there\\nmay be a range of different styles of project and governance needs, such as:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.651,472.237,211.787,482.737 '• One-off self-service analytics\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,455.637,209.981,466.137 '• Internally consumed models\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,439.037,244.494,449.537 '• Models embedded in public websites\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,422.436,284.562,432.936 '• Models deployed to Internet of Things devices\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.997,362.036,432.004,410.336 'In  these  cases,  the  differences  between  some  processes  may  be  so  great  it  is  best  to\\nthink in terms of several parallel processes. Ultimately, every governance measure for\\neach  use  case  should  be  associated  with  a  process  step  and  with  a  team  that  is  ulti‐\\nmately responsible, as presented here:\\n'>\n",
      "<LTTextBoxHorizontal(8) 75.597,311.650,122.262,346.500 'Process step\\nBusiness scoping\\nIdeation\\n'>\n",
      "<LTTextBoxHorizontal(9) 75.600,286.600,112.806,295.600 'Development\\n'>\n",
      "<LTTextBoxHorizontal(10) 75.600,251.650,114.894,260.650 'Preproduction\\n'>\n",
      "<LTTextBoxHorizontal(11) 75.600,226.600,109.530,235.600 'Deployment\\n'>\n",
      "<LTTextBoxHorizontal(12) 141.661,324.700,391.042,346.500 'Example activities and governance considerations\\nRecord objectives, define KPIs, and record sign-off: for internal governance considerations\\n'>\n",
      "<LTTextBoxHorizontal(13) 141.664,300.200,326.965,320.100 'Data discovery: data quality and regulatory compliance constraints\\nAlgorithm choice: impacted by explainability requirements\\n'>\n",
      "<LTTextBoxHorizontal(14) 141.664,265.250,416.461,295.050 'Data preparation: consider PII compliance, separation of legal regional scopes, avoid input bias\\nModel development: consider model reproducibility and auditabilityModel testing and verification:\\nbias and harm testing, explainability\\n'>\n",
      "<LTTextBoxHorizontal(15) 141.664,240.200,268.915,260.100 'Verify performance/bias with production data\\nProduction-ready testing: verify scalability\\n'>\n",
      "<LTTextBoxHorizontal(16) 141.664,205.250,407.128,235.050 'Deployment strategy: driven by the level of operationalization\\nDeployment verification testsUse of shadow challenger or A/B test techniques for in-production\\nverification\\n'>\n",
      "<LTTextBoxHorizontal(17) 75.600,180.850,118.827,200.650 'Monitoring and\\nfeedback\\n'>\n",
      "<LTTextBoxHorizontal(18) 141.664,180.200,281.092,200.100 'Performance metrics and alerting\\nPrediction log analysis for input drift with alerting\\n'>\n",
      "<LTTextBoxHorizontal(19) 71.998,93.755,432.003,165.394 'Step 6: Select the Tools for Centralized Governance Management\\nThe MLOps governance process impacts both the complete ML life cycle and many\\nteams across the organization. Each step requires a specific sequence of actions and\\nchecks to be executed. Traceability of both the development of the model and the exe‐\\ncution of governance activities is a complex challenge.\\n'>\n",
      "<LTTextBoxHorizontal(20) 72.002,40.500,84.440,49.500 '122 \\n'>\n",
      "<LTTextBoxHorizontal(21) 92.018,40.500,95.330,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(22) 102.908,40.500,189.560,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,335.600,138.064,349.400>\n",
      "<LTRect 138.064,335.600,432.000,349.400>\n",
      "<LTLine 72.000,322.675,138.164,322.675>\n",
      "<LTLine 137.964,322.675,432.000,322.675>\n",
      "<LTLine 72.000,297.625,138.164,297.625>\n",
      "<LTLine 137.964,297.625,432.000,297.625>\n",
      "<LTLine 72.000,262.675,138.164,262.675>\n",
      "<LTLine 137.964,262.675,432.000,262.675>\n",
      "<LTLine 72.000,237.625,138.164,237.625>\n",
      "<LTLine 137.964,237.625,432.000,237.625>\n",
      "<LTLine 72.000,202.675,138.164,202.675>\n",
      "<LTLine 137.964,202.675,432.000,202.675>\n",
      "<LTLine 72.000,177.625,138.164,177.625>\n",
      "<LTLine 137.964,177.625,432.000,177.625>\n",
      "<LTTextBoxHorizontal(0) 71.996,519.436,432.005,605.537 'Most organizations still have a “paper form” mindset for process management, where\\nforms  are  filled  in,  circulated,  signed,  and  filed.  The  forms  may  be  text  documents,\\ncirculated via email, and filed electronically, but the limitations of paper remain. It is\\nhard to track progress, associate artifacts, review many projects at once, prompt for\\naction,  and  remind  teams  of  responsibilities.  The  complete  record  of  events  is  typi‐\\ncally spread across multiple systems and owned by individual teams, making a simple\\noverview of analytics projects effectively impossible.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,475.636,432.003,511.336 'While teams will always have tools specific to their roles, MLOps governance is much\\nmore  effective  if  the  overarching  process  is  managed  and  tracked  from  one  system.\\nThis system should:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.652,453.037,431.997,463.537 '• Centralize the definition of the governance process flows for each class of analyt‐\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,440.436,140.990,450.936 'ics use cases\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,423.836,381.908,434.336 '• Enable tracking and enforcement of the complete governance process\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.655,407.236,392.523,417.736 '• Provide a single point of reference for the discovery of analytics projects\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,390.636,431.997,401.136 '• Enable collaboration between teams, in particular, the transfer of work between\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.002,378.036,114.729,388.536 'teams\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,361.436,318.708,371.936 '• Integrate with existing tools used for project execution\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.997,301.036,432.005,349.336 'The current workflow, project management, and MLOps tools can only partially sup‐\\nport these objectives. A new category of ML governance tools is emerging to support\\nthis need directly and more fully. These new tools focus on the specific challenges of\\nML governance, including:\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.657,278.436,418.595,288.936 '• A single view of the status of all models (otherwise known as a model registry)\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.655,261.836,431.997,272.336 '• Process gates with a sign-off mechanism to allow ready traceability of the history\\n'>\n",
      "<LTTextBoxHorizontal(12) 90.002,249.236,170.148,259.736 'of decision making\\n'>\n",
      "<LTTextBoxHorizontal(13) 80.660,232.636,249.629,243.136 '• Ability to track all versions of a model\\n'>\n",
      "<LTTextBoxHorizontal(14) 80.655,216.036,385.656,226.536 '• Ability to link to artifact stores, metrics snapshots, and documentation\\n'>\n",
      "<LTTextBoxHorizontal(15) 80.655,199.436,392.964,209.936 '• Ability to tailor processes specifically for each class of analytics use cases\\n'>\n",
      "<LTTextBoxHorizontal(16) 80.655,182.836,431.997,193.336 '• Ability to integrate health monitoring from production systems and to track the\\n'>\n",
      "<LTTextBoxHorizontal(17) 90.002,170.236,331.187,180.736 'performance of models against the original business KPIs\\n'>\n",
      "<LTTextBoxHorizontal(18) 71.996,74.427,432.004,158.667 'Step 7: Engage and Educate\\nWithout a program of engagement and training for the groups involved in overseeing\\nand executing the governance process, the chances of it being even partially adopted\\nare slim. It is essential that the importance of MLOps governance to the business, and\\nthe  necessity  of  each  team’s  contribution,  is  communicated.  Building  on  this\\nunderstanding,  every  individual  needs  to  learn  what  they  must  do,  when,  and  how.\\n'>\n",
      "<LTTextBoxHorizontal(19) 300.139,40.500,402.514,49.500 'A Template for MLOps Governance \\n'>\n",
      "<LTTextBoxHorizontal(20) 410.092,40.500,413.404,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(21) 420.982,40.500,431.998,49.500 '123\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'This  exercise  will  require  considerable  documentation,  training,  and,  most  of  all,\\ntime.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,538.636,432.004,574.337 'Start  by  communicating  the  broad  vision  for  MLOps  governance  in  the  business.\\nHighlight the dangers of the status quo, outline a process, and detail how it is tailored\\nto the range of use cases.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,482.236,432.001,530.536 'Engage directly with each team involved and build a training program directly with\\nthem. Do not be afraid to leverage their experience to shape not only the training, but\\nalso the detailed implementation of their governance responsibilities. The result will\\nbe much stronger buy-in and more effective governance.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,399.027,432.005,470.667 'Step 8: Monitor and Refine\\nIs the newly implemented governance working? Are the prescribed steps being imple‐\\nmented, and are the objectives being met? What actions should be taken if things are\\ngoing  poorly?  How  do  we  measure  the  gap  between  today’s  reality  and  where  the\\nbusiness needs to be?\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,330.027,432.002,390.927 'Measuring success requires metrics and checks. It requires people to be tasked with\\nmonitoring and a way to address problems. The governance process and the way it is\\nimplemented will need to be refined over time, based both on lessons learned and on\\nevolving requirements (including, as discussed earlier in this chapter, evolving regula‐\\ntory requirements).\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.000,298.827,432.003,321.927 'A  big  factor  in  the  success  of  the  process  will  be  the  diligence  of  the  individuals\\nresponsible for the individual measures in the process, and incentivizing them is key.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,217.227,432.005,290.727 'Monitoring the governance process starts with a clear understanding of the key per‐\\nformance  metrics  and  targets—KPIs  for  governance.  These  should  aim  to  measure\\nboth whether the process is being enacted and whether objectives are being achieved.\\nMonitoring and auditing can be time consuming, so look to automate metrics as far\\nas  possible  and  encourage  individual  teams  to  own  the  monitoring  of  metrics  that\\nrelate to their area of responsibility.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,160.827,432.004,209.127 'It’s  hard  to  make  people  carry  out  tasks  that  seemingly  deliver  nothing  concrete  to\\nthose doing the work. One popular tactic to address this is gamification. This is not\\nabout making everything look like a video game, but about introducing incentives for\\npeople to carry out tasks where the main benefit is derived by others.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,79.227,432.005,152.727 'Look to gamify the governance process in simple ways: publishing KPI results widely\\nis  the  simplest  place  to  start.  Just  being  able  to  see  targets  being  met  is  a  source  of\\nsatisfaction  and  motivation.  Leaderboards,  whether  at  the  team  or  individual  level,\\ncan add some constructive element of competition. For example, people whose work\\nconsistently  passes  compliance  checks  the  first  time,  or  meets  deadlines,  should  be\\nable to feel their efforts are visible.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.999,40.500,84.437,49.500 '124 \\n'>\n",
      "<LTTextBoxHorizontal(10) 92.015,40.500,95.327,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 102.905,40.500,189.557,49.500 'Chapter 8: Model Governance\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.004,605.537 'However, excessive competition can be disruptive and demotivating. A balance must\\nbe struck, and this is best achieved by building up gamification elements slowly over\\ntime.  Start  with  the  least  competition-oriented  and  add  new  elements  one  by  one,\\nmeasuring their effectiveness before adding the next.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,500.836,432.001,549.136 'Monitoring  changes  in  the  governance  landscape  is  essential.  This  might  be  regula‐\\ntory, or it might be about public opinion. Those with responsibility for the strategic\\nvision  must  continue  to  monitor  it  as  well  as  have  a  process  to  evaluate  potential\\nchanges.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,406.636,432.005,492.736 'Finally,  all  monitoring  of  the  process  is  only  worthwhile  if  issues  are  acted  upon.\\nEstablish  a  process  for  agreeing  on  change  and  for  enacting  it.  This  may  result  in\\nrevisiting  policies,  processes,  tools,  responsibilities,  education,  and  monitoring!  It’s\\nnecessary to iterate and refine, but the balance between efficiency and effectiveness is\\nhard to find; many lessons can only be learned the hard way. Build a culture where\\npeople see iteration and refinement as a measure of a successful process, not a failed\\none.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,306.236,432.003,394.297 'Closing Thoughts\\nIt’s hard to separate MLOps from its governance. It’s not possible to successfully man‐\\nage the model life cycle, mitigate the risks, and deliver value at scale without gover‐\\nnance. Governance impacts everything from how the business can acceptably exploit\\nML,  to  the  data  and  algorithms  that  can  be  used,  to  the  style  of  operationalization,\\nmonitoring, and retraining.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,249.836,432.004,298.136 'MLOps at scale is in its infancy. Few businesses are doing it, and even fewer are doing\\nit well. While governance is the key to improving the effectiveness of MLOps, there\\nare  few  tools  today  that  directly  address  this  challenge,  and  there  is  only  piecemeal\\nadvice.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,180.836,432.003,241.736 'Public trust in ML is at risk. Even slow-moving organizations like the EU understand\\nthis. If trust is lost, then so too will be many of the benefits to be derived from ML.\\nAdditional  legislation  is  being  prepared,  but  even  without  this,  businesses  need  to\\nworry  about  the  potential  damage  to  their  public  image  that  can  be  caused  by  an\\ninadvertently harmful model.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.997,111.836,432.004,172.736 'When planning to scale MLOps, start with governance and use it to drive the process.\\nDon’t bolt it on at the end. Think through the policies; think about using tooling to\\ngive a centralized view; engage across the organization. It will take time and iteration,\\nbut  ultimately  the  business  will  be  able  to  look  back  and  be  proud  that  it  took  its\\nresponsibilities seriously.\\n'>\n",
      "<LTTextBoxHorizontal(7) 350.371,40.500,402.517,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(8) 410.095,40.500,413.407,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 420.985,40.500,432.001,49.500 '125\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 161.161,527.418,431.991,581.880 'PART III\\nMLOps: Real-World Examples\\n'>\n",
      "<LTLine 72.000,558.814,432.000,558.814>\n",
      "<LTTextBoxHorizontal(0) 194.754,509.542,431.998,582.331 'CHAPTER 9\\nMLOps in Practice: Consumer\\nCredit Risk Management\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,332.109,432.005,393.009 'In  the  final  chapters  of  this  book,  we  explore  three  examples  of  how  MLOps  might\\nlook in practice. We explicitly chose these three examples because they represent fun‐\\ndamentally different use cases for machine learning and illustrate how MLOps meth‐\\nodology  might  differ  to  suit  the  needs  of  the  business  and  its  ML  model  life  cycle\\npractices.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.994,244.309,432.003,319.770 'Background: The Business Use Case\\nWhen  a  consumer  asks  for  a  loan,  the  credit  institution  has  to  make  a  decision  on\\nwhether or not to grant it. Depending on the case, the amount of automation in the\\nprocess  may  vary.  However,  it  is  very  likely  that  the  decision  will  be  informed  by\\nscores that estimate the probability that the loan will or will not be repaid as expected.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,225.709,315.502,236.209 'Scores are routinely used at different stages of the process:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.652,203.109,431.997,213.609 '• At the prescreen stage, a score computed with a small number of features allows\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,190.509,307.404,201.009 'the institution to quickly discard some applications.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,173.909,431.997,184.409 '• At  the  underwriting  stage,  a  score  computed  with  all  the  required  information\\n'>\n",
      "<LTTextBoxHorizontal(7) 90.002,161.309,267.462,171.809 'gives a more precise basis for the decision.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.660,144.709,431.997,155.209 '• After the underwriting stage, scores can be used to assess the risk associated with\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,132.109,180.533,142.609 'loans in the portfolio.\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.996,71.709,432.005,120.009 'Analytics  methods  have  been  used  for  decades  to  compute  these  probabilities.  For\\nexample,  the  FICO  score  has  been  used  since  1995  in  the  United  States.  Given  the\\ndirect  impact  they  have  on  the  institutions’  revenues  and  on  customers’  lives,  these\\npredictive  models  have  always  been  under  great  scrutiny.  Consequently,  processes,\\n'>\n",
      "<LTTextBoxHorizontal(11) 420.980,40.500,431.996,49.500 '129\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'methods,  and  skills  have  been  formalized  into  a  highly  regulated  environment  to\\nensure the sustainable performance of models.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.436,432.005,574.337 'Whether the models are based on expert-made rules, on classical statistical models,\\nor on more recent machine learning algorithms, they all have to comply with similar\\nregulations. Consumer credit risk management can therefore be seen as a precursor\\nof  MLOps:  parallels  with  other  use  cases  as  well  as  best  practices  can  be  analyzed\\nbased on this use case.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,431.836,432.005,505.336 'At the time a credit decision is made, information about the customer’s historical and\\ncurrent situation is usually available. How much credit does the customer hold? Has\\nthe customer ever not repaid a loan (in credit jargon, is the customer a delinquent)?\\nIn  some  countries,  organizations  called  credit  bureaus  collect  this  information  and\\nmake it available to creditors either directly or through the form of a score (like the\\naforementioned FICO score).\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,337.636,432.003,423.736 'The definition of the target to be predicted is more complex. A customer not repay‐\\ning as expected is a “bad” outcome in credit risk modeling. In theory, one should wait\\nfor the complete repayment to determine a “good” outcome and for the loss charge\\noff to determine a “bad” outcome. However, it may take a long time to obtain these\\nultimate figures, and waiting for them would deter reactivity to changing conditions.\\nAs a result, trade-offs are usually made, based on various indicators, to declare “bad”\\noutcomes before the losses are certain.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,199.436,432.005,325.296 'Model Development\\nHistorically,  credit  risk  modeling  is  based  on  a  mix  of  rules  (“manual  feature  engi‐\\nneering” in modern ML jargon) and logistic regression. Expert knowledge is vital to\\ncreating a good model. Building adapted customer segmentation as well as studying\\nthe  influence  of  each  variable  and  the  interactions  between  variables  requires  enor‐\\nmous  time  and  effort.  Combined  with  advanced  techniques  like  two-stage  models\\nwith offset, advanced general linear models based on Tweedie distribution, or monot‐\\nonicity  constraints  on  one  side  and  financial  risk  management  techniques  on  the\\nother side, this makes the field a playground for actuaries.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,105.236,432.004,191.336 'Gradient boosting algorithms like XGBoost have reduced the cost to build good mod‐\\nels. However, their validation is made more complex by the black box effect: it’s hard\\nto get the feeling that such models give sensible results whatever the inputs. Never‐\\ntheless, credit risk modelers have learned to use and validate these new types of mod‐\\nels.  They  have  developed  new  validation  methodologies  based,  for  example,  on\\nindividual explanations (e.g., Shapley values) to build trust into their models, which is\\na critical component of MLOps, as we’ve explored throughout this book.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,40.500,84.436,49.500 '130 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.014,40.500,95.326,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.904,40.500,293.308,49.500 'Chapter 9: MLOps in Practice: Consumer Credit Risk Management\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.998,545.112,432.004,607.973 'Model Bias Considerations\\nThe modeler also has to take into account selection biases, as the model will inevita‐\\nbly be used to reject applicants. As a result, the population to which a loan is granted\\nis not representative of the applicant population.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,463.512,432.003,537.012 'By training a model version on the population selected by the previous model version\\nwithout care, the data scientist would make a model unable to accurately predict on\\nthe rejected population because it is not represented in the training dataset, while it is\\nexactly  what  is  expected  from  the  model.  This  effect  is  called  cherry-picking.  As  a\\nresult,  special  methods,  like  reweighting  based  on  the  applicant  population  or  cali‐\\nbrating the model based on external data, have to be used.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,381.912,432.005,455.412 'Models that are used for risk assessment and not only to make decisions about grant‐\\ning  loans  have  to  produce  probabilities  and  not  only  yes/no  outcomes.  Usually,  the\\nprobability produced directly by prediction models is not accurate. While it is not an\\nissue  if  data  scientists  apply  thresholding  to  obtain  a  binary  classification,  they  will\\nusually  need  a  monotonous  transformation  called  a  calibration  to  recover  “true”\\nprobabilities as evaluated on historical data.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.003,363.312,317.220,373.812 'The model validation for this use case typically consists of:\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.658,340.712,431.997,351.212 '• Testing  its  performance  on  out-of-sample  datasets,  chosen  after  (or,  in  some\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,328.112,268.533,338.612 'cases, before, as well) the training datasets.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,248.512,432.005,322.012 '• Investigating the performance not only overall, but also per subpopulation. The\\nsubpopulations  would  typically  have  customer  segments  based  on  revenue,  and\\nwith  the  rise  of  Responsible  AI,  other  segmenting  variables  like  gender  or  any\\nprotected attribute according to local regulation. Risks of not doing so can result\\nin serious damages, as Apple learned the hard way in 2019 when its credit card\\nwas said to be “sexist” against women applying for credit.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.992,173.312,432.005,236.172 'Prepare for Production\\nGiven  the  significant  impact  of  credit  risk  models,  their  validation  process  involves\\nsignificant work with regard to the modeling part of the life cycle, and it includes the\\nfull documentation of:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.654,150.712,148.513,161.212 '• The data used\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,134.112,285.948,144.612 '• The model and the hypothesis made to build it\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.655,117.512,317.658,128.012 '• The validation methodology and the validation results\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.655,100.912,215.871,111.412 '• The monitoring methodology\\n'>\n",
      "<LTTextBoxHorizontal(12) 323.601,40.500,402.513,49.500 'Model Bias Considerations \\n'>\n",
      "<LTTextBoxHorizontal(13) 410.091,40.500,413.403,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(14) 420.981,40.500,431.997,49.500 '131\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,544.636,432.005,605.537 'The monitoring methodology in this scenario is twofold: data and performance drift.\\nAs the delay between the prediction and obtaining the ground truth is long (typically\\nthe duration of the loan plus a few months to take into account late payments), it is\\nnot enough to monitor the model performance: data drift also has to be monitored\\ncarefully.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.998,425.236,432.977,536.536 'For  example,  should  an  economic  recession  occur  or  should  the  commercial  policy\\nchange, it is likely that the applicant population would change in such a way that the\\nmodel’s performance could not be guaranteed without further validation. Data drift is \\nusually performed by customer segment with generic statistical metrics that measure\\ndistances  between  probability  distributions  (like  Kolmogorov-Smirnov  or  Wasser‐\\nstein distances) and also with metrics that are specific to financial services, like popu‐\\nlation  stability  index  and  characteristic  stability  index.  Performance  drift  is  also\\nregularly assessed on subpopulations with generic metrics (AUC) or specific metrics\\n(Kolmogorov-Smirnov, Gini).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,331.036,432.005,417.136 'The model documentation is usually reviewed by an MRM team in a very formal and\\nstandalone process. Such an independent review is a good practice to make sure that\\nthe right questions are asked of the model development team. In some critical cases,\\nthe validation team may rebuild the model from scratch given the documentation. In\\nsome  cases,  the  second  implementation  is  made  using  an  alternative  technology  to\\nestablish  confidence  in  documented  understanding  of  the  model  and  to  highlight\\nunseen bugs deriving from the original toolset.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,287.236,432.002,322.936 'Complex and time-consuming model validation processes have an implication on the\\nentire MLOps life cycle. Quick-fixes and rapid model iteration are not possible with\\nsuch lengthy QA and lead to a very slow and deliberate MLOps life cycle.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,199.436,432.003,274.896 'Deploy to Production\\nIn a typical large financial services organization, the production environment is not\\nonly separate from the design environment, but also likely to be based on a different\\ntechnical  stack.  The  technical  stack  for  critical  operations—like  transaction  valida‐\\ntion, but also potentially loan validation—will always evolve slowly.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,117.836,432.004,191.336 'Historically,  the  production  environments  have  mainly  supported  rules  and  linear\\nmodels  like  logistic  regression.  Some  can  handle  more  complex  models  such  as\\nPMML  or  JAR  file.  For  less  critical  use  cases,  Docker  deployment  or  deployment\\nthrough integrated data science and machine learning platforms may be possible. As\\na result, the operationalization of the model may involve operations that range from\\nclicking on a button to writing a formula based on a Microsoft Word document.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,74.036,432.005,109.736 'Activity  logging  of  the  deployed  model  is  essential  for  monitoring  model  perfor‐\\nmance in such a high-value use case. Depending on the frequency of the monitoring,\\nthe  feedback  loop  may  be  automated  or  not.  For  example,  automation  may  not  be\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,40.500,84.434,49.500 '132 \\n'>\n",
      "<LTTextBoxHorizontal(8) 92.012,40.500,95.324,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 102.902,40.500,293.306,49.500 'Chapter 9: MLOps in Practice: Consumer Credit Risk Management\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'necessary if the task is performed only once or twice a year and the largest amount of\\ntime is spent asking questions of the data. On the other hand, automation might be\\nessential if the assessment is done weekly, which may be the case for short-term loans\\nwith durations of a few months.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,419.036,432.004,544.897 'Closing Thoughts\\nFinancial services have been developing schemes for prediction model validation and\\nmonitoring for decades. They have been able to continuously adapt to new modeling\\ntechnologies like gradient boosting methods. Given their important impact, the pro‐\\ncesses around the life cycle management of these models are well formalized and even\\nincorporated into many regulations. As a result, they can be a source of best practices\\nfor MLOps in other domains, though adaptations are needed as the trade-off between\\nrobustness  on  one  side  and  cost  efficiency,  time  to  value,  and—importantly—team\\nfrustration on the other may be different in other businesses.\\n'>\n",
      "<LTTextBoxHorizontal(2) 350.369,40.500,402.515,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(3) 410.093,40.500,413.405,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(4) 420.983,40.500,431.999,49.500 '133\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 191.374,509.542,431.999,582.331 'CHAPTER 10\\nMLOps in Practice: Marketing\\nRecommendation Engines\\n'>\n",
      "<LTTextBoxHorizontal(1) 339.725,412.542,431.999,425.542 'Makoto Miyazaki\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,323.509,432.003,384.409 'Recommendation  engines  have  become  very  popular  in  the  last  20  years,  from  the\\nfirst  Amazon  book  recommendations  to  today’s  generalized  use  in  digital  shops,\\nadvertisements, and music and video streaming. We have all become accustomed to\\nthem. However, throughout the years, the underlying technologies behind these rec‐\\nommendation engines have evolved.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.999,279.709,432.005,315.409 'This  chapter  covers  a  use  case  that  illustrates  the  adaption  of  and  need  for  MLOps\\nstrategies  given  the  particularities  of  a  fast-paced  and  rapidly  changing  machine\\nlearning model life cycle.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,154.109,432.005,267.370 'The Rise of Recommendation Engines\\nHistorically,  marketing  recommendations  were  human-built.  Based  on  qualitative\\nand quantitative marketing studies, marketing moguls would set up rules that stati‐\\ncally  defined  the  impression  (in  the  sense  of  advertising  views)  sent  to  a  customer\\nwith  given  characteristics.  This  technique  gave  rise  to  the  marketing  data  mining\\nurban legend that a grocery chain discovered that men who bought diapers on Thurs‐\\ndays  and  Saturdays  were  more  likely  to  buy  beer  as  well  and  hence  placing  the  two\\nnext to each other will increase beer sales.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,85.109,432.004,146.009 'Overall, recommendation engines created manually presented numerous bottlenecks\\nthat  resulted  in  a  significant  amount  of  wasted  money:  it  was  hard  to  build  rules\\nbased  on  many  different  customer  features  because  the  rule  creation  process  was\\nmanual, it was hard to set up experiments to test many different kinds of impressions,\\nand it was hard to update the rules when the behavior of the customers changed.\\n'>\n",
      "<LTTextBoxHorizontal(6) 420.982,40.500,431.998,49.500 '135\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.996,485.855,432.005,607.894 'The Role of Machine Learning\\nThe  rise  of  ML  has  brought  a  new  paradigm  to  recommendation  engines,  allowing\\nfor the elimination of rules based on human insight. A new class of algorithm called\\ncollaborative filtering dominates the field. This algorithm is able to analyze customer\\nand purchase data with millions of customers and tens of thousands of products to\\nperform  recommendations  without  any  prior  marketing  knowledge.  By  identifying\\nefficiently what customers that look like the current customer bought, marketers can\\nrely  on  automatic  strategies  that  outperform  traditional  ones  both  in  cost  and\\nefficiency.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,391.655,432.005,477.755 'Because the process of building strategies is automatic, it is possible to update them\\nregularly and to compare them using A/B testing or shadow scoring schemes (includ‐\\ning  the  way  to  choose  the  impression  among  all  possibilities).  Note  that  these  algo‐\\nrithms may be combined with more classical business rules for various reasons—e.g.,\\navoiding  the  filtering  bubble,  not  selling  a  product  in  a  given  geographical  area,  or\\npreventing the use of a specific association that is statistically meaningful but unethi‐\\ncal to use (like proposing alcohol to a recovering alcoholic), to name a few.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,308.446,432.001,380.085 'Push or Pull?\\nWhen implementing a recommendation engine, it is important to keep in mind that\\nits  structure  will  depend  on  whether  the  recommendations  are  pushed  or  pulled. \\nPush  channels  are  the  easiest  to  handle;  for  example,  they  can  consist  of  sending\\nemails or making outbound calls. \\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,252.046,432.002,300.346 'The recommendation engine can be run on a regular basis in batch mode (typically\\nbetween  once  a  day  and  once  a  month),  and  it  is  easy  to  split  the  customer  dataset\\ninto several parts to perform analysis within a sound experimental design. The regu‐\\nlarity of the process allows for regular review and optimization of the strategy.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,183.046,432.004,243.946 'Pull channels are often more effective because they provide information to customers\\nwhen they need it—for example, when doing an online search or when calling a cus‐\\ntomer service line. Specific information from the session can be used (e.g., what the\\nuser has searched for) to precisely tailor the recommendation. Music streaming plat‐\\nforms, for instance, provide pull-channel recommendations for playlists.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,139.246,432.003,174.946 'Recommendations  can  be  prebaked,  as  illustrated  in  the  in-depth  example  in  this\\nchapter, or made in real time. In the latter case, a special architecture has to be set up\\nto compute recommendations on the fly.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.995,70.246,432.004,131.146 'Comparing strategies in a pull context is more challenging. First, the customers who\\ncall  in  on  a  given  channel  are  likely  to  differ  from  the  average  customer.  In  simple\\ncases, it is possible to randomly choose the strategy to use for each recommendation,\\nbut it also happens that the strategy needs to be used consistently over a given period\\nfor  a  given  customer.  This  usually  results  in  an  unbalanced  proportion  of\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.004,40.500,84.442,49.500 '136 \\n'>\n",
      "<LTTextBoxHorizontal(8) 92.020,40.500,95.332,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 102.910,40.500,302.926,49.500 'Chapter 10: MLOps in Practice: Marketing Recommendation Engines\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.004,605.537 'recommendations made with each strategy, which makes the statistical treatment to\\ndecide which one is the best more complex. However, once a good framework is set,\\nthis allows a very quick improvement cycle, as many strategies can be tested in real\\ntime.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.999,494.636,431.792,544.897 'Data Preparation\\nThe customer data that is usually accessible to a recommendation engine is composed\\nof the following:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.654,472.036,383.808,482.536 '• Structural information about the customer (e.g., age, gender, location)\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.655,455.436,407.223,465.936 '• Information about historical activities (e.g., past views, purchases, searches)\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,438.836,315.978,449.336 '• Current context (e.g., current search, viewed product)\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,391.036,432.000,426.736 'Whatever  the  technique  used,  all  customer  information  has  to  be  aggregated  into  a\\nvector (a list of fixed size) of characteristics. For example, from the historical activi‐\\nties, the following characteristics could be extracted:\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,368.436,342.186,378.936 '• Amount of purchases during the last week or the last month\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,351.836,245.943,362.336 '• Number of views during past periods\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.655,335.236,319.012,345.736 '• Increase in spending or in views during the last month\\n'>\n",
      "<LTTextBoxHorizontal(9) 80.655,318.636,312.796,329.136 '• Previously seen impressions and customer’s response\\n'>\n",
      "<LTTextBoxHorizontal(10) 71.997,258.236,432.005,306.536 'In  addition  to  customer  data,  the  recommendation  context  can  also  be  taken  into\\naccount.  For  example,  days  to  summer  for  seasonal  products  like  above-ground\\nswimming pools or days to monthly pay day, as some customers delay purchases for\\ncash flow reasons.\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.996,201.836,432.002,250.136 'Once the customer and context data is formatted, it is important to define the set of\\npossible recommendations, and there are many choices to make. The same product\\nmay  be  presented  with  different  offers,  which  may  be  communicated  in  different\\nways.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.997,132.836,432.005,193.736 'It is of the utmost importance not to forget the “do not recommend anything” option.\\nIndeed, most of us have the personal experience that not all recommendations have a\\npositive  impact.  Sometimes  not  showing  anything  might  be  better  than  the  alterna‐\\ntives. It’s also important to consider that some customers may not be entitled to see\\ncertain recommendations, for example depending on their geographical origin.\\n'>\n",
      "<LTTextBoxHorizontal(13) 350.429,40.500,402.512,49.500 'Data Preparation \\n'>\n",
      "<LTTextBoxHorizontal(14) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(15) 420.980,40.500,431.996,49.500 '137\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,519.912,432.003,607.973 'Design and Manage Experiments\\nTo leverage the continuous improvement potential of recommendation engines, it is\\nnecessary  to  experiment  with  different  strategies  within  a  sound  framework.  When\\ndesigning a prediction model for a recommendation engine, the data scientist might\\nwell  focus  on  a  simple  strategy,  such  as  predicting  the  probability  that  a  given  cus‐\\ntomer clicks on a given recommendation.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,425.712,432.005,511.812 'This may seem a reasonable compromise compared to the more precise approach of\\ntrying to gather information about whether the customer purchased the product and\\nwhether to attribute this purchase to a given recommendation. However, this is not\\nadequate from a business perspective, as phenomena like cannibalization may occur\\n(i.e., by showing a low-margin product to a customer, one might prevent them from\\nbuying  a  high-margin  product).  As  a  result,  even  if  the  predictions  were  good  and\\nresulted in increased sales volume, the overall revenue might be reduced.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,356.712,432.005,417.612 'On the other hand, bluntly promoting the organization’s interest and not the custom‐\\ner’s could also have detrimental long-term consequences. The overarching KPI that is\\nused  to  assess  if  a  given  strategy  yields  better  results  should  be  carefully  chosen,\\ntogether with the time period over which it is evaluated. Choosing the revenue over a\\ntwo-week period after the recommendation as the main KPI is common practice.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,224.712,432.005,348.612 'To be as close as possible to an experimental setting, also called A/B testing, the con‐\\ntrol  group  and  the  experimental  groups  have  to  be  carefully  chosen.  Ideally,  the\\ngroups are defined before the start of the experiment by randomly splitting the cus‐\\ntomer base. If possible, customers should not have been involved in another experi‐\\nmentation recently so that their historical data is not polluted by its impact. However,\\nthis may not be possible in a pull setting in which many new customers are coming\\nin. In this case, the assignment could be decided on the fly. The size of the groups as\\nwell as the duration of the experiments depend on the expected magnitude of the KPI\\ndifference  between  the  two  groups:  the  larger  the  expected  effect,  the  smaller  the\\ngroup size and the shorter the duration.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.998,168.312,432.004,216.612 'The experimentation could also be done in two steps: in the first one, two groups of\\nequal but limited size could be selected. If the experimentation is positive, the deploy‐\\nment could start with 10% on the new strategy, a proportion that can be raised pro‐\\ngressively if results are in line with expectations.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,80.512,432.004,155.972 'Model Training and Deployment\\nTo better illustrate the MLOps process for this type of use case, the following sections\\nfocus  on  the  specific  example  of  a  hypothetical  company  deploying  an  automated\\npipeline to train and deploy recommendation engines. The company is a global soft‐\\nware company (let’s call it MarketCloud) headquartered in Silicon Valley.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.003,40.500,84.441,49.500 '138 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.019,40.500,95.331,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.909,40.500,302.925,49.500 'Chapter 10: MLOps in Practice: Marketing Recommendation Engines\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'One  of  MarketCloud’s  products  is  a  software-as-a-service  (SaaS)  platform  called\\nSalesCore. SalesCore is a B2B product that allows its users (businesses) to drive sales\\nto  customers  in  a  simple  manner  by  keeping  track  of  deals,  clearing  tedious\\nadministration tasks off their desks, and creating customized product offers for each\\ncustomer (see Figure 10-1).\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.003,536.536 'From time to time, MarketCloud’s clients use the cloud-based SalesCore while on a\\ncall with their customers, adjusting their sales strategy by looking at past interactions\\nwith  the  customers  as  well  as  at  the  product  offers  and  discounts  suggested  by\\nSalesCore.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,444.436,432.000,480.136 'MarketCloud is a mid-sized company with an annual revenue of around $200 million\\nand a few thousand employees. From salespeople at a brewing company to those in a\\ntelecommunication entity, MarketCloud’s clients represent a range of businesses.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.005,231.386,427.661,254.486 'Figure 10-1. Mock-up of the SalesCore platform, the basis of the theoretical company on\\nwhich this section’s example is based\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,131.186,432.005,217.286 'MarketCloud would like to automatically display product suggestions on SalesCore to\\nthe salespeople trying to sell products to the customers. Suggestions would be made\\nbased on customers’ information and their past interaction records with the salesper‐\\nson;  suggestions  would  therefore  be  customized  for  each  customer.  In  other  words,\\nSalesCore  is  based  on  a  recommendation  engine  used  in  a  pull  (inbound  calls)  or\\npush (outbound calls) context. Salespeople would be able to incorporate in their sales\\nstrategy the suggested products while on a call with their customers.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,87.386,432.005,123.086 'To implement this idea, MarketCloud needs to build a recommendation engine and\\nembed  it  into  SalesCore’s  platform,  which,  from  a  model  training  and  deployment\\nstandpoint, presents several challenges. We’ll present these challenges in this section,\\n'>\n",
      "<LTTextBoxHorizontal(6) 305.852,40.500,402.512,49.500 'Model Training and Deployment \\n'>\n",
      "<LTTextBoxHorizontal(7) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 420.980,40.500,431.996,49.500 '139\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,438.175,432.500,438.175>\n",
      "<LTLine 432.375,260.050,432.375,438.300>\n",
      "<LTLine 72.000,260.175,432.500,260.175>\n",
      "<LTLine 72.125,260.050,72.125,438.300>\n",
      "<LTFigure(I1) 79.450,266.030,425.050,433.050 matrix=[345.60,0.00,0.00,167.02, (79.45,266.03)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'and in the next section we’ll show MLOps strategies that allow the company to handle\\neach of them.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,499.227,432.001,570.867 'Scalability and Customizability\\nMarketCloud’s business model (selling software for client companies to help them sell\\ntheir  own  products)  presents  an  interesting  situation.  Each  client  company  has  its\\nown  dataset,  mainly  about  its  products  and  customers,  and  it  doesn’t  wish  to  share\\nthe data with other companies.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,430.227,432.003,491.127 'If MarketCloud has around four thousand clients using SalesCore, that means instead\\nof having a universal recommender system for all the clients, it would need to create\\nfour thousand different systems. MarketCloud needs to come up with a way to build\\nfour  thousand  recommendation  systems  as  efficiently  as  possible  since  there  is  no\\nway it can handcraft that many systems one by one.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,284.018,432.005,418.658 'Monitoring and Retraining Strategy\\nEach  of  the  four  thousand  recommendation  engines  would  be  trained  on  the  cus‐\\ntomer data of the corresponding client. Therefore, each of them would be a different\\nmodel,  yielding  a  different  performance  result  and  making  it  nearly  impossible  for\\nthe company to manually keep an eye on all four thousand. For example, the recom‐\\nmendation engine for client A in the beverage industry might consistently give good\\nproduct  suggestions,  while  the  engine  for  client  B  in  the  telecommunication  sector\\nmight seldom provide good suggestions. MarketCloud needed to come up with a way\\nto automate the monitoring and the subsequent model retraining strategy in case the\\nperformance degraded.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,188.209,432.003,272.449 'Real-Time Scoring\\nIn  many  situations,  MarketCloud’s  clients  use  SalesCore  when  they  are  talking  to\\ntheir customers on the phone. The sales negotiation evolves every single minute dur‐\\ning  the  call,  and  the  salesperson  needs  to  adjust  the  strategy  during  the  interaction\\nwith the customer, so the recommendation engine has to be responsive to real-time\\nrequests.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.995,94.009,432.005,180.109 'For  example,  imagine  you  as  a  salesperson  are  on  a  call  with  your  customer  to  sell\\ntelecommunication  devices.  The  customer  tells  you  what  his  office  looks  like,  the\\nexisting infrastructure at the office such as an optic fiber, the type of WiFi network,\\nand so forth. Upon entering this information in SalesCore, you want the platform to\\ngive  you  a  suggestion  for  the  products  that  your  customer  could  feasibly  purchase.\\nThis response from the platform needs to be in real time, not 10 minutes later, after\\nthe call, or on the following day.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.001,40.500,84.439,49.500 '140 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.017,40.500,95.329,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.907,40.500,302.923,49.500 'Chapter 10: MLOps in Practice: Marketing Recommendation Engines\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.996,498.455,432.004,607.894 'Ability to Turn Recommendations On and Off\\nResponsible AI principles acknowledge that retaining human involvement is impor‐\\ntant. This can be done through a human-in-command design,1 by which it should be\\npossible  not  to  use  the  AI.  In  addition,  adoption  is  likely  to  be  low  if  users  cannot\\noverride  AI  recommendations.  Some  clients  value  using  their  own  intuition  about\\nwhich  products  to  recommend  to  their  customers.  For  this  reason,  MarketCloud\\nwants to give its clients full control to turn the recommendation engine on and off so\\nthat the clients can use the recommendations when they want.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,385.455,432.003,486.115 'Pipeline Structure and Deployment Strategy\\nTo efficiently build four thousand recommendation engines, MarketCloud decided to\\nmake one data pipeline as a prototype and duplicate it four thousand times. This pro‐\\ntotype pipeline consists of the necessary data preprocessing steps and a single recom‐\\nmendation  engine,  built  on  an  example  dataset.  The  algorithms  used  in  the\\nrecommendation engines will be the same across all four thousand pipelines, but they\\nwill be trained with the specific data associated with each client (see Figure 10-2).\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,211.504,428.724,234.604 'Figure 10-2. Image of data pipeline structure for MarketCloud’s recommendation engine\\nproject\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,149.104,432.005,197.404 'In this way, MarketCloud can efficiently launch four thousand recommendation sys‐\\ntems. The users will still retain some room for customization, because the engine is\\ntrained with their own data, and each algorithm will work with different parameters\\n—i.e., it’s adopted to the customer and product information of each client.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,117.904,431.999,141.004 'What makes it possible to scale up a single pipeline to four thousand pipelines is the\\nuniversal schema of the dataset. If a dataset from client A has 100 columns whereas\\n'>\n",
      "<LTTextBoxHorizontal(5) 73.140,81.954,415.240,89.954 '1 For an explanation of human-in-command design, see Karen Yeung, “Responsibility and AI” (Council of\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.000,71.954,228.848,79.954 'Europe study, DGI(2019)05), 64, footnote 229.\\n'>\n",
      "<LTTextBoxHorizontal(7) 270.590,40.500,402.512,49.500 'Pipeline Structure and Deployment Strategy \\n'>\n",
      "<LTTextBoxHorizontal(8) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 420.980,40.500,431.996,49.500 '141\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,379.193,432.500,379.193>\n",
      "<LTLine 432.375,240.168,432.375,379.318>\n",
      "<LTLine 72.000,240.293,432.500,240.293>\n",
      "<LTLine 72.125,240.168,72.125,379.318>\n",
      "<LTLine 72.000,97.900,162.000,97.900>\n",
      "<LTFigure(I1) 79.809,246.148,424.691,374.068 matrix=[344.88,0.00,0.00,127.92, (79.81,246.15)]>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'client  B  has  50,  or  if  the  column  “number  of  purchased  items”  from  client  A  is  an\\ninteger  whereas  the  same  column  from  client  B  is  a  string,  they  would  need  to  go\\nthrough different preprocessing pipelines.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.436,432.005,561.736 'Although each client has different customer and product data, at the point that this\\ndata is registered on SalesCore, it acquires the same number of columns and the same\\ndata types for each column. This makes things easier, as MarketCloud simply needs\\nto copy a single pipeline four thousand times.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,431.836,432.005,505.336 'Each recommendation system embedded in the four thousand pipelines will have dif‐\\nferent API endpoints. On the surface, it looks like when a user clicks the “show prod‐\\nuct recommendations” button, SalesCore displays a list of suggested products. But in\\nthe background, what is happening is that by clicking the button, the user is hitting\\nthe  specific  API  endpoint  associated  with  the  ranked  product  lists  for  the  specific\\ncustomer.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,331.436,432.004,419.497 'Monitoring and Feedback\\nMaintaining  four  thousand  recommendation  systems  is  not  an  easy  task,  and  while\\nthere  have  already  been  many  MLOps  considerations  until  this  point,  this  is  maybe\\nthe most complex part. Each system’s performance needs to be monitored and upda‐\\nted as needed. To implement this monitoring strategy at a large scale, MarketCloud\\ncan automate the scenario for retraining and updating the models.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,223.027,432.005,319.866 'Retraining Models\\nClients obtain  new customers, some of the customers churn, every once in a while\\nnew  products  are  added  to  or  dropped  from  their  catalogs;  the  bottom  line  is  that\\ncustomer  and  product  data  are  constantly  changing,  and  recommendation  systems\\nhave  to  reflect  the  latest  data.  It’s  the  only  way  they  can  maintain  the  quality  of  the\\nrecommendation, and, more importantly, avoid a situation such as recommending a\\nWiFi router that is outdated and no longer supported.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,166.627,432.005,214.927 'To reflect the latest data, the team could program a scenario to automatically update\\nthe database with the newest customer and product data, retraining the model with\\nthe  latest  datasets  every  day  at  midnight.  This  automation  scenario  could  then  be\\nimplemented in all four thousand data pipelines.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,110.227,432.002,158.527 'The  retraining  frequency  can  differ  depending  on  the  use  case.  Thanks  to  the  high\\ndegree of automation, retraining every night in this case is possible. In other contexts,\\nretraining  could  be  triggered  by  various  signals  (e.g.,  signification  volume  of  new\\ninformation or drift in customer behavior, be it aperiodic or seasonal).\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,79.027,432.003,102.127 'In addition, the delay between the recommendation and the point in time at which its\\neffect  is  evaluated  has  to  be  taken  into  account.  If  the  impact  is  only  known  with  a\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.000,40.500,84.438,49.500 '142 \\n'>\n",
      "<LTTextBoxHorizontal(9) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(10) 102.906,40.500,302.922,49.500 'Chapter 10: MLOps in Practice: Marketing Recommendation Engines\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,557.236,432.005,605.537 'delay of several months, it is unlikely that retraining every day is adequate. Indeed, if\\nthe behavior changes so fast that retraining it every day is needed, it is likely that the\\nmodel  is  completely  outdated  when  it  is  used  to  make  recommendations  several\\nmonths after the most recent ones in the training data.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,461.427,432.004,545.667 'Updating Models\\nUpdating models is also one of the key features of automation strategies at scale. In\\nthis case, for each of the four thousand pipelines, retrained models must be compared\\nto  the  existing  models.  Their  performances  can  be  compared  using  metrics  such  as\\nRMSE  (root-mean-square  error),  and  only  when  the  performance  of  the  retrained\\nmodel beats the prior one does the retrained model get deployed to SalesCore.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,365.618,432.003,449.858 'Runs Overnight, Sleeps During Daytime\\nAlthough  the  model  is  retrained  every  day,  users  do  not  interact  directly  with  the\\nmodel. Using the updated model, the platform actually finishes calculating the ranked\\nlist of products for all the customers during the night. On the following day, when a\\nuser hits the “show product recommendations” button, the platform simply looks at\\nthe customer ID and returns the ranked list of products for the specific customer.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,309.218,432.004,357.518 'To the user, it looks as if the recommendation engine is running in real time. In real‐\\nity, however, everything is already prepared overnight, and the engine is sleeping dur‐\\ning daytime. This makes it possible to get the recommendation instantly without any\\ndowntime.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,226.009,432.001,297.649 'Option to Manually Control Models\\nAlthough the monitoring, retraining, and updating of the models is fully automated,\\nMarketCloud still leaves room for its clients to turn the models on and off. More pre‐\\ncisely, MarketCloud allows the users to choose from three options to interact with the\\nmodels:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.657,203.409,387.735,213.909 '• Turn on to get the recommendation based on the most updated dataset\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.655,186.809,404.503,197.309 '• Freeze to stop retraining with the new data, but keep using the same model\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,170.209,431.111,180.709 '• Turn off to completely stop using the recommendation functionality of SalesCore\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.997,97.209,432.005,158.109 'Machine learning algorithms attempt to convert practical knowledge into meaningful\\nalgorithms  to  automate  processing  tasks.  However,  it  is  still  good  practice  to  leave\\nroom  for  users  to  rely  on  their  domain  knowledge,  as  they  are  presumed  to  be  far\\nmore  capable  of  identifying,  articulating,  and  demonstrating  day-to-day  process\\nproblems in business.\\n'>\n",
      "<LTTextBoxHorizontal(9) 325.703,40.500,402.518,49.500 'Monitoring and Feedback \\n'>\n",
      "<LTTextBoxHorizontal(10) 410.096,40.500,413.408,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 420.986,40.500,432.002,49.500 '143\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,532.036,432.005,605.537 'The second option is important because it allows users to stay in the current quality\\nof  the  recommendation  without  having  the  recommendation  engines  updated  with\\nthe newer data. Whether the current model is replaced with a retrained one depends\\non the mathematical evaluation based on metrics such as the RMSE. However, if users\\nfeel  that  the  product  recommendations  on  SalesCore  are  already  working  well  for\\npushing sales, they have the choice not to risk changing the recommendation quality.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,423.627,432.005,520.467 'Option to Automatically Control Models\\nFor those that don’t want to manually handle the models, the platform could also pro‐\\npose A/B testing so that the impact of new versions is tested before fully switching to\\nthem. Multi-armed bandit algorithms (an algorithm that allows for maximization of\\nthe revenue of a user facing multiple slot machines, each with a different probability\\nto win and a different proportion of the money given back on average) are used for\\nthis purpose.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,329.427,432.004,415.527 'Let’s assume that several model versions are available. The goal is to use the most effi‐\\ncient one, but to do that, the algorithm obviously has to first learn which is the most\\nefficient.  Therefore,  it  balances  these  two  objectives:  sometimes,  it  tries  algorithms\\nthat  may  not  be  the  most  efficient  to  learn  if  they  are  efficient  (exploration),  and\\nsometimes it uses the version that is likely to be the most efficient to maximize the\\nrevenue  (exploitation).  In  addition,  it  forgets  past  information,  as  the  algorithm\\nknows the most efficient today may not be the most efficient tomorrow.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,285.627,432.004,321.327 'The  most  advanced  option  consists  in  training  different  models  for  different  KPIs\\n(click, buy, expected revenue, etc.). A method inspired from ensemble models would\\nthen allow for the solving of conflicts between models.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,177.218,432.005,274.058 'Monitoring Performance\\nWhen  a  salesperson  suggests  a  customer  buy  the  products  recommended  by  Sales‐\\nCore,  the  interaction  of  the  customer  with  the  recommended  products  as  well  as\\nwhether the customer bought them or not is recorded. This record can then be used\\nto  keep  track  of  the  performance  of  the  recommender  system,  overwriting  the  cus‐\\ntomer and product dataset with this record to feed the most updated information to\\nthe model when it is retrained.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,83.018,432.005,169.118 'Thanks  to  this  ground  truth  recording  process,  dashboards  showing  model  perfor‐\\nmance  can  be  presented  to  the  user,  including  performance  comparison  from  A/B\\ntesting. Because the ground truth is obtained quickly, data drift monitoring is secon‐\\ndary. A version of the model is trained every night, but, thanks to the freeze mecha‐\\nnism, the user can choose the active version based on the quantitative information. It\\nis customary to keep the human in the loop on these high-impact decisions where the\\nperformance metrics have a hard time capturing the full context around the decision.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.999,40.500,84.437,49.500 '144 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.015,40.500,95.327,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.905,40.500,302.921,49.500 'Chapter 10: MLOps in Practice: Marketing Recommendation Engines\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'In the case of A/B testing, it is important that only one experiment be done at a time\\non a group of customers; the impact of combined strategies cannot be simply added.\\nWith such considerations in mind, it is possible to build a sound baseline to perform\\na  counterfactual  analysis  and  derive  the  increased  revenue  and/or  the  decreased\\nchurn linked to a new strategy.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,488.236,432.003,536.536 'Apart  from  this,  MarketCloud  can  also  monitor  the  algorithm  performance  at  a\\nmacro level, by checking how many clients froze or turned off the recommender sys‐\\ntems.  If  many  clients  turned  off  the  recommender  systems,  that’s  a  strong  indicator\\nthat they are not satisfied with the recommendation quality.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,375.236,432.003,475.897 'Closing Thoughts\\nThis  use  case  is  peculiar  in  the  sense  that  MarketCloud  built  a  sales  platform  that\\nmany other companies use to sell products, where the ownership of the data belongs\\nto each company, and the data cannot be shared across companies. This brings a chal‐\\nlenging situation where MarketCloud must create different recommender systems for\\neach of the users instead of pooling all the data to create a universal recommendation\\nengine.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.997,306.236,432.005,367.136 'MarketCloud can overcome this obstacle by creating a single pipeline into which data\\nfrom many different companies can be fed. By having the data go through an automa‐\\nted  recommendation  engine  training  scenario,  MarketCloud  created  many  recom‐\\nmendation  engines  trained  on  different  datasets.  Good  MLOps  processes  are  what\\nallow the company to do this at scale.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.996,161.636,432.005,298.136 'It’s worth noting that though this use case is fictionalized, it is based on reality. The\\nreal-life team that tackled a similar project took around three months to finish. The\\nteam used a data science and machine learning platform to orchestrate the duplica‐\\ntion of a single pipeline to four thousand copies and to automate the processes to feed\\ncorresponding  datasets  to  each  pipeline  and  train  the  models.  Of  necessity,  they\\naccepted trade-offs between the recommendation quality and scalability to efficiently\\nlaunch  the  product.  If  the  team  had  carefully  crafted  a  custom  recommendation\\nengine  for  each  of  the  four  thousand  pipelines  by,  for  example,  choosing  the  best\\nalgorithm for each client, the recommendation engines would have been of a higher\\nquality, but they would have never been able to complete the project with such a small\\nteam in such a short period of time.\\n'>\n",
      "<LTTextBoxHorizontal(5) 350.366,40.500,402.512,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(6) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 420.980,40.500,431.996,49.500 '145\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 93.824,533.502,431.999,582.331 'CHAPTER 11\\nMLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTTextBoxHorizontal(1) 353.829,436.501,431.998,449.501 'Nicolas Omont\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,309.668,432.004,408.369 'Predictions  at  various  times  and  geographical  scales  play  an  important  role  in  the\\noperation of a power grid. They allow for simulation of possible future states of the\\nsystem and for making sure that it can safely operate. This chapter will walk through\\na machine learning model life cycle and MLOps use case for consumption forecast‐\\ning,  including  business  considerations,  data  collection,  and  implementation  deci‐\\nsions.  Though  this  particular  chapter  is  focused  on  power  grids,  the  considerations\\nand particularities of the use case can be generalized to other industrial cases that use\\nconsumption forecasting.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,196.668,432.005,297.329 'Power Systems\\nBulk  power  systems  are  the  backbone  of  power  grids.  Also  called  transmission  net‐\\nworks, they form the core of the system that keeps the lights on. These systems are\\nmainly  composed  of  lines  and  transformers,  which  are  indirectly  connected  with\\nmost  producers  and  consumers  through  distribution  networks  that  take  care  of  the\\nlast few kilometers of transmission. As illustrated in Figure 11-1, only the largest pro‐\\nducers and consumers are directly connected to the bulk system.\\n'>\n",
      "<LTTextBoxHorizontal(4) 420.984,40.500,432.000,49.500 '147\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,561.816,432.000,561.816>\n",
      "<LTTextBoxHorizontal(0) 72.000,338.166,422.794,361.266 'Figure 11-1. A sample bulk power system, to which only the largest producers and con‐\\nsumers are directly connected\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,237.966,432.004,324.066 'The longer the transmission distance and the larger the energy volume to be trans‐\\nmitted, the higher the voltage used: on the lower end, a few tens of kilovolts for a few\\ntens of megawatts over a few tens of kilometers; on the upper end, one million volts\\nfor a few thousand megawatts over a few thousand kilometers. (A line with a capacity\\nof one megawatt can be used to provide power to around one thousand inhabitants in\\nEurope.) The operation of transmission systems has always required a lot of commu‐\\nnications and computations because of its properties:\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.003,218.366,144.411,228.866 'No energy storage\\n'>\n",
      "<LTTextBoxHorizontal(3) 89.997,155.366,432.005,216.266 'The  network  stores  a  meaningless  amount  of  energy—less  than  one  second  of\\nconsumption in the grid and up to 30 seconds in the alternators and motors. By\\nway  of  contrast,  a  gas  network  stores  several  hours  of  consumption  in  its  pipe‐\\nline. Therefore, actions have to be taken very quickly to balance production and\\nconsumption and avoid blackouts.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.000,135.766,145.721,146.266 'Weak flow control\\n'>\n",
      "<LTTextBoxHorizontal(5) 89.996,72.766,432.002,133.666 'On  telecommunication  networks,  congestions  are  handled  by  dropping  packets\\nor by not establishing a call. There is no equivalent mechanism in power grids,\\nwhich means the power flow on a grid element can be higher than its operating\\nlimit.  Actions  have  to  be  taken  after  a  few  seconds  to  a  few  hours  of  overload\\ndepending on the technology and the severity. Flow control technologies do exist,\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.003,40.500,84.441,49.500 '148 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.019,40.500,95.331,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.909,40.500,259.059,49.500 'Chapter 11: MLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,366.829,432.375,607.500>\n",
      "<LTLine 72.000,366.954,432.500,366.954>\n",
      "<LTLine 72.125,366.829,72.125,607.500>\n",
      "<LTFigure(I1) 81.249,372.809,423.251,602.250 matrix=[342.00,0.00,0.00,229.44, (81.25,372.81)]>\n",
      "<LTTextBoxHorizontal(0) 89.997,582.437,431.997,605.537 'but  there  is  a  trade-off  between  flow  control  and  instantaneous  balance:  the\\npower has to find a path from generation to consumption.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,526.036,432.005,574.337 'Because of these two properties, the grid operator always has to anticipate the contin‐\\ngencies: if this grid element fails, will the overload on the remaining elements remain\\nacceptable? The anticipation is done on several timescales, from the next five minutes\\nto the next five decades. The actions to be taken depend on the horizon. For example:\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.651,503.437,431.997,513.937 '• Below  five  minutes:  no  human  action  is  possible.  Automatic  actions  should\\n'>\n",
      "<LTTextBoxHorizontal(3) 90.002,490.837,188.586,501.337 'already be well defined.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.660,474.237,431.997,484.737 '• Between five minutes and a few hours ahead: production schedule and grid top‐\\n'>\n",
      "<LTTextBoxHorizontal(5) 90.002,461.637,408.026,472.137 'ology adaptation (opening of breakers and other flow control technologies).\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.660,445.037,314.056,455.537 '• A few days ahead: maintenance schedule adaptations.\\n'>\n",
      "<LTTextBoxHorizontal(7) 80.655,403.236,432.002,438.936 '• A few seasons ahead: maintenance schedule adaptations, contracts with produc‐\\ners  or  consumers  to  guarantee  power  capacity  or  limit  power  generation  or\\nconsumption.\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.654,361.436,432.002,397.136 '• From  5  to  50  years  ahead:  investment  in  grid  elements.  Lines  and  transformers\\nhave standard life expectancies of several decades; practically, it is expected that\\nsome grid elements will last over one hundred years.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,301.036,432.003,349.336 'Another concern is anticipating at different geographical scales. While some contin‐\\ngencies  only  have  effects  on  a  small  part  of  the  grid,  some  may  have  a  continental\\nimpact  and  may  require  coordinated  actions  among  several  countries  to  mitigate\\ntheir effects. As a result, operating the grid requires:\\n'>\n",
      "<LTTextBoxHorizontal(10) 77.316,278.436,402.949,288.936 '1. Collecting data over a wide geographical area with strong time constraints.\\n'>\n",
      "<LTTextBoxHorizontal(11) 77.316,261.836,295.881,272.336 '2. Processing data to anticipate and act accordingly.\\n'>\n",
      "<LTTextBoxHorizontal(12) 71.996,174.036,432.004,249.497 'Data Collection\\nCollecting past data is the first step to making forecasts. There are two largely inde‐\\npendent sources of data: the SCADA (supervisory control and data acquisition) sys‐\\ntem and the metering system. Depending on the prediction use case, one or the other\\nmay be used.\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.998,117.636,432.001,165.936 'The  SCADA  system  collects  data  in  real  time  to  provide  an  up-to-date  view  of  the\\nsystem to the operator. It also allows commands to be sent to network equipment—\\nfor example to open and close a breaker. The most impressive representation of the\\nsystem is the synoptic screen found in most control rooms as shown in Figure 11-2.\\n'>\n",
      "<LTTextBoxHorizontal(14) 356.428,40.500,402.517,49.500 'Data Collection \\n'>\n",
      "<LTTextBoxHorizontal(15) 410.095,40.500,413.407,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 420.985,40.500,432.001,49.500 '149\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,336.918,419.046,360.018 'Figure 11-2. The SCADA system typically refreshes thousands of measurements about\\nflows, consumption, and generation on the grid every 10 seconds or less\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.997,261.918,432.005,322.818 'Some  measurements  are  intentionally  redundant,  such  as  measuring  power  loss.  If\\nthe power flow is measured at each end of a line, then the difference between them is\\nequal  to  the  losses  on  the  line.  These  losses  can  be  physically  estimated  so  that  it  is\\npossible to handle the case when one measure is missing, to detect anomalies, or to\\nimprove the precision of the estimates.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,192.918,432.003,253.818 'The process that uses this redundancy to produce a state of the network is called the\\nstate estimation, and it is run every few minutes. When an operating limit is not satis‐\\nfied, the SCADA system raises an alarm. However, the SCADA cannot raise an alarm\\nwhen an operating limit would not be satisfied if one of the grid elements went out of\\norder.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,111.318,432.005,184.818 'Simulations  of  network  element  loss  (N-1  simulation)  on  the  consistent  state  pro‐\\nduced by the state estimation are run on a regular basis, and the value of SCADA data\\nfades  quickly;  therefore,  when  it  is  historized,  it  is  not  consolidated;  missing  values\\nare usually not input, and anomalies are usually not corrected. State estimations are\\nused by a variety of processes so that they are usually historized over a few months to\\na few years.\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.000,80.118,432.003,103.218 'The metering system that is used for invoicing does not need to be as reactive as the\\nSCADA system, but should be precise. It focuses on generation and consumption, not\\n'>\n",
      "<LTTextBoxHorizontal(5) 72.000,40.500,84.438,49.500 '150 \\n'>\n",
      "<LTTextBoxHorizontal(6) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(7) 102.906,40.500,259.056,49.500 'Chapter 11: MLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,607.375,432.500,607.375>\n",
      "<LTLine 432.375,365.582,432.375,607.500>\n",
      "<LTLine 72.000,365.707,432.500,365.707>\n",
      "<LTLine 72.125,365.582,72.125,607.500>\n",
      "<LTFigure(I1) 79.450,371.562,425.050,602.250 matrix=[345.60,0.00,0.00,230.69, (79.45,371.56)]>\n",
      "<LTTextBoxHorizontal(0) 71.997,582.437,432.000,605.537 'flow. Rather than monitoring instantaneous power, it records the withdrawn or injec‐\\nted energy over a period of time that ranges between a few minutes and one hour.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,513.437,432.004,574.337 'The  information  it  gathers  was  previously  made  available  after  a  delay  of  a  day  or\\nmore. Newer systems make it available within a few minutes. However, consolidation\\nand validation are usually needed when there are missing measurements or anoma‐\\nlies so that the final data is still usually available within a few working days. This data\\nis well historized.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,407.662,432.002,501.097 'Problem Definition: Machine Learning,\\nor Not Machine Learning?\\nNot  all  use  cases  are  appropriate  for  machine  learning.  Some  can  be  solved  more\\neasily and cheaply in other ways. The techniques used to make forecasts for the type\\nof  use  case  presented  here  are  different  in  these  three  situations  as  shown  in\\nTable 11-1.\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.005,383.156,249.175,393.156 'Table 11-1. Forecasting techniques by use case\\n'>\n",
      "<LTTextBoxHorizontal(4) 75.601,321.926,144.703,376.126 'Use case\\nThe forecast uncertainty\\ncomes from a part of the\\nsystem that the operator\\ncannot change.\\n'>\n",
      "<LTTextBoxHorizontal(5) 75.600,254.876,144.702,296.276 'The forecast uncertainty\\ncomes from a part of the\\nsystem that the operator\\ncan somehow influence.\\n'>\n",
      "<LTTextBoxHorizontal(6) 75.600,187.826,144.468,240.026 'The forecast uncertainty\\ncomes from a part of the\\nsystem that some other\\nactors can control and\\nanticipate.\\n'>\n",
      "<LTTextBoxHorizontal(7) 158.114,144.626,428.195,376.126 'Forecasting technique\\nChanging the weather is, practically speaking, impossible. As a result, wind and photovoltaic\\n(PV) generation, as well as heating and air conditioning, can safely be considered exogenous.\\nThis makes them good candidates for direct machine learning forecasting. These forecasts can\\nleverage meteorological forecasts or climatic scenarios, depending on the horizon.\\nMeteorological forecasts are available only a few days ahead, though some models now predict\\ntrends over a few months.\\nFor example, strictly speaking, the consumption should not be forecasted, but rather the\\ndemand. The difference between consumption and demand is that the consumption is somehow\\nat the hand of the operator that can choose not to serve the demand by switching off the\\nconsumers. For the same reason, the photovoltaic and wind production potential is forecasted,\\nnot the actual production.\\nFor example, for dispatchable power units where the operator can switch them on or off, it is\\nbetter to ask for the schedules from the operator. If this is not possible, it may be better to\\nreproduce the way the schedules are made—for instance, the operator may start the plant if\\nthe power price is higher than the plant fuel cost. In such cases, the forecasts may rely on\\ntechniques like agent-based modeling.\\nLarge factories are likely to have consumption schedules based on their operational production\\nschedules. Distribution grid topology is also likely to be scheduled ahead of time, as\\nmaintenance operations require advanced planning. In all these cases, it is often better to ask for\\nthe schedules than to use machine learning to forecast them.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.997,80.012,432.000,130.272 'Spatial and Temporal Resolution\\nDue  to  the  law  of  large  numbers,  the  forecast  uncertainty  decreases  when  the  con‐\\nsumption is spatially or temporally aggregated. While it is hard to forecast the hourly\\n'>\n",
      "<LTTextBoxHorizontal(9) 209.696,40.500,402.512,49.500 'Problem Definition: Machine Learning, or Not Machine Learning? \\n'>\n",
      "<LTTextBoxHorizontal(10) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(11) 420.980,40.500,431.996,49.500 '151\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTRect 72.000,365.226,154.515,379.026>\n",
      "<LTRect 154.515,365.226,432.000,379.026>\n",
      "<LTLine 72.000,298.301,154.615,298.301>\n",
      "<LTLine 154.415,298.301,432.000,298.301>\n",
      "<LTLine 72.000,242.051,154.615,242.051>\n",
      "<LTLine 154.415,242.051,432.000,242.051>\n",
      "<LTLine 72.000,142.601,154.615,142.601>\n",
      "<LTLine 154.415,142.601,432.000,142.601>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'consumption of an individual household because people are not machines, it is sur‐\\nprisingly  easy  for  populations  of  a  few  million,  and  is  relatively  easy  to  forecast  the\\nmonthly consumption of such a population as well.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.999,526.036,432.004,561.736 'As a result, a forecast system is often hierarchical, with several levels of forecasts that\\nare  linked  together  by  constraints.  That  is,  regional  forecasts  should  sum  up  to  the\\ncountry-wide forecasts, and hourly forecasts should sum up to the daily forecast.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.996,457.036,432.003,517.936 'Let’s take a striking example to illustrate this. Electric traction trains have a worrying\\nconsumption pattern for grid operators because they move, with a typical train line\\nbeing fed by a different substation every 10 to 50 kilometers. As a result, the operator\\nsees consumption of a few megawatts switching from substation to substation every\\n10 minutes or so. It creates several issues:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.652,409.236,432.002,444.936 '• The forecast is relatively easy at the line level because the train is always consum‐\\ning somewhere and because trains usually circulate at fixed hours. As a result, a\\nmachine learning approach is likely to work.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.654,367.436,432.002,403.136 '• The forecast of the energy withdrawn over a long period at a given substation is\\nalso  relatively  easy,  because  the  train  will  go  through  the  corresponding  part  of\\nthe line.\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.654,350.836,431.997,361.336 '• But because the operator wants to know if the train will create an overload when\\n'>\n",
      "<LTTextBoxHorizontal(6) 90.002,338.236,297.114,348.736 'circulating, a consistent set of forecasts is needed:\\n'>\n",
      "<LTTextBoxHorizontal(7) 91.074,321.636,369.549,332.136 '— The train should withdraw power at one location at a time only.\\n'>\n",
      "<LTTextBoxHorizontal(8) 91.069,305.036,431.996,315.536 '— Each substation should see a consumption spike at some point in time so that\\n'>\n",
      "<LTTextBoxHorizontal(9) 103.000,292.436,272.648,302.936 'a fine-grained time resolution is needed.\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.004,269.836,332.729,280.336 'As a result, the solution depends on the goal of the prediction:\\n'>\n",
      "<LTTextBoxHorizontal(11) 80.659,196.836,432.002,257.736 '• On a day-to-day basis, an average solution that splits the train consumption over\\nall substations is not acceptable, as potential overloads may be missed. A worst-\\ncase solution that assigns the train consumption to all substations may be more\\nacceptable, though it will anticipate spurious overloads as the overall consump‐\\ntion will be too large.\\n'>\n",
      "<LTTextBoxHorizontal(12) 80.652,155.036,432.002,190.736 '• However, to schedule the maintenance of one of the lines that feeds the region,\\nthe exact location of the consumption is likely to have no impact as long as it is\\nnot counted several times.\\n'>\n",
      "<LTTextBoxHorizontal(13) 71.996,94.636,432.003,142.936 'When  designing  the  forecast  system,  trade-offs  will  have  to  be  made,  as  the  perfect\\nsystem is unlikely to exist. If the system has a lot of margin, few or no overloads are\\nexpected so that the forecasting system can be coarse. However, if the grid is operated\\nclose to its limits, the system has to be carefully crafted.\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.003,40.500,84.441,49.500 '152 \\n'>\n",
      "<LTTextBoxHorizontal(15) 92.019,40.500,95.331,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(16) 102.909,40.500,259.059,49.500 'Chapter 11: MLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.998,557.712,432.001,607.973 'Implementation\\nOnce data is collected, either by the SCADA system or by the metering system, it has\\nto be historized. In addition to storing the raw data, some processing is required:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.653,497.312,432.002,545.613 '• Temporal aggregations, for example over a five-minute period: Either the average\\nvalue or a high quantile value is kept. The average is representative of the energy\\nconsumed over the period, and the high quantile is useful to assess if constraints\\noccurred.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.658,442.912,432.002,491.213 '• Disaggregations: When only the withdrawal is measured, the production and the\\nconsumption  have  to  be  separately  estimated.  Usually,  consumption  is  what\\nremains  after  removing  the  best  possible  estimation  of  distributed  generation\\n(wind, PV, etc.). Machine learning can be useful to perform these estimations.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.658,363.312,432.005,436.812 '• Spatial aggregations: As the system is balanced, it is possible to compute the con‐\\nsumption of a region by computing the difference between the local production\\nand the exchanges with the neighboring regions. This was historically very useful\\nbecause the production was easy to monitor because there were only a few very\\nlarge generation units and a few lines with neighboring countries. Nowadays, it\\ntends to be more complex as distributed generation is more widespread.\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.657,308.912,432.002,357.212 '• Missing value imputation: A measurement may be missing. In the SCADA sys‐\\ntem, rules exist to replace a missing value with an older or a typical value in real\\ntime. In the metering system, the imputation is a heavy impact process as it will\\nbe reflected directly on the customer’s invoice.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,210.712,432.005,296.812 'Data is then stored in different databases. Data used in short-term critical processes is\\nstored in high-availability systems in which redundancy allows rapid recovery from\\nthe loss of a data center. Data used in longer-term processes (invoicing, reports, ML\\nmodel training) is stored in ordinary IT databases. Overall, the number of monitored\\ngrid elements will range between 1,000 and 100,000. This means that they generate a\\nreasonable volume of data by today’s standards. Scalability is not such an issue either,\\nas bulk power grids do not grow anymore in developed countries.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.994,85.112,432.004,198.372 'Modeling\\nOnce the data preparation has been finished, the data scientist typically has access to\\na  few  hundred  time  series  of  production  and  consumption  at  various  withdrawal\\npoints of the grid. They have to develop methods to predict some of them at various\\nhorizons. Their focus is usually on wind, PV, and sometimes run-of-the river hydro‐\\nelectricity production potential and on demand. While wind and PV mainly depend\\non meteorological factors, the demand is mainly driven by economic activity, but part\\nof it is also dependent on meteorology (for example heating and cooling).\\n'>\n",
      "<LTTextBoxHorizontal(7) 352.441,40.500,402.517,49.500 'Implementation \\n'>\n",
      "<LTTextBoxHorizontal(8) 410.095,40.500,413.407,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(9) 420.985,40.500,432.001,49.500 '153\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,595.037,354.051,605.537 'Depending on the horizon, the modeling might look very different:\\n'>\n",
      "<LTTextBoxHorizontal(1) 80.655,534.636,432.002,582.937 '• Short-term: Up to a few days ahead, the last known values are very important to\\nmake predictions. In addition, for the same reasons, meteorological forecasts are\\navailable. Therefore, methods will leverage this information. In this case, deter‐\\nministic forecasts make sense.\\n'>\n",
      "<LTTextBoxHorizontal(2) 80.658,467.637,432.001,528.537 '• Mid-term:  Between  a  few  days  and  a  few  years,  the  meteorology  is  not  known,\\nbut  the  climate  is.  Statistical  extrapolation  of  past  year  tendencies  make  sense,\\nexcept if an economic crisis occurs. As a result, it is possible to draw scenarios to\\nobtain statistical indicators (mean, confidence intervals, quantiles, etc.) about the\\nfuture consumptions.\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.652,400.636,432.002,461.537 '• Long-term: Investment decisions require forecasts over several decades. On this\\nhorizon, statistical extrapolations of the current trend are not enough, neither on\\nthe  socio-economic  side  nor  on  the  climatic  side  given  global  warming.  As  a\\nresult,  statistical  approaches  have  to  be  completed  with  bottom-up  usage-based\\napproaches and expert-made diversified scenarios about the future.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,352.836,432.005,388.536 'ML  and  MLOps  mainly  concern  short-term  and  mid-term  forecasts.  Of  the  two,  in\\nthis case, mid-term models are easier to start with: given a few years of data, the goal\\nis to predict consumption based on:\\n'>\n",
      "<LTTextBoxHorizontal(5) 80.657,305.036,432.001,340.736 '• The calendar, with a superposition of daily, weekly, and annual cycles. Bank holi‐\\ndays and school vacations also have a big impact, in addition to daylight saving\\ntime.\\n'>\n",
      "<LTTextBoxHorizontal(6) 80.654,263.236,432.001,298.936 '• The  meteorological  variables  (temperature,  wind,  sun).  As  buildings  have  very\\nlarge thermal inertia, at least two days and up to three weeks of past temperatures\\nmay be needed.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,177.636,432.003,251.136 'While any kind of ML algorithm can be used, the smoothness of the predicted curve\\nis important because the predictions are not used individually, but as daily, weekly, or\\nannual  scenarios.  Many  algorithms  do  not  consider  smoothness  in  their  metrics\\nbecause they rely on the hypothesis that the data is independent and identically dis‐\\ntributed, which in our case is incorrect, since the consumption of a given day is usu‐\\nally correlated with the one of the previous day and the one of the previous week.\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.996,121.236,432.005,169.536 'Generalized additive models (GAM) are often a good starting point: they are based\\non  splines,  so  that  the  smoothness  is  guaranteed.  In  fact,  consumption  forecasting\\nwas one of the use cases for which they were developed. Combined with climatic sce‐\\nnarios, the ML model is then able to yield yearly consumption scenarios.\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.996,77.436,432.003,113.136 'Short-term forecasts are more complex. The simplest way to proceed is to remove the\\nmid-term forecast from the recent historical data and use standard time series techni‐\\nques,  such  as  ARIMA  (autoregressive  integrated  moving  average)  or  exponential\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.000,40.500,84.438,49.500 '154 \\n'>\n",
      "<LTTextBoxHorizontal(11) 92.016,40.500,95.328,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 102.906,40.500,259.056,49.500 'Chapter 11: MLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.997,569.837,432.000,605.537 'smoothing, on the residuals. This allows the generation of forecasts over several days.\\nAn integrated short-term model trained on several years of data has potential advan‐\\ntages over this simple approach.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,475.636,432.004,561.736 'For example, the mid-term model is trained on realized meteorological data and not\\non meteorological forecasts. As a result, it gives too much importance to meteorolog‐\\nical forecasts even though they may be wrong. A short-term model trained on mete‐\\norological  forecasts  would  address  this  issue.  However,  although  new  algorithms,\\nsuch as long short-term memory (LSTM) neural networks, are promising, it is hard\\nto  find  a  method  that  allows  for  forecasting  at  any  time  of  the  day  for  several  time\\nhorizons at once in a consistent way.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.997,431.836,432.005,467.536 'When  the  resolution  is  such  that  the  stochasticity  is  too  large  to  make  meaningful\\npredictions, it is better to aggregate time series spatially or temporally and then use\\nnon-ML heuristics to split the aggregated forecasts:\\n'>\n",
      "<LTTextBoxHorizontal(3) 80.657,409.236,401.238,419.736 '• A sharing key based on past observations in the case of spatial aggregation\\n'>\n",
      "<LTTextBoxHorizontal(4) 80.655,392.636,431.510,403.136 '• An average profile based on past observations in the case of temporal aggregation\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,307.036,432.005,380.536 'Because the grid is under constant evolution, it is likely that new injections and with‐\\ndrawals  appear  for  which  no  historical  data  is  available  and  that  ruptures  occur  in\\nconsumption patterns, so that past data is not relevant anymore. The forecast method\\nhas to take into account these edge cases. Ruptures could be spotted using anomaly\\ndetection  methods.  As  soon  as  a  rupture  is  identified,  a  simplified  model  could  be\\nused for as long as necessary until enough historical data is available.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,238.036,432.003,298.936 'Once again, neural networks could become an appealing alternative with the promise\\nthat only one model could be trained for all the consumptions instead of one model\\nper consumption with standard methods. Indeed, with only one model, the forecast\\nof a consumption with shallow historical data would be possible provided that its pat‐\\ntern looks similar to an existing pattern.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.999,162.836,432.005,225.696 'Deployment\\nNowadays, the models are likely to be prototyped by a data scientist in R, Python, or\\nMATLAB scripts. The prototype is able to prepare the data, train the model on one\\ndataset, and score it on another. The operationalization could follow several paths:\\n'>\n",
      "<LTTextBoxHorizontal(8) 80.654,140.236,431.997,150.736 '• The prototype is fully rewritten. This is costly and not flexible but may be neces‐\\n'>\n",
      "<LTTextBoxHorizontal(9) 90.002,127.636,388.044,138.136 'sary if embedding in an operational technology (OT) system is needed.\\n'>\n",
      "<LTTextBoxHorizontal(10) 80.660,73.236,432.002,121.536 '• Only the data preparation and the scoring are rewritten, which allows for train‐\\ning on a different schedule. It makes sense if the training occurs once a year or so\\nbecause it is good practice to regularly perform a model review to ensure that it\\nworks well and that the skills to maintain it are in place.\\n'>\n",
      "<LTTextBoxHorizontal(11) 364.682,40.500,402.518,49.500 'Deployment \\n'>\n",
      "<LTTextBoxHorizontal(12) 410.096,40.500,413.408,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(13) 420.986,40.500,432.002,49.500 '155\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 80.655,519.436,432.005,605.537 '• Data  science  and  machine  learning  platforms  can  be  used  to  operationalize  the\\nprototype.  These  platforms  are  flexible  and  allow  the  transfer  of  prototypes  to\\nproduction environments in which security and scalability are guaranteed. Most\\nconsumption  forecast  models  will  be  run  periodically  in  batch  mode.  For  more\\nspecific use cases, these platforms are able to export trained models as JAR files,\\nSQL,  PMML,  PFA,  and  ONNX  so  that  they  can  be  flexibly  integrated  into  any\\nkind of application.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,406.436,432.004,507.097 'Monitoring\\nThis section mainly discusses short-term forecasts. Indeed, mid-term and long-term\\nforecasts are systematically impacted by drift, as the past never looks like the future,\\nso they are almost systematically trained again before being used to make predictions.\\nFor  short-term  forecasts,  besides  IT  monitoring  to  raise  alarms  if  forecasts  are  not\\nproduced on time and warnings for events that may result in missing deadlines, the\\nmodels themselves should be monitored.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,324.836,432.004,398.336 'The  first  kind  of  monitoring  is  drift  monitoring.  For  electricity  consumption,  it  is\\ncritical that drift monitoring is deployed together with the model. Anomaly detection\\nand rupture detection allow teams to make sure that the trained model can be used. If\\nnot, fallback models based on shallow historical data or normative disaggregation of\\nmultiple  consumption  forecasts  should  be  used.  This  first  layer  will  detect  drastic\\ndrifts online.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.995,230.636,432.003,316.736 'Though the data scientist will try to design models that are adaptive to the consump‐\\ntion level (like ARIMA), it can be useful to detect that some consumption levels are\\nhigher or lower than in the training period. This may have happened slowly, so that it\\nwas  not  detected  online.  The  offline  analysis  of  the  forecasts,  for  example  once  a\\nmonth if the forecasts are computed every day for the next day, offers the possibility\\nto detect these slow drifts. In these cases, if no additional ground truth is available, it\\nwould make sense to shift to a fallback model for these consumptions.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.995,161.636,432.004,222.536 'Lastly, after the operations, it is possible to assess the performance of the prediction\\nthrough  various  metrics  like  mean  absolute  percentage  error  (MAPE).  If  a  perfor‐\\nmance  drop  is  detected  during  a  significant  amount  of  time  (for  example,  one\\nmonth),  retraining  the  corresponding  models  is  an  option  as  new  data  is  available,\\nand the retrained models may increase performance.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.997,80.036,432.005,153.536 'This requires a tight integration of the design and the production environment with\\nCI/CD processes (as discussed at length in Chapter 6). If it is possible to handle man‐\\nually the deployment of new models once a year, it is usually too costly to do so once\\na  month.  With  an  advanced  data  science  and  machine  learning  platform,  it  is  also\\npossible to perform shadow scoring with the new model for a few days before using it\\nfor the forecasts.\\n'>\n",
      "<LTTextBoxHorizontal(6) 72.002,40.500,84.440,49.500 '156 \\n'>\n",
      "<LTTextBoxHorizontal(7) 92.018,40.500,95.330,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(8) 102.908,40.500,259.058,49.500 'Chapter 11: MLOps in Practice: Consumption Forecast\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 71.995,532.512,432.003,607.973 'Closing Thoughts\\nIn this chapter, we have seen how to make the data speak to assist the operation of a\\ntransmission power grid. Various ML and non-ML techniques can be used to provide\\nforecasts for up to thousands of consumptions on timescales ranging from minutes to\\ndecades.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,463.512,432.004,524.412 'Thanks  to  MLOps,  design,  deployment,  and  monitoring  processes  have  been  stan‐\\ndardized across several industries, and data science and machine learning platforms\\nhave been developed to support this process. Designers of consumption forecast sys‐\\ntems can leverage these standard processes and platforms to improve the efficiency of\\nthese systems from the cost, quality, or time to value perspective.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,381.912,432.004,455.412 'Taking  a  larger  step  back,  it’s  clear  that  different  industries  have  a  wide  range  of\\nmachine learning use cases, all of which have their own intricacies when it comes to\\ndefining  the  problem,  building  models,  pushing  to  production—everything  we’ve\\ncovered in this book. But no matter what the industry or use case, MLOps processes\\nare consistently the thread that allows data teams (and more widely, entire organiza‐\\ntions) to scale their machine learning efforts.\\n'>\n",
      "<LTTextBoxHorizontal(3) 350.367,40.500,402.513,49.500 'Closing Thoughts \\n'>\n",
      "<LTTextBoxHorizontal(4) 410.091,40.500,413.403,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(5) 420.981,40.500,431.997,49.500 '157\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTTextBoxHorizontal(0) 385.358,561.061,431.990,586.280 'Index\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.999,422.508,112.697,446.841 'A\\nA/B testing\\n'>\n",
      "<LTTextBoxHorizontal(2) 84.239,379.308,242.288,420.708 'canary releases, 79\\nconsiderations in MLOps context, 102\\nof new and existing model versions, 33\\nperformance monitoring for marketing rec‐\\n'>\n",
      "<LTTextBoxHorizontal(3) 96.479,368.508,190.979,377.508 'ommendation engine, 144\\n'>\n",
      "<LTTextBoxHorizontal(4) 84.239,357.708,241.577,366.708 'for recommendation engines using collabo‐\\n'>\n",
      "<LTTextBoxHorizontal(5) 96.479,346.908,164.654,355.908 'rative filtering, 136\\n'>\n",
      "<LTTextBoxHorizontal(6) 84.239,336.108,229.976,345.108 'using in online evaluation of models, 99,\\n'>\n",
      "<LTTextBoxHorizontal(7) 96.479,325.308,109.439,334.308 '101\\n'>\n",
      "<LTTextBoxHorizontal(8) 71.999,314.508,122.930,323.508 'accountability\\n'>\n",
      "<LTTextBoxHorizontal(9) 71.999,282.108,196.631,312.708 'GxP guidelines focus on, 109\\nin Responsible AI, 10, 112\\naccuracy, precision, and recall, 132\\n'>\n",
      "<LTTextBoxHorizontal(10) 84.239,271.308,233.801,280.308 'metrics collected in preproduction model\\n'>\n",
      "<LTTextBoxHorizontal(11) 71.999,238.908,150.839,269.508 'testing, 65\\nadversarial attacks, 68\\nAI, 112\\n'>\n",
      "<LTTextBoxHorizontal(12) 84.239,217.308,214.199,237.108 '(see also Responsible AI)\\nnew wave of AI-specific regulations,\\n'>\n",
      "<LTTextBoxHorizontal(13) 96.479,206.508,125.603,215.508 '111-112\\n'>\n",
      "<LTTextBoxHorizontal(14) 84.239,195.708,191.654,204.708 'Responsible AI, MLOps for, 9\\n'>\n",
      "<LTTextBoxHorizontal(15) 71.999,174.108,207.746,193.908 'AIOps versus MLOps, 3\\nalgorithms (machine learning), 23, 44\\n'>\n",
      "<LTTextBoxHorizontal(16) 84.239,152.508,234.593,172.308 'computing power considerations, 45\\nMLOps considerations by algorithm type,\\n'>\n",
      "<LTTextBoxHorizontal(17) 96.479,141.708,105.119,150.708 '45\\n'>\n",
      "<LTTextBoxHorizontal(18) 84.239,109.308,220.796,139.908 'online learning, 88\\nrequirement for tabular input data, 47\\nsmoothness of predicted curve, 154\\n'>\n",
      "<LTTextBoxHorizontal(19) 259.499,414.208,425.531,444.808 'anonymizing or pseudo-anonymizing data, 36\\nApache Spark, 78\\nAPIs\\n'>\n",
      "<LTTextBoxHorizontal(20) 271.739,403.408,415.838,412.408 'marketing recommendation engine API\\n'>\n",
      "<LTTextBoxHorizontal(21) 283.979,392.608,337.052,401.608 'endpoints, 142\\n'>\n",
      "<LTTextBoxHorizontal(22) 271.739,381.808,419.807,390.808 'REST API for model-as-a-service or live-\\n'>\n",
      "<LTTextBoxHorizontal(23) 283.979,371.008,348.158,380.008 'scoring model, 28\\n'>\n",
      "<LTTextBoxHorizontal(24) 259.499,360.208,413.363,369.208 'ARIMA (autoregressive integrated moving\\n'>\n",
      "<LTTextBoxHorizontal(25) 259.499,317.008,348.545,358.408 'average), 154, 156\\nartifacts (ML), 75-76\\nassumptions (model), 57\\nauditability, 112, 116\\n'>\n",
      "<LTTextBoxHorizontal(26) 271.739,295.408,417.629,315.208 'aiding with QA for machine learning, 67\\nand reproducibility, 67\\n'>\n",
      "<LTTextBoxHorizontal(27) 259.499,284.608,301.214,293.608 'automation\\n'>\n",
      "<LTTextBoxHorizontal(28) 271.739,241.408,422.408,282.808 'automated feature selection, 48\\nautomated model deployment, 29\\nautomated model documentation, 27\\nautomated model packaging and delivery,\\n'>\n",
      "<LTTextBoxHorizontal(29) 283.979,230.608,305.354,239.608 '14, 18\\n'>\n",
      "<LTTextBoxHorizontal(30) 271.739,209.008,431.849,228.808 'automated reporting tools on all models, 14\\nautomatically controlling models, marketing\\n'>\n",
      "<LTTextBoxHorizontal(31) 283.979,198.208,390.008,207.208 'recommendation system, 144\\n'>\n",
      "<LTTextBoxHorizontal(32) 271.739,187.408,425.342,196.408 'in experimentation during model develop‐\\n'>\n",
      "<LTTextBoxHorizontal(33) 283.979,176.608,315.407,185.608 'ment, 50\\n'>\n",
      "<LTTextBoxHorizontal(34) 271.739,144.208,423.506,174.808 'feedback loop, 132\\nof tests in testing pipeline, 76\\nof versioning and reproducibility tasks, 58\\n'>\n",
      "<LTTextBoxHorizontal(35) 259.496,108.108,320.576,132.441 'B\\nbatch scoring, 77\\n'>\n",
      "<LTTextBoxHorizontal(36) 71.999,98.508,241.532,107.508 'analytics use cases, understanding and classify‐\\n'>\n",
      "<LTTextBoxHorizontal(37) 271.742,97.308,427.523,106.308 'volume of data becoming too large, distrib‐\\n'>\n",
      "<LTTextBoxHorizontal(38) 84.239,87.708,112.841,96.708 'ing, 118\\n'>\n",
      "<LTTextBoxHorizontal(39) 71.999,76.908,151.595,85.908 'anomaly detection, 71\\n'>\n",
      "<LTTextBoxHorizontal(40) 283.982,86.508,364.118,95.508 'uting computation, 82\\n'>\n",
      "<LTTextBoxHorizontal(41) 259.502,75.708,321.827,84.708 'Bayesian tests, 34\\n'>\n",
      "<LTTextBoxHorizontal(42) 420.986,40.500,432.002,49.500 '159\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 72.000,589.375,432.000,589.375>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 72.000,596.817,93.366,605.817 'biases\\n'>\n",
      "<LTTextBoxHorizontal(1) 84.240,542.817,240.129,595.017 'analyzing models for fairness, 66\\ninappropriate biases in models, 36\\nintroduced by ground truth monitoring, 90\\nin ML black box judgments, 106\\nmodel bias considerations in consumer\\n'>\n",
      "<LTTextBoxHorizontal(2) 96.480,532.017,198.945,541.017 'credit risk management, 131\\n'>\n",
      "<LTTextBoxHorizontal(3) 84.240,488.817,204.381,530.217 'reputational risk due to, 64\\nResponsible AI position on, 114\\nsample selection bias, 93\\ntuning bias/variance trade-off, 50\\n'>\n",
      "<LTTextBoxHorizontal(4) 283.984,596.817,429.343,605.817 'problem definition and data acquisition,\\n'>\n",
      "<LTTextBoxHorizontal(5) 296.224,586.017,309.184,595.017 '130\\n'>\n",
      "<LTTextBoxHorizontal(6) 271.744,564.417,401.569,584.217 'deploying model to production, 132\\nmodel development, 130\\n'>\n",
      "<LTTextBoxHorizontal(7) 283.984,553.617,370.024,562.617 'bias considerations, 131\\n'>\n",
      "<LTTextBoxHorizontal(8) 259.504,532.017,425.050,551.817 'preparing model for production, 131\\nconsumption forecast for power grid, 147-157\\n'>\n",
      "<LTTextBoxHorizontal(9) 271.744,499.617,369.241,530.217 'data collection, 149-151\\ndeployment of models, 155\\nimplementation, 153-155\\n'>\n",
      "<LTTextBoxHorizontal(10) 283.984,488.817,335.401,497.817 'modeling, 153\\n'>\n",
      "<LTTextBoxHorizontal(11) 72.000,445.617,235.035,487.017 'black box models, 54\\nblue-green deployment, 28, 78\\nbounds for retraining frequency, 87\\nbusiness concerns in monitoring ML models,\\n'>\n",
      "<LTTextBoxHorizontal(12) 84.240,434.817,92.880,443.817 '31\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.000,413.217,241.695,433.017 'business decision modeling, 15\\nbusiness metrics for model performance moni‐\\n'>\n",
      "<LTTextBoxHorizontal(14) 84.240,402.417,131.931,411.417 'toring, 89, 94\\n'>\n",
      "<LTTextBoxHorizontal(15) 271.744,456.417,428.902,487.017 'monitoring, 156\\npower systems, 147-149\\nproblem definition, using machine learning\\n'>\n",
      "<LTTextBoxHorizontal(16) 283.984,434.817,413.404,454.617 'or not, 151-152\\nspatial and temporal resolution, 151\\n'>\n",
      "<LTTextBoxHorizontal(17) 259.504,424.017,343.069,433.017 'containerization, 79-81\\n'>\n",
      "<LTTextBoxHorizontal(18) 271.744,413.217,418.174,422.217 'solving problems of dependencies in ML\\n'>\n",
      "<LTTextBoxHorizontal(19) 283.984,402.417,367.864,411.417 'model deployments, 28\\n'>\n",
      "<LTTextBoxHorizontal(20) 72.000,391.617,241.299,400.617 'business objectives, establishing for ML model,\\n'>\n",
      "<LTTextBoxHorizontal(21) 259.504,391.617,431.044,400.617 'continuous integration and continuous delivery\\n'>\n",
      "<LTTextBoxHorizontal(22) 84.240,380.817,92.880,389.817 '24\\n'>\n",
      "<LTTextBoxHorizontal(23) 271.744,380.817,350.053,389.817 '(see CI/CD pipelines)\\n'>\n",
      "<LTTextBoxHorizontal(24) 72.000,370.017,233.307,379.017 'business use case, consumer credit risk man‐\\n'>\n",
      "<LTTextBoxHorizontal(25) 259.504,370.017,277.252,379.017 'costs\\n'>\n",
      "<LTTextBoxHorizontal(26) 84.240,359.217,174.177,368.217 'agement application, 129\\n'>\n",
      "<LTTextBoxHorizontal(27) 72.004,312.317,237.712,347.450 'C\\ncanary releases, 78\\nCCPA (California Consumer Privacy Act), 35,\\n'>\n",
      "<LTTextBoxHorizontal(28) 84.244,301.517,97.204,310.517 '111\\n'>\n",
      "<LTTextBoxHorizontal(29) 72.004,269.117,178.420,299.717 'champion/challenger, 66, 100\\nChi-squared test, 94\\nCI/CD pipelines, 73-74\\n'>\n",
      "<LTTextBoxHorizontal(30) 84.244,258.317,244.345,267.317 'continuous delivery for end-to-end machine\\n'>\n",
      "<LTTextBoxHorizontal(31) 96.484,247.517,168.007,256.517 'learning process, 95\\n'>\n",
      "<LTTextBoxHorizontal(32) 84.244,193.517,208.183,245.717 'DevOps role in managing, 20\\nfor different models, 82\\nJenkins build system, 76\\nML artifacts in, 75\\nrobust, for model deployments, 29\\n'>\n",
      "<LTTextBoxHorizontal(33) 72.004,171.917,235.552,191.717 'collaborative filtering, 136\\ncompression techniques, use in model defini‐\\n'>\n",
      "<LTTextBoxHorizontal(34) 84.244,161.117,160.168,170.117 'tion optimization, 61\\n'>\n",
      "<LTTextBoxHorizontal(35) 72.004,139.517,237.172,159.317 'computational metrics from model testing, 65\\ncomputing power\\n'>\n",
      "<LTTextBoxHorizontal(36) 84.244,117.917,228.280,137.717 'ML model development and, 45\\nrequred for inference on ML models, 62\\n'>\n",
      "<LTTextBoxHorizontal(37) 72.004,85.517,228.613,116.117 'concept drift, 92\\nconformal prediction, 72\\nconsumer credit risk management, 129-133\\n'>\n",
      "<LTTextBoxHorizontal(38) 84.244,74.717,162.598,83.717 'business use case, 129\\n'>\n",
      "<LTTextBoxHorizontal(39) 271.744,348.417,421.774,368.217 'for algorithms that train themselves, 88\\nfor retraining models versus performance\\n'>\n",
      "<LTTextBoxHorizontal(40) 259.504,305.217,419.767,346.617 'improvement, 86\\ncross-trained models, 88\\ncurse of dimensionality, 71\\ncustomizability, marketing recommendation\\n'>\n",
      "<LTTextBoxHorizontal(41) 271.744,294.417,337.777,303.417 'engine model, 140\\n'>\n",
      "<LTTextBoxHorizontal(42) 259.503,258.317,319.480,282.650 'D\\ndark launch, 100\\n'>\n",
      "<LTTextBoxHorizontal(43) 271.744,247.517,383.758,256.517 '(see also champion/challenger)\\n'>\n",
      "<LTTextBoxHorizontal(44) 259.504,236.717,274.732,245.717 'data\\n'>\n",
      "<LTTextBoxHorizontal(45) 271.744,225.917,412.567,234.917 'collection for consumption forecast for\\n'>\n",
      "<LTTextBoxHorizontal(46) 283.984,215.117,356.596,224.117 'power grid, 149-151\\n'>\n",
      "<LTTextBoxHorizontal(47) 271.744,193.517,421.126,213.317 'considerations in Responsible AI, 113\\ncustomer data, preparation for marketing\\n'>\n",
      "<LTTextBoxHorizontal(48) 283.984,182.717,389.455,191.717 'recommendation engine, 137\\n'>\n",
      "<LTTextBoxHorizontal(49) 271.744,171.917,431.881,180.917 'processing for consumption forecast system,\\n'>\n",
      "<LTTextBoxHorizontal(50) 283.984,161.117,296.944,170.117 '153\\n'>\n",
      "<LTTextBoxHorizontal(51) 271.744,150.317,338.587,159.317 'reproducibility, 57\\n'>\n",
      "<LTTextBoxHorizontal(52) 259.504,139.517,431.035,148.517 'data access before validation and launch to pro‐\\n'>\n",
      "<LTTextBoxHorizontal(53) 259.504,96.317,324.250,137.717 'duction, 62\\ndata architects, 21\\ndata cleansing, 25\\ndata drift, 91\\n'>\n",
      "<LTTextBoxHorizontal(54) 271.744,85.517,407.770,94.517 'for consumer credit risk management\\n'>\n",
      "<LTTextBoxHorizontal(55) 283.984,74.717,323.854,83.717 'model, 132\\n'>\n",
      "<LTTextBoxHorizontal(56) 72.004,40.500,84.442,49.500 '160 \\n'>\n",
      "<LTTextBoxHorizontal(57) 92.020,40.500,95.332,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(58) 102.910,40.500,119.551,49.500 'Index\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 2.700,-9.000,2.700,54.000>\n",
      "<LTLine 2.700,52.000,2.700,609.500>\n",
      "<LTLine 2.700,607.500,2.700,670.500>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 84.240,596.817,161.154,605.817 'example causes of, 93\\n'>\n",
      "<LTTextBoxHorizontal(1) 72.000,564.417,143.568,595.017 'data engineers, 19\\ndata exploration, 46\\ndata governance, 36\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.000,542.817,230.112,562.617 'questions for ML model data sources, 25\\ndata pipeline structure for recommendation\\n'>\n",
      "<LTTextBoxHorizontal(3) 84.240,532.017,152.784,541.017 'engine project, 141\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.000,521.217,145.584,530.217 'data privacy, 35, 108\\n'>\n",
      "<LTTextBoxHorizontal(5) 271.740,575.217,426.351,605.817 'strategies for, 77-79\\ndeployment, defined, 77\\nmarketing recommendation engine model,\\n'>\n",
      "<LTTextBoxHorizontal(6) 283.980,564.417,313.104,573.417 '138-141\\n'>\n",
      "<LTTextBoxHorizontal(7) 271.740,542.817,421.311,562.617 'model deployment types and contents, 28\\nmodel deploymnt requirements, 29\\n'>\n",
      "<LTTextBoxHorizontal(8) 259.500,532.017,428.115,541.017 'development environments, adaptation to pro‐\\n'>\n",
      "<LTTextBoxHorizontal(9) 271.740,521.217,376.338,530.217 'duction environments, 60-62\\n'>\n",
      "<LTTextBoxHorizontal(10) 84.240,510.417,231.723,519.417 'GDPR and CCPA regulations on, 35, 110\\n'>\n",
      "<LTTextBoxHorizontal(11) 259.500,510.417,301.584,519.417 'DevOps, 20\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.000,499.617,146.457,508.617 'data scientists, 17-19\\n'>\n",
      "<LTTextBoxHorizontal(13) 84.240,488.817,235.368,497.817 'collaboration with SMEs in ML model life\\n'>\n",
      "<LTTextBoxHorizontal(14) 96.480,478.017,127.062,487.017 'cycle, 16\\n'>\n",
      "<LTTextBoxHorizontal(15) 84.240,467.217,222.192,476.217 'concerns in ML model monitoring, 30\\n'>\n",
      "<LTTextBoxHorizontal(16) 96.480,445.617,156.267,465.417 'ground truth, 30\\ninput drift, 31\\n'>\n",
      "<LTTextBoxHorizontal(17) 84.240,424.017,243.396,443.817 'role in and needs from MLOps, 18\\nrole in machine learning model life cycle, 17\\n'>\n",
      "<LTTextBoxHorizontal(18) 72.000,380.817,240.111,422.217 'data sources for machine learning models, 24\\nDataOps, 7\\ndecision modeling (business), 16\\ndecision-making processes, statistically driven,\\n'>\n",
      "<LTTextBoxHorizontal(19) 84.240,370.017,97.200,379.017 '105\\n'>\n",
      "<LTTextBoxHorizontal(20) 72.000,348.417,197.829,368.217 'deep learning, 45, 48, 54\\ndegradation of model performance\\n'>\n",
      "<LTTextBoxHorizontal(21) 271.740,456.417,430.896,508.617 'concerns in ML model monitoring, 30\\nMLOps and, 6\\nmonitoring of ML models, 86\\nrole in and needs from MLOps, 21\\nrole in machine learning model life cycle, 20\\n'>\n",
      "<LTTextBoxHorizontal(22) 259.500,424.017,421.284,454.617 'DI (Data Integrity), 109\\ndimensionality, curse of, 71\\ndimensioning constraints on model develop‐\\n'>\n",
      "<LTTextBoxHorizontal(23) 271.740,413.217,303.168,422.217 'ment, 54\\n'>\n",
      "<LTTextBoxHorizontal(24) 259.500,359.217,431.589,411.417 'disaggregations of data, 153, 156\\ndistances between probability distributions, 132\\ndistillation (model), 61\\ndistributed computation, 82\\ndistribution of data, 92\\n'>\n",
      "<LTTextBoxHorizontal(25) 271.740,348.417,415.587,357.417 'divergence between training and testing\\n'>\n",
      "<LTTextBoxHorizontal(26) 84.240,326.817,225.666,346.617 'common approaches to discovering, 30\\nunderstanding, 89-92\\n'>\n",
      "<LTTextBoxHorizontal(27) 283.980,337.617,320.619,346.617 'phases, 92\\n'>\n",
      "<LTTextBoxHorizontal(28) 259.500,326.817,298.875,335.817 'Docker, 80\\n'>\n",
      "<LTTextBoxHorizontal(29) 72.000,316.017,113.724,325.017 'delivery, 77\\n'>\n",
      "<LTTextBoxHorizontal(30) 84.240,294.417,238.302,314.217 '(see also CI/CD pipelines)\\ncontinuous delivery versus deployment, 77\\n'>\n",
      "<LTTextBoxHorizontal(31) 72.000,283.617,120.888,292.617 'dependencies\\n'>\n",
      "<LTTextBoxHorizontal(32) 271.740,305.217,400.557,325.017 'deployment of models through, 132\\nusing Kubernetes with, 80\\n'>\n",
      "<LTTextBoxHorizontal(33) 259.500,283.617,425.730,303.417 'documentation of model development, 26\\ndomain knowledge, importance in data explo‐\\n'>\n",
      "<LTTextBoxHorizontal(34) 84.240,262.017,234.036,281.817 'partial dependency plots, 27\\non production environment, reducing, 28\\n'>\n",
      "<LTTextBoxHorizontal(35) 271.740,272.817,347.061,281.817 'ration for models, 46\\n'>\n",
      "<LTTextBoxHorizontal(36) 259.500,262.017,290.802,271.017 'domains\\n'>\n",
      "<LTTextBoxHorizontal(37) 72.000,251.217,175.095,260.217 'deployment strategies, 77-79\\n'>\n",
      "<LTTextBoxHorizontal(38) 84.240,218.817,232.146,249.417 'categories of model deployment, 77\\nconcepts and terminology, 77\\nconsiderations in sending models to pro‐\\n'>\n",
      "<LTTextBoxHorizontal(39) 96.480,208.017,137.223,217.017 'duction, 78\\n'>\n",
      "<LTTextBoxHorizontal(40) 84.240,197.217,232.992,206.217 'maintenance of models in production, 79\\n'>\n",
      "<LTTextBoxHorizontal(41) 72.000,186.417,118.287,195.417 'deployments\\n'>\n",
      "<LTTextBoxHorizontal(42) 84.240,175.617,230.517,184.617 'broader model deployment, greater risks\\n'>\n",
      "<LTTextBoxHorizontal(43) 96.480,164.817,127.179,173.817 'from, 69\\n'>\n",
      "<LTTextBoxHorizontal(44) 84.240,110.817,219.870,163.017 'consumption forecast models, 155\\ndeploying to production, 73-84\\nbuilding ML artifacts, 75-76\\nCI/CD pipelines, 73\\nconsumer credit risk management\\n'>\n",
      "<LTTextBoxHorizontal(45) 108.720,100.017,148.590,109.017 'model, 132\\n'>\n",
      "<LTTextBoxHorizontal(46) 96.480,78.417,194.274,98.217 'containerization, 79-81\\nscaling deployments, 81-83\\n'>\n",
      "<LTTextBoxHorizontal(47) 271.740,240.417,399.018,260.217 'domain classsifier, 94\\nmodel retraining frequency and, 86\\n'>\n",
      "<LTTextBoxHorizontal(48) 259.500,229.617,288.147,238.617 'drift, 91\\n'>\n",
      "<LTTextBoxHorizontal(49) 271.740,208.017,369.966,227.817 '(see also input drift)\\ndetection in practice, 92-95\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.740,175.617,425.190,206.217 'example causes of data drift, 93\\ninput drift detection techniques, 93\\nmeasuring for consumer credit risk assess‐\\n'>\n",
      "<LTTextBoxHorizontal(51) 283.980,164.817,344.586,173.817 'ment model, 132\\n'>\n",
      "<LTTextBoxHorizontal(52) 271.740,143.217,421.365,163.017 'monitoring, 156\\nmonitoring and mitigation measures, 103\\n'>\n",
      "<LTTextBoxHorizontal(53) 259.496,96.317,413.000,131.450 \"E\\nEDA (exploratory data analysis), 24, 25, 46\\neffienciency, data scientists' need for from\\n\">\n",
      "<LTTextBoxHorizontal(54) 259.496,74.717,324.575,94.517 'MLOps, 19\\nelastic systems, 81\\n'>\n",
      "<LTTextBoxHorizontal(55) 384.449,40.500,402.512,49.500 'Index \\n'>\n",
      "<LTTextBoxHorizontal(56) 410.090,40.500,413.402,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(57) 420.980,40.500,431.996,49.500 '161\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 72.000,575.217,240.363,605.817 'embedded models, 28\\nembeddings, 48\\nengaging and educating groups responsible for\\n'>\n",
      "<LTTextBoxHorizontal(1) 84.240,564.417,142.857,573.417 'governance, 123\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.000,553.617,234.738,562.617 'environmental, social, and governance (ESG)\\n'>\n",
      "<LTTextBoxHorizontal(3) 84.240,542.817,181.431,551.817 'performance indicators, 36\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.000,532.017,121.968,541.017 'environments\\n'>\n",
      "<LTTextBoxHorizontal(5) 84.240,521.217,236.232,530.217 'changing rapidly, multiplying model risks,\\n'>\n",
      "<LTTextBoxHorizontal(6) 96.480,510.417,105.120,519.417 '70\\n'>\n",
      "<LTTextBoxHorizontal(7) 259.500,596.817,424.677,605.817 'federated learning approach to model retrain‐\\n'>\n",
      "<LTTextBoxHorizontal(8) 271.740,586.017,296.022,595.017 'ing, 34\\n'>\n",
      "<LTTextBoxHorizontal(9) 259.500,575.217,338.475,584.217 'feedback loop, 95-103\\n'>\n",
      "<LTTextBoxHorizontal(10) 271.740,510.417,412.968,573.417 'automating or not, 132\\ninfrastructure, main components of, 96\\nlogging, 96\\nmodel evaluation, 97-99\\nlogical model, 97\\nmodel evaluation store, 98\\n'>\n",
      "<LTTextBoxHorizontal(11) 84.240,488.817,237.321,508.617 'information needed by data scientists, 53\\nproviding exact description for model ver‐\\n'>\n",
      "<LTTextBoxHorizontal(12) 96.480,478.017,173.412,487.017 'sion management, 79\\n'>\n",
      "<LTTextBoxHorizontal(13) 84.240,467.217,166.752,476.217 'reproducibility and, 58\\n'>\n",
      "<LTTextBoxHorizontal(14) 72.000,445.617,230.472,465.417 'ethical position, establishing in MLOps, 118\\nEU\\n'>\n",
      "<LTTextBoxHorizontal(15) 271.740,499.617,425.766,508.617 'online evaluation of models in production,\\n'>\n",
      "<LTTextBoxHorizontal(16) 259.500,445.617,421.185,497.817 '99-102\\nA/B testing, 101\\nchampion/challenger, 100\\nfinancial crisis of 2007-2008, 109\\nfinancial model risk management regulation,\\n'>\n",
      "<LTTextBoxHorizontal(17) 84.240,434.817,213.210,443.817 'General Data Protection Regulation\\n'>\n",
      "<LTTextBoxHorizontal(18) 271.740,434.817,284.700,443.817 '109\\n'>\n",
      "<LTTextBoxHorizontal(19) 96.480,424.017,156.204,433.017 '(GDPR), 35, 110\\n'>\n",
      "<LTTextBoxHorizontal(20) 84.240,413.217,238.455,422.217 'key requirements for trustworthy AI appli‐\\n'>\n",
      "<LTTextBoxHorizontal(21) 96.480,402.417,139.095,411.417 'cations, 111\\n'>\n",
      "<LTTextBoxHorizontal(22) 72.000,380.817,239.211,400.617 'evaluation datasets, 44\\nexperimentation in model development, 49-51\\n'>\n",
      "<LTTextBoxHorizontal(23) 84.240,359.217,232.299,379.017 'impacts on MLOps strategy, 50\\nmarketing recommendation engines, 138\\n'>\n",
      "<LTTextBoxHorizontal(24) 72.000,348.417,133.587,357.417 'explainability, 27\\n'>\n",
      "<LTTextBoxHorizontal(25) 84.240,337.617,239.202,346.617 'for decisions made by ML models affecting\\n'>\n",
      "<LTTextBoxHorizontal(26) 96.480,326.817,138.177,335.817 'humans, 54\\n'>\n",
      "<LTTextBoxHorizontal(27) 84.240,316.017,165.339,325.017 'in Responsible AI, 113\\n'>\n",
      "<LTTextBoxHorizontal(28) 72.000,305.217,200.295,314.217 'exploratory data analysis (see EDA)\\n'>\n",
      "<LTTextBoxHorizontal(29) 71.999,258.317,224.541,293.450 'F\\nFacebook-Cambridge Analytica affair, 106\\nfairness\\n'>\n",
      "<LTTextBoxHorizontal(30) 84.240,215.117,239.058,256.517 'reassuring public that ML is fair, 106\\nrequirements having dimensioning con‐\\nstraints on model development, 54\\nsubpopulation analysis and model fairness,\\n'>\n",
      "<LTTextBoxHorizontal(31) 72.000,171.917,132.840,213.317 '66\\nfeature drift, 92\\nfeature stores, 49\\nfeatures, 24\\n'>\n",
      "<LTTextBoxHorizontal(32) 84.240,150.317,238.527,170.117 'controlling feature-value intervals, 71\\ndrift attributed to, use in mitigating impact\\n'>\n",
      "<LTTextBoxHorizontal(33) 96.480,139.517,134.424,148.517 'of drift, 95\\n'>\n",
      "<LTTextBoxHorizontal(34) 84.240,128.717,213.804,137.717 'engineering and selection, 25, 47-49\\n'>\n",
      "<LTTextBoxHorizontal(35) 96.480,107.117,235.863,126.917 'feature engineering techniques, 47\\nimpacts of feature selection on MLOps\\n'>\n",
      "<LTTextBoxHorizontal(36) 108.720,96.317,149.751,105.317 'strategy, 48\\n'>\n",
      "<LTTextBoxHorizontal(37) 259.500,402.417,413.472,433.017 'financial risk management techniques, 130\\nFlask framework, 29\\nforecasting, 147\\n'>\n",
      "<LTTextBoxHorizontal(38) 271.740,391.617,419.583,400.617 '(see also consumption forecast for power\\n'>\n",
      "<LTTextBoxHorizontal(39) 283.980,380.817,301.809,389.817 'grid)\\n'>\n",
      "<LTTextBoxHorizontal(40) 271.740,359.217,431.967,379.017 'forecasting techniques by use case, 151\\nspatial and temporal resolution of consump‐\\n'>\n",
      "<LTTextBoxHorizontal(41) 283.980,348.417,315.705,357.417 'tion, 151\\n'>\n",
      "<LTTextBoxHorizontal(42) 259.495,301.517,422.497,336.650 'G\\nGAM (generalized additive models), 154\\nGDPR (General Data Protection Regulation),\\n'>\n",
      "<LTTextBoxHorizontal(43) 271.738,290.717,297.433,299.717 '35, 110\\n'>\n",
      "<LTTextBoxHorizontal(44) 259.498,258.317,403.435,288.917 'generalization capacity (models), 42\\nGit, 75\\nGoogle, smartphone keyboard software,\\n'>\n",
      "<LTTextBoxHorizontal(45) 271.738,247.517,313.012,256.517 'GBoard, 34\\n'>\n",
      "<LTTextBoxHorizontal(46) 259.498,236.717,358.858,245.717 'governance, 34-38, 105-125\\n'>\n",
      "<LTTextBoxHorizontal(47) 271.738,225.917,403.750,234.917 'AI-specific regulations, new wave of,\\n'>\n",
      "<LTTextBoxHorizontal(48) 283.978,204.317,406.045,224.117 '111-112\\nstatus of AI governance initiatives\\n'>\n",
      "<LTTextBoxHorizontal(49) 296.218,193.517,351.550,202.517 'worldwide, 111\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.738,161.117,367.201,191.717 'application to MLOps, 36\\ndata governance, 36\\nprocess governance, 37\\n'>\n",
      "<LTTextBoxHorizontal(51) 271.738,139.517,430.102,159.317 'critical role in machine learning security, 69\\ncurrent regulations driving MLOps gover‐\\n'>\n",
      "<LTTextBoxHorizontal(52) 283.978,107.117,421.741,137.717 'nance, 108-111\\nfinancial model risk management, 109\\nGDPR and CCPA data privacy regula‐\\n'>\n",
      "<LTTextBoxHorizontal(53) 296.218,96.317,331.246,105.317 'tions, 110\\n'>\n",
      "<LTTextBoxHorizontal(54) 84.240,85.517,237.852,94.517 'statistical test on data from source and tar‐\\n'>\n",
      "<LTTextBoxHorizontal(55) 283.978,85.517,422.848,94.517 'pharmaceutical regulation in US, GxP,\\n'>\n",
      "<LTTextBoxHorizontal(56) 96.480,74.717,165.024,83.717 'get distribution, 94\\n'>\n",
      "<LTTextBoxHorizontal(57) 296.218,74.717,309.178,83.717 '109\\n'>\n",
      "<LTTextBoxHorizontal(58) 71.998,40.500,84.436,49.500 '162 \\n'>\n",
      "<LTTextBoxHorizontal(59) 92.014,40.500,95.326,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(60) 102.904,40.500,119.545,49.500 'Index\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 2.700,-9.000,2.700,54.000>\n",
      "<LTLine 2.700,52.000,2.700,609.500>\n",
      "<LTLine 2.700,607.500,2.700,670.500>\n",
      "<LTTextBoxHorizontal(0) 84.240,596.817,229.851,605.817 'key elements of Responsible AI, 112-116\\n'>\n",
      "<LTTextBoxHorizontal(1) 96.480,542.817,213.093,595.017 'bias, 114\\ndata, 113\\ngovernance techniques, 116\\ninclusiveness, 115\\nmodel management at scale, 116\\n'>\n",
      "<LTTextBoxHorizontal(2) 84.240,521.217,190.854,541.017 'matching with risk level, 107\\ntemplate for MLOps, 117-125\\n'>\n",
      "<LTTextBoxHorizontal(3) 96.480,467.217,231.264,519.417 'determining governance policies, 120\\nengaging end educating, 123\\nestablishing an ethical position, 118\\nestablishing responsibilities, 119\\nintegrating governance policies into\\n'>\n",
      "<LTTextBoxHorizontal(4) 271.739,542.817,394.589,605.817 'detection of, 91\\ndetection techniques, 93\\ndomain classifier, 94\\ninterpretation of results, 94\\nunivariate statistical tests, 93\\nmodel retraining motivated by, 33\\n'>\n",
      "<LTTextBoxHorizontal(5) 259.499,532.017,311.969,541.017 'integration, 77\\n'>\n",
      "<LTTextBoxHorizontal(6) 271.739,521.217,366.212,530.217 '(see also CI/CD pipelines)\\n'>\n",
      "<LTTextBoxHorizontal(7) 259.499,499.617,420.572,519.417 'intentionality, Responsible AI, 9, 112\\ninteractions between models, risks generated\\n'>\n",
      "<LTTextBoxHorizontal(8) 271.739,488.817,293.177,497.817 'by, 70\\n'>\n",
      "<LTTextBoxHorizontal(9) 259.499,478.017,422.201,487.017 'interpretability (ML), bias detection through,\\n'>\n",
      "<LTTextBoxHorizontal(10) 271.739,467.217,284.699,476.217 '114\\n'>\n",
      "<LTTextBoxHorizontal(11) 108.720,456.417,182.304,465.417 'MLOps process, 121\\n'>\n",
      "<LTTextBoxHorizontal(12) 259.499,456.417,358.670,465.417 'iterations of ML models, 32\\n'>\n",
      "<LTTextBoxHorizontal(13) 96.480,445.617,230.076,454.617 'monitoring and refining governance,\\n'>\n",
      "<LTTextBoxHorizontal(14) 108.720,434.817,121.680,443.817 '124\\n'>\n",
      "<LTTextBoxHorizontal(15) 271.739,434.817,423.740,454.617 'feedback loop, 33\\niterating on model deployed to millions of\\n'>\n",
      "<LTTextBoxHorizontal(16) 96.480,424.017,226.881,433.017 'selecting tools for centralized gover‐\\n'>\n",
      "<LTTextBoxHorizontal(17) 283.979,424.017,322.805,433.017 'devices, 34\\n'>\n",
      "<LTTextBoxHorizontal(18) 108.720,413.217,196.173,422.217 'nance management, 122\\n'>\n",
      "<LTTextBoxHorizontal(19) 96.480,402.417,237.339,411.417 'understanding/classifying analytics use\\n'>\n",
      "<LTTextBoxHorizontal(20) 108.720,391.617,143.964,400.617 'cases, 118\\n'>\n",
      "<LTTextBoxHorizontal(21) 84.240,380.817,227.502,389.817 'who decides on organization needs, 105\\n'>\n",
      "<LTTextBoxHorizontal(22) 72.000,359.217,193.482,379.017 'gradient boosting algorithms, 130\\nground truth, 30\\n'>\n",
      "<LTTextBoxHorizontal(23) 84.240,348.417,243.666,357.417 'evaluation of, in monitoring of performance\\n'>\n",
      "<LTTextBoxHorizontal(24) 96.480,337.617,152.496,346.617 'degradation, 89\\n'>\n",
      "<LTTextBoxHorizontal(25) 72.000,326.817,227.124,335.817 'GxP pharmaceutical regulations in US, 109\\n'>\n",
      "<LTTextBoxHorizontal(26) 71.998,236.717,227.671,315.050 'H\\nhealth checks, 79\\nhealth diagnosis or prediction example, 43\\nhigh bias model (underfitting), 50\\nhigh variance model (overfitting), 50\\nhistorizing data, 153\\nhouse price example, model prediction and\\n'>\n",
      "<LTTextBoxHorizontal(27) 84.238,225.917,147.967,234.917 'generalization, 43\\n'>\n",
      "<LTTextBoxHorizontal(28) 71.998,182.717,210.958,224.117 'human-centered approach, 113\\nhuman-in-command, 141\\nhuman-in-the-loop (HITL), 115\\nhyperparameters of ML algorithms, 44\\n'>\n",
      "<LTTextBoxHorizontal(29) 71.994,135.817,242.954,170.950 'I\\nimpact coding, 47\\nimplementation, reproducibility of models and,\\n'>\n",
      "<LTTextBoxHorizontal(30) 84.239,125.017,92.879,134.017 '57\\n'>\n",
      "<LTTextBoxHorizontal(31) 71.999,103.417,239.372,123.217 'inclusiveness, 115\\nindividual conditional expectation (ICE) com‐\\n'>\n",
      "<LTTextBoxHorizontal(32) 84.239,92.617,130.967,101.617 'putations, 54\\n'>\n",
      "<LTTextBoxHorizontal(33) 71.999,71.017,231.983,90.817 'inference, computing power required for, 62\\ninput drift, 31\\n'>\n",
      "<LTTextBoxHorizontal(34) 259.497,387.917,421.494,412.250 'J\\nJava-based environment, model developed in\\n'>\n",
      "<LTTextBoxHorizontal(35) 271.743,377.117,310.740,386.117 'Python, 61\\n'>\n",
      "<LTTextBoxHorizontal(36) 259.503,366.317,346.416,375.317 'Jenkins build system, 76\\n'>\n",
      "<LTTextBoxHorizontal(37) 259.496,319.417,413.774,354.550 'K\\nKolmogorov-Smirnov test, 94, 132\\nKPIs (key performance indicators), 24, 107\\n'>\n",
      "<LTTextBoxHorizontal(38) 271.736,243.817,428.885,317.617 'in business scoping, 122\\nfor governance, 124\\nmeasurement of, 25\\nmonitoring for deployed ML models, 31\\nsubject matter experts contributing, 15\\ntracking model performance against, 123\\ntraining different models for different KPIs,\\n'>\n",
      "<LTTextBoxHorizontal(39) 259.496,222.217,325.745,242.017 '144\\nKubernetes, 28, 80\\n'>\n",
      "<LTTextBoxHorizontal(40) 271.736,200.617,364.103,220.417 'autoscaling properties, 81\\nusing with Spark, 82\\n'>\n",
      "<LTTextBoxHorizontal(41) 259.496,164.517,412.966,188.850 'L\\nlabeled sample subset, problems in ground\\n'>\n",
      "<LTTextBoxHorizontal(42) 271.738,153.717,346.447,162.717 'truth monitoring, 90\\n'>\n",
      "<LTTextBoxHorizontal(43) 259.498,142.917,284.986,151.917 'latency\\n'>\n",
      "<LTTextBoxHorizontal(44) 271.738,132.117,413.398,141.117 'metrics on, collection in preproduction\\n'>\n",
      "<LTTextBoxHorizontal(45) 283.978,121.317,345.736,130.317 'model testing, 65\\n'>\n",
      "<LTTextBoxHorizontal(46) 271.738,110.517,367.975,119.517 'model conversions and, 61\\n'>\n",
      "<LTTextBoxHorizontal(47) 259.498,78.117,363.664,108.717 'linear algorithms, 45\\nlinear models, advanced, 130\\nLinux cgroups, 29\\n'>\n",
      "<LTTextBoxHorizontal(48) 384.451,40.500,402.514,49.500 'Index \\n'>\n",
      "<LTTextBoxHorizontal(49) 410.092,40.500,413.404,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(50) 420.982,40.500,431.998,49.500 '163\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 72.000,586.017,152.064,605.817 'live-scoring model, 28\\nlogging\\n'>\n",
      "<LTTextBoxHorizontal(1) 84.240,553.617,244.305,584.217 'activity logging of deployed model, 132\\ncomponent in ML feedback loop, 96\\nusing to generate centralized datasets for use\\n'>\n",
      "<LTTextBoxHorizontal(2) 96.480,542.817,175.410,551.817 'by model designer, 83\\n'>\n",
      "<LTTextBoxHorizontal(3) 72.000,532.017,133.119,541.017 'logical model, 98\\n'>\n",
      "<LTTextBoxHorizontal(4) 259.504,596.817,287.107,605.817 'MLOps\\n'>\n",
      "<LTTextBoxHorizontal(5) 271.744,532.017,402.415,595.017 'considerations with A/B testing, 102\\ndefining, and its challenges, 4-7\\ngovernance template for, 117-125\\nmitigating risks of ML models, 7-10\\nversus ModelOps versus AIOps, 3\\nreal-world examples\\n'>\n",
      "<LTTextBoxHorizontal(6) 84.240,521.217,236.727,530.217 'comparing performance between different\\n'>\n",
      "<LTTextBoxHorizontal(7) 283.984,521.217,409.426,530.217 'consumer credit risk management,\\n'>\n",
      "<LTTextBoxHorizontal(8) 96.480,510.417,139.077,519.417 'versions, 99\\n'>\n",
      "<LTTextBoxHorizontal(9) 84.240,499.617,180.729,508.617 'versioning evolution of, 99\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.000,488.817,234.495,497.817 'LTSM (long short-term memory) neural net‐\\n'>\n",
      "<LTTextBoxHorizontal(11) 84.240,478.017,123.156,487.017 'works, 155\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.000,420.317,242.185,466.250 'M\\nmachine learning, 41\\n(see also models)\\ncontinuous delivery for end-to-end process,\\n'>\n",
      "<LTTextBoxHorizontal(13) 96.484,409.517,105.124,418.517 '95\\n'>\n",
      "<LTTextBoxHorizontal(14) 84.244,355.517,227.713,407.717 'deciding whether to use or not, 151-152\\nprimer, 23\\nquality assurance for, 64-66\\nrole in recommendation engines, 136\\nsecurity issues, 67-69\\n'>\n",
      "<LTTextBoxHorizontal(15) 72.004,344.717,184.522,353.717 'machine learning architects, 21\\n'>\n",
      "<LTTextBoxHorizontal(16) 84.244,323.117,243.400,342.917 'role in and needs from MLOps, 22\\nrole in machine learning model life cycle, 22\\n'>\n",
      "<LTTextBoxHorizontal(17) 72.004,247.517,236.227,321.317 'machine learning metrics, monitoring, 79\\nmachine learning models (see models)\\nmaintenance of models in production, 79\\nmanual control of models, 143\\nMAPE (mean absolute percentage error), 156\\nmarketing data mining urban legend, 135\\nmarketing recommendation engines, 135-145\\n'>\n",
      "<LTTextBoxHorizontal(18) 84.244,225.917,232.897,245.717 'data preparation for, 137\\ndesigning and managing experiments on,\\n'>\n",
      "<LTTextBoxHorizontal(19) 96.484,215.117,109.444,224.117 '138\\n'>\n",
      "<LTTextBoxHorizontal(20) 84.244,204.317,231.934,213.317 'model training and deployment, 138-141\\n'>\n",
      "<LTTextBoxHorizontal(21) 96.484,193.517,160.123,202.517 'challenges in, 139\\n'>\n",
      "<LTTextBoxHorizontal(22) 84.244,171.917,241.141,191.717 'monitoring and feedback, 142-145\\npipeline structure and deployment strategy,\\n'>\n",
      "<LTTextBoxHorizontal(23) 96.484,161.117,109.444,170.117 '141\\n'>\n",
      "<LTTextBoxHorizontal(24) 84.244,128.717,233.401,159.317 'rise of recommendation engines, 135-137\\npush or pull recommendations, 136\\nrole of machine learning, 136\\n'>\n",
      "<LTTextBoxHorizontal(25) 72.004,107.117,229.297,126.917 'MATLAB scripts, 155\\nminority populations, representation in ML\\n'>\n",
      "<LTTextBoxHorizontal(26) 72.004,74.717,180.382,105.317 'model data sources, 25\\nmisbehavior of models, 71\\nmissing value imputation, 153\\n'>\n",
      "<LTTextBoxHorizontal(27) 71.996,40.500,84.434,49.500 '164 \\n'>\n",
      "<LTTextBoxHorizontal(28) 92.012,40.500,95.324,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(29) 102.902,40.500,119.543,49.500 'Index\\n'>\n",
      "<LTTextBoxHorizontal(30) 296.224,510.417,325.348,519.417 '129-133\\n'>\n",
      "<LTTextBoxHorizontal(31) 283.984,488.817,417.040,508.617 'consumption forecast, 147-157\\nmarketing recommendation engines,\\n'>\n",
      "<LTTextBoxHorizontal(32) 296.224,478.017,325.348,487.017 '135-145\\n'>\n",
      "<LTTextBoxHorizontal(33) 271.744,467.217,314.278,476.217 'for scale, 10\\n'>\n",
      "<LTTextBoxHorizontal(34) 259.504,445.617,373.606,465.417 'model evaluation stores, 97, 98\\nmodel risk manager/auditor, 21\\n'>\n",
      "<LTTextBoxHorizontal(35) 271.744,424.017,430.900,443.817 'role in and needs from MLOps, 21\\nrole in machine learning model life cycle, 21\\n'>\n",
      "<LTTextBoxHorizontal(36) 259.504,402.417,422.206,422.217 'model-as-a-service, or live-scoring model, 28\\nmodels\\n'>\n",
      "<LTTextBoxHorizontal(37) 271.744,391.617,360.601,400.617 'developing, 24-27, 41-58\\n'>\n",
      "<LTTextBoxHorizontal(38) 283.984,380.817,407.374,389.817 'consumer credit risk management\\n'>\n",
      "<LTTextBoxHorizontal(39) 283.984,348.417,429.505,379.017 'model, 130\\ndata exploration, 46\\ndata sources and exploratory data analy‐\\n'>\n",
      "<LTTextBoxHorizontal(40) 296.224,337.617,317.977,346.617 'sis, 24\\n'>\n",
      "<LTTextBoxHorizontal(41) 283.984,294.417,430.657,335.817 'establishing business objectives, 24\\nevaluating and comparing models, 51-56\\nexperimentation, 49-51\\nfeature engineering and selection, 25,\\n'>\n",
      "<LTTextBoxHorizontal(42) 283.984,251.217,414.826,292.617 '47-49\\nin practice, 43\\nin theory, 42\\nMLOps considerations by algorithm\\n'>\n",
      "<LTTextBoxHorizontal(43) 296.224,240.417,324.376,249.417 'type, 45\\n'>\n",
      "<LTTextBoxHorizontal(44) 283.975,197.217,425.248,238.617 'required components, 44\\nResponsible AI, 26\\ntraining and evaluation, 26\\nversion management and reproducibil‐\\n'>\n",
      "<LTTextBoxHorizontal(45) 296.215,186.417,330.082,195.417 'ity, 56-58\\n'>\n",
      "<LTTextBoxHorizontal(46) 271.735,143.217,430.973,184.617 'iterations and life cycle, 32-34\\nlife cycle, 4\\nmitigating risks with MLOps, 7-10\\nmonitoring after deployment to production,\\n'>\n",
      "<LTTextBoxHorizontal(47) 283.976,132.417,304.460,141.417 '29-32\\n'>\n",
      "<LTTextBoxHorizontal(48) 271.736,121.617,409.832,130.617 'productionalization and deploymeent,\\n'>\n",
      "<LTTextBoxHorizontal(49) 283.976,110.817,304.460,119.817 '27-29\\n'>\n",
      "<LTTextBoxHorizontal(50) 259.496,89.217,420.848,109.017 'monitoring and refining governance, 124\\nmonitoring machine learning models, 29-32,\\n'>\n",
      "<LTTextBoxHorizontal(51) 271.736,78.417,296.540,87.417 '85-103\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 2.700,-9.000,2.700,54.000>\n",
      "<LTLine 2.700,52.000,2.700,609.500>\n",
      "<LTLine 2.700,607.500,2.700,670.500>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 84.240,586.017,234.540,605.817 'business concerns, 31\\nconsumer credit risk management model,\\n'>\n",
      "<LTTextBoxHorizontal(1) 96.480,575.217,126.495,584.217 '132, 132\\n'>\n",
      "<LTTextBoxHorizontal(2) 84.240,564.417,243.477,573.417 'consumption forecast for power grid model,\\n'>\n",
      "<LTTextBoxHorizontal(3) 96.480,553.617,109.440,562.617 '156\\n'>\n",
      "<LTTextBoxHorizontal(4) 84.240,521.217,244.197,551.817 'data scientist concerns, 18, 30\\ndeciding how often to retrain models, 86-89\\ndegradation of performance, understanding,\\n'>\n",
      "<LTTextBoxHorizontal(5) 96.480,488.817,195.759,519.417 '89-92\\nground truth evaluation, 89\\ninput drift detection, 91\\n'>\n",
      "<LTTextBoxHorizontal(6) 84.240,467.217,200.421,487.017 'DevOps concerns, 30\\ndrift detection in practice, 92-95\\n'>\n",
      "<LTTextBoxHorizontal(7) 96.480,456.417,223.308,465.417 'input drift detection techniques, 93\\n'>\n",
      "<LTTextBoxHorizontal(8) 84.240,445.617,163.215,454.617 'feedback loop, 95-103\\n'>\n",
      "<LTTextBoxHorizontal(9) 96.480,413.217,236.988,443.817 'logging, 96\\nmodel evaluation, 97-99\\nonline evaluation of models in produc‐\\n'>\n",
      "<LTTextBoxHorizontal(10) 108.720,402.417,152.289,411.417 'tion, 99-102\\n'>\n",
      "<LTTextBoxHorizontal(11) 84.240,391.617,231.048,400.617 'marketing recommendation engine, 140,\\n'>\n",
      "<LTTextBoxHorizontal(12) 96.480,370.017,242.415,389.817 '142-145\\nmodel runs overnight, sleeps in daytime,\\n'>\n",
      "<LTTextBoxHorizontal(13) 108.720,359.217,121.680,368.217 '143\\n'>\n",
      "<LTTextBoxHorizontal(14) 96.480,337.617,238.878,357.417 'monitoring performance, 144\\noption to automatically control models,\\n'>\n",
      "<LTTextBoxHorizontal(15) 108.720,326.817,121.680,335.817 '144\\n'>\n",
      "<LTTextBoxHorizontal(16) 96.480,294.417,238.725,325.017 'option to manually control models, 143\\nretraining models, 142\\nupdating models, 143\\n'>\n",
      "<LTTextBoxHorizontal(17) 84.240,272.817,225.279,292.617 'model risk manager/auditor, role in, 21\\nin production, 79\\n'>\n",
      "<LTTextBoxHorizontal(18) 72.000,251.217,201.978,271.017 'monotonicity constraints, 130\\nMRM (model risk management), 21\\n'>\n",
      "<LTTextBoxHorizontal(19) 84.240,229.617,233.181,249.417 '(see also model risk manager/auditor)\\nprinciples for good MRM, defined by UK\\n'>\n",
      "<LTTextBoxHorizontal(20) 96.480,218.817,130.410,227.817 'PRA, 109\\n'>\n",
      "<LTTextBoxHorizontal(21) 84.240,208.017,216.288,217.017 'regulation for fianancial models, 109\\n'>\n",
      "<LTTextBoxHorizontal(22) 72.000,197.217,182.196,206.217 'multi-armed bandit testing, 34\\n'>\n",
      "<LTTextBoxHorizontal(23) 72.001,161.117,143.227,185.450 'N\\nneural networks, 45\\n'>\n",
      "<LTTextBoxHorizontal(24) 72.001,128.717,237.178,159.317 'adversarial attacks on, 68\\nlong short-term memory (LSTM), 155\\nNLP (natural language processing) pretrained\\n'>\n",
      "<LTTextBoxHorizontal(25) 84.241,117.917,210.070,126.917 'models, compression used with, 61\\n'>\n",
      "<LTTextBoxHorizontal(26) 72.001,107.117,180.937,116.117 'null hypothesis to p-values, 90\\n'>\n",
      "<LTTextBoxHorizontal(27) 71.997,71.017,148.902,95.350 'O\\none-hot encoding, 47\\n'>\n",
      "<LTTextBoxHorizontal(28) 259.497,596.817,413.523,605.817 'online evaluation of models in production,\\n'>\n",
      "<LTTextBoxHorizontal(29) 271.737,564.417,365.418,595.017 '99-102\\nA/B testing, 101\\nchampion/challenger, 100\\n'>\n",
      "<LTTextBoxHorizontal(30) 259.497,532.017,383.337,562.617 'online machine learning, 87\\noperationalization and MLOps, 18\\norchestration of containers, 28\\n'>\n",
      "<LTTextBoxHorizontal(31) 259.495,495.917,302.456,520.250 'P\\np-values, 89\\n'>\n",
      "<LTTextBoxHorizontal(32) 271.739,485.117,389.234,494.117 'advantages and drawbacks of, 94\\n'>\n",
      "<LTTextBoxHorizontal(33) 259.499,463.517,383.168,483.317 'parallelization of batch scoring, 78\\npartitioning (or sharding), 78\\n'>\n",
      "<LTTextBoxHorizontal(34) 271.739,452.717,424.397,461.717 'distributing batch processing by partition‐\\n'>\n",
      "<LTTextBoxHorizontal(35) 283.979,441.917,325.532,450.917 'ing data, 82\\n'>\n",
      "<LTTextBoxHorizontal(36) 259.499,431.117,418.034,440.117 'people in MLOps, roles and responsibilities,\\n'>\n",
      "<LTTextBoxHorizontal(37) 271.739,333.917,404.516,429.317 '13-22\\ndata engineers, 19\\ndata scientists, 17-19\\nDevOps, 20\\nmachine learning architects, 21\\nmodel risk manager/auditor, 21\\noverview, 13\\nsoftware engineers, 20\\nsubject matter experts (SMEs), 15-17\\n'>\n",
      "<LTTextBoxHorizontal(38) 259.499,323.117,305.849,332.117 'performance\\n'>\n",
      "<LTTextBoxHorizontal(39) 271.739,301.517,431.084,321.317 'considerations when converting from devel‐\\nopment to production environments, 61\\n'>\n",
      "<LTTextBoxHorizontal(40) 271.739,290.717,424.163,299.717 'degradation of model performance, moni‐\\n'>\n",
      "<LTTextBoxHorizontal(41) 283.979,279.917,330.779,288.917 'toring, 89-92\\n'>\n",
      "<LTTextBoxHorizontal(42) 271.739,269.117,426.746,278.117 'drift, assessing for subpopulations in credit\\n'>\n",
      "<LTTextBoxHorizontal(43) 283.979,258.317,388.379,267.317 'risk management model, 132\\n'>\n",
      "<LTTextBoxHorizontal(44) 271.739,236.717,429.473,256.517 'model retraining and, 86\\nmonitoring for marketing recommendation\\n'>\n",
      "<LTTextBoxHorizontal(45) 283.979,225.917,328.457,234.917 'engines, 144\\n'>\n",
      "<LTTextBoxHorizontal(46) 271.739,182.717,422.201,224.117 'monitoring ML models for, 86\\ntesting on out-of-sample datasets for con‐\\nsumer credit risk management model,\\n131\\n'>\n",
      "<LTTextBoxHorizontal(47) 259.499,150.317,421.562,180.917 'performance metrics, 44\\npersonally identifiable information (PII), 25\\nmaking sure no PII used to train a model,\\n'>\n",
      "<LTTextBoxHorizontal(48) 283.979,139.517,292.619,148.517 '36\\n'>\n",
      "<LTTextBoxHorizontal(49) 259.499,85.517,416.675,137.717 'pharmaceutical regulations in US, GxP, 109\\npoisoning attacks, 68\\nportable formats for models, 28, 156\\npower systems, 147-149\\npredictions\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.739,74.717,321.986,83.717 'conformal, 72\\n'>\n",
      "<LTTextBoxHorizontal(51) 384.452,40.500,402.515,49.500 'Index \\n'>\n",
      "<LTTextBoxHorizontal(52) 410.093,40.500,413.405,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(53) 420.983,40.500,431.999,49.500 '165\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 84.240,586.017,240.984,605.817 'decoupled ground truth and prediction, 90\\nexamples of model prediction and generali‐\\n'>\n",
      "<LTTextBoxHorizontal(1) 96.480,575.217,131.445,584.217 'zation, 43\\n'>\n",
      "<LTTextBoxHorizontal(2) 283.980,596.817,431.877,605.817 'adaptation from development to produc‐\\n'>\n",
      "<LTTextBoxHorizontal(3) 296.220,586.017,387.480,595.017 'tion environments, 60-62\\n'>\n",
      "<LTTextBoxHorizontal(4) 283.980,575.217,428.241,584.217 'data access before validation and launch\\n'>\n",
      "<LTTextBoxHorizontal(5) 84.240,553.617,224.388,573.417 'ground truth for model predictions, 31\\ninput drift and, 31\\n'>\n",
      "<LTTextBoxHorizontal(6) 283.980,553.617,358.356,573.417 'to poduction, 62\\nfinal thoughts on, 62\\n'>\n",
      "<LTTextBoxHorizontal(7) 72.000,542.817,117.126,551.817 'probabilities\\n'>\n",
      "<LTTextBoxHorizontal(8) 84.240,532.017,241.326,541.017 'distances between probability distributions,\\n'>\n",
      "<LTTextBoxHorizontal(9) 96.480,521.217,151.650,530.217 'measuring, 132\\n'>\n",
      "<LTTextBoxHorizontal(10) 84.240,510.417,230.841,519.417 'produced by models used for risk assess‐\\n'>\n",
      "<LTTextBoxHorizontal(11) 96.480,499.617,132.228,508.617 'ment, 131\\n'>\n",
      "<LTTextBoxHorizontal(12) 72.000,467.217,238.374,497.817 'problem definition and data acquisition, com‐\\nsumer credit risk management MLOps\\napplication, 130\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.000,456.417,239.130,465.417 'problem definition, using machine learning or\\n'>\n",
      "<LTTextBoxHorizontal(14) 84.240,445.617,129.717,454.617 'not, 151-152\\n'>\n",
      "<LTTextBoxHorizontal(15) 72.000,434.817,155.223,443.817 'process governance, 37\\n'>\n",
      "<LTTextBoxHorizontal(16) 84.240,424.017,237.969,433.017 'effective implementation, difficulties of, 37\\n'>\n",
      "<LTTextBoxHorizontal(17) 72.000,402.417,186.543,422.217 'production, deploying to, 73-84\\nbuilding ML artifacts, 75-76\\n'>\n",
      "<LTTextBoxHorizontal(18) 96.480,391.617,222.615,400.617 'using testing pipeline on model, 75\\n'>\n",
      "<LTTextBoxHorizontal(19) 84.240,370.017,234.540,389.817 'CI/CD pipelines, 73\\nconsumer credit risk management model,\\n'>\n",
      "<LTTextBoxHorizontal(20) 96.480,359.217,109.440,368.217 '132\\n'>\n",
      "<LTTextBoxHorizontal(21) 84.240,348.417,230.148,357.417 'consumption forecast prototype models,\\n'>\n",
      "<LTTextBoxHorizontal(22) 96.480,337.617,109.440,346.617 '155\\n'>\n",
      "<LTTextBoxHorizontal(23) 84.240,316.017,187.335,335.817 'containerization, 79-81\\ndeployment strategies, 77-79\\n'>\n",
      "<LTTextBoxHorizontal(24) 96.480,294.417,244.386,314.217 'categories of model deployment, 77\\nconsiderations in sending models to pro‐\\n'>\n",
      "<LTTextBoxHorizontal(25) 108.720,283.617,149.463,292.617 'duction, 78\\n'>\n",
      "<LTTextBoxHorizontal(26) 96.480,272.817,234.549,281.817 'maintenance of models in production,\\n'>\n",
      "<LTTextBoxHorizontal(27) 108.720,262.017,117.360,271.017 '79\\n'>\n",
      "<LTTextBoxHorizontal(28) 84.240,251.217,182.034,260.217 'scaling deployments, 81-83\\n'>\n",
      "<LTTextBoxHorizontal(29) 96.480,240.417,212.247,249.417 'requirements and challenges, 83\\n'>\n",
      "<LTTextBoxHorizontal(30) 72.000,229.617,189.396,238.617 'production, preparing for, 59-72\\n'>\n",
      "<LTTextBoxHorizontal(31) 84.240,218.817,234.540,227.817 'consumer credit risk management model,\\n'>\n",
      "<LTTextBoxHorizontal(32) 96.480,208.017,109.440,217.017 '131\\n'>\n",
      "<LTTextBoxHorizontal(33) 84.240,164.817,202.203,206.217 'key ideas, summary of, 72\\nmachine learning security, 67-69\\nmodel risk evaluation, 63-64\\nmodel risk mitigation, 69-72\\n'>\n",
      "<LTTextBoxHorizontal(34) 96.480,132.417,212.103,163.017 'changing environments, 70\\ninteractions between models, 70\\nmodel misbehavior, 71\\n'>\n",
      "<LTTextBoxHorizontal(35) 84.240,121.617,226.089,130.617 'quality assurance for machine learning,\\n'>\n",
      "<LTTextBoxHorizontal(36) 96.480,110.817,116.964,119.817 '64-66\\n'>\n",
      "<LTTextBoxHorizontal(37) 84.240,100.017,239.877,109.017 'reproducibility and auditability for models,\\n'>\n",
      "<LTTextBoxHorizontal(38) 96.480,89.217,105.120,98.217 '66\\n'>\n",
      "<LTTextBoxHorizontal(39) 84.240,78.417,190.053,87.417 'runtime environments, 60-63\\n'>\n",
      "<LTTextBoxHorizontal(40) 71.997,40.500,84.435,49.500 '166 \\n'>\n",
      "<LTTextBoxHorizontal(41) 92.013,40.500,95.325,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(42) 102.903,40.500,119.544,49.500 'Index\\n'>\n",
      "<LTTextBoxHorizontal(43) 259.500,542.817,427.404,551.817 'productionalization and deploymnt of models,\\n'>\n",
      "<LTTextBoxHorizontal(44) 271.740,510.417,421.311,541.017 '27-29\\nmodel deployment requirements, 29\\nmodel deployment types and contents, 28\\n'>\n",
      "<LTTextBoxHorizontal(45) 259.500,478.017,404.409,508.617 'progressive or canary rollouts, 69\\nproject criticality and operationalization\\napproaches to risk assessment, 107\\n'>\n",
      "<LTTextBoxHorizontal(46) 259.500,445.617,422.319,476.217 'provenance of data, 113\\npruning models, 61\\npublic opinion, influence on ML governance,\\n'>\n",
      "<LTTextBoxHorizontal(47) 271.740,434.817,284.700,443.817 '106\\n'>\n",
      "<LTTextBoxHorizontal(48) 259.500,413.217,387.822,433.017 'push or pull recommendations, 136\\nPython, 9, 61, 155\\n'>\n",
      "<LTTextBoxHorizontal(49) 259.497,377.117,422.220,401.450 'Q\\nQA (quality assurance) for machine learning,\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.740,344.717,427.665,375.317 '64-66\\nkey testing considerations, 65\\nproviding clear view of model performance\\n'>\n",
      "<LTTextBoxHorizontal(51) 283.980,333.917,393.735,342.917 'and facilitating auditability, 67\\n'>\n",
      "<LTTextBoxHorizontal(52) 259.500,323.117,317.703,332.117 'quantization, 61\\n'>\n",
      "<LTTextBoxHorizontal(53) 259.497,276.217,414.648,311.350 'R\\nR language, 155\\nRACI (responsible, accountable, consulted,\\n'>\n",
      "<LTTextBoxHorizontal(54) 271.737,265.417,325.782,274.417 'informed), 119\\n'>\n",
      "<LTTextBoxHorizontal(55) 259.497,254.617,316.512,263.617 'randomness, 57\\n'>\n",
      "<LTTextBoxHorizontal(56) 271.737,243.817,348.435,252.817 'random sampling, 91\\n'>\n",
      "<LTTextBoxHorizontal(57) 259.497,233.017,333.810,242.017 'real-time scoring, 77\\n'>\n",
      "<LTTextBoxHorizontal(58) 271.737,211.417,426.348,231.217 'logging streaming data, 83\\nmarketing recommendation engine model,\\n'>\n",
      "<LTTextBoxHorizontal(59) 283.977,200.617,296.937,209.617 '140\\n'>\n",
      "<LTTextBoxHorizontal(60) 259.497,189.817,368.271,198.817 'recommendation engines, 135\\n'>\n",
      "<LTTextBoxHorizontal(61) 271.737,179.017,405.549,188.017 '(see also marketing recommendation\\n'>\n",
      "<LTTextBoxHorizontal(62) 271.737,157.417,327.132,177.217 'engines)\\nrise of, 135-137\\n'>\n",
      "<LTTextBoxHorizontal(63) 283.977,146.617,425.340,155.617 'deciding on push or pull recommenda‐\\n'>\n",
      "<LTTextBoxHorizontal(64) 296.217,135.817,331.245,144.817 'tions, 136\\n'>\n",
      "<LTTextBoxHorizontal(65) 271.737,125.017,408.177,134.017 'turning recommendations on/off, 141\\n'>\n",
      "<LTTextBoxHorizontal(66) 259.497,103.417,351.450,123.217 'red-black deployment, 78\\nregulations, 118\\n'>\n",
      "<LTTextBoxHorizontal(67) 271.737,81.817,403.938,101.617 'AI-specific, new wave of, 111-112\\ncurrent, driving MLOps governance,\\n'>\n",
      "<LTTextBoxHorizontal(68) 283.977,71.017,313.101,80.017 '108-111\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 2.700,-9.000,2.700,54.000>\n",
      "<LTLine 2.700,52.000,2.700,609.500>\n",
      "<LTLine 2.700,607.500,2.700,670.500>\n",
      "<LTTextBoxHorizontal(0) 96.480,586.017,234.243,605.817 'financial model risk management, 109\\nGDPR and CCPA data privacy regula‐\\n'>\n",
      "<LTTextBoxHorizontal(1) 108.720,575.217,143.748,584.217 'tions, 110\\n'>\n",
      "<LTTextBoxHorizontal(2) 283.980,596.817,340.734,605.817 '(see also biases)\\n'>\n",
      "<LTTextBoxHorizontal(3) 271.740,586.017,429.366,595.017 'in consumer credit risk management model\\n'>\n",
      "<LTTextBoxHorizontal(4) 283.980,575.217,315.273,584.217 'bias, 131\\n'>\n",
      "<LTTextBoxHorizontal(5) 96.480,564.417,235.350,573.417 'pharmaceutical regulation in US, GxP,\\n'>\n",
      "<LTTextBoxHorizontal(6) 271.740,564.417,368.346,573.417 'feature that causes drift, 95\\n'>\n",
      "<LTTextBoxHorizontal(7) 108.720,553.617,121.680,562.617 '109\\n'>\n",
      "<LTTextBoxHorizontal(8) 259.500,553.617,276.321,562.617 'risks\\n'>\n",
      "<LTTextBoxHorizontal(9) 84.240,542.817,241.083,551.817 'governance and regulatory checks in model\\n'>\n",
      "<LTTextBoxHorizontal(10) 96.480,532.017,155.502,541.017 'deployments, 29\\n'>\n",
      "<LTTextBoxHorizontal(11) 271.740,532.017,417.153,551.817 'assessing, considerations in MLOps, 107\\ncredit risk modeling, 130\\n'>\n",
      "<LTTextBoxHorizontal(12) 84.240,521.217,237.600,530.217 'government regulations on ML to mitigate\\n'>\n",
      "<LTTextBoxHorizontal(13) 283.980,521.217,424.155,530.217 '(see also consumer credit risk manage‐\\n'>\n",
      "<LTTextBoxHorizontal(14) 96.480,510.417,199.728,519.417 'negative impact of its use, 35\\n'>\n",
      "<LTTextBoxHorizontal(15) 296.220,510.417,318.027,519.417 'ment)\\n'>\n",
      "<LTTextBoxHorizontal(16) 84.240,499.617,237.987,508.617 'government regulations on use of personal\\n'>\n",
      "<LTTextBoxHorizontal(17) 271.740,499.617,420.078,508.617 'financial model risk management regula‐\\n'>\n",
      "<LTTextBoxHorizontal(18) 84.240,478.017,183.132,497.817 'data by businesses, 35\\nprocess governance and, 38\\n'>\n",
      "<LTTextBoxHorizontal(19) 72.000,456.417,126.108,476.217 'releases, 77\\nreproducibility\\n'>\n",
      "<LTTextBoxHorizontal(20) 84.240,413.217,211.869,454.617 'auditability and, 67\\nof experiments with ML models, 26\\nimportance as model property, 56\\nin MLOps versus academia, 66\\n'>\n",
      "<LTTextBoxHorizontal(21) 72.000,402.417,175.185,411.417 'resource demands of models\\n'>\n",
      "<LTTextBoxHorizontal(22) 84.240,370.017,211.869,400.617 'capped, 29\\ncomputing power, 45\\nscalability of compute resources for\\n'>\n",
      "<LTTextBoxHorizontal(23) 96.480,359.217,170.244,368.217 'deployed models, 30\\n'>\n",
      "<LTTextBoxHorizontal(24) 72.000,337.617,159.102,357.417 'resource monitoring, 79\\nresources\\n'>\n",
      "<LTTextBoxHorizontal(25) 72.000,294.417,231.219,335.817 'monitoring for ML models, 85\\noptimizing usage with elastic systems, 81\\nresponsibilities, establishing in MLOps, 119\\nResponsible AI, 112-116\\n'>\n",
      "<LTTextBoxHorizontal(26) 84.240,262.017,187.407,292.617 'businesses engaging with, 36\\nimpacts on modeling, 53\\nkey elements of, 113-116\\n'>\n",
      "<LTTextBoxHorizontal(27) 96.480,208.017,213.093,260.217 'bias, 114\\ndata, 113\\ngovernance, 116\\ninclusiveness, 115\\nmodel management at scale, 116\\n'>\n",
      "<LTTextBoxHorizontal(28) 84.240,164.817,238.374,206.217 'in marketing recommendation engine, 141\\nin ML model development, 26\\nMLOps for, 9\\nmodel explainability, 27\\n'>\n",
      "<LTTextBoxHorizontal(29) 72.000,154.017,240.948,163.017 'results of in-depth analysis of models, compar‐\\n'>\n",
      "<LTTextBoxHorizontal(30) 84.240,143.217,108.522,152.217 'ing, 57\\n'>\n",
      "<LTTextBoxHorizontal(31) 72.000,132.417,136.242,141.417 'retraining models\\n'>\n",
      "<LTTextBoxHorizontal(32) 84.240,110.817,231.048,130.617 'deciding how often to retrain, 86-89\\nmarketing recommendation engine, 140,\\n'>\n",
      "<LTTextBoxHorizontal(33) 72.000,89.217,127.440,109.017 '142\\nreweighting, 93\\n'>\n",
      "<LTTextBoxHorizontal(34) 84.240,78.417,175.779,87.417 'for biased samples, 91, 93\\n'>\n",
      "<LTTextBoxHorizontal(35) 283.980,488.817,315.705,497.817 'tion, 109\\n'>\n",
      "<LTTextBoxHorizontal(36) 271.740,467.217,418.602,487.017 'matching governance with risk level, 107\\nmodel risk evaluation, 63-64\\n'>\n",
      "<LTTextBoxHorizontal(37) 283.980,445.617,398.190,465.417 'origins of ML model risk, 64\\npurpose of model validation, 63\\n'>\n",
      "<LTTextBoxHorizontal(38) 271.740,424.017,385.842,443.817 'model risk manager/auditor, 21\\nmodel risk mitigation, 69-72\\n'>\n",
      "<LTTextBoxHorizontal(39) 283.980,391.617,399.603,422.217 'changing environments, 70\\ninteractions between models, 70\\nmodel misbehavior, 71\\n'>\n",
      "<LTTextBoxHorizontal(40) 259.500,326.817,425.856,389.817 'need for proactively addressing in ML, 106\\nrisk assessment, 8\\nrisk mitigation, MLOps for, 9\\nRMSE (root-mean-square error), 143\\nrollbacks to previous model versions, 79\\nruntime environments, 60-63\\n'>\n",
      "<LTTextBoxHorizontal(41) 271.740,316.017,431.103,325.017 'adaptation from development to production\\n'>\n",
      "<LTTextBoxHorizontal(42) 283.980,283.617,394.005,314.217 'environments, 60-62\\nperformane considerations, 61\\ntooling considerations, 61\\n'>\n",
      "<LTTextBoxHorizontal(43) 271.740,272.817,425.379,281.817 'data access before validation and launch to\\n'>\n",
      "<LTTextBoxHorizontal(44) 271.740,251.217,346.116,271.017 'production, 62\\nfinal thoughts on, 62\\n'>\n",
      "<LTTextBoxHorizontal(45) 259.498,215.117,292.497,239.450 'S\\nsampling\\n'>\n",
      "<LTTextBoxHorizontal(46) 271.743,193.517,332.466,213.317 'random, 91\\nselection bias, 93\\n'>\n",
      "<LTTextBoxHorizontal(47) 259.503,182.717,427.659,191.717 'SCADA (supervisory control and data acquisi‐\\n'>\n",
      "<LTTextBoxHorizontal(48) 271.743,171.917,333.303,180.917 'tion) system, 149\\n'>\n",
      "<LTTextBoxHorizontal(49) 259.503,161.117,276.666,170.117 'scale\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.743,139.517,419.793,159.317 'MLOps for, 10\\nmodel management at scale (Responsible\\n'>\n",
      "<LTTextBoxHorizontal(51) 283.983,128.717,313.440,137.717 'AI), 116\\n'>\n",
      "<LTTextBoxHorizontal(52) 271.743,117.917,399.372,126.917 'scalability of compute resources for\\n'>\n",
      "<LTTextBoxHorizontal(53) 283.983,107.117,357.747,116.117 'deployed models, 30\\n'>\n",
      "<LTTextBoxHorizontal(54) 271.743,96.317,420.459,105.317 'scalability of marketing recommendation\\n'>\n",
      "<LTTextBoxHorizontal(55) 271.743,74.717,369.537,94.517 'engine model, 140\\nscaling deployments, 81-83\\n'>\n",
      "<LTTextBoxHorizontal(56) 384.456,40.500,402.519,49.500 'Index \\n'>\n",
      "<LTTextBoxHorizontal(57) 410.097,40.500,413.409,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(58) 420.987,40.500,432.003,49.500 '167\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 96.480,575.217,215.658,605.817 'requirements and challenges, 83\\nscalable and elastic systems, 81\\nscaling the number of models, 82\\n'>\n",
      "<LTTextBoxHorizontal(1) 84.240,564.417,197.163,573.417 'scaling inference on models, 62\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.000,553.617,121.077,562.617 'scikit-learn, 9\\n'>\n",
      "<LTTextBoxHorizontal(3) 259.502,586.017,383.315,605.817 'temporal aggegrations of data, 153\\ntesting\\n'>\n",
      "<LTTextBoxHorizontal(4) 271.742,553.617,428.882,584.217 'key testing considerations, 65\\nfor models using self-training algorithm, 88\\ntesting pipeline for ML artifacts, 75\\n'>\n",
      "<LTTextBoxHorizontal(5) 84.240,542.817,237.060,551.817 'models developed with, adpatation to pro‐\\n'>\n",
      "<LTTextBoxHorizontal(6) 259.502,542.817,277.007,551.817 'tools\\n'>\n",
      "<LTTextBoxHorizontal(7) 96.480,532.017,186.822,541.017 'ducton environments, 61\\n'>\n",
      "<LTTextBoxHorizontal(8) 72.000,521.217,199.341,530.217 'security in machine learning, 67-69\\n'>\n",
      "<LTTextBoxHorizontal(9) 84.240,499.617,169.542,519.417 'adversarial attacks, 68\\nother vulnerabilities, 68\\n'>\n",
      "<LTTextBoxHorizontal(10) 72.000,467.217,186.075,497.817 'selection bias, 93\\nsettings, reproducibility and, 57\\nshadow testing, 99\\n'>\n",
      "<LTTextBoxHorizontal(11) 84.240,445.617,233.109,465.417 '(see also champion/challenger)\\nof new model version deployed alongside\\n'>\n",
      "<LTTextBoxHorizontal(12) 96.480,434.817,162.153,443.817 'existing model, 33\\n'>\n",
      "<LTTextBoxHorizontal(13) 72.000,391.617,193.194,433.017 'Shapley values, 27, 53, 130\\nsharding, 78\\nSMEs (see subject matter experts)\\nsoftware engineers, 20\\n'>\n",
      "<LTTextBoxHorizontal(14) 84.240,370.017,243.396,389.817 'role in and needs from MLOps, 20\\nrole in machine learning model life cycle, 20\\n'>\n",
      "<LTTextBoxHorizontal(15) 72.000,337.617,242.604,368.217 'Spark, 82\\nspatial aggregations of data, 153\\nspatial and temporal resolution, forecast uncer‐\\n'>\n",
      "<LTTextBoxHorizontal(16) 84.240,326.817,137.700,335.817 'tainty and, 151\\n'>\n",
      "<LTTextBoxHorizontal(17) 72.000,316.017,102.555,325.017 'statistics\\n'>\n",
      "<LTTextBoxHorizontal(18) 84.240,294.417,232.452,314.217 'from null hypothesis to p-values, 90\\nstatistical metrics for model performance\\n'>\n",
      "<LTTextBoxHorizontal(19) 96.480,283.617,150.732,292.617 'monitoring, 89\\n'>\n",
      "<LTTextBoxHorizontal(20) 84.240,262.017,230.085,281.817 'statistical results on model testing, 65\\nstatistically driven decision-making pro‐\\n'>\n",
      "<LTTextBoxHorizontal(21) 96.480,251.217,159.183,260.217 'cesses in ML, 105\\n'>\n",
      "<LTTextBoxHorizontal(22) 84.240,240.417,186.795,249.417 'univariate statistical tests, 93\\n'>\n",
      "<LTTextBoxHorizontal(23) 72.000,197.217,243.396,238.617 'stochasticity, 155\\nsubject matter experts (SMEs), 15-17\\nrole in and needs from MLOps, 16\\nrole in machine learning model life cycle, 15\\n'>\n",
      "<LTTextBoxHorizontal(24) 72.000,186.417,127.485,195.417 'subpopulations\\n'>\n",
      "<LTTextBoxHorizontal(25) 84.240,132.417,236.808,184.617 'analyses of, 27\\nanalysis and model fairness, 66\\ninvesting model performance for in con‐\\nsumer credit risk management, 131\\nstatistical tests on results in preproduction\\n'>\n",
      "<LTTextBoxHorizontal(26) 72.000,110.817,158.238,130.617 'model testing, 66\\nsupervised learning, 44\\n'>\n",
      "<LTTextBoxHorizontal(27) 71.994,74.717,209.648,99.050 'T\\ntechnical debt in machine learning, 49\\n'>\n",
      "<LTTextBoxHorizontal(28) 71.999,40.500,84.437,49.500 '168 \\n'>\n",
      "<LTTextBoxHorizontal(29) 92.015,40.500,95.327,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(30) 102.905,40.500,119.546,49.500 'Index\\n'>\n",
      "<LTTextBoxHorizontal(31) 271.742,510.417,427.667,541.017 'considerations in adaptation from develop‐\\nment to production environments, 61\\nselecting for centralized governance man‐\\n'>\n",
      "<LTTextBoxHorizontal(32) 283.982,499.617,331.718,508.617 'agement, 122\\n'>\n",
      "<LTTextBoxHorizontal(33) 259.502,478.017,416.570,497.817 'traceability in pharmaceutical industry, 109\\ntraining data, 23, 44\\n'>\n",
      "<LTTextBoxHorizontal(34) 271.742,424.017,430.070,476.217 'adversarial attacks on, 68\\ndata governance concerns, 36\\nkeeping up to date for deployed models, 30\\nnon-stationary environment, 93\\nquality of, determining model performance,\\n'>\n",
      "<LTTextBoxHorizontal(35) 283.982,413.217,292.622,422.217 '86\\n'>\n",
      "<LTTextBoxHorizontal(36) 259.502,402.417,329.315,411.417 'training models, 26\\n'>\n",
      "<LTTextBoxHorizontal(37) 271.742,370.017,429.926,400.617 'automation in, 50\\ndeciding how often to retrain models, 86-89\\nmarketing recommendation engine model,\\n'>\n",
      "<LTTextBoxHorizontal(38) 283.982,359.217,313.106,368.217 '138-141\\n'>\n",
      "<LTTextBoxHorizontal(39) 271.742,348.417,424.310,357.417 'retraining existing model with latest train‐\\n'>\n",
      "<LTTextBoxHorizontal(40) 283.982,337.617,325.535,346.617 'ing data, 32\\n'>\n",
      "<LTTextBoxHorizontal(41) 259.502,316.017,332.231,335.817 'transfer learning, 48\\ntransparency\\n'>\n",
      "<LTTextBoxHorizontal(42) 271.742,283.617,428.594,314.217 \"data scientists' need for from MLOps, 18\\nsubject matter experts, role in MLOps, 17\\ntransparent strategies for machine learning,\\n\">\n",
      "<LTTextBoxHorizontal(43) 283.982,272.817,292.622,281.817 '11\\n'>\n",
      "<LTTextBoxHorizontal(44) 259.502,251.217,350.627,271.017 'tree-based algorithms, 45\\ntrust\\n'>\n",
      "<LTTextBoxHorizontal(45) 271.742,240.417,430.097,249.417 \"EU's requirements for trustworthy AI appli‐\\n\">\n",
      "<LTTextBoxHorizontal(46) 283.982,229.617,326.597,238.617 'cations, 111\\n'>\n",
      "<LTTextBoxHorizontal(47) 271.742,218.817,431.906,227.817 'importance to consumers and businesses, 36\\n'>\n",
      "<LTTextBoxHorizontal(48) 259.502,186.417,379.463,217.017 'Tweedie distribution, 130\\nTwitter chatbot by Microsoft, 68\\ntwo-stage models with offset, 130\\n'>\n",
      "<LTTextBoxHorizontal(49) 259.497,128.717,431.372,174.650 'U\\nU.S. Food and Drug Administration (FDA), 109\\nUCI Wine Quality dataset, 91\\nUK Prudential Regulation Authority’s (PRA)\\n'>\n",
      "<LTTextBoxHorizontal(50) 271.739,117.917,325.676,126.917 'regulation, 109\\n'>\n",
      "<LTTextBoxHorizontal(51) 259.499,85.517,362.054,116.117 'underfitting, 50\\nunivariate statistical tests, 93\\nupdating models, 143\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 2.700,-9.000,2.700,54.000>\n",
      "<LTLine 2.700,52.000,2.700,609.500>\n",
      "<LTLine 2.700,607.500,2.700,670.500>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 72.000,583.517,108.012,607.850 'V\\nvalidation\\n'>\n",
      "<LTTextBoxHorizontal(1) 84.243,561.917,229.638,581.717 'data access before model validation, 62\\nmodel for consumer credit risk manage‐\\n'>\n",
      "<LTTextBoxHorizontal(2) 96.483,551.117,132.231,560.117 'ment, 131\\n'>\n",
      "<LTTextBoxHorizontal(3) 84.243,529.517,224.274,549.317 'purpose of model validation, 63\\ntesting on recent production data from\\n'>\n",
      "<LTTextBoxHorizontal(4) 72.003,486.317,198.255,527.717 'models, 76\\nvariance in models, 50\\nversion control systems, central, 75\\nversion management, 79\\n'>\n",
      "<LTTextBoxHorizontal(5) 259.503,596.817,421.638,605.817 'version management and reproducibility, 26,\\n'>\n",
      "<LTTextBoxHorizontal(6) 271.743,586.017,292.227,595.017 '56-58\\n'>\n",
      "<LTTextBoxHorizontal(7) 259.503,575.217,424.770,584.217 'VMs (virtual machines), containers versus, 80\\n'>\n",
      "<LTTextBoxHorizontal(8) 259.500,528.317,351.094,563.450 'W\\nWasserstein distance, 132\\nwhat-if analysis, 27\\n'>\n",
      "<LTTextBoxHorizontal(9) 259.496,492.217,347.041,516.550 'X\\nXGBoost algorithm, 130\\n'>\n",
      "<LTTextBoxHorizontal(10) 384.451,40.500,402.514,49.500 'Index \\n'>\n",
      "<LTTextBoxHorizontal(11) 410.092,40.500,413.404,49.500 '| \\n'>\n",
      "<LTTextBoxHorizontal(12) 420.982,40.500,431.998,49.500 '169\\n'>\n",
      "<LTLine 72.000,54.125,432.000,54.125>\n",
      "<LTLine 501.300,-9.000,501.300,54.000>\n",
      "<LTLine 501.300,52.000,501.300,609.500>\n",
      "<LTLine 501.300,607.500,501.300,670.500>\n",
      "<LTTextBoxHorizontal(0) 72.000,582.144,166.720,597.894 'About the Authors\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.995,524.024,432.004,572.324 'Mark Treveil has designed products in fields as diverse as telecommunications, bank‐\\ning, and online trading. His own startup led a revolution in governance in UK local\\ngovernment,  where  it  still  dominates.  He  is  now  part  of  the  Dataiku  Product  Team\\nbased in Paris.\\n'>\n",
      "<LTTextBoxHorizontal(2) 71.995,455.024,432.003,515.924 'Nicolas Omont is VP of operations at Artelys, where he is developing mathematical\\noptimization solutions for energy and transport. He previously held the role of Data‐\\niku  product  manager  for  ML  and  advanced  analytics.  He  holds  a  PhD  in  computer\\nscience,  and  he’s  been  working  in  operations  research  and  statistics  for  the  past  15\\nyears, mainly in the telecommunications and energy utility sectors.\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,373.424,432.005,446.924 'Clément  Stenac  is  a  passionate  software  engineer,  CTO,  and  cofounder  at  Dataiku.\\nHe oversees the design and development of the Dataiku DSS Enterprise AI Platform.\\nClément was previously head of product development at Exalead, leading the design\\nand implementation of web-scale search engine software. He also has extensive expe‐\\nrience with open source software, as a former developer of the VideoLAN (VLC) and\\nDebian projects.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,317.024,432.005,365.324 'Kenji Lefèvre is VP of product at Dataiku. He oversees the product road map and the\\nuser experience of the Dataiku DSS Enterprise AI Platform. He holds a PhD in pure\\nmathematics  from  University  of  Paris  VII,  and  he  directed  documentary  movies\\nbefore switching to data science and product management.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,260.624,432.002,308.924 'Du Phan is a machine learning engineer at Dataiku, where he works in democratiz‐\\ning data science. In the past few years, he has been dealing with a variety of data prob‐\\nlems,  from  geospatial  analysis  to  deep  learning.  His  work  now  focuses  on  different\\nfacets and challenges of MLOps.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.996,166.424,432.005,252.524 'Joachim Zentici is an engineering director at Dataiku. Joachim graduated in applied\\nmathematics  from  Ecole  Centrale  Paris.  Prior  to  joining  Dataiku  in  2014,  he  was  a\\nresearch  engineer  in  computer  vision  at  Siemens  Molecular  Imaging  and  Inria.  He\\nhas also been a teacher and a lecturer. At Dataiku, Joachim has made multiple contri‐\\nbutions including managing the engineers in charge of the core infrastructure, build‐\\ning the team for the plug-ins and ecosystem efforts, and leading the global technology\\ntraining program for customer-facing engineers.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.996,110.024,432.004,158.324 'Adrien  Lavoillotte  is  an  engineering  director  at  Dataiku  where  he  leads  the  team\\nresponsible for machine learning and statistics features in the software. He studied at\\nECE Paris, a graduate school of engineering, and worked for several startups before\\njoining Dataiku in 2015.\\n'>\n",
      "<LTLine 72.000,579.413,432.000,579.413>\n",
      "<LTTextBoxHorizontal(0) 71.997,544.636,432.005,605.537 'Makoto Miyazaki is a data scientist at Dataiku and responsible for delivering hands-\\non consulting services using Dataiku DSS for European and Japanese clients. Makoto\\nholds a bachelor’s degree in economics and a master’s degree in data science, and he\\nwas  also  a  former  financial  journalist  with  a  wide  range  of  beats,  including  nuclear\\nenergy and economic recoveries from the tsunami.\\n'>\n",
      "<LTTextBoxHorizontal(1) 71.996,450.436,432.005,536.536 'Lynn Heidmann received her bachelor’s degree in journalism/mass communications\\nand anthropology from the University of Wisconsin-Madison in 2008 and decided to\\nbring  her  passion  for  research  and  writing  into  the  world  of  tech.  She  spent  seven\\nyears in the San Francisco Bay Area writing and running operations with Google and\\nsubsequently Niantic before moving to Paris to head content initiatives at Dataiku. In\\nher  current  role,  Lynn  follows  and  writes  about  technological  trends  and  develop‐\\nments in the world of data and AI.\\n'>\n",
      "<LTTextBoxHorizontal(2) 72.002,424.944,121.110,440.694 'Colophon\\n'>\n",
      "<LTTextBoxHorizontal(3) 71.996,366.824,432.002,415.124 'The animal on the cover of Introducing MLOps is an African moth called Bunaeopsis\\noubie, also known as Zaddach’s Emperor, that can be found across central and eastern\\nAfrica,  from  Angola  to  Eritrea.  It  is  a  member  of  the  Saturniidae  family,  which\\nincludes one thousand species of the world’s largest moths.\\n'>\n",
      "<LTTextBoxHorizontal(4) 71.997,285.224,432.005,358.724 'This African moth has one of the largest wingspans, stretching up to 10 inches, mak‐\\ning it bigger than some birds. Its wings have distinctive markings: one reddish brown\\ncircle  on  each  of  the  four  wings,  dark  brown  stripes  underneath,  and  white  strokes\\nbordering  the  thorax  and  along  the  outer  edges  of  each  wing.  Moth  antennae  are\\nthick  and  feathered.  Their  entire  bodies  repel  water  with  a  wax  coating  that  covers\\ntheir hairs and the scales on their wings.\\n'>\n",
      "<LTTextBoxHorizontal(5) 71.996,228.824,432.003,277.124 'Moths  tend  to  be  attracted  to  white,  fragrant  flowers,  which  they  sniff  out  easily  at\\nnight  and  pollinate  well  with  their  fuzzy,  sticky  bodies.  Many  animals  and  birds\\ndepend on moths in their diets, including owls and bats. Moth caterpillars are prey to\\nlizards, birds, and many small mammals.\\n'>\n",
      "<LTTextBoxHorizontal(6) 71.998,197.624,432.001,220.724 'Many of the animals on O’Reilly’s covers are endangered; all of them are important to\\nthe world.\\n'>\n",
      "<LTTextBoxHorizontal(7) 71.998,141.224,432.005,189.524 'The cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom  Encyclopedie  D’Histoire  Naturelle.  The  cover  fonts  are  Gilroy  Semibold  and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\\n'>\n",
      "<LTLine 72.000,422.212,432.000,422.212>\n"
     ]
    }
   ],
   "source": [
    "for page_layout in extract_pages(\"D:\\Learnbay\\Introduction to ML ops.pdf\"):\n",
    "    for element in page_layout:\n",
    "        print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604e31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "Mastering MLOps \n",
      "with Dataiku\n",
      "\n",
      "Dataiku is the only platform that provides one simple, consistent UI \n",
      "for data connection, wrangling, mining, visualization, machine \n",
      "learning, deployment, and model monitoring, all at enterprise scale.\n",
      "\n",
      "KEY FEATURES FOR A SCALABLE MLOPS STRATEGY INCLUDE:\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "Model input drift detection that looks at the recent data the model has had to score and statistically \n",
      "compares it with the data on which the model was evaluated.\n",
      "\n",
      "Easier creation of validation feedback loops via Dataiku Evaluation Recipes to compute the true \n",
      "performance of a saved model against a new validation dataset, plus automated retraining and \n",
      "redeployment.\n",
      "\n",
      " Everyday AI, \n",
      "\n",
      "3\n",
      "\n",
      "Dashboard interfaces dedicated to the monitoring of global pipelines.\n",
      "\n",
      "4\n",
      "\n",
      "...and more! See MLOps features with Dataiku in action at dataiku.com\n",
      "\n",
      " Extraordinary People \n",
      "\n",
      "Elastic Architecture Built for the Cloud\n",
      "\n",
      "Machine Learning\n",
      "\n",
      "Visualization\n",
      "\n",
      "Data Preparation\n",
      "\n",
      "Age\n",
      "\n",
      "Integer\n",
      "\n",
      "22\n",
      "\n",
      "38\n",
      "\n",
      "26\n",
      "\n",
      "35\n",
      "\n",
      "35\n",
      "\n",
      "29\n",
      "\n",
      "Name\n",
      "\n",
      "Natural lang.\n",
      "\n",
      "Braund, Mr. Owen Harris\n",
      "\n",
      "Moran, Mr. James\n",
      "\n",
      "Heikkinen, Miss. Laina\n",
      "\n",
      "Remove rows containing Mr.\n",
      "\n",
      "Sex\n",
      "\n",
      "Gender\n",
      "\n",
      "male\n",
      "\n",
      "male\n",
      "\n",
      "female\n",
      "\n",
      "Futrelle, Mrs. Jacques Heath\n",
      "\n",
      "Keep only rows containing Mr.\n",
      "\n",
      "female\n",
      "\n",
      "Allen, Mr. William Henry\n",
      "\n",
      "McCarthy, Mr. Robert\n",
      "\n",
      "Split column on Mr.\n",
      "\n",
      "Hewlett, Mrs (Mary D Kingcome)\n",
      "\n",
      "Replace Mr. by ...\n",
      "\n",
      "male\n",
      "\n",
      "male\n",
      "\n",
      "Remove rows equal to Moran, Mr. James\n",
      "\n",
      "Keep only rows equal to Moran, Mr. James\n",
      "\n",
      "Clear cells equal to Moran, Mr. James\n",
      "\n",
      "Filter on Moran, Mr. James\n",
      "\n",
      "Filter on Mr.\n",
      "\n",
      "Toggle row highlight\n",
      "\n",
      "Show complete value\n",
      "\n",
      "DataOps\n",
      "\n",
      "Governance & MLOps\n",
      "\n",
      "Applications\n",
      "\n",
      "450+\n",
      "\n",
      "CUSTOMERS\n",
      "\n",
      "45,000+\n",
      "\n",
      "ACTIVE USERS\n",
      "\n",
      "©Dataiku2021 | dataiku.com\n",
      "\n",
      "Dataiku is the world’s leading platform for Everyday AI, systemizing the use of data for \n",
      "\n",
      "exceptional business results. Organizations that use Dataiku elevate their people (whether \n",
      "\n",
      "technical and working in code or on the business side and low- or no-code) to extraordinary, \n",
      "\n",
      "arming them with the ability to make better day-to-day decisions with data.\n",
      "\n",
      "©2021 dataiku | dataiku.com\n",
      "\n",
      "\f",
      "Introducing MLOps\n",
      "How to Scale Machine Learning in the Enterprise\n",
      "\n",
      "Mark Treveil and the Dataiku Team\n",
      "\n",
      "Beijing\n",
      "Beijing\n",
      "\n",
      "Boston\n",
      "Boston\n",
      "\n",
      "Farnham Sebastopol\n",
      "Farnham Sebastopol\n",
      "\n",
      "Tokyo\n",
      "Tokyo\n",
      "\n",
      "\f",
      "Introducing MLOps\n",
      "by Mark Treveil, and the Dataiku Team\n",
      "\n",
      "Copyright © 2020 Dataiku. All rights reserved.\n",
      "\n",
      "Printed in the United States of America.\n",
      "\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "\n",
      "Indexer: Ellen Troutman-Zaig\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Kate Dullea\n",
      "\n",
      "Acquisitions Editor: Rebecca Novack\n",
      "Development Editor: Angela Rufino\n",
      "Production Editor: Katherine Tozer\n",
      "Copyeditor: Penelope Perkins\n",
      "Proofreader: Kim Wimpsett\n",
      "\n",
      "December 2020:\n",
      "\n",
      " First Edition\n",
      "\n",
      "Revision History for the First Edition\n",
      "2020-11-30:  First Release\n",
      "\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492083290 for release details.\n",
      "\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introducing MLOps, the cover image,\n",
      "and related trade dress are trademarks of O’Reilly Media, Inc.\n",
      "\n",
      "The  views  expressed  in  this  work  are  those  of  the  authors,  and  do  not  represent  the  publisher’s  views.\n",
      "While  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and\n",
      "instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\n",
      "licenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\n",
      "thereof complies with such licenses and/or rights.\n",
      "\n",
      "This  work  is  part  of  a  collaboration  between  O’Reilly  and  Dataiku.  See  our  statement  of  editorial\n",
      "independence.\n",
      "\n",
      "978-1-492-08330-6\n",
      "\n",
      "[LSI]\n",
      "\n",
      "\f",
      "Table of Contents\n",
      "\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   ix\n",
      "\n",
      "Part I.  MLOps: What and Why\n",
      "\n",
      "1. Why Now and Challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\n",
      "Defining MLOps and Its Challenges                                                                             4\n",
      "MLOps to Mitigate Risk                                                                                                  7\n",
      "Risk Assessment                                                                                                            8\n",
      "Risk Mitigation                                                                                                              9\n",
      "MLOps for Responsible AI                                                                                          9\n",
      "MLOps for Scale                                                                                                             10\n",
      "Closing Thoughts                                                                                                           11\n",
      "\n",
      "2. People of MLOps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13\n",
      "Subject Matter Experts                                                                                                   15\n",
      "Data Scientists                                                                                                                 17\n",
      "Data Engineers                                                                                                                19\n",
      "Software Engineers                                                                                                         20\n",
      "DevOps                                                                                                                            20\n",
      "Model Risk Manager/Auditor                                                                                       21\n",
      "Machine Learning Architect                                                                                         21\n",
      "Closing Thoughts                                                                                                           22\n",
      "\n",
      "3. Key MLOps Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   23\n",
      "A Primer on Machine Learning                                                                                   23\n",
      "Model Development                                                                                                       24\n",
      "Establishing Business Objectives                                                                              24\n",
      "\n",
      "iii\n",
      "\n",
      "\f",
      "Data Sources and Exploratory Data Analysis                                                         24\n",
      "Feature Engineering and Selection                                                                           25\n",
      "Training and Evaluation                                                                                            26\n",
      "Reproducibility                                                                                                            26\n",
      "Responsible AI                                                                                                            26\n",
      "Productionalization and Deployment                                                                         27\n",
      "Model Deployment Types and Contents                                                                 28\n",
      "Model Deployment Requirements                                                                           29\n",
      "Monitoring                                                                                                                      29\n",
      "DevOps Concerns                                                                                                       30\n",
      "Data Scientist Concerns                                                                                             30\n",
      "Business Concerns                                                                                                      31\n",
      "Iteration and Life Cycle                                                                                                 32\n",
      "Iteration                                                                                                                        32\n",
      "The Feedback Loop                                                                                                    33\n",
      "Governance                                                                                                                     34\n",
      "Data Governance                                                                                                        36\n",
      "Process Governance                                                                                                   37\n",
      "Closing Thoughts                                                                                                           38\n",
      "\n",
      "Part II.  MLOps: How\n",
      "\n",
      "4. Developing Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41\n",
      "What Is a Machine Learning Model?                                                                           42\n",
      "In Theory                                                                                                                     42\n",
      "In Practice                                                                                                                    43\n",
      "Required Components                                                                                               44\n",
      "Different ML Algorithms, Different MLOps Challenges                                      45\n",
      "Data Exploration                                                                                                            46\n",
      "Feature Engineering and Selection                                                                              47\n",
      "Feature Engineering Techniques                                                                              47\n",
      "How Feature Selection Impacts MLOps Strategy                                                  48\n",
      "Experimentation                                                                                                             49\n",
      "Evaluating and Comparing Models                                                                             51\n",
      "Choosing Evaluation Metrics                                                                                    51\n",
      "Cross-Checking Model Behavior                                                                             53\n",
      "Impact of Responsible AI on Modeling                                                                   53\n",
      "Version Management and Reproducibility                                                                 56\n",
      "Closing Thoughts                                                                                                           58\n",
      "\n",
      "iv \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\f",
      "5. Preparing for Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   59\n",
      "Runtime Environments                                                                                                 60\n",
      "Adaptation from Development to Production Environments                             60\n",
      "Data Access Before Validation and Launch to Production                                   62\n",
      "Final Thoughts on Runtime Environments                                                            62\n",
      "Model Risk Evaluation                                                                                                   63\n",
      "The Purpose of Model Validation                                                                            63\n",
      "The Origins of ML Model Risk                                                                                 64\n",
      "Quality Assurance for Machine Learning                                                                   64\n",
      "Key Testing Considerations                                                                                          65\n",
      "Reproducibility and Auditability                                                                                 66\n",
      "Machine Learning Security                                                                                           67\n",
      "Adversarial Attacks                                                                                                     68\n",
      "Other Vulnerabilities                                                                                                  68\n",
      "Model Risk Mitigation                                                                                                   69\n",
      "Changing Environments                                                                                            70\n",
      "Interactions Between Models                                                                                    70\n",
      "Model Misbehavior                                                                                                     71\n",
      "Closing Thoughts                                                                                                           72\n",
      "\n",
      "6. Deploying to Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      " 73\n",
      "CI/CD Pipelines                                                                                                              73\n",
      "Building ML Artifacts                                                                                                    75\n",
      "What’s in an ML Artifact?                                                                                          75\n",
      "The Testing Pipeline                                                                                                   75\n",
      "Deployment Strategies                                                                                                   77\n",
      "Categories of Model Deployment                                                                             77\n",
      "Considerations When Sending Models to Production                                         78\n",
      "Maintenance in Production                                                                                      79\n",
      "Containerization                                                                                                             79\n",
      "Scaling Deployments                                                                                                     81\n",
      "Requirements and Challenges                                                                                      83\n",
      "Closing Thoughts                                                                                                           84\n",
      "\n",
      "7. Monitoring and Feedback Loop. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85\n",
      "How Often Should Models Be Retrained?                                                                  86\n",
      "Understanding Model Degradation                                                                             89\n",
      "Ground Truth Evaluation                                                                                          89\n",
      "Input Drift Detection                                                                                                 91\n",
      "Drift Detection in Practice                                                                                            92\n",
      "Example Causes of Data Drift                                                                                   93\n",
      "Input Drift Detection Techniques                                                                            93\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "v\n",
      "\n",
      "\f",
      "The Feedback Loop                                                                                                        95\n",
      "Logging                                                                                                                         96\n",
      "Model Evaluation                                                                                                        97\n",
      "Online Evaluation                                                                                                       99\n",
      "Closing Thoughts                                                                                                         103\n",
      "\n",
      "8. Model Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      " 105\n",
      "Who Decides What Governance the Organization Needs?                                   105\n",
      "Matching Governance with Risk Level                                                                     107\n",
      "Current Regulations Driving MLOps Governance                                                 108\n",
      "Pharmaceutical Regulation in the US: GxP                                                          109\n",
      "Financial Model Risk Management Regulation                                                   109\n",
      "GDPR and CCPA Data Privacy Regulations                                                        110\n",
      "The New Wave of AI-Specific Regulations                                                               111\n",
      "The Emergence of Responsible AI                                                                             112\n",
      "Key Elements of Responsible AI                                                                                113\n",
      "Element 1: Data                                                                                                         113\n",
      "Element 2: Bias                                                                                                          114\n",
      "Element 3: Inclusiveness                                                                                          115\n",
      "Element 4: Model Management at Scale                                                                116\n",
      "Element 5: Governance                                                                                            116\n",
      "A Template for MLOps Governance                                                                         117\n",
      "Step 1: Understand and Classify the Analytics Use Cases                                  118\n",
      "Step 2: Establish an Ethical Position                                                                      118\n",
      "Step 3: Establish Responsibilities                                                                            119\n",
      "Step 4: Determine Governance Policies                                                                120\n",
      "Step 5: Integrate Policies into the MLOps Process                                               121\n",
      "Step 6: Select the Tools for Centralized Governance Management                   122\n",
      "Step 7: Engage and Educate                                                                                     123\n",
      "Step 8: Monitor and Refine                                                                                     124\n",
      "Closing Thoughts                                                                                                         125\n",
      "\n",
      "Part III.  MLOps: Real-World Examples\n",
      "\n",
      "9. MLOps in Practice: Consumer Credit Risk Management. . . . . . . . . . . . . . . . . . . . . . . . .  129\n",
      "Background: The Business Use Case                                                                         129\n",
      "Model Development                                                                                                    130\n",
      "Model Bias Considerations                                                                                         131\n",
      "Prepare for Production                                                                                                131\n",
      "Deploy to Production                                                                                                  132\n",
      "Closing Thoughts                                                                                                         133\n",
      "\n",
      "vi \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\f",
      "10. MLOps in Practice: Marketing Recommendation Engines. . . . . . . . . . . . . . . . . . . . . . .   135\n",
      "The Rise of Recommendation Engines                                                                     135\n",
      "The Role of Machine Learning                                                                               136\n",
      "Push or Pull?                                                                                                              136\n",
      "Data Preparation                                                                                                          137\n",
      "Design and Manage Experiments                                                                              138\n",
      "Model Training and Deployment                                                                              138\n",
      "Scalability and Customizability                                                                              140\n",
      "Monitoring and Retraining Strategy                                                                      140\n",
      "Real-Time Scoring                                                                                                    140\n",
      "Ability to Turn Recommendations On and Off                                                   141\n",
      "Pipeline Structure and Deployment Strategy                                                           141\n",
      "Monitoring and Feedback                                                                                           142\n",
      "Retraining Models                                                                                                    142\n",
      "Updating Models                                                                                                      143\n",
      "Runs Overnight, Sleeps During Daytime                                                              143\n",
      "Option to Manually Control Models                                                                     143\n",
      "Option to Automatically Control Models                                                             144\n",
      "Monitoring Performance                                                                                         144\n",
      "Closing Thoughts                                                                                                         145\n",
      "\n",
      "11. MLOps in Practice: Consumption Forecast. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   147\n",
      "Power Systems                                                                                                              147\n",
      "Data Collection                                                                                                             149\n",
      "Problem Definition: Machine Learning, or Not Machine Learning?                   151\n",
      "Spatial and Temporal Resolution                                                                               151\n",
      "Implementation                                                                                                            153\n",
      "Modeling                                                                                                                        153\n",
      "Deployment                                                                                                                   155\n",
      "Monitoring                                                                                                                    156\n",
      "Closing Thoughts                                                                                                         157\n",
      "\n",
      "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   159\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "vii\n",
      "\n",
      "\f",
      "\f",
      "Preface\n",
      "\n",
      "We’ve reached a turning point in the story of machine learning where the technology\n",
      "has moved from the realm of theory and academics and into the “real world”—that is,\n",
      "businesses  providing  all  kinds  of  services  and  products  to  people  across  the  globe.\n",
      "While  this  shift  is  exciting,  it’s  also  challenging,  as  it  combines  the  complexities  of\n",
      "machine learning models with the complexities of the modern organization.\n",
      "\n",
      "One difficulty, as organizations move from experimenting with machine learning to\n",
      "scaling it in production environments, is maintenance. How can companies go from\n",
      "managing just one model to managing tens, hundreds, or even thousands? This is not\n",
      "only where MLOps comes into play, but it’s also where the aforementioned complexi‐\n",
      "ties, both on the technical and business sides, appear. This book will introduce read‐\n",
      "ers to the challenges at hand, while also offering practical insights and solutions for\n",
      "developing MLOps capabilities.\n",
      "\n",
      "Who This Book Is For\n",
      "We wrote this book specifically for analytics and IT operations team managers, that\n",
      "is, the people directly facing the task of scaling machine learning (ML) in production.\n",
      "Given  that  MLOps  is  a  new  field,  we  developed  this  book  as  a  guide  for  creating  a\n",
      "successful  MLOps  environment,  from  the  organizational  to  the  technical  challenges\n",
      "involved.\n",
      "\n",
      "How This Book Is Organized\n",
      "This  book  is  divided  into  three  parts.  The  first  is  an  introduction  to  the  topic  of\n",
      "MLOps, diving into how (and why) it has developed as a discipline, who needs to be\n",
      "involved to execute MLOps successfully, and what components are required.\n",
      "\n",
      "The second part roughly follows the machine learning model life cycle, with chapters\n",
      "on developing models, preparing for production, deploying to production, monitor‐\n",
      "ing,  and  governance.  These  chapters  cover  not  only  general  considerations,  but\n",
      "\n",
      "ix\n",
      "\n",
      "\f",
      "MLOps  considerations  at  each  stage  of  the  life  cycle,  providing  more  detail  on  the\n",
      "topics touched on in Chapter 3.\n",
      "\n",
      "The final part provides tangible examples of how MLOps looks in companies today,\n",
      "so  that  readers  can  understand  the  setup  and  implications  in  practice.  Though  the\n",
      "company names are fictitious, the stories are based on real-life companies’ experience\n",
      "with MLOps and model management at scale.\n",
      "\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "\n",
      "Italic\n",
      "\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "\n",
      "Constant width\n",
      "\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments  such  as  variable  or  function  names,  databases,  data  types,  environment\n",
      "variables, statements, and keywords.\n",
      "\n",
      "Constant width bold\n",
      "\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "\n",
      "Constant width italic\n",
      "\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context.\n",
      "\n",
      "O’Reilly Online Learning\n",
      "\n",
      "For more than 40 years, O’Reilly Media has provided technol‐\n",
      "ogy  and  business  training,  knowledge,  and  insight  to  help\n",
      "companies succeed.\n",
      "\n",
      "Our unique network of experts and innovators share their knowledge and expertise\n",
      "through books, articles, and our online learning platform. O’Reilly’s online learning\n",
      "platform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning\n",
      "paths, interactive coding environments, and a vast collection of text and video from\n",
      "O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n",
      "\n",
      "x \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\f",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. You can access this page at https://oreil.ly/intro-mlops.\n",
      "\n",
      "Email  bookquestions@oreilly.com  to  comment  or  ask  technical  questions  about  this\n",
      "book.\n",
      "\n",
      "For news and information about our books and courses, visit http://oreilly.com.\n",
      "\n",
      "Find us on Facebook: http://facebook.com/oreilly\n",
      "\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "\n",
      "Watch us on YouTube: http://www.youtube.com/oreillymedia\n",
      "\n",
      "Acknowledgments\n",
      "We would like to thank the entire Dataiku team for their support in developing this\n",
      "book,  from  conception  to  completion.  It’s  been  a  true  team  effort  and,  like  most\n",
      "things we do at Dataiku, rooted in fundamental collaboration between countless peo‐\n",
      "ple and teams.\n",
      "\n",
      "Thanks to those who supported our vision from the beginning of writing this book\n",
      "with  O’Reilly.  Thanks  to  those  who  stepped  in  to  help  with  writing  and  editing.\n",
      "Thanks  to  those  who  provided  honest  feedback  (even  when  it  meant  more  writing\n",
      "and rewriting and re-rewriting). Thanks to those who were internal cheerleaders and,\n",
      "of course, those who helped us promote the finished product to the world.\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xi\n",
      "\n",
      "\f",
      "\f",
      "PART I\n",
      "MLOps: What and Why\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 1\n",
      "Why Now and Challenges\n",
      "\n",
      "Machine  learning  operations  (MLOps)  is  quickly  becoming  a  critical  component  of\n",
      "successful data science project deployment in the enterprise (Figure 1-1). It’s a process\n",
      "that  helps  organizations  and  business  leaders  generate  long-term  value  and  reduce\n",
      "risk associated with data science, machine learning, and AI initiatives. Yet it’s a rela‐\n",
      "tively new concept; so why has it seemingly skyrocketed into the data science lexicon\n",
      "overnight?  This  introductory  chapter  delves  into  what  MLOps  is  at  a  high  level,  its\n",
      "challenges,  why  it  has  become  essential  to  a  successful  data  science  strategy  in  the\n",
      "enterprise, and, critically, why it is coming to the forefront now.\n",
      "\n",
      "MLOps Versus ModelOps Versus AIOps\n",
      "MLOps  (or  ModelOps)  is  a  relatively  new  discipline,  emerging  under  these  names\n",
      "particularly in late 2018 and 2019. The two—MLOps and ModelOps—are, at the time\n",
      "this book is being written, largely being used interchangeably. However, some argue\n",
      "that ModelOps is more general than MLOps, as it’s not only about machine learning\n",
      "models but any kind of model (e.g., rule-based models). For the purpose of this book,\n",
      "we’ll be specifically discussing the machine learning model life cycle and will thus use\n",
      "the term “MLOps.”\n",
      "\n",
      "AIOps, though sometimes confused with MLOps, is another topic entirely and refers\n",
      "to  the  process  of  solving  operational  challenges  through  the  use  of  artificial  intelli‐\n",
      "gence (i.e., AI for DevOps). An example would be a form of predictive maintenance\n",
      "for  network  failures,  alerting  DevOps  teams  to  possible  problems  before  they  arise.\n",
      "While important and interesting in its own right, AIOps is outside the scope of this\n",
      "book.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Figure 1-1. Representation of the exponential growth of MLOps (not the parallel growth\n",
      "of the term “ModelOps”)\n",
      "\n",
      "Defining MLOps and Its Challenges\n",
      "At its core, MLOps is the standardization and streamlining of machine learning life\n",
      "cycle management (Figure 1-2). But taking a step back, why does the machine learn‐\n",
      "ing life cycle need to be streamlined? On the surface, just looking at the steps to go\n",
      "from  business  problem  to  a  machine  learning  model  at  a  very  high  level,  it  seems\n",
      "straightforward.\n",
      "\n",
      "For  most  traditional  organizations,  the  development  of  multiple  machine  learning\n",
      "models and their deployment in a production environment are relatively new. Until\n",
      "recently, the number of models may have been manageable at a small scale, or there\n",
      "was  simply  less  interest  in  understanding  these  models  and  their  dependencies  at  a\n",
      "company-wide  level.  With  decision  automation  (that  is,  an  increasing  prevalence  of\n",
      "decision  making  that  happens  without  human  intervention),  models  become  more\n",
      "critical,  and,  in  parallel,  managing  model  risks  becomes  more  important  at  the  top\n",
      "level.\n",
      "\n",
      "The  reality  of  the  machine  learning  life  cycle  in  an  enterprise  setting  is  much  more\n",
      "complex, in terms of needs and tooling (Figure 1-3).\n",
      "\n",
      "4 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Why Now and Challenges\n",
      "\n",
      "\f",
      "Figure 1-2. A simple representation of the machine learning model life cycle, which often\n",
      "underplays the need for MLOps, compared to Figure 1-3\n",
      "\n",
      "There  are  three  key  reasons  that  managing  machine  learning  life  cycles  at  scale  is\n",
      "challenging:\n",
      "\n",
      "• There are many dependencies. Not only is data constantly changing, but business\n",
      "needs shift as well. Results need to be continually relayed back to the business to\n",
      "ensure that the reality of the model in production and on production data aligns\n",
      "with  expectations  and,  critically,  addresses  the  original  problem  or  meets  the\n",
      "original goal.\n",
      "\n",
      "• Not everyone speaks the same language. Even though the machine learning life\n",
      "cycle involves people from the business, data science, and IT teams, none of these\n",
      "groups  are  using  the  same  tools  or  even,  in  many  cases,  share  the  same  funda‐\n",
      "mental skills to serve as a baseline of communication.\n",
      "\n",
      "• Data scientists are not software engineers. Most are specialized in model building\n",
      "and  assessment,  and  they  are  not  necessarily  experts  in  writing  applications.\n",
      "Though this may start to shift over time as some data scientists become special‐\n",
      "ists  more  on  the  deployment  or  operational  side,  for  now  many  data  scientists\n",
      "find themselves having to juggle many roles, making it challenging to do any of\n",
      "them  thoroughly.  Data  scientists  being  stretched  too  thin  becomes  especially\n",
      "problematic at scale with increasingly more models to manage. The complexity\n",
      "becomes exponential when considering the turnover of staff on data teams and,\n",
      "suddenly, data scientists have to manage models they did not create.\n",
      "\n",
      "Defining MLOps and Its Challenges \n",
      "\n",
      "| \n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Figure 1-3. The realistic picture of a machine learning model life cycle inside an average\n",
      "organization today, which involves many different people with completely different skill\n",
      "sets and who are often using entirely different tools.\n",
      "\n",
      "If  the  definition  (or  even  the  name  MLOps)  sounds  familiar,  that’s  because  it  pulls\n",
      "heavily  from  the  concept  of  DevOps,  which  streamlines  the  practice  of  software\n",
      "changes and updates. Indeed, the two have quite a bit in common. For example, they\n",
      "both center around:\n",
      "\n",
      "• Robust automation and trust between teams\n",
      "\n",
      "• The idea of collaboration and increased communication between teams\n",
      "\n",
      "6 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Why Now and Challenges\n",
      "\n",
      "\f",
      "• The end-to-end service life cycle (build, test, release)\n",
      "\n",
      "• Prioritizing continuous delivery and high quality\n",
      "\n",
      "Yet there is one critical difference between MLOps and DevOps that makes the latter\n",
      "not immediately transferable to data science teams: deploying software code into pro‐\n",
      "duction is fundamentally different than deploying machine learning models into pro‐\n",
      "duction.  While  software  code  is  relatively  static  (“relatively”  because  many  modern\n",
      "software-as-a-service [SaaS] companies do have DevOps teams that can iterate quite\n",
      "quickly  and  deploy  in  production  multiple  times  per  day),  data  is  always  changing,\n",
      "which means machine learning models are constantly learning and adapting—or not,\n",
      "as the case may be—to new inputs. The complexity of this environment, including the\n",
      "fact that machine learning models are made up of both code and data, is what makes\n",
      "MLOps a new and unique discipline.\n",
      "\n",
      "What About DataOps?\n",
      "To  add  to  the  complexity  of  MLOps  versus  DevOps,  there  is  also  DataOps,  a  term\n",
      "introduced  in  2014  by  IBM.  DataOps  seeks  to  provide  business-ready  data  that  is\n",
      "quickly  available  for  use,  with  a  large  focus  on  data  quality  and  metadata  manage‐\n",
      "ment. For example, if there’s a sudden change in data that a model relies on, a Data‐\n",
      "Ops  system  would  alert  the  business  team  to  deal  more  carefully  with  the  latest\n",
      "insights,  and  the  data  team  would  be  notified  to  investigate  the  change  or  revert  a\n",
      "library upgrade and rebuild the related partition.\n",
      "\n",
      "The rise of MLOps, therefore, intersects with DataOps at some level, though MLOps\n",
      "goes a step further and brings even more robustness through additional key features\n",
      "(discussed in more detail in Chapter 3).\n",
      "\n",
      "As was the case with DevOps and later DataOps, until recently teams have been able\n",
      "to get by without defined and centralized processes mostly because—at an enterprise\n",
      "level—they  weren’t  deploying  machine  learning  models  into  production  at  a  large\n",
      "enough scale. Now, the tables are turning and teams are increasingly looking for ways\n",
      "to  formalize  a  multi-stage,  multi-discipline,  multi-phase  process  with  a  heterogene‐\n",
      "ous environment and a framework for MLOps best practices, which is no small task.\n",
      "Part II of this book, “MLOps: How,” will provide this guidance.\n",
      "\n",
      "MLOps to Mitigate Risk\n",
      "MLOps  is  important  to  any  team  that  has  even  one  model  in  production  because,\n",
      "depending on the model, continuous performance monitoring and adjusting is essen‐\n",
      "tial.  By  allowing  safe  and  reliable  operations,  MLOps  is  key  in  mitigating  the  risks\n",
      "\n",
      "MLOps to Mitigate Risk \n",
      "\n",
      "| \n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "induced by the use of ML models. However, MLOps practices do come at a cost, so a\n",
      "proper cost-benefit evaluation should be performed for each use case.\n",
      "\n",
      "Risk Assessment\n",
      "When  it  comes  to  machine  learning  models,  risks  vary  widely.  For  example,  the\n",
      "stakes  are  much  lower  for  a  recommendation  engine  used  once  a  month  to  decide\n",
      "which  marketing  offer  to  send  a  customer  than  for  a  travel  site  whose  pricing  and\n",
      "revenue depend on a machine learning model. Therefore, when looking at MLOps as\n",
      "a way to mitigate risk, an analysis should cover:\n",
      "\n",
      "• The risk that the model is unavailable for a given period of time\n",
      "\n",
      "• The risk that the model returns a bad prediction for a given sample\n",
      "\n",
      "• The risk that the model accuracy or fairness decreases over time\n",
      "\n",
      "• The risk that the skills necessary to maintain the model (i.e., data science talent)\n",
      "\n",
      "are lost\n",
      "\n",
      "Risks are usually larger for models that are deployed widely and used outside of the\n",
      "organization. As shown in Figure 1-4, risk assessment is generally based on two met‐\n",
      "rics:  the  probability  and  the  impact  of  the  adverse  event.  Mitigation  measures  are\n",
      "generally  based  on  the  combination  of  the  two,  i.e.,  the  model’s  severity.  Risk\n",
      "assessment should be performed at the beginning of each project and reassessed peri‐\n",
      "odically, as models may be used in ways that were not foreseen initially.\n",
      "\n",
      "Figure 1-4. A table that helps decision makers with quantitative risk analysis\n",
      "\n",
      "8 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Why Now and Challenges\n",
      "\n",
      "\f",
      "Risk Mitigation\n",
      "MLOps  really  tips  the  scales  as  critical  for  risk  mitigation  when  a  centralized  team\n",
      "(with unique reporting of its activities, meaning that there can be multiple such teams\n",
      "at any given enterprise) has more than a handful of operational models. At this point,\n",
      "it  becomes  difficult  to  have  a  global  view  of  the  states  of  these  models  without  the\n",
      "standardization that allows the appropriate mitigation measures to be taken for each\n",
      "of them (see “Matching Governance with Risk Level” on page 107).\n",
      "\n",
      "Pushing machine learning models into production without MLOps infrastructure is\n",
      "risky  for  many  reasons,  but  first  and  foremost  because  fully  assessing  the  perfor‐\n",
      "mance of a machine learning model can often only be done in the production envi‐\n",
      "ronment.  Why?  Because  prediction  models  are  only  as  good  as  the  data  they  are\n",
      "trained  on,  which  means  the  training  data  must  be  a  good  reflection  of  the  data\n",
      "encountered in the production environment. If the production environment changes,\n",
      "then the model performance is likely to decrease rapidly (see Chapter 5 for details).\n",
      "\n",
      "Another major risk factor is that machine learning model performance is often very\n",
      "sensitive  to  the  production  environment  it  is  running  in,  including  the  versions  of\n",
      "software and operating systems in use. They tend not to be buggy in the classic soft‐\n",
      "ware  sense,  because  most  weren’t  written  by  hand,  but  rather  were  machine-\n",
      "generated. Instead, the problem is that they are often built on a pile of open source\n",
      "software  (e.g.,  libraries,  like  scikit-learn,  Python,  or  Linux),  and  having  versions  of\n",
      "this software in production that match those that the model was verified on is criti‐\n",
      "cally important.\n",
      "\n",
      "Ultimately, pushing models into production is not the final step of the machine learn‐\n",
      "ing life cycle—far from it. It’s often just the beginning of monitoring its performance\n",
      "and ensuring that it behaves as expected. As more data scientists start pushing more\n",
      "machine learning models into production, MLOps becomes critical in mitigating the\n",
      "potential risks, which (depending on the model) can be devastating for the business if\n",
      "things  go  wrong.  Monitoring  is  also  essential  so  that  the  organization  has  a  precise\n",
      "knowledge of how broadly each model is used.\n",
      "\n",
      "MLOps for Responsible AI\n",
      "A  responsible  use  of  machine  learning  (more  commonly  referred  to  as  Responsible\n",
      "AI) covers two main dimensions:\n",
      "\n",
      "Intentionality\n",
      "\n",
      "Ensuring  that  models  are  designed  and  behave  in  ways  aligned  with  their  pur‐\n",
      "pose. This includes assurance that data used for AI projects comes from compli‐\n",
      "ant  and  unbiased  sources  plus  a  collaborative  approach  to  AI  projects  that\n",
      "ensures multiple checks and balances on potential model bias. Intentionality also\n",
      "\n",
      "MLOps to Mitigate Risk \n",
      "\n",
      "| \n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "includes explainability, meaning the results of AI systems should be explainable\n",
      "by humans (ideally, not just the humans who created the system).\n",
      "\n",
      "Accountability\n",
      "\n",
      "Centrally  controlling,  managing,  and  auditing  the  enterprise  AI  effort—no\n",
      "shadow  IT!  Accountability  is  about  having  an  overall  view  of  which  teams  are\n",
      "using what data, how, and in which models. It also includes the need for trust that\n",
      "data  is  reliable  and  being  collected  in  accordance  with  regulations  as  well  as  a\n",
      "centralized understanding of which models are used for what business processes.\n",
      "This  is  closely  tied  to  traceability:  if  something  goes  wrong,  is  it  easy  to  find\n",
      "where in the pipeline it happened?\n",
      "\n",
      "These principles may seem obvious, but it’s important to consider that machine learn‐\n",
      "ing models lack the transparency of traditional imperative code. In other words, it is\n",
      "much harder to understand what features are used to determine a prediction, which\n",
      "in turn can make it much harder to demonstrate that models comply with the neces‐\n",
      "sary regulatory or internal governance requirements.\n",
      "\n",
      "The  reality  is  that  introducing  automation  vis-à-vis  machine  learning  models  shifts\n",
      "the fundamental onus of accountability from the bottom of the hierarchy to the top.\n",
      "That is, decisions that were perhaps previously made by individual contributors who\n",
      "operated within a margin of guidelines (for example, what the price of a given prod‐\n",
      "uct  should  be  or  whether  or  not  a  person  should  be  accepted  for  a  loan)  are  now\n",
      "being made by a model. The person responsible for the automated decisions of said\n",
      "model is likely a data team manager or even executive, and that brings the concept of\n",
      "Responsible AI even more to the forefront.\n",
      "\n",
      "Given the previously discussed risks as well as these particular challenges and princi‐\n",
      "ples,  it’s  easy  to  see  the  interplay  between  MLOps  and  Responsible  AI.  Teams  must\n",
      "have good MLOps principles to practice Responsible AI, and Responsible AI necessi‐\n",
      "tates MLOps strategies. Given the gravity of this topic, we’ll come back to it multiple\n",
      "times throughout this book, examining how it should be addressed at each stage of\n",
      "the ML model life cycle.\n",
      "\n",
      "MLOps for Scale\n",
      "MLOps  isn’t  just  important  because  it  helps  mitigate  the  risk  of  machine  learning\n",
      "models  in  production;  it  is  also  an  essential  component  to  massively  deploying\n",
      "machine learning efforts (and in turn benefiting from the corresponding economies\n",
      "of scale). Going from one or a handful of models in production to tens, hundreds, or\n",
      "thousands that have a positive business impact requires MLOps discipline.\n",
      "\n",
      "10 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Why Now and Challenges\n",
      "\n",
      "\f",
      "Good MLOps practices will help teams at a minimum:\n",
      "\n",
      "• Keep track of versioning, especially with experiments in the design phase\n",
      "\n",
      "• Understand whether retrained models are better than the previous versions (and\n",
      "\n",
      "promoting models to production that are performing better)\n",
      "\n",
      "• Ensure (at defined periods—daily, monthly, etc.) that model performance is not\n",
      "\n",
      "degrading in production\n",
      "\n",
      "Closing Thoughts\n",
      "Key features will be discussed at length in Chapter 3, but the point here is that these\n",
      "are not optional practices. They are essential tasks for not only efficiently scaling data\n",
      "science and machine learning at the enterprise level, but also doing it in a way that\n",
      "doesn’t  put  the  business  at  risk.  Teams  that  attempt  to  deploy  data  science  without\n",
      "proper MLOps practices in place will face issues with model quality and continuity—\n",
      "or, worse, they will introduce models that have a real, negative impact on the business\n",
      "(e.g., a model that makes biased predictions that reflect poorly on the company).\n",
      "\n",
      "MLOps is also, at a higher level, a critical part of transparent strategies for machine\n",
      "learning. Upper management and the C-suite should be able to understand as well as\n",
      "data  scientists  what  machine  learning  models  are  deployed  in  production  and  what\n",
      "effect  they’re  having  on  the  business.  Beyond  that,  they  should  arguably  be  able  to\n",
      "drill down to understand the whole data pipeline (i.e., the steps taken to go from raw\n",
      "data to final output) behind those machine learning models. MLOps, as described in\n",
      "this book, can provide this level of transparency and accountability.\n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 2\n",
      "People of MLOps\n",
      "\n",
      "Even though machine learning models are primarily built by data scientists, it’s a mis‐\n",
      "conception  that  only  data  scientists  can  benefit  from  robust  MLOps  processes  and\n",
      "systems.  In  fact,  MLOps  is  an  essential  piece  of  enterprise  AI  strategy  and  affects\n",
      "everyone working on, or benefiting from, the machine learning model life cycle.\n",
      "\n",
      "This chapter covers the roles each of these people plays in the machine learning life\n",
      "cycle, who they should ideally be connected and working together with under a top-\n",
      "notch  MLOps  program  to  achieve  the  best  possible  results  from  machine  learning\n",
      "efforts, and what MLOps requirements they may have.\n",
      "\n",
      "It’s important to note that this field is constantly evolving, bringing with it many new\n",
      "job titles that may not be listed here and presenting new challenges (or overlaps) in\n",
      "MLOps responsibilities.\n",
      "\n",
      "Before  we  dive  into  the  details,  let’s  look  at  the  following  table,  which  provides  an\n",
      "overview:\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Role\n",
      "Subject\n",
      "matter\n",
      "experts\n",
      "\n",
      "Data\n",
      "scientists\n",
      "\n",
      "Data\n",
      "engineers\n",
      "\n",
      "Software\n",
      "engineers\n",
      "\n",
      "DevOps\n",
      "\n",
      "Model risk\n",
      "managers/\n",
      "auditors\n",
      "\n",
      "Machine\n",
      "learning\n",
      "architects\n",
      "\n",
      "Role in machine learning model life cycle\n",
      "\n",
      "MLOps requirements\n",
      "\n",
      "• Provide business questions, goals, or KPIs around\n",
      "\n",
      "• Easy way to understand deployed model\n",
      "\n",
      "which ML models should be framed.\n",
      "\n",
      "performance in business terms.\n",
      "\n",
      "• Continually evaluate and ensure that model\n",
      "\n",
      "• Mechanism or feedback loop for flagging model\n",
      "\n",
      "performance aligns with or resolves the initial need.\n",
      "\n",
      "results that don’t align with business\n",
      "expectations.\n",
      "\n",
      "• Build models that address the business question or\n",
      "\n",
      "needs brought by subject matter experts.\n",
      "\n",
      "• Deliver operationalizable models so that they can be\n",
      "properly used in the production environment and\n",
      "with production data.\n",
      "\n",
      "• Assess model quality (of both original and tests) in\n",
      "tandem with subject matter experts to ensure they\n",
      "answer initial business questions or needs.\n",
      "\n",
      "• Automated model packaging and delivery for\n",
      "quick and easy (yet safe) deployment to\n",
      "production.\n",
      "\n",
      "• Ability to develop tests to determine the quality\n",
      "of deployed models and to make continual\n",
      "improvements.\n",
      "\n",
      "• Visibility into the performance of all deployed\n",
      "models (including side-by-side for tests) from\n",
      "one central location.\n",
      "\n",
      "• Ability to investigate data pipelines of each\n",
      "model to make quick assessments and\n",
      "adjustments regardless of who originally built\n",
      "the model.\n",
      "\n",
      "• Optimize the retrieval and use of data to power ML\n",
      "\n",
      "• Visibility into performance of all deployed\n",
      "\n",
      "models.\n",
      "\n",
      "models.\n",
      "\n",
      "• Ability to see the full details of individual data\n",
      "pipelines to address underlying data plumbing\n",
      "issues.\n",
      "\n",
      "• Integrate ML models in the company’s applications\n",
      "\n",
      "and systems.\n",
      "\n",
      "• Versioning and automatic tests.\n",
      "• The ability to work in parallel on the same\n",
      "\n",
      "• Ensure that ML models work seamlessly with other\n",
      "\n",
      "application.\n",
      "\n",
      "non-machine-learning-based applications.\n",
      "\n",
      "• Conduct and build operational systems and test for\n",
      "\n",
      "• Seamless integration of MLOps into the larger\n",
      "\n",
      "security, performance, availability.\n",
      "\n",
      "DevOps strategy of the enterprise.\n",
      "\n",
      "• Continuous Integration/Continuous Delivery (CI/CD)\n",
      "\n",
      "• Seamless deployment pipeline.\n",
      "\n",
      "pipeline management.\n",
      "\n",
      "• Minimize overall risk to the company as a result of\n",
      "\n",
      "• Robust, likely automated, reporting tools on all\n",
      "\n",
      "ML models in production.\n",
      "\n",
      "• Ensure compliance with internal and external\n",
      "requirements before pushing ML models to\n",
      "production.\n",
      "\n",
      "• Ensure a scalable and flexible environment for ML\n",
      "model pipelines, from design to development and\n",
      "monitoring.\n",
      "\n",
      "• Introduce new technologies when appropriate that\n",
      "improve ML model performance in production.\n",
      "\n",
      "models (currently or ever in production),\n",
      "including data lineage.\n",
      "\n",
      "• High-level overview of models and their\n",
      "\n",
      "resources consumed.\n",
      "\n",
      "• Ability to drill down into data pipelines to assess\n",
      "\n",
      "and adjust infrastructure needs.\n",
      "\n",
      "14 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: People of MLOps\n",
      "\n",
      "\f",
      "Subject Matter Experts\n",
      "The  first  profile  to  consider  as  part  of  MLOps  efforts  is  the  subject  matter  experts\n",
      "(SMEs); after all, the ML model life cycle starts and ends with them. While the data-\n",
      "oriented profiles (data scientist, engineer, architect, etc.) have expertise across many\n",
      "areas,  they  tend  to  lack  a  deep  understanding  of  the  business  and  the  problems  or\n",
      "questions that need to be addressed using machine learning.\n",
      "\n",
      "Subject matter experts usually come to the table—or, at least, they should come to the\n",
      "table—with clearly defined goals, business questions, and/or key performance indica‐\n",
      "tors  (KPIs)  that  they  want  to  achieve  or  address.  In  some  cases,  they  might  be\n",
      "extremely well defined (e.g., “To hit our numbers for the quarter, we need to reduce\n",
      "customer churn by 10%” or “We’re losing $N per quarter due to unscheduled mainte‐\n",
      "nance;  how  can  we  better  predict  downtime?”).  In  other  cases,  the  goals  and  ques‐\n",
      "tions may be less well defined (e.g., “Our service staff needs to better understand our\n",
      "customers to upsell them” or “How can we get people to buy more widgets?”).\n",
      "\n",
      "In  organizations  with  healthy  processes,  starting  the  machine  learning  model  life\n",
      "cycle with a more defined business question isn’t necessarily always an imperative, or\n",
      "even  an  ideal,  scenario.  Working  with  a  less  defined  business  goal  can  be  a  good\n",
      "opportunity for subject matter experts to work directly with data scientists up front to\n",
      "better  frame  the  problem  and  brainstorm  possible  solutions  before  even  beginning\n",
      "any data exploration or model experimentation.\n",
      "\n",
      "Without this critical starting point from subject matter experts, other data professio‐\n",
      "nals (particularly data scientists) risk starting the machine learning life cycle process\n",
      "trying  to  solve  problems  or  provide  solutions  that  don’t  serve  the  larger  business.\n",
      "Ultimately,  this  is  detrimental  not  only  to  the  subject  matter  experts  who  need  to\n",
      "partner with data scientists and other data experts to build solutions, but to data sci‐\n",
      "entists themselves who might struggle to provide larger value.\n",
      "\n",
      "Another negative outcome when SMEs are not involved in the ML life cycle is that,\n",
      "without real business outcomes, data teams subsequently struggle to gain traction and\n",
      "additional  budget  or  support  to  continue  advanced  analytics  initiatives.  Ultimately,\n",
      "this is bad for data teams, for SMEs, and for the business as a whole.\n",
      "\n",
      "To add more structure around SME involvement, business decision modeling meth‐\n",
      "odologies can be applied to formalize the business problems to be solved and frame\n",
      "the role of machine learning in the solution.\n",
      "\n",
      "Subject Matter Experts \n",
      "\n",
      "| \n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Business Decision Modeling\n",
      "Decision  modeling  creates  a  business  blueprint  of  the  decision-making  process,\n",
      "allowing  subject  matter  experts  to  directly  structure  and  describe  their  needs.  Deci‐\n",
      "sion models can be helpful because they put machine learning in context for subject\n",
      "matter experts. This allows the models to be integrated with the business rules, as well\n",
      "as helps the SMEs to fully understand decision contexts and the potential impact of\n",
      "model changes.\n",
      "\n",
      "MLOps strategies that include a component of business decision modeling for subject\n",
      "matter experts can be an effective tool for ensuring that real-world machine learning\n",
      "model results are properly contextualized for those who don’t have deep knowledge of\n",
      "how the underlying models themselves work.1\n",
      "\n",
      "Subject matter experts have a role to play not only at the beginning of the ML model\n",
      "life  cycle,  but  at  the  end  (post-production)  as  well.  Oftentimes,  to  understand  if  an\n",
      "ML  model  is  performing  well  or  as  expected,  data  scientists  need  subject  matter\n",
      "experts  to  close  the  feedback  loop  because  traditional  metrics  (accuracy,  precision,\n",
      "recall, etc.) are not enough.\n",
      "\n",
      "For  example,  data  scientists  could  build  a  simple  churn  prediction  model  that  has\n",
      "very high accuracy in a production environment; however, marketing does not man‐\n",
      "age  to  prevent  anyone  from  churning.  From  a  business  perspective,  that  means  the\n",
      "model didn’t work, and that’s important information that needs to make its way back\n",
      "to those building the ML model so that they can find another possible solution, such\n",
      "as introducing uplift modeling that helps marketing better target potential churners\n",
      "who might be receptive to marketing messaging.\n",
      "\n",
      "Given the role of SMEs in the ML model life cycle, it’s critical when building MLOps\n",
      "processes to have an easy way for them to understand deployed model performance\n",
      "in  business  terms.  That  is,  they  need  to  understand  not  just  model  accuracy,  preci‐\n",
      "sion, and recall, but the results or impact of the model on the business process identi‐\n",
      "fied up front. In addition, when there are unexpected shifts in performance, subject\n",
      "matter experts need a scalable way, through MLOps processes, to flag model results\n",
      "that don’t align with business expectations.\n",
      "\n",
      "On  top  of  these  explicit  feedback  mechanisms,  more  generally,  MLOps  should  be\n",
      "built  in  a  way  that  increases  transparency  for  subject  matter  experts.  That  is,  they\n",
      "should be able to use MLOps processes as a jumping-off point for exploring the data\n",
      "\n",
      "1 Decision requirements models are based on Decision Model and Notation, a framework for improving pro‐\n",
      "\n",
      "cesses, effectively managing business rules projects, framing predictive analytics efforts, and ensuring decision\n",
      "support systems and dashboards are action-oriented.\n",
      "\n",
      "16 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: People of MLOps\n",
      "\n",
      "\f",
      "pipelines  behind  the  models,  understanding  what  data  is  being  used,  how  it’s  being\n",
      "transformed and enhanced, and what kind of machine learning techniques are being\n",
      "applied.\n",
      "\n",
      "For subject matter experts who are also concerned with compliance of machine learn‐\n",
      "ing models with internal or external regulations, MLOps serves as an additional way\n",
      "to bring transparency and understanding to these processes. This includes being able\n",
      "to dig into individual decisions made by a model to understand why the model came\n",
      "to  that  decision.  This  should  be  complementary  to  statistical  and  aggregated\n",
      "feedback.\n",
      "\n",
      "Ultimately, MLOps is most relevant for subject matter experts as a feedback mecha‐\n",
      "nism  and  a  platform  for  communication  with  data  scientists  about  the  models  they\n",
      "are  building.  However,  there  are  other  MLOps  needs  as  well—specifically  around\n",
      "transparency,  which  ties  into  Responsible  AI—that  are  relevant  for  subject  matter\n",
      "experts and make them an important part of the MLOps picture.\n",
      "\n",
      "Data Scientists\n",
      "The needs of data scientists are the most critical ones to consider when building an\n",
      "MLOps strategy. To be sure, they have a lot to gain; data scientists at most organiza‐\n",
      "tions  today  often  deal  with  siloed  data,  processes,  and  tools,  making  it  difficult  to\n",
      "effectively scale their efforts. MLOps is well positioned to change this.\n",
      "\n",
      "Though most see data scientists’ role in the ML model life cycle as strictly the model\n",
      "building portion, it is—or at least, it should be—much wider. From the very begin‐\n",
      "ning, data scientists need to be involved with subject matter experts, understanding\n",
      "and  helping  to  frame  business  problems  in  such  a  way  that  they  can  build  a  viable\n",
      "machine learning solution.\n",
      "\n",
      "The reality is that this very first, critical step in the ML model life cycle is often the\n",
      "hardest.  It’s  challenging  particularly  for  data  scientists  because  it’s  not  where  their\n",
      "training  lies.  Both  formal  and  informal  data  science  programs  in  universities  and\n",
      "online heavily emphasize technical skills and not necessarily skills for communicating\n",
      "effectively with subject matter experts from the business side of the house, who usu‐\n",
      "ally are not intimately familiar with machine learning techniques. Once again, busi‐\n",
      "ness decision modeling techniques can help here.\n",
      "\n",
      "It’s also a challenge because it can take time. For data scientists who want to dive in\n",
      "and get their hands dirty, spending weeks framing and outlining the problem before\n",
      "getting  started  on  solving  it  can  be  torture.  To  top  it  off,  data  scientists  are  often\n",
      "siloed (physically, culturally, or both) from the core of the business and from subject\n",
      "matter  experts,  so  they  simply  don’t  have  access  to  an  organizational  infrastructure\n",
      "that facilitates easy collaboration between these profiles. Robust MLOps systems can\n",
      "help address some of these challenges.\n",
      "\n",
      "Data Scientists \n",
      "\n",
      "| \n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "After overcoming the first hurdle, depending on the organization, the project might\n",
      "get handed off to either data engineers or analysts to do some of the initial data gath‐\n",
      "ering, preparation, and exploration. In some cases, data scientists themselves manage\n",
      "these  parts  of  the  ML  model  life  cycle.  But  in  any  case,  data  scientists  step  back  in\n",
      "when it comes time to build, test, robustify, and then deploy the model.\n",
      "\n",
      "Following deployment, data scientists’ roles include constantly assessing model qual‐\n",
      "ity to ensure the way it’s working in production answers initial business questions or\n",
      "needs. The underlying question in many organizations is often whether data scientists\n",
      "monitor  only  the  models  they  have  had  a  hand  in  building  or  whether  one  person\n",
      "handles  all  monitoring.  In  the  former  scenario,  what  happens  when  there  is  staff\n",
      "turnover? In the latter scenario, building good MLOps practices is critical, as the per‐\n",
      "son monitoring also needs to be able to quickly jump in and take action should the\n",
      "model drift and start negatively affecting the business. If they weren’t the ones who\n",
      "built it, how can MLOps make this process seamless?\n",
      "\n",
      "Operationalization and MLOps\n",
      "Throughout  2018  and  the  beginning  of  2019,  operationalization  was  the  key  buzz‐\n",
      "word when it came to ML model life cycles and AI in the enterprise. Put simply, oper‐\n",
      "ationalization  of  data  science  is  the  process  of  pushing  models  to  production  and\n",
      "measuring their performance against business goals. So how does operationalization\n",
      "fit into the MLOps story? MLOps takes operationalization one step further, encom‐\n",
      "passing  not  just  the  push  to  production  but  the  maintenance  of  those  models—and\n",
      "the entire data pipeline—in production.\n",
      "\n",
      "Though  they  are  distinct,  MLOps  might  be  considered  the  new  operationalization.\n",
      "That is, where many of the major hurdles for businesses to operationalize have disap‐\n",
      "peared, MLOps is the next frontier and presents the next big challenge for machine\n",
      "learning efforts in the enterprise.\n",
      "\n",
      "All  of  the  questions  in  the  previous  section  lead  directly  here:  data  scientists’  needs\n",
      "when  it  comes  to  MLOps.  Starting  from  the  end  of  the  process  and  working  back‐\n",
      "ward, MLOps must provide data scientists with visibility into the performance of all\n",
      "deployed models as well as any models being A/B tested. But taking that one step fur‐\n",
      "ther, it’s not just about monitoring—it’s also about action. Top-notch MLOps should\n",
      "allow  data  scientists  the  flexibility  to  select  winning  models  from  tests  and  easily\n",
      "deploy them.\n",
      "\n",
      "Transparency is an overarching theme in MLOps, so it’s no surprise that it’s also a key\n",
      "need for data scientists. The ability to drill down into data pipelines and make quick\n",
      "assessments and adjustments (regardless of who originally built the model) is critical. \n",
      "Automated model packaging and delivery for quick and easy (yet safe) deployment to\n",
      "production is another important point for transparency, and it’s a crucial component\n",
      "\n",
      "18 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: People of MLOps\n",
      "\n",
      "\f",
      "of MLOps, especially to bring data scientists together to a place of trust with software\n",
      "engineers and DevOps teams.\n",
      "\n",
      "In addition to transparency, another theme for mastering MLOps—especially when it\n",
      "comes to meeting the needs of data scientists—is pure efficiency. In an enterprise set‐\n",
      "ting, agility and speed matter. It’s true for DevOps, and the story for MLOps is no dif‐\n",
      "ferent.  Of  course,  data  scientists  can  deploy,  test,  and  monitor  models  in  an  ad  hoc\n",
      "fashion.  But  they  will  spend  enormous  amounts  of  time  reinventing  the  wheel  with\n",
      "every single ML model, and that will never add up to scalable ML processes for the\n",
      "organization.\n",
      "\n",
      "Data Engineers\n",
      "Data pipelines are at the core of the ML model life cycle, and data engineers are, in\n",
      "turn,  at  the  core  of  data  pipelines.  Because  data  pipelines  can  be  abstract  and  com‐\n",
      "plex, data engineers have a lot of efficiencies to gain from MLOps.\n",
      "\n",
      "In  large  organizations,  managing  the  flow  of  data,  outside  of  the  application  of  ML\n",
      "models, is a full-time job.  Depending on the technical stack and organizational struc‐\n",
      "ture of the enterprise, data engineers might, therefore, be more focused on the data‐\n",
      "bases  themselves  than  on  pipelines  (especially  if  the  company  is  leveraging  data\n",
      "science and ML platforms that facilitate the visual building of pipelines by other data\n",
      "practitioners, like business analysts).\n",
      "\n",
      "Ultimately,  despite  these  slight  variations  in  the  role  by  an  organization,  the  role  of\n",
      "data engineers in the life cycle is to optimize the retrieval and use of data to eventually\n",
      "power ML models. Generally, this means working closely with business teams, partic‐\n",
      "ularly  subject  matter  experts,  to  identify  the  right  data  for  the  project  at  hand  and\n",
      "possibly also prepare it for use. On the other end, they work closely with data scien‐\n",
      "tists to resolve any data plumbing issues that might cause a model to behave undesira‐\n",
      "bly in production.\n",
      "\n",
      "Given data engineers’ central role in the ML model life cycle, underpinning both the\n",
      "building and monitoring portions, MLOps can bring significant efficiency gains. Data\n",
      "engineers require not only visibility into the performance of all models deployed in\n",
      "production,  but  the  ability  to  take  it  one  step  further  and  directly  drill  down  into\n",
      "individual data pipelines to address any underlying issues.\n",
      "\n",
      "Ideally, for maximum efficiency for the data engineer profile (and for others as well,\n",
      "including  data  scientists),  MLOps  must  not  consist  of  simple  monitoring,  but  be  a\n",
      "bridge to underlying systems for investigating and tweaking ML models.\n",
      "\n",
      "Data Engineers \n",
      "\n",
      "| \n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Software Engineers\n",
      "It would be easy to exclude classical software engineers from MLOps consideration,\n",
      "but  it  is  crucial  from  a  wider  organizational  perspective  to  consider  their  needs  to\n",
      "build a cohesive enterprise-wide strategy for machine learning.\n",
      "\n",
      "Software  engineers  don’t  usually  build  ML  models,  but,  on  the  other  hand,  most\n",
      "organizations  are  not  only  producing  ML  models,  but  classic  software  and  applica‐\n",
      "tions as well. It’s important that software engineers and data scientists work together\n",
      "to ensure the functioning of the larger system. After all, ML models aren’t just stand‐\n",
      "alone experiments; the machine learning code, training, testing, and deployment have\n",
      "to  fit  into  the  Continuous  Integration/Continuous  Delivery  (CI/CD)  pipelines  that\n",
      "the rest of the software is using.\n",
      "\n",
      "For example, consider a retail company that has built an ML-based recommendation\n",
      "engine  for  their  website.  The  ML  model  was  built  by  the  data  scientist,  but  to  inte‐\n",
      "grate it into the larger functioning of the site, software engineers will necessarily need\n",
      "to  be  involved.  Similarly,  software  engineers  are  responsible  for  the  maintenance  of\n",
      "the  website  as  a  whole,  and  a  large  part  of  that  includes  the  functioning  of  the  ML\n",
      "models in production.\n",
      "\n",
      "Given  this  interplay,  software  engineers  need  MLOps  to  provide  them  with  model\n",
      "performance  details  as  part  of  a  larger  picture  of  software  application  performance\n",
      "for the enterprise. MLOps is a way for data scientists and software engineers to speak\n",
      "the same language and have the same baseline understanding of how different models\n",
      "deployed across the silos of the enterprise are working together in production.\n",
      "\n",
      "Other  important  features  for  software  engineers  include  versioning,  to  be  sure  of\n",
      "what  they  are  currently  dealing  with;  automatic  tests,  to  be  as  sure  as  possible  that\n",
      "what they are currently dealing with is working; and the ability to work in parallel on\n",
      "the same application (thanks to a system that allows branches and merges like Git). \n",
      "\n",
      "DevOps\n",
      "MLOps was born out of DevOps principles, but that doesn’t mean they can be run in\n",
      "parallel as completely separate and siloed systems.\n",
      "\n",
      "DevOps teams have two primary roles in the ML model life cycle. First, they are the\n",
      "people conducting and building operational systems as well as tests to ensure security,\n",
      "performance, and availability of ML models. Second, they are responsible for CI/CD\n",
      "pipeline management. Both of these roles require tight collaboration with data scien‐\n",
      "tists, data engineers, and data architects. Tight collaboration is, of course, easier said\n",
      "than done, but that is where MLOps can add value.\n",
      "\n",
      "20 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: People of MLOps\n",
      "\n",
      "\f",
      "For DevOps teams, MLOps needs to be integrated into the larger DevOps strategy of\n",
      "the  enterprise,  bridging  the  gap  between  traditional  CI/CD  and  modern  ML.  That\n",
      "means systems that are fundamentally complementary and that allow DevOps teams\n",
      "to automate tests for ML just as they can automate tests for traditional software.\n",
      "\n",
      "Model Risk Manager/Auditor\n",
      "In certain industries (particularly the financial services sector), the model risk man‐\n",
      "agement (MRM) function is crucial for regulatory compliance. But it’s not only highly\n",
      "regulated industries that should be concerned or that should have a similar function;\n",
      "MRM  can  protect  companies  in  any  industry  from  catastrophic  loss  introduced  by\n",
      "poorly  performing  ML  models.  What’s  more,  audits  play  a  role  in  many  industries\n",
      "and can be labor intensive, which is where MLOps comes into the picture.\n",
      "\n",
      "When it comes to the ML model life cycle, model risk managers play the critical role\n",
      "of analyzing not just model outcomes, but the initial goal and business questions ML\n",
      "models  seek  to  resolve  to  minimize  overall  risk  to  the  company.  They  should  be\n",
      "involved  along  with  subject  matter  experts  at  the  very  beginning  of  the  life  cycle  to\n",
      "ensure that an automated, ML-based approach in and of itself doesn’t present risk.\n",
      "\n",
      "And, of course, they have a role to play in monitoring—their more traditional place\n",
      "in the model life cycle—to ensure that risks are kept at bay once models are in pro‐\n",
      "duction.  In  between  conception  and  monitoring,  MRM  also  is  a  factor  post-model\n",
      "development and preproduction, ensuring initial compliance with internal and exter‐\n",
      "nal requirements.\n",
      "\n",
      "MRM professionals and teams have a lot to gain from MLOps, because their work is\n",
      "often painstakingly manual. As MRM and the teams with which they work often use\n",
      "different tools, standardization can offer a huge leg up in the speed at which auditing\n",
      "and risk management can occur.\n",
      "\n",
      "When  it  comes  to  specific  MLOps  needs,  robust  reporting  tools  on  all  models\n",
      "(whether they are currently in production or have been in production in the past) is\n",
      "the primary one. This reporting should include not just performance details, but the\n",
      "ability to see data lineage. Automated reporting adds an extra layer of efficiency for\n",
      "MRM and audit teams in MLOps systems and processes.\n",
      "\n",
      "Machine Learning Architect\n",
      "Traditional  data  architects  are  responsible  for  understanding  the  overall  enterprise\n",
      "architecture and ensuring that it meets the requirements for data needs from across\n",
      "the  business.  They  generally  play  a  role  in  defining  how  data  will  be  stored  and\n",
      "consumed.\n",
      "\n",
      "Model Risk Manager/Auditor \n",
      "\n",
      "| \n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Today, demands on architects are much greater, and they often have to be knowledge‐\n",
      "able not only on the ins and outs of data storage and consumption, but on how ML\n",
      "models work in tandem. This adds a lot of complexity to the role and increases their\n",
      "responsibility  in  the  MLOps  life  cycle,  and  it’s  why  in  this  section,  we  have  called\n",
      "them machine learning architects instead of the more traditional “data architect” title.\n",
      "\n",
      "Machine learning architects play a critical role in the ML model life cycle, ensuring a\n",
      "scalable and flexible environment for model pipelines. In addition, data teams need\n",
      "their  expertise  to  introduce  new  technologies  (when  appropriate)  that  improve  ML\n",
      "model performance in production. It is for this reason that the data architect title isn’t\n",
      "enough; they need to have an intimate understanding of machine learning, not just\n",
      "enterprise architecture, to play this key role in the ML model life cycle.\n",
      "\n",
      "This role requires collaboration across the enterprise, from data scientists and engi‐\n",
      "neers  to  DevOps  and  software  engineers.  Without  a  complete  understanding  of  the\n",
      "needs of each of these people and teams, machine learning architects cannot properly\n",
      "allocate resources to ensure optimal performance of ML models in production.\n",
      "\n",
      "When  it  comes  to  MLOps,  the  machine  learning  architects’  role  is  about  having  a\n",
      "centralized  view  of  resource  allocation.  As  they  have  a  strategic,  tactical  role,  they\n",
      "need an overview of the situation to identify bottlenecks and use that information to\n",
      "find long-term improvements. Their role is one of pinpointing possible new technol‐\n",
      "ogy or infrastructure for investment, not necessarily operational quick fixes that don’t\n",
      "address the heart of the scalability of the system.\n",
      "\n",
      "Closing Thoughts\n",
      "MLOps isn’t just for data scientists; a diverse group of experts across the organization\n",
      "has a role to play not only in the ML model life cycle, but the MLOps strategy as well.\n",
      "In fact, each person—from the subject matter expert on the business side to the most\n",
      "technical machine learning architect—plays a critical part in the maintenance of ML\n",
      "models in production. This is ultimately important not only to ensure the best possi‐\n",
      "ble results from ML models (good results generally lead to more trust in ML-based\n",
      "systems as well as increased budget to build more), but, perhaps more pointedly, to\n",
      "protect the business from the risks outlined in Chapter 1.\n",
      "\n",
      "22 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: People of MLOps\n",
      "\n",
      "\f",
      "CHAPTER 3\n",
      "Key MLOps Features\n",
      "\n",
      "Mark Treveil\n",
      "\n",
      "MLOps affects many different roles across the organization and, in turn, many parts\n",
      "of the machine learning life cycle. This chapter introduces the five key components of\n",
      "MLOps (development, deployment, monitoring, iteration, and governance) at a high\n",
      "level as a foundation for Chapters 4 through 8, which delve into the more technical\n",
      "details and requirements of these components.\n",
      "\n",
      "A Primer on Machine Learning\n",
      "To  understand  the  key  features  of  MLOps,  it’s  essential  first  to  understand  how\n",
      "machine learning works and be intimately familiar with its specificities. Though often\n",
      "overlooked  in  its  role  as  a  part  of  MLOps,  ultimately  algorithm  selection  (or  how\n",
      "machine learning models are built) can have a direct impact on MLOps processes.\n",
      "\n",
      "At its core, machine learning is the science of computer algorithms that automatically\n",
      "learn  and  improve  from  experience  rather  than  being  explicitly  programmed.  The\n",
      "algorithms  analyze  sample  data,  known  as  training  data,  to  build  a  software  model\n",
      "that can make predictions.\n",
      "\n",
      "For example, an image recognition model might be able to identify the type of elec‐\n",
      "tricity meter from a photograph by searching for key patterns in the image that dis‐\n",
      "tinguish each type of meter. Another example is an insurance recommender model,\n",
      "which might suggest additional insurance products that a specific existing customer\n",
      "is most likely to buy based on the previous behavior of similar customers.\n",
      "\n",
      "When faced with unseen data, be it a photo or a customer, the ML model uses what it\n",
      "has  learned  from  previous  data  to  make  the  best  prediction  it  can  based  on  the\n",
      "assumption that the unseen data is somehow related to the previous data.\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "ML algorithms use a wide range of mathematical techniques, and the models can take\n",
      "many different forms, from simple decision trees to logistic regression algorithms to\n",
      "much  more  complex  deep  learning  models  (see  “What  Is  a  Machine  Learning\n",
      "Model?” on page 42 for details).\n",
      "\n",
      "Model Development\n",
      "Let’s  take  a  deeper  look  into  ML  model  development  as  a  whole  for  an  even  more\n",
      "complete  understanding  of  its  components,  all  of  which  can  have  an  impact  on\n",
      "MLOps after deployment.\n",
      "\n",
      "Establishing Business Objectives\n",
      "The process of developing a machine learning model typically starts with a business\n",
      "objective,  which  can  be  as  simple  as  reducing  fraudulent  transactions  to  <  0.1%  or\n",
      "having  the  ability  to  identify  people’s  faces  on  their  social  media  photos.  Business\n",
      "objectives naturally come with performance targets, technical infrastructure require‐\n",
      "ments, and cost constraints; all of these factors can be captured as key performance\n",
      "indicators, or KPIs, which will ultimately enable the business performance of models\n",
      "in production to be monitored.\n",
      "\n",
      "It’s important to recognize that ML projects don’t happen in a vacuum. They are gen‐\n",
      "erally part of a larger project that in turn impacts technologies, processes, and people.\n",
      "That means part of establishing objectives also includes change management, which\n",
      "may even provide some guidance for how the ML model should be built. For exam‐\n",
      "ple,  the  required  degree  of  transparency  will  strongly  influence  the  choice  of  algo‐\n",
      "rithms and may drive the need to provide explanations together with predictions so\n",
      "that predictions are turned into valuable decisions at the business level.\n",
      "\n",
      "Data Sources and Exploratory Data Analysis\n",
      "With  clear  business  objectives  defined,  it  is  time  to  bring  together  subject  matter\n",
      "experts  and  data  scientists  to  begin  the  journey  of  developing  the  ML  model.  This\n",
      "starts with the search for suitable input data. Finding data sounds simple, but in prac‐\n",
      "tice, it can be the most arduous part of the journey.\n",
      "\n",
      "Key questions for finding data to build ML models include:\n",
      "\n",
      "• What relevant datasets are available?\n",
      "\n",
      "• Is this data sufficiently accurate and reliable?\n",
      "\n",
      "• How can stakeholders get access to this data?\n",
      "\n",
      "• What  data  properties  (known  as  features)  can  be  made  available  by  combining\n",
      "\n",
      "multiple sources of data?\n",
      "\n",
      "24 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "• Will this data be available in real time?\n",
      "\n",
      "• Is there a need to label some of the data with the “ground truth” that is to be pre‐\n",
      "dicted, or does unsupervised learning make sense? If so, how much will this cost\n",
      "in terms of time and resources?\n",
      "\n",
      "• What platform should be used?\n",
      "\n",
      "• How will data be updated once the model is deployed?\n",
      "\n",
      "• Will the use of the model itself reduce the representativeness of the data?\n",
      "\n",
      "• How will the KPIs, which were established along with the business objectives, be\n",
      "\n",
      "measured?\n",
      "\n",
      "The constraints of data governance bring even more questions, including:\n",
      "\n",
      "• Can the selected datasets be used for this purpose?\n",
      "\n",
      "• What are the terms of use?\n",
      "\n",
      "• Is there personally identifiable information (PII) that must be redacted or anony‐\n",
      "\n",
      "mized?\n",
      "\n",
      "• Are  there  features,  such  as  gender,  that  legally  cannot  be  used  in  this  business\n",
      "\n",
      "context?\n",
      "\n",
      "• Are minority populations sufficiently well represented that the model has equiva‐\n",
      "\n",
      "lent performances on each group?\n",
      "\n",
      "Since data is the essential ingredient to power ML algorithms, it always helps to build\n",
      "an understanding of the patterns in data before attempting to train models. Explora‐\n",
      "tory data analysis (EDA) techniques can help build hypotheses about the data, iden‐\n",
      "tify  data  cleaning  requirements,  and  inform  the  process  of  selecting  potentially\n",
      "significant features. EDA can be carried out visually for intuitive insight and statisti‐\n",
      "cally if more rigor is required.\n",
      "\n",
      "Feature Engineering and Selection\n",
      "EDA leads naturally into feature engineering and feature selection. Feature engineer‐\n",
      "ing  is  the  process  of  taking  raw  data  from  the  selected  datasets  and  transforming  it\n",
      "into “features” that better represent the underlying problem to be solved. “Features”\n",
      "are arrays of numbers of fixed size, as it is the only object that ML algorithms under‐\n",
      "stand.  Feature  engineering  includes  data  cleansing,  which  can  represent  the  largest\n",
      "part  of  an  ML  project  in  terms  of  time  spent.  For  details,  see  “Feature  Engineering\n",
      "and Selection” on page 47.\n",
      "\n",
      "Model Development \n",
      "\n",
      "| \n",
      "\n",
      "25\n",
      "\n",
      "\f",
      "Training and Evaluation\n",
      "After  data  preparation  by  way  of  feature  engineering  and  selection,  the  next  step  is\n",
      "training. The process of training and optimizing a new ML model is iterative; several\n",
      "algorithms may be tested, features can be automatically generated, feature selections\n",
      "may be adapted, and algorithm hyperparameters tuned. In addition to—or in many\n",
      "cases  because  of—its  iterative  nature,  training  is  also  the  most  intensive  step  of  the\n",
      "ML model life cycle when it comes to computing power.\n",
      "\n",
      "Keeping  track  of  the  results  of  each  experiment  when  iterating  becomes  complex\n",
      "quickly. Nothing is more frustrating to data scientists than not being able to re-create\n",
      "the best results because they cannot remember the precise configuration. An experi‐\n",
      "ment tracking tool can greatly simplify the process of remembering the data, the fea‐\n",
      "tures  selection,  and  model  parameters  alongside  the  performance  metrics.  These\n",
      "enable  experiments  to  be  compared  side-by-side,  highlighting  the  differences  in\n",
      "performance.\n",
      "\n",
      "Deciding  what  is  the  best  solution  requires  both  quantitative  criteria,  such  as  accu‐\n",
      "racy or average error, and qualitative criteria regarding the explainability of the algo‐\n",
      "rithm or its ease of deployment.\n",
      "\n",
      "Reproducibility\n",
      "While many experiments may be short-lived, significant versions of a model need to\n",
      "be  saved  for  possible  later  use.  The  challenge  here  is  reproducibility,  which  is  an\n",
      "important  concept  in  experimental  science  in  general.  The  aim  in  ML  is  to  save\n",
      "enough information about the environment the model was developed in so that the\n",
      "model can be reproduced with the same results from scratch.\n",
      "\n",
      "Without reproducibility, data scientists have little chance of being able to confidently\n",
      "iterate on models, and worse, they are unlikely to be able to hand over the model to\n",
      "DevOps to see if what was created in the lab can be faithfully reproduced in produc‐\n",
      "tion.  True  reproducibility  requires  version  control  of  all  the  assets  and  parameters\n",
      "involved, including the data used to train and evaluate the model, as well as a record\n",
      "of  the  software  environment  (see  “Version  Management  and  Reproducibility”  on\n",
      "page 56 for details).\n",
      "\n",
      "Responsible AI\n",
      "Being  able  to  reproduce  the  model  is  only  part  of  the  operationalization  challenge;\n",
      "the DevOps team also needs to understand how to verify the model (i.e., what does\n",
      "the model do, how should it be tested, and what are the expected results?). Those in\n",
      "highly regulated industries are likely required to document even more detail, includ‐\n",
      "ing how the model was built and how it was tuned. In critical cases, the model may be\n",
      "independently recoded and rebuilt.\n",
      "\n",
      "26 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "Documentation is still the standard  solution to this communication challenge. Auto‐\n",
      "mated model document generation, whereby a tool automatically creates documenta‐\n",
      "tion associated with any trained model, can make the task less onerous. But in almost\n",
      "all cases, some documentation will need to be written by hand to explain the choices\n",
      "made.\n",
      "\n",
      "It is a fundamental consequence of their statistical nature that ML models are chal‐\n",
      "lenging  to  understand.  While  model  algorithms  come  with  standard  performance\n",
      "measures  to  assess  their  efficacy,  these  don’t  explain  how  the  predictions  are  made.\n",
      "The  “how”  is  important  as  a  way  to  sanity-check  the  model  or  help  better  engineer\n",
      "features,  and  it  may  be  necessary  to  ensure  that  fairness  requirements  (e.g.,  around\n",
      "features like sex, age, or race) have been met. This is the field of explainability, which\n",
      "is connected to Responsible AI as discussed in Chapter 1 and which will be discussed\n",
      "in further detail in Chapter 4.\n",
      "\n",
      "Explainability  techniques  are  becoming  increasingly  important  as  global  concerns\n",
      "grow about the impact of unbridled AI. They offer a way to mitigate uncertainty and\n",
      "help prevent unintended consequences. The techniques most commonly used today\n",
      "include:\n",
      "\n",
      "• Partial  dependence  plots,  which  look  at  the  marginal  impact  of  features  on  the\n",
      "\n",
      "predicted outcome \n",
      "\n",
      "• Subpopulation analyses, which look at how the model treats specific subpopula‐\n",
      "\n",
      "tions and that are the basis of many fairness analyses\n",
      "\n",
      "• Individual  model  predictions,  such  as  Shapley  values,  which  explain  how  the\n",
      "\n",
      "value of each feature contributes to a specific prediction\n",
      "\n",
      "• What-if analysis, which helps the ML model user to understand the sensitivity of\n",
      "\n",
      "the prediction to its inputs\n",
      "\n",
      "As we’ve seen in this section, even though model development happens very early on,\n",
      "it’s still an important place to incorporate MLOps practices. Any MLOps work done\n",
      "up front during the model development stage will make the models easier to manage\n",
      "down the line (especially when pushing to production).\n",
      "\n",
      "Productionalization and Deployment\n",
      "Productionalizing and deploying models is a key component of MLOps that presents\n",
      "an  entirely  different  set  of  technical  challenges  than  developing  the  model.  It  is  the\n",
      "domain of the software engineer and the DevOps team, and the organizational chal‐\n",
      "lenges  in  managing  the  information  exchange  between  the  data  scientists  and  these\n",
      "teams must not be underestimated. As touched on in Chapter 1, without effective col‐\n",
      "laboration between the teams, delays or failures to deploy are inevitable.\n",
      "\n",
      "Productionalization and Deployment \n",
      "\n",
      "| \n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Model Deployment Types and Contents\n",
      "To understand what happens in these phases, it’s helpful to take a step back and ask:\n",
      "what exactly is going into production, and what does a model consist of? There are\n",
      "commonly two types of model deployment:\n",
      "\n",
      "Model-as-a-service, or live-scoring model\n",
      "\n",
      "Typically the model is deployed into a simple framework to provide a REST API\n",
      "endpoint  (the  means  from  which  the  API  can  access  the  resources  it  needs  to\n",
      "perform the task) that responds to requests in real time.\n",
      "\n",
      "Embedded model\n",
      "\n",
      "Here the model is packaged into an application, which is then published. A com‐\n",
      "mon example is an application that provides batch-scoring of requests.\n",
      "\n",
      "What  to-be-deployed  models  consist  of  depends,  of  course,  on  the  technology\n",
      "chosen, but typically they comprise a set of code (commonly Python, R, or Java) and\n",
      "data artifacts. Any of these can have version dependencies on runtimes and packages\n",
      "that need to match in the production environment because the use of different ver‐\n",
      "sions may cause model predictions to differ.\n",
      "\n",
      "One approach to reducing dependencies on the production environment is to export\n",
      "the model to a portable format such as PMML, PFA, ONNX, or POJO. These aim to\n",
      "improve model portability between systems and simplify deployment. However, they\n",
      "come  at  a  cost:  each  format  supports  a  limited  range  of  algorithms,  and  sometimes\n",
      "the portable models behave in subtly different ways than the original. Whether or not\n",
      "to use a portable format is a choice to be made based on a thorough understanding of\n",
      "the technological and business context.\n",
      "\n",
      "Containerization\n",
      "Containerization  is  an  increasingly  popular  solution  to  the  headaches  of  dependen‐\n",
      "cies  when  deploying  ML  models.  Container  technologies  such  as  Docker  are  light‐\n",
      "weight  alternatives  to  virtual  machines,  allowing  applications  to  be  deployed  in\n",
      "independent, self-contained environments, matching the exact requirements of each\n",
      "model.\n",
      "\n",
      "They also enable new models to be seamlessly deployed using the blue-green deploy‐\n",
      "ment technique.1 Compute resources for models can be scaled elastically using multi‐\n",
      "ple containers, too. Orchestrating many containers is the role of technologies such as\n",
      "Kubernetes and can be used both in the cloud and on-premise.\n",
      "\n",
      "1 Describing the blue-green deployment technique will require more space than we have here. For more infor‐\n",
      "\n",
      "mation, see Martin Fowler’s blog.\n",
      "\n",
      "28 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "Model Deployment Requirements\n",
      "So  what  about  the  productionalization  process  between  completing  model  develop‐\n",
      "ment  and  physically  deploying  into  production—what  needs  to  be  addressed?  One\n",
      "thing is for sure: rapid, automated deployment is always preferred to labor-intensive\n",
      "processes.\n",
      "\n",
      "For  short-lifetime,  self-service  applications,  there  often  isn’t  much  need  to  worry\n",
      "about testing and validation. If the maximum resource demands of the model can be\n",
      "securely capped by technologies such as Linux cgroups, then a fully automated single-\n",
      "step push-to-production may be entirely adequate. It is even possible to handle sim‐\n",
      "ple  user  interfaces  with  frameworks  like  Flask  when  using  this  lightweight\n",
      "deployment  mode.  Along  with  integrated  data  science  and  machine  learning  plat‐\n",
      "forms, some business rule management systems may also allow some sort of autono‐\n",
      "mous deployment of basic ML models.\n",
      "\n",
      "In  customer-facing,  mission-critical  use  cases,  a  more  robust  CI/CD  pipeline  is\n",
      "required. This typically involves:\n",
      "\n",
      "1. Ensuring all coding, documentation and sign-off standards have been met\n",
      "\n",
      "2. Re-creating the model in something approaching the production environment\n",
      "\n",
      "3. Revalidating the model accuracy\n",
      "\n",
      "4. Performing explainability checks\n",
      "\n",
      "5. Ensuring all governance requirements have been met\n",
      "\n",
      "6. Checking the quality of any data artifacts\n",
      "\n",
      "7. Testing resource usage under load\n",
      "\n",
      "8. Embedding into a more complex application, including integration tests\n",
      "\n",
      "In  heavily  regulated  industries  (e.g.,  finance  and  pharmaceuticals),  governance  and\n",
      "regulatory checks will be extensive and are likely to involve manual intervention. The\n",
      "desire in MLOps, just as in DevOps, is to automate the CI/CD pipeline as far as possi‐\n",
      "ble.  This  not  only  speeds  up  the  deployment  process,  but  it  enables  more  extensive\n",
      "regression testing and reduces the likelihood of errors in the deployment. \n",
      "\n",
      "Monitoring\n",
      "Once a model is deployed to production, it is crucial that it continue to perform well\n",
      "over time. But good performance means different things to different people, in par‐\n",
      "ticular to the DevOps team, to data scientists, and to the business.\n",
      "\n",
      "Monitoring \n",
      "\n",
      "| \n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "DevOps Concerns\n",
      "The concerns of the DevOps team are very familiar and include questions like:\n",
      "\n",
      "• Is the model getting the job done quickly enough?\n",
      "\n",
      "• Is it using a sensible amount of memory and processing time?\n",
      "\n",
      "This is traditional IT performance monitoring, and DevOps teams know how to do\n",
      "this well already. The resource demands of ML models are not so different from tra‐\n",
      "ditional software in this respect.\n",
      "\n",
      "Scalability  of  compute  resources  can  be  an  important  consideration,  for  example,  if\n",
      "you are retraining models in production. Deep learning models have greater resource\n",
      "demands  than  much  simpler  decision  trees.  But  overall,  the  existing  expertise  in\n",
      "DevOps teams for monitoring and managing resources can be readily applied to ML\n",
      "models.\n",
      "\n",
      "Data Scientist Concerns\n",
      "The data scientist is interested in monitoring ML models for a new, more challenging\n",
      "reason:  they  can  degrade  over  time,  since  ML  models  are  effectively  models  of  the\n",
      "data they were trained on. This is not a problem faced by traditional software, but it is\n",
      "inherent to machine learning. ML mathematics builds a concise representation of the\n",
      "important patterns in the training data with the hope that this is a good reflection of\n",
      "the real world. If the training data reflects the real world well, then the model should\n",
      "be accurate and, thus, useful.\n",
      "\n",
      "But the real world doesn’t stand still. The training data used to build a fraud detection\n",
      "model six months ago won’t reflect a new type of fraud that has started to occur in the\n",
      "last  three  months.  If  a  given  website  starts  to  attract  an  increasingly  younger  user\n",
      "base, then a model that generates advertisements is likely to produce less and less rel‐\n",
      "evant adverts. At some point, the performance will become unacceptable, and model\n",
      "retraining  becomes  necessary.  How  soon  models  need  to  be  retrained  depends  on\n",
      "how fast the real world is changing and how accurate the model needs to be, but also,\n",
      "importantly, on how easy it is to build and deploy a better model.\n",
      "\n",
      "But  first,  how  can  data  scientists  tell  a  model’s  performance  is  degrading?  It’s  not\n",
      "always easy. There are two common approaches, one based on ground truth and the\n",
      "other on input drift.\n",
      "\n",
      "Ground truth\n",
      "\n",
      "The  ground  truth,  put  simply,  is  the  correct  answer  to  the  question  that  the  model\n",
      "was asked to solve—for example, “Is this credit card transaction actually fraudulent?”\n",
      "\n",
      "30 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "In knowing the ground truth for all the predictions a model has made, one can judge\n",
      "how well that model is performing.\n",
      "\n",
      "Sometimes ground truth is obtained rapidly after a prediction—for example, in mod‐\n",
      "els that decide which advertisements to display to a user on a web page. The user is\n",
      "likely to click on the advertisements within seconds, or not at all. However, in many\n",
      "use cases, obtaining the ground truth is much slower. If a model predicts that a trans‐\n",
      "action is fraudulent, how can this be confirmed? In some cases, verification may only\n",
      "take a few minutes, such as a phone call placed to the cardholder. But what about the\n",
      "transactions the model thought were OK but actually weren’t? The best hope is that\n",
      "they will be reported by the cardholder when they review their monthly transactions,\n",
      "but this could happen up to a month after the event (or not at all).\n",
      "\n",
      "In the fraud example, ground truth isn’t going to enable data science teams to moni‐\n",
      "tor performance accurately on a daily basis. If the situation requires rapid feedback,\n",
      "then input drift may be a better approach.\n",
      "\n",
      "Input drift\n",
      "\n",
      "Input drift is based on the principle that a model is only going to predict accurately if\n",
      "the data it was trained on is an accurate reflection of the real world. So if a compari‐\n",
      "son  of  recent  requests  to  a  deployed  model  against  the  training  data  shows  distinct\n",
      "differences, then there is a strong likelihood that the model performance is compro‐\n",
      "mised. This is the basis of input drift monitoring. The beauty of this approach is that\n",
      "all the data required for this test already exists, so there is no need to wait for ground\n",
      "truth or any other information.\n",
      "\n",
      "Identifying  drift  is  one  of  the  most  important  components  of  an  adaptable  MLOps\n",
      "strategy, and one that can bring agility to the organization’s enterprise AI efforts over‐\n",
      "all. Chapter 7 will go into more technical depth about data scientists’ concerns when\n",
      "it comes to model monitoring.\n",
      "\n",
      "Business Concerns\n",
      "The business has a holistic outlook on monitoring, and some of its concerns might\n",
      "include questions like:\n",
      "\n",
      "• Is the model delivering value to the enterprise?\n",
      "\n",
      "• Do the benefits of the model outweigh the cost of developing and deploying it?\n",
      "\n",
      "(And how can we measure this?)\n",
      "\n",
      "The  KPIs  identified  for  the  original  business  objective  are  one  part  of  this  process.\n",
      "Where  possible,  these  should  be  monitored  automatically,  but  this  is  rarely  trivial.\n",
      "The  objective  of  reducing  fraud  to  less  than  0.1%  of  transactions  in  our  previous\n",
      "\n",
      "Monitoring \n",
      "\n",
      "| \n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "example is reliant on establishing the ground truth. But even monitoring this doesn’t\n",
      "answer the question: what is the net gain to the business in dollars?\n",
      "\n",
      "This  is  an  age-old  challenge  for  software,  and  with  ever-increasing  expenditure  on\n",
      "ML, the pressure for data scientists to demonstrate value is only going to grow. In the\n",
      "absence  of  a  “dollar-o-meter,”  effectively  monitoring  the  business  KPIs  is  the  best\n",
      "option available (see “Design and Manage Experiments” on page 138). The choice of\n",
      "the baseline is important here and should ideally allow for differentiation of the value\n",
      "of the ML subproject specifically, rather than that of the global project. For example,\n",
      "the  ML  performance  can  be  assessed  with  respect  to  a  rule-based  decision  model\n",
      "based on subject matter expertise to set apart the contribution of decision automation\n",
      "and of ML per se.\n",
      "\n",
      "Iteration and Life Cycle\n",
      "Developing  and  deploying  improved  versions  of  a  model  is  an  essential  part  of  the\n",
      "MLOps  life  cycle,  and  one  of  the  more  challenging.  There  are  various  reasons  to\n",
      "develop a new model version, one of which is model performance degradation due to\n",
      "model  drift,  as  discussed  in  the  prior  section.  Sometimes  there  is  a  need  to  reflect\n",
      "refined business objectives and KPIs, and other times, it’s just that the data scientists\n",
      "have come up with a better way to design the model.\n",
      "\n",
      "Iteration\n",
      "In  some  fast-moving  business  environments,  new  training  data  becomes  available\n",
      "every  day.  Daily  retraining  and  redeployment  of  the  model  are  often  automated  to\n",
      "ensure that the model reflects recent experience as closely as possible.\n",
      "\n",
      "Retraining an existing model with the latest training data is the simplest scenario for\n",
      "iterating a new model version. But while there are no changes to feature selection or\n",
      "algorithm, there are still plenty of pitfalls. In particular:\n",
      "\n",
      "• Does  the  new  training  data  look  as  expected?  Automated  validation  of  the  new\n",
      "\n",
      "data through predefined metrics and checks is essential.\n",
      "\n",
      "• Is the data complete and consistent?\n",
      "\n",
      "• Are the distributions of features broadly similar to those in the previous training\n",
      "\n",
      "set? Remember that the goal is to refine the model, not radically change it.\n",
      "\n",
      "With a new model version built, the next step is to compare the metrics with the cur‐\n",
      "rent live model version. Doing so requires evaluating both models on the same devel‐\n",
      "opment  dataset,  whether  it  be  the  previous  or  latest  version.  If  metrics  and  checks\n",
      "suggest a wide variation between the models, automated scripts should not be rede‐\n",
      "ployed, and manual intervention should be sought.\n",
      "\n",
      "32 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "Even in the “simple” automated retraining scenario with new training data, there is a\n",
      "need  for  multiple  development  datasets  based  on  scoring  data  reconciliation  (with\n",
      "ground truth when it becomes available), data cleaning and validation, the previous\n",
      "model version, and a set of carefully considered checks. Retraining in other scenarios\n",
      "is likely to be even more complicated, rendering automated redeployment unlikely.\n",
      "\n",
      "As  an  example,  consider  retraining  motivated  by  the  detection  of  significant  input\n",
      "drift. How can the model be improved? If new training data is available, then retrain‐\n",
      "ing with this data is the action with the highest cost-benefit ratio, and it may suffice.\n",
      "However,  in  environments  where  it’s  slow  to  obtain  the  ground  truth,  there  may  be\n",
      "little new labeled data.\n",
      "\n",
      "This case requires direct invention from data scientists who need to understand the\n",
      "cause  of  the  drift  and  work  out  how  the  existing  training  data  could  be  adjusted  to\n",
      "more  accurately  reflect  the  latest  input  data.  Evaluating  a  model  generated  by  such\n",
      "changes is difficult. The data scientist has to spend time assessing the situation—time\n",
      "that  increases  with  the  amount  of  modeling  debt—as  well  as  estimate  the  potential\n",
      "impact  on  performance  and  design  custom  mitigation  measures.  For  example,\n",
      "removing a specific feature or sampling the existing rows of training data may lead to\n",
      "a better-tuned model.\n",
      "\n",
      "The Feedback Loop\n",
      "In large enterprises, DevOps best practices typically dictate that the live model scor‐\n",
      "ing environment and the model retraining environment are distinct. As a result, the\n",
      "evaluation of a new model version on the retraining environment is likely to be com‐\n",
      "promised.\n",
      "\n",
      "One approach to mitigating this uncertainty is shadow testing, where the new model\n",
      "version  is  deployed  into  the  live  environment  alongside  the  existing  model.  All  live\n",
      "scoring  is  handled  by  the  incumbent  model  version,  but  each  new  request  is  then\n",
      "scored again by the new model version and the results logged, but not returned to the\n",
      "requestor. Once sufficient requests have been scored by both versions, the results can\n",
      "be  compared  statistically.  Shadow  scoring  also  gives  more  visibility  to  the  SMEs  on\n",
      "the future versions of the model and may thus allow for a smoother transition.\n",
      "\n",
      "In the advertisement generation model previously discussed, it is impossible to tell if\n",
      "the  ads  selected  by  the  model  are  good  or  bad  without  allowing  the  end  user  the\n",
      "chance to click on them. In this use case, shadow testing has limited benefits, and A/B\n",
      "testing is more common.\n",
      "\n",
      "In  A/B  testing,  both  models  are  deployed  into  the  live  environment,  but  input\n",
      "requests  are  split  between  the  two  models.  Each  request  is  processed  by  one  or  the\n",
      "other model, not both. Results from the two models are logged for analysis (but never\n",
      "\n",
      "Iteration and Life Cycle \n",
      "\n",
      "| \n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "for the same request). Drawing statistically meaningful conclusions from an A/B test\n",
      "requires careful planning of the test.\n",
      "\n",
      "Chapter 7 will cover the how-to of A/B testing in more detail, but as a preview, the\n",
      "simplest form of A/B testing is often referred to as a fixed-horizon test. That’s because\n",
      "in the search for a statistically meaningful conclusion, one has to wait until the care‐\n",
      "fully  predetermined  number  of  samples  have  been  tested.  “Peeking”  at  the  result\n",
      "before the test is finished is unreliable. However, if the test is running live in a com‐\n",
      "mercial environment, every bad prediction is likely to cost money, so not being able\n",
      "to stop a test early could be expensive.\n",
      "\n",
      "Bayesian,  and  in  particular  multi-armed  bandit,  tests  are  an  increasingly  popular\n",
      "alternative  to  the  “frequentist”  fixed-horizon  test,  with  the  aim  of  drawing  conclu‐\n",
      "sions more quickly. Multi-armed bandit testing is adaptive: the algorithm that decides\n",
      "the split between models adapts according to live results and reduces the workload of\n",
      "underperforming models. While multi-armed bandit testing is more complex, it can\n",
      "reduce the business cost of sending traffic to a poorly performing model.\n",
      "\n",
      "Iterating on the Edge\n",
      "Iterating on an ML model deployed to millions of devices, such as smartphones, sen‐\n",
      "sors, or cars, presents different challenges to iteration in a corporate IT environment.\n",
      "One  approach  is  to  relay  all  the  feedback  from  the  millions  of  model  instances  to  a\n",
      "central point and perform training centrally. Tesla’s autopilot system, running in more\n",
      "than 500,000 cars, does exactly this. Full retraining of their 50 or so neural networks\n",
      "takes 70,000 GPU hours.\n",
      "\n",
      "Google  has  taken  a  different  approach  with  its  smartphone  keyboard  software,\n",
      "GBoard.  Instead  of  centralized  retraining,  every  smartphone  retrains  the  model\n",
      "locally and sends a summary of the improvements it has found to Google centrally.\n",
      "These improvements from every device are averaged and the shared model updated. \n",
      "This  federated  learning  approach  means  that  an  individual  user’s  personal  data\n",
      "doesn’t need to be collected centrally, the improved model on each phone can be used\n",
      "immediately, and the overall power consumption goes down. \n",
      "\n",
      "Governance\n",
      "Governance is the set of controls placed on a business to ensure that it delivers on its\n",
      "responsibilities  to  all  stakeholders,  from  shareholders  and  employees  to  the  public\n",
      "and national governments. These responsibilities include financial, legal, and ethical\n",
      "obligations. Underpinning all three of these is the fundamental principle of fairness.\n",
      "\n",
      "Legal obligations are the easiest to understand. Businesses were constrained by regu‐\n",
      "lations long before the advent of machine learning. Many regulations target specific\n",
      "\n",
      "34 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "industries;  for  example,  financial  regulations  aim  to  protect  the  public  and  wider\n",
      "economy  from  finance  mismanagement  and  fraud,  while  pharmaceutical  industries\n",
      "must  comply  with  rules  to  safeguard  the  health  of  the  public.  Business  practice  is\n",
      "impacted by broader legislation to protect vulnerable sectors of society and to ensure\n",
      "a level playing field on criteria such as sex, race, age, or religion.\n",
      "\n",
      "Recently, governments across the world have imposed regulations to protect the pub‐\n",
      "lic from the impact of the use of personal data by businesses.  The 2016 EU General\n",
      "Data Protection Regulation (GDPR) and the 2018 California Consumer Privacy Act\n",
      "(CCPA) typify this trend, and their impact on ML—with its total dependency on data\n",
      "—has  been  immense.  For  example,  GDPR  attempts  to  protect  personal  data  from\n",
      "industrial  misuse  with  a  goal  of  limiting  the  potential  discrimination  against\n",
      "individuals.\n",
      "\n",
      "GDPR Principles\n",
      "The GDPR sets out principles for the processing of personal data, and it’s worth not‐\n",
      "ing that the CCPA was built to closely mirror its principles, though it does have some\n",
      "significant differences.2 Processing includes the collection, storage, alteration, and use\n",
      "of personal data. These principles are:\n",
      "\n",
      "• Lawfulness, fairness, and transparency\n",
      "\n",
      "• Purpose limitation\n",
      "\n",
      "• Data minimization\n",
      "\n",
      "• Accuracy\n",
      "\n",
      "• Storage limitation\n",
      "\n",
      "• Integrity and confidentiality (security)\n",
      "\n",
      "• Accountability\n",
      "\n",
      "Governments are now starting to turn their regulatory eye to ML specifically, hoping\n",
      "to  mitigate  the  negative  impact  of  its  use.  The  European  Union  is  leading  the  way\n",
      "with planned legislation to define the acceptable uses of various forms of AI. This is\n",
      "not necessarily about reducing use; for example, it may enable beneficial applications\n",
      "of facial recognition technology that are currently restricted by data privacy regula‐\n",
      "tions. But what is clear is that businesses will have to take heed of yet more regulation\n",
      "when applying ML.\n",
      "\n",
      "Do businesses care about moral responsibilities to society, beyond formal legislation?\n",
      "Increasingly, the answer is yes, as seen in the current development of environmental,\n",
      "\n",
      "2 Delve into the differences between GDPR and CCPA.\n",
      "\n",
      "Governance \n",
      "\n",
      "| \n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "social,  and  governance  (ESG)  performance  indicators.  Trust  matters  to  consumers,\n",
      "and a lack of trust is bad for business. With increasing public activism on the subject,\n",
      "businesses  are  engaging  with  ideas  of  Responsible  AI,  the  ethical,  transparent,  and\n",
      "accountable application of AI technology. Trust matters to shareholders, too, and full\n",
      "disclosure of ML risks is on its way.\n",
      "\n",
      "Applying good governance to MLOPs is challenging. The processes are complex, the\n",
      "technology is opaque, and the dependence on data is fundamental. Governance ini‐\n",
      "tiatives in MLOps broadly fall into one of two categories:\n",
      "\n",
      "Data governance\n",
      "\n",
      "A framework for ensuring appropriate use and management of data\n",
      "\n",
      "Process governance\n",
      "\n",
      "The  use  of  well-defined  processes  to  ensure  all  governance  considerations  have\n",
      "been addressed at the correct point in the life cycle of the model and that a full\n",
      "and accurate record has been kept\n",
      "\n",
      "Data Governance\n",
      "Data  governance  concerns  itself  with  the  data  being  used,  especially  that  for  model\n",
      "training, and it can address questions like:\n",
      "\n",
      "• What is the data’s provenance?\n",
      "\n",
      "• How was the original data collected and under what terms of use?\n",
      "\n",
      "• Is the data accurate and up to date?\n",
      "\n",
      "• Is there PII or other forms of sensitive data that should not be used?\n",
      "\n",
      "ML projects usually involve significant pipelines, consisting of data cleaning, combi‐\n",
      "nation,  and  transformation  steps.  Understanding  the  data  lineage  is  complex,  espe‐\n",
      "cially  at  the  feature  level,  but  it  is  essential  for  compliance  with  GDPR-style\n",
      "regulations. How can teams—and more broadly organizations, as it matters at the top\n",
      "as well—be sure that no PII is used to train a given model? Anonymizing or pseudo-\n",
      "anonymizing data is not always a sufficient solution to managing personal informa‐\n",
      "tion.  If  these  processes  are  not  performed  correctly,  it  can  still  be  possible  to  single\n",
      "out an individual and their data, contrary to the requirements of GDPR.3\n",
      "\n",
      "Inappropriate biases in models can arise quite accidentally despite the best intentions\n",
      "of data scientists. An ML recruitment model famously discriminated against women\n",
      "by identifying that certain schools—all-female schools—were less well represented in\n",
      "\n",
      "3 For more on anonymization, pseudo-anonymization, and why they don’t solve all data privacy woes, we rec‐\n",
      "\n",
      "ommend Executing Data Privacy-Compliant Data Projects by Dataiku.\n",
      "\n",
      "36 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "the company’s upper management, which reflected the historical dominance of men\n",
      "in  the  organization.4  The  point  is  that  making  predictions  based  on  experience  is  a\n",
      "powerful  technique,  but  sometimes  the  consequences  are  not  only  counter-\n",
      "productive, but illegal.\n",
      "\n",
      "Data  governance  tools  that  can  address  these  problems  are  in  their  infancy.  Most\n",
      "focus on answering these two questions about data lineage:\n",
      "\n",
      "• Where did the information in this dataset come from, and what does this tell me\n",
      "\n",
      "about how I can use it?\n",
      "\n",
      "• How is this dataset used, and if I change it in some way, what are the implications\n",
      "\n",
      "downstream?\n",
      "\n",
      "Neither question is easy to answer fully and accurately in real-world data preparation\n",
      "pipelines. For example, if a data scientist writes a Python function to in-memory pro‐\n",
      "cess several input datasets and output a single dataset, how can one be sure from what\n",
      "information each cell of the new dataset was derived?\n",
      "\n",
      "Process Governance\n",
      "Process governance focuses on formalizing the steps in the MLOps process and asso‐\n",
      "ciating actions with them. Typically these actions are reviews, sign-offs, and the cap‐\n",
      "ture of supporting materials, such as documentation. The aim is twofold:\n",
      "\n",
      "• To ensure every governance-related consideration is made at the correct time and\n",
      "correctly acted upon. For example, models should not be deployed to production\n",
      "until all validation checks have been passed.\n",
      "\n",
      "• To  enable  oversight  from  outside  of  the  strict  MLOps  process.  Auditors,  risk\n",
      "managers, compliance officers, and the business as a whole all have an interest in\n",
      "being able to track progress and review decisions at a later stage.\n",
      "\n",
      "Effective implementation of process governance is hard:\n",
      "\n",
      "• Formal  processes  for  the  ML  life  cycle  are  rarely  easy  to  define  accurately.  The\n",
      "understanding of the complete process is usually spread across the many teams\n",
      "involved,  often  with  no  one  person  having  a  detailed  understanding  of  it  as  a\n",
      "whole.\n",
      "\n",
      "• For the process to be applied successfully, every team must be willing to adopt it\n",
      "\n",
      "wholeheartedly.\n",
      "\n",
      "4 In 2018, Amazon famously scrapped an AI recruiting tool because of its bias against women.\n",
      "\n",
      "Governance \n",
      "\n",
      "| \n",
      "\n",
      "37\n",
      "\n",
      "\f",
      "• If  the  process  is  just  too  heavy-weight  for  some  use-cases,  teams  will  certainly\n",
      "\n",
      "subvert it, and much of the benefit will be lost.\n",
      "\n",
      "Today,  process  governance  is  most  commonly  found  in  organizations  with  a  tradi‐\n",
      "tionally  heavy  burden  of  regulation  and  compliance,  such  as  finance.  Outside  of\n",
      "these,  it  is  rare.  With  ML  creeping  into  all  spheres  of  commercial  activity  and  with\n",
      "rising  concern  about  Responsible  AI,  we  will  need  new  and  innovative  solutions  to\n",
      "this problem that can work for all businesses. \n",
      "\n",
      "Closing Thoughts\n",
      "Given  this  overview  of  features  required  for  and  processes  affected  by  MLOps,  it’s\n",
      "clearly not something data teams—or even the data-driven organization at large—can\n",
      "ignore.  Nor  is  it  an  item  to  check  off  of  a  list  (“yes,  we  do  MLOps!”),  but  rather  a\n",
      "complex  interplay  between  technologies,  processes,  and  people  that  requires  disci‐\n",
      "pline and time to do correctly.\n",
      "\n",
      "The following chapters go deeper into each of the ML model life cycle components at\n",
      "play in MLOps, providing a look at how each should be done to get closer to the ideal\n",
      "MLOps implementation.\n",
      "\n",
      "38 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Key MLOps Features\n",
      "\n",
      "\f",
      "PART II\n",
      "MLOps: How\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 4\n",
      "Developing Models\n",
      "\n",
      "Adrien Lavoillotte\n",
      "\n",
      "Anyone who wants to be serious about MLOps needs to have at least a cursory under‐\n",
      "standing  of  the  model  development  process,  which  is  presented  in  Figure  4-1  as  an\n",
      "element  of  the  larger  ML  project  life  cycle.  Depending  on  the  situation,  the  model\n",
      "development process can range from quite simple to extremely complex, and it dic‐\n",
      "tates the constraints of subsequent usage, monitoring, and maintenance of models.\n",
      "\n",
      "Figure 4-1. Model development highlighted in the larger context of the ML project life\n",
      "cycle\n",
      "\n",
      "The implications of the data collection process on the rest of the model’s life is quite\n",
      "straightforward, and one easily sees how a model can become stale. For other parts of\n",
      "the model, the effects may be less obvious.\n",
      "\n",
      "For  example,  take  feature  creation,  where  feeding  a  date  to  the  model  versus  a  flag\n",
      "indicating whether the day is a public holiday may make a big difference in perfor‐\n",
      "mance, but also comes with significantly different constraints on updating the model.\n",
      "Or consider how the metrics used for evaluating and comparing models may enable\n",
      "\n",
      "41\n",
      "\n",
      "\f",
      "automatic switching to the best possible version down the line, should the situation\n",
      "require it.\n",
      "\n",
      "This  chapter  therefore  covers  the  basics  of  model  development,  specifically  in  the\n",
      "context  of  MLOps,  that  is,  how  models  might  be  built  and  developed  in  ways  that\n",
      "make MLOps considerations easier to implement down the line.\n",
      "\n",
      "What Is a Machine Learning Model?\n",
      "Machine learning models are leveraged both in academia and in the real world (i.e.,\n",
      "business contexts), so it’s important to distinguish what they represent in theory ver‐\n",
      "sus how they are implemented in practice. Let’s dive into both, building on what we’ve\n",
      "already seen in Chapter 3.\n",
      "\n",
      "In Theory\n",
      "A machine learning model is a projection of reality; that is, it’s a partial and approxi‐\n",
      "mate  representation  of  some  aspect  (or  aspects)  of  a  real  thing  or  process.  Which\n",
      "aspects  are  represented  often  depends  on  what  is  available  and  useful.  A  machine\n",
      "learning model, once trained, boils down a mathematical formula that yields a result\n",
      "when fed some inputs—say, a probability estimation of some event happening or the\n",
      "estimated value of a raw number.\n",
      "\n",
      "Machine learning models are based on statistical theory, and machine learning algo‐\n",
      "rithms are the tools that build models from training data. Their goal is to find a syn‐\n",
      "thetic representation of the data they are fed, and this data represents the world as it\n",
      "was  at  the  time  of  collection.  Therefore,  machine  learning  models  can  be  used  to\n",
      "make predictions when the future looks like the past, because their synthetic repre‐\n",
      "sentation is still valid.\n",
      "\n",
      "Generalization Capacity\n",
      "Machine  learning  models’  ability  to  accurately  predict  for  cases  that  are  not  exactly\n",
      "like the input data is called their generalization capacity. Even when they yield outputs\n",
      "like horses with zebra stripes1 that do not exist in training datasets, they do it by mod‐\n",
      "eling a probability distribution that allows them to have this kind of surprising gener‐\n",
      "alization capacity.\n",
      "\n",
      "1 CycleGAN is the implementation of recent research by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\n",
      "\n",
      "A. Efros.\n",
      "\n",
      "42 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "An often-used example for how machine learning models can predict and generalize\n",
      "is the price of a house. Of course, the selling price of a house will depend on too many\n",
      "factors too complex to model precisely, but getting close enough to be useful is not so\n",
      "difficult. The input data for that model may be things inherent to the house like sur‐\n",
      "face  area,  number  of  bedrooms  and  bathrooms,  year  of  construction,  location,  etc.,\n",
      "but also other more contextual information like the state of the housing market at the\n",
      "time of sale, whether the seller is in a hurry, and so on. With complete enough histor‐\n",
      "ical data, and provided the market conditions do not change too much, an algorithm\n",
      "can compute a formula that provides a reasonable estimate.\n",
      "\n",
      "Another  frequent  example  is  a  health  diagnosis  or  prediction  that  someone  will\n",
      "develop a certain disease within a given timeframe. This kind of classification model\n",
      "often  outputs  the  probability  of  some  event,  sometimes  also  with  a  confidence\n",
      "interval.\n",
      "\n",
      "In Practice\n",
      "A model is the set of parameters necessary to rebuild and apply the formula. It is usu‐\n",
      "ally  stateless  and  deterministic  (i.e.,  the  same  inputs  always  give  the  same  outputs,\n",
      "with some exceptions; see “Online Learning” on page 88).\n",
      "\n",
      "This  includes  the  parameters  of  the  end  formula  itself,  but  it  also  includes  all  the\n",
      "transformations  to  go  from  the  input  data  that  will  be  fed  to  the  model  to  the  end\n",
      "formula that will yield a value plus the possible derived data (like a classification or a\n",
      "decision).  Given  this  description  in  practice,  it  usually  does  not  make  a  difference\n",
      "whether the model is ML-based or not: it is just a computable mathematical function\n",
      "applied to the input data, one row at a time.\n",
      "\n",
      "In the house price case, for instance, it may not be practical to gather enough pricing\n",
      "data for every zip code to get a model that’s accurate enough in all target locations.\n",
      "Instead,  maybe  the  zip  codes  will  be  replaced  with  some  derived  inputs  that  are\n",
      "deemed  to  have  the  most  influence  on  price—say,  average  income,  population  den‐\n",
      "sity,  or  proximity  to  some  amenities.  But  since  end  users  will  continue  to  input  the\n",
      "zip code and not these derived inputs, for all intents and purposes, all of this transfor‐\n",
      "mation is also part of the pricing model.\n",
      "\n",
      "Outputs  can  also  be  richer  than  a  single  number.  A  system  that  detects  fraud,  for\n",
      "example, will often provide some kind of probability (and in some cases maybe also a\n",
      "confidence interval) rather than a binary answer. Depending on the acceptability of\n",
      "fraud and the cost of subsequent verification or denial of the transaction, it may be set\n",
      "up  to  only  classify  fraudulent  instances  where  the  probability  reaches  some  fine-\n",
      "tuned threshold. Some models even include recommendations or decisions, such as\n",
      "which product to show a visitor to maximize spending or which treatment provides\n",
      "the most probable recovery.\n",
      "\n",
      "What Is a Machine Learning Model? \n",
      "\n",
      "| \n",
      "\n",
      "43\n",
      "\n",
      "\f",
      "All of these transformations and associated data are part of the model to some degree;\n",
      "however, this does not mean they are always bundled in a monolithic package, as one\n",
      "single  artifact  compiled  together.  This  could  quickly  get  unwieldy,  and,  in  addition,\n",
      "some parts of this information come with varying constraints (different refresh rates,\n",
      "external sources, etc.).\n",
      "\n",
      "Required Components\n",
      "Building  a  machine  learning  model  requires  many  components  as  outlined  in\n",
      "Table 4-1.\n",
      "\n",
      "Table 4-1. Required components of a machine learning model\n",
      "\n",
      "ML component\n",
      "Training data\n",
      "\n",
      "A performance\n",
      "metric\n",
      "\n",
      "ML algorithm\n",
      "\n",
      "Hyperparameters\n",
      "\n",
      "Description\n",
      "Training data is usually labeled for the prediction case with examples of what is being modeled\n",
      "(supervised learning). It might sound obvious, but it’s important to have good training data. An\n",
      "illustrative example of when this was not the case was data from damaged planes during World War II,\n",
      "which suffered from survivor bias and therefore was not good training data.\n",
      "A performance metric is what the model being developed seeks to optimize. It should be chosen\n",
      "carefully to avoid unintended consequences, like the cobra effect (named for a famous anecdote, where\n",
      "a reward for dead cobras led some to breed them). For example, if 95% of the data has class A,\n",
      "optimizing for raw accuracy may produce a model that always predicts A and is 95% accurate.\n",
      "There are a variety of models that work in various ways and have different pros and cons. It is\n",
      "important to note that some algorithms are more suited to certain tasks than others, but their selection\n",
      "also depends on what needs to be prioritized: performance, stability, interpretability, computation cost,\n",
      "etc.\n",
      "Hyperparameters are configurations for ML algorithms. The algorithm contains the basic formula, the\n",
      "parameters it learns are the operations and operands that make up this formula for that particular\n",
      "prediction task, and the hyperparameters are the ways that the algorithm may go to find these\n",
      "parameters.\n",
      "For example, in a decision tree (where data continues to be split in two according to what looks to be\n",
      "the best predictor in the subset that reached this path), one hyperparameter is the depth of the tree\n",
      "(i.e., the number of splits).\n",
      "\n",
      "Evaluation dataset When using labeled data, an evaluation dataset that is different from the training set will be required\n",
      "\n",
      "to evaluate how the model performs on unseen data (i.e., how well it can generalize).\n",
      "\n",
      "The sheer number and complexity of each individual component is part of what can\n",
      "make good MLOps a challenging undertaking. But the complexity doesn’t stop here,\n",
      "as algorithm choice is yet another piece of the puzzle.\n",
      "\n",
      "44 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "Different ML Algorithms, Different MLOps Challenges\n",
      "What ML algorithms all have in common is that they model patterns in past data to\n",
      "make inferences, and the quality and relevance of this experience are the key factors\n",
      "in  their  effectiveness.  Where  they  differ  is  that  each  style  of  algorithm  has  specific\n",
      "characteristics and presents different challenges in MLOps (outlined in Table 4-2).\n",
      "\n",
      "Table 4-2. MLOps considerations by algorithm type\n",
      "\n",
      "Algorithm\n",
      "type\n",
      "Linear\n",
      "\n",
      "Tree-based\n",
      "\n",
      "Deep learning\n",
      "\n",
      "Name\n",
      "\n",
      "MLOps considerations\n",
      "\n",
      "Linear\n",
      "regression\n",
      "Logistic\n",
      "regression\n",
      "Decision tree\n",
      "\n",
      "Random forest\n",
      "\n",
      "Gradient\n",
      "boosting\n",
      "Neural\n",
      "networks\n",
      "\n",
      "There is a tendency for overfitting.\n",
      "\n",
      "There is a tendency for overfitting.\n",
      "\n",
      "Can be unstable—small changes in data can lead to a large change in the structure of the\n",
      "optimal decision tree.\n",
      "Predictions can be difficult to understand, which is challenging from a Responsible AI\n",
      "perspective. Random forest models can also be relatively slow to output predictions, which\n",
      "can present challenges for applications.\n",
      "Like random forest, predictions can be difficult to understand. Also, a small change in the\n",
      "feature or training set can create radical changes in the model.\n",
      "In terms of interpretability, deep learning models are almost impossible to understand.\n",
      "Deep learning algorithms, including neural networks, are also extremely slow to train and\n",
      "require a lot of power (and data). Is it worth the resources, or would a simpler model work\n",
      "just as well?\n",
      "\n",
      "Some ML algorithms can best support specific use cases, but governance considera‐\n",
      "tions may also play a part in the choice of algorithm. In particular, highly regulated\n",
      "environments where decisions must be explained (e.g., financial services) cannot use\n",
      "opaque algorithms such as neural networks; rather, they have to favor simpler techni‐\n",
      "ques, such as decision trees. In many use cases, it’s not so much a trade-off on perfor‐\n",
      "mance but rather a trade-off on cost. That is, simpler techniques usually require more\n",
      "costly  manual  feature  engineering  to  reach  the  same  level  of  performance  as  more\n",
      "complex techniques.\n",
      "\n",
      "Computing Power\n",
      "When talking about components of machine learning model development, it’s impos‐\n",
      "sible to ignore computing power. Some say planes fly thanks to human ingenuity, but\n",
      "it’s also thanks to a lot of fuel. This holds true with machine learning as well: its devel‐\n",
      "opment is inversely proportional to the cost of computing power.\n",
      "\n",
      "From hand-computed linear regression of the early twentieth century to today’s larg‐\n",
      "est deep learning models, new algorithms arose when the required computing power\n",
      "\n",
      "What Is a Machine Learning Model? \n",
      "\n",
      "| \n",
      "\n",
      "45\n",
      "\n",
      " \n",
      "\f",
      "became available. For example, mainstream algorithms like random forest and gradi‐\n",
      "ent boosting both require a computing power that was expensive 20 years ago.\n",
      "\n",
      "In exchange, they brought an ease of use that considerably lowered the cost of devel‐\n",
      "oping ML models, thus putting new use cases within the reach of the average organi‐\n",
      "zation. The decrease in the cost of data also helped, but it was not the first driver: very\n",
      "few algorithms leverage big data technology in which both data and computation are\n",
      "distributed over a large number of computers; rather, most of them still operate with\n",
      "all the training data in memory.\n",
      "\n",
      "Data Exploration\n",
      "When data scientists or analysts consider data sources to train a model, they need to\n",
      "first  get  a  grasp  of  what  that  data  looks  like.  Even  a  model  trained  using  the  most\n",
      "effective  algorithm  is  only  as  good  as  its  training  data.  At  this  stage,  a  number  of\n",
      "issues can prevent all or part of the data from being useful, including incompleteness,\n",
      "inaccuracy, inconsistency, etc.\n",
      "\n",
      "Examples of such processes can include:\n",
      "\n",
      "• Documenting  how  the  data  was  collected  and  what  assumptions  were  already\n",
      "\n",
      "made\n",
      "\n",
      "• Looking  at  summarizing  statistics  of  the  data:  What  is  the  domain  of  each  col‐\n",
      "umn? Are there some rows with missing values? Obvious mistakes? Strange out‐\n",
      "liers? No outliers at all?\n",
      "\n",
      "• Taking a closer look at the distribution of the data\n",
      "\n",
      "• Cleaning, filling, reshaping, filtering, clipping, sampling, etc.\n",
      "\n",
      "• Checking correlations between the different columns, running statistical tests on\n",
      "\n",
      "some subpopulations, fitting distribution curves\n",
      "\n",
      "• Comparing that data to other data or models in the literature: Is there some usual\n",
      "\n",
      "information that seems to be missing? Is this data comparably distributed?\n",
      "\n",
      "Of  course,  domain  knowledge  is  required  to  make  informed  decisions  during  this\n",
      "exploration. Some oddities may be hard to spot without specific insight, and assump‐\n",
      "tions made can have consequences that are not obvious to the untrained eye. Indus‐\n",
      "trial  sensor  data  is  a  good  example:  unless  the  data  scientist  is  also  a  mechanical\n",
      "engineer or expert in the equipment, they might not know what constitutes normal\n",
      "versus strange outliers for a particular machine.\n",
      "\n",
      "46 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "Feature Engineering and Selection\n",
      "Features are how data is presented to a model, serving to inform that model on things\n",
      "it  may  not  infer  by  itself.  This  table  provides  examples  of  how  features  may  be\n",
      "engineered:\n",
      "\n",
      "Feature\n",
      "engineering\n",
      "category\n",
      "Derivatives\n",
      "Enrichment\n",
      "Encoding\n",
      "Combination\n",
      "\n",
      "Description\n",
      "\n",
      "Infer new information from existing information—e.g., what day of the week is this date?\n",
      "Add new external information—e.g., is this day a public holiday?\n",
      "Present the same information differently—e.g., day of the week or weekday versus weekend.\n",
      "Link features together—e.g., the size of the backlog might need to be weighted by the complexity of\n",
      "the different items in it.\n",
      "\n",
      "For instance, in trying to estimate the potential duration of a business process given\n",
      "the current backlog, if one of the inputs is a date, it is pretty common to derive the\n",
      "corresponding day of the week or how far ahead the next public holiday is from that\n",
      "date.  If  the  business  serves  multiple  locations  that  observe  different  business  calen‐\n",
      "dars, that information may also be important.\n",
      "\n",
      "Another example, to follow up on the house pricing scenario from the previous sec‐\n",
      "tion, would be using average income and population density, which ideally allows the\n",
      "model to better generalize and train on more diverse data than trying to segment by\n",
      "area (i.e., zip code).\n",
      "\n",
      "Feature Engineering Techniques\n",
      "A whole market exists for such complementary data that extends far beyond the open\n",
      "data  that  public  institutions  and  companies  share.  Some  services  provide  direct\n",
      "enrichment that can save a lot of time and effort.\n",
      "\n",
      "There are, however, many cases when information that data scientists need for their\n",
      "models is not available. In this case, there are techniques like impact coding, whereby\n",
      "data scientists replace a modality by the average value of the target for that modality,\n",
      "thus allowing the model to benefit from data in a similar range (at the cost of some\n",
      "information loss).\n",
      "\n",
      "Ultimately, most ML algorithms require a table of numbers as input, each row repre‐\n",
      "senting a sample, and all samples coming from the same dataset. When the input data\n",
      "is not tabular, data scientists can use other tricks to transform it.\n",
      "\n",
      "The most common one is one-hot encoding. For example, a feature that can take three\n",
      "values (e.g., Raspberry, Blueberry, and Strawberry) is transformed into three features\n",
      "\n",
      "Feature Engineering and Selection \n",
      "\n",
      "| \n",
      "\n",
      "47\n",
      "\n",
      "\f",
      "that  can  take  only  two  values—yes  or  no  (e.g.,  Raspberry  yes/no,  Blueberry  yes/no,\n",
      "Strawberry yes/no).\n",
      "\n",
      "Text  or  image  inputs,  on  the  other  hand,  require  more  complex  engineering.  Deep\n",
      "learning  has  recently  revolutionized  this  field  by  providing  models  that  transform\n",
      "images and text into tables of numbers that are usable by ML algorithms. These tables\n",
      "are  called  embeddings,  and  they  allow  data  scientists  to  perform  transfer  learning\n",
      "because they can be used in domains on which they were not trained.\n",
      "\n",
      "Transfer Learning\n",
      "Transfer  learning  is  the  technique  of  using  information  gained  from  solving  one\n",
      "problem in solving a different problem. Transfer learning can be used to significantly\n",
      "accelerate learning of second or subsequent tasks, and it is very popular in deep learn‐\n",
      "ing, where the resources needed to train models can be enormous.\n",
      "\n",
      "For example, even if a particular deep learning model was trained on images that did\n",
      "not contain any forks, it may give a useful embedding to be used by a model that is\n",
      "trained  to  detect  them,  because  a  fork  is  an  object,  and  that  model  was  trained  to\n",
      "detect similar human-made objects.\n",
      "\n",
      "How Feature Selection Impacts MLOps Strategy\n",
      "When it comes to feature creation and selection, the question of how much and when\n",
      "to  stop  comes  up  regularly.  Adding  more  features  may  produce  a  more  accurate\n",
      "model, achieve more fairness when splitting into more precise groups, or compensate\n",
      "for some other useful missing information. However, it also comes with downsides,\n",
      "all of which can have a significant impact on MLOps strategies down the line:\n",
      "\n",
      "• The model can become more and more expensive to compute.\n",
      "\n",
      "• More features require more inputs and more maintenance down the line.\n",
      "\n",
      "• More features mean a loss of some stability.\n",
      "\n",
      "• The sheer number of features can raise privacy concerns.\n",
      "\n",
      "Automated  feature  selection  can  help  by  using  heuristics  to  estimate  how  critical\n",
      "some features will be for the predictive performance of the model. For instance, one\n",
      "can look at the correlation with the target variable or quickly train a simple model on\n",
      "a representative subset of the data and then look at which features are the strongest\n",
      "predictors.\n",
      "\n",
      "Which inputs to use, how to encode them, how they interact or interfere with each\n",
      "other—such  decisions  require  a  certain  understanding  of  the  inner  workings  of  the\n",
      "ML algorithm. The good news is that some of this can be partly automated, e.g., by\n",
      "\n",
      "48 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "using tools such as Auto-sklearn or AutoML applications that cross-reference features\n",
      "against  a  given  target  to  estimate  which  features,  derivatives,  or  combinations  are\n",
      "likely  to  yield  the  best  results,  leaving  out  all  the  features  that  would  probably  not\n",
      "make that much of a difference.\n",
      "\n",
      "Other  choices  still  require  human  intervention,  such  as  deciding  whether  to  try  to\n",
      "collect additional information that might improve the model. Spending time to build\n",
      "business-friendly  features  will  often  improve  the  final  performance  and  ease  the\n",
      "adoption  by  end  users,  as  model  explanations  are  likely  to  be  simpler.  It  can  also\n",
      "reduce  modeling  debt,  allowing  data  scientists  to  understand  the  main  prediction\n",
      "drivers  and  ensure  that  they  are  robust.  Of  course,  there  are  trade-offs  to  consider\n",
      "between  the  cost  of  time  spent  to  understand  the  model  and  the  expected  value,  as\n",
      "well as risks associated with the model’s use.\n",
      "\n",
      "Feature Stores\n",
      "Feature  factories,  or  feature  stores,  are  repositories  of  different  features  associated\n",
      "with business entities that are created and stored in a central location for easier reuse.\n",
      "They usually combine an offline part (slower, but potentially more powerful) and an\n",
      "online part (quicker and more useful for real-time needs), making sure they remain\n",
      "consistent with each other.\n",
      "\n",
      "Given  how  time-consuming  feature  engineering  is  for  data  scientists,  feature  stores\n",
      "have huge potential to free up their time for even more valuable tasks. Machine learn‐\n",
      "ing  is  still  often  the  “high-interest  credit  card  of  technical  debt”.  Reversing  this  will\n",
      "require huge efficiency gains in the data-to-model-to-production process and in the\n",
      "MLOps process, and feature stores are one way to get there.\n",
      "\n",
      "The bottom line is that when building models, the process of engineering and select‐\n",
      "ing  features,  like  many  other  ML  model  components,  is  a  delicate  balance  between\n",
      "considering MLOps components and performance.\n",
      "\n",
      "Experimentation\n",
      "Experimentation takes place throughout the entire model development process, and\n",
      "usually every important decision or assumption comes with at least some experiment\n",
      "or previous research to justify it. Experimentation can take many shapes, from build‐\n",
      "ing full-fledged predictive ML models to doing statistical tests or charting data. Goals\n",
      "of experimentation include:\n",
      "\n",
      "• Assessing  how  useful  or  how  good  of  a  model  can  be  built  given  the  elements\n",
      "outlined in Table 4-1. (The next section will cover model evaluation and compar‐\n",
      "ison in more detail.)\n",
      "\n",
      "Experimentation \n",
      "\n",
      "| \n",
      "\n",
      "49\n",
      "\n",
      "\f",
      "• Finding the best modeling parameters (algorithms, hyperparameters, feature pre‐\n",
      "\n",
      "processing, etc.).\n",
      "\n",
      "• Tuning the bias/variance trade-off for a given training cost to fit that definition of\n",
      "\n",
      "“best.”\n",
      "\n",
      "• Finding  a  balance  between  model  improvement  and  improved  computation\n",
      "costs. (Since there’s always room for improvement, how good is good enough?)\n",
      "\n",
      "Bias and Variance\n",
      "A high bias model (also known as underfitting) fails to discover some of the rules that\n",
      "could  have  been  learned  from  the  training  data,  possibly  because  of  reductive\n",
      "assumptions making the model overly simplistic.\n",
      "\n",
      "A  high  variance  model  (or  overfitting)  sees  patterns  in  noise  and  seeks  to  predict\n",
      "every  single  variation,  resulting  in  a  complex  model  that  does  not  generalize  well\n",
      "beyond its training data.\n",
      "\n",
      "When experimenting, data scientists need to be able to quickly iterate through all the\n",
      "possibilities for each of the model building blocks outlined in Table 4-1. Fortunately,\n",
      "there are tools to do all of this semiautomatically, where you only need to define what\n",
      "should  be  tested  (the  space  of  possibilities)  depending  on  prior  knowledge  (what\n",
      "makes sense) and the constraints (e.g., computation, budget).\n",
      "\n",
      "Some tools allow for even more automation, for instance by offering stratified model\n",
      "training. For example, say the business wants to predict customer demand for prod‐\n",
      "ucts to optimize inventory, but behavior varies a lot from one store to the next. Strati‐\n",
      "fied modeling consists of training one model per store that can be better optimized\n",
      "for each store rather than a model that tries to predict in all stores.\n",
      "\n",
      "Trying  all  combinations  of  every  possible  hyperparameter,  feature  handling,  etc.,\n",
      "quickly becomes untraceable. Therefore, it is useful to define a time and/or computa‐\n",
      "tion budget for experiments as well as an acceptability threshold for usefulness of the\n",
      "model (more on that notion in the next section).\n",
      "\n",
      "Notably, all or part of this process may need to be repeated every time anything in the\n",
      "situation  changes  (including  whenever  the  data  and/or  problem  constraints  change\n",
      "significantly; see “Drift Detection in Practice” on page 92). Ultimately, this means that\n",
      "all experiments that informed the final decisions data scientists made to arrive at the\n",
      "model as well as all the assumptions and conclusions along the way may need to be\n",
      "rerun and reexamined.\n",
      "\n",
      "Fortunately,  more  and  more  data  science  and  machine  learning  platforms  allow  for\n",
      "automating these workflows not only on the first run, but also to preserve all the pro‐\n",
      "\n",
      "50 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "cessing operations for repeatability. Some also allow for the use of version control and\n",
      "experimental branch spin-off to test theories and then merge, discard, or keep them\n",
      "(see “Version Management and Reproducibility” on page 56).\n",
      "\n",
      "Evaluating and Comparing Models\n",
      "George E. P. Box, a twentieth century British statistician, once said that all models are\n",
      "wrong, but some are useful. In other words, a model should not aim to be perfect, but\n",
      "it  should  pass  the  bar  of  “good  enough  to  be  useful”  while  keeping  an  eye  on  the\n",
      "uncanny valley—typically a model that looks like it’s doing a good job but does a bad\n",
      "(or  catastrophic)  job  for  a  specific  subset  of  cases  (say,  an  underrepresented\n",
      "population).\n",
      "\n",
      "With this in mind, it’s important to evaluate a model in context and have some ability\n",
      "to compare it to what existed before the model—whether a previous model or a rules-\n",
      "based process—to get an idea of what the outcome would be if the current model or\n",
      "decision process were replaced by the new one.\n",
      "\n",
      "A  model  with  an  absolute  performance  that  could  technically  be  considered  disap‐\n",
      "pointing  can  still  possibly  enhance  an  existing  situation.  For  instance,  having  a\n",
      "slightly more accurate forecast of demand for a certain product or service may result\n",
      "in huge cost-savings.\n",
      "\n",
      "Conversely,  a  model  that  gets  a  perfect  score  is  suspicious,  as  most  problems  have\n",
      "noise in the data that’s at least somewhat hard to predict. A perfect or nearly-perfect\n",
      "score may be a sign that there is a leak in the data (i.e., that the target to be predicted\n",
      "is also in the input data or that an input feature is very correlated to the target but,\n",
      "practically,  available  only  once  the  target  is  known)  or  that  the  model  overfits  the\n",
      "training data and will not generalize well.\n",
      "\n",
      "Choosing Evaluation Metrics\n",
      "Choosing the proper metric by which to evaluate and compare different models for a\n",
      "given problem can lead to very different models (think of the cobra effect mentioned\n",
      "in Table 4-1). A simple example: accuracy is often used for automated classification\n",
      "problems but is rarely the best fit when the classes are unbalanced (i.e., when one of\n",
      "the outcomes is very unlikely compared to the other). In a binary classification prob‐\n",
      "lem where the positive class (i.e., the one that is interesting to predict because its pre‐\n",
      "diction  triggers  an  action)  is  rare,  say  5%  of  occurrences,  a  model  that  constantly\n",
      "predicts the negative class is therefore 95% accurate, while also utterly useless.\n",
      "\n",
      "Unfortunately, there is no one-size-fits-all metric. You need to pick one that matches\n",
      "the  problem  at  hand,  which  means  understanding  the  limits  and  trade-offs  of  the\n",
      "metric (the mathematics side) and their impact on the optimization of the model (the\n",
      "business side).\n",
      "\n",
      "Evaluating and Comparing Models \n",
      "\n",
      "| \n",
      "\n",
      "51\n",
      "\n",
      "\f",
      "To get an idea of how well a model will generalize, that metric should be evaluated on\n",
      "a  part  of  the  data  that  was  not  used  for  the  model’s  training  (a  holdout  dataset),  a\n",
      "method called cross-testing. There can be multiple steps where some data is held for\n",
      "evaluation and the rest is used for training or optimizing, such as metric evaluation or\n",
      "hyperparameter  optimization.  There  are  different  strategies  as  well,  not  necessarily\n",
      "just  a  simple  split.  In  k-fold  cross-validation,  for  example,  data  scientists  rotate  the\n",
      "parts that they hold out to evaluate and train multiple times. This multiplies the train‐\n",
      "ing time but gives an idea of the stability of the metric.\n",
      "\n",
      "With a simple split, the holdout dataset can consist of the most recent records instead\n",
      "of randomly chosen ones. Indeed, as models are usually used for future predictions, it\n",
      "is  likely  that  assessing  them  as  if  they  were  used  for  prediction  on  the  most  recent\n",
      "data leads to more realistic estimations. In addition, it allows one to assess whether\n",
      "the data drifted between the training and the holdout dataset (see “Drift Detection in\n",
      "Practice” on page 92 for more details).\n",
      "\n",
      "As  an  example,  Figure  4-2  shows  a  scheme  in  which  a  test  dataset  is  a  holdout  (in\n",
      "gray) in order to perform the evaluation. The remaining data is split into three parts\n",
      "to find the best hyperparameter combination by training the model three times with a\n",
      "given  combination  on  each  of  the  blue  datasets,  and  validating  its  performance  on\n",
      "their respective green datasets. The gray dataset is used only once with the best hyper‐\n",
      "parameter combination, while the other datasets are used with all of them.\n",
      "\n",
      "Figure 4-2. An example of dataset split for model evaluation\n",
      "\n",
      "Oftentimes,  data  scientists  want  to  periodically  retrain  models  with  the  same  algo‐\n",
      "rithms, hyperparameters, features, etc., but on more recent data. Naturally, the next\n",
      "step  is  to  compare  the  two  models  and  see  how  the  new  version  fares.  But  it’s  also\n",
      "important  to  make  sure  all  previous  assumptions  still  hold:  that  the  problem  hasn’t\n",
      "fundamentally  shifted,  that  the  modeling  choices  made  previously  still  fit  the  data,\n",
      "etc.  This  is  more  specifically  part  of  performance  and  drift  monitoring  (find  more\n",
      "details on this in Chapter 7).\n",
      "\n",
      "52 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "Cross-Checking Model Behavior\n",
      "Beyond  the  raw  metrics,  when  evaluating  a  model,  it’s  critical  to  understand  how  it\n",
      "will behave. Depending on the impact of the model’s predictions, decisions, or classi‐\n",
      "fications, a more or less deep understanding may be required. For example, data sci‐\n",
      "entists  should  take  reasonable  steps  (with  respect  to  that  impact)  to  ensure  that  the\n",
      "model is not actively harmful: a model that would predict that all patients need to be\n",
      "checked by a doctor may score high in terms of raw prevention, but not so much on\n",
      "realistic resource allocation.\n",
      "\n",
      "Examples of these reasonable steps include:\n",
      "\n",
      "• Cross-checking different metrics (and not only the ones on which the model was\n",
      "\n",
      "initially optimized)\n",
      "\n",
      "• Checking how the model reacts to different inputs—e.g., plot the average predic‐\n",
      "tion (or probability for classification models) for different values of some inputs\n",
      "and see whether there are oddities or extreme variability\n",
      "\n",
      "• Splitting  one  particular  dimension  and  checking  the  difference  in  behavior  and\n",
      "metrics across different subpopulations—e.g., is the error rate the same for males\n",
      "and females?\n",
      "\n",
      "These kinds of global analyses should not be understood as causality, just as correla‐\n",
      "tion. They do not necessarily imply a specific causal relationship between some vari‐\n",
      "ables  and  an  outcome;  they  merely  show  how  the  model  sees  that  relationship.  In\n",
      "other words, the model should be used with care for what-if analysis. If one feature\n",
      "value is changed, the model prediction is likely to be wrong if the new feature value\n",
      "has never been seen in the training dataset or if it has never been seen in combination\n",
      "with the values of the other features in this dataset.\n",
      "\n",
      "When comparing models, those different aspects should be accessible to data scien‐\n",
      "tists, who need to be able to go deeper than a single metric. That means the full envi‐\n",
      "ronment  (interactive  tooling,  data,  etc.)  needs  to  be  available  for  all  models,  ideally\n",
      "allowing for comparison from all angles and between all components. For example,\n",
      "for  drift,  the  comparison  might  use  the  same  settings  but  different  data,  while  for\n",
      "modeling performance, it might use the same data but different settings.\n",
      "\n",
      "Impact of Responsible AI on Modeling\n",
      "Depending on the situation (and sometimes depending on the industry or sector of\n",
      "the  business),  on  top  of  a  general  understanding  of  model  behavior,  data  scientists\n",
      "may also need models’ individual predictions to be explainable, including having an\n",
      "idea of what specific features pushed the prediction one way or the other. Sometimes\n",
      "predictions may be very different for a specific record than on average. Popular meth‐\n",
      "ods to compute individual prediction explanations include Shapley value (the average\n",
      "\n",
      "Evaluating and Comparing Models \n",
      "\n",
      "| \n",
      "\n",
      "53\n",
      "\n",
      "\f",
      "marginal contribution of a feature value across all possible coalitions) and individual\n",
      "conditional  expectation  (ICE)  computations,  which  show  the  dependence  between\n",
      "the target functions and features of interest.\n",
      "\n",
      "For example, the measured level of a specific hormone could generally push a model\n",
      "to predict someone has a health issue, but for a pregnant woman, that level makes the\n",
      "model  infer  she  is  at  no  such  risk.  Some  legal  frameworks  mandate  some  kind  of\n",
      "explainability for decisions made by a model that have consequences on humans, like\n",
      "recommending a loan to be denied. “Element 2: Bias” on page 114 discusses this topic\n",
      "in detail.\n",
      "\n",
      "Note  that  the  notion  of  explainability  has  several  dimensions.  In  particular,  deep\n",
      "learning networks are sometimes called black-box models because of their complexity\n",
      "(though when reading the model coefficients, a model is fully specified, and it is usu‐\n",
      "ally a conceptually remarkably simple formula, but a very large formula that becomes\n",
      "impossible to intuitively apprehend). Conversely, global and local explanation tools—\n",
      "such as partial dependence plots or Shapley value computations—give some insights\n",
      "but  arguably  do  not  make  the  model  intuitive.  To  actually  communicate  a  rigorous\n",
      "and  intuitive  understanding  of  what  exactly  the  model  is  doing,  limiting  the  model\n",
      "complexity is required.\n",
      "\n",
      "Fairness  requirements  can  also  have  dimensioning  constraints  on  model  develop‐\n",
      "ment. Consider this theoretical example to better understand what is at stake when it\n",
      "comes to bias: a US-based organization regularly hires people who do the same types\n",
      "of  jobs.  Data  scientists  could  train  a  model  to  predict  the  workers’  performance\n",
      "according  to  various  characteristics,  and  people  would  then  be  hired  based  on  the\n",
      "probability that they would be high-performing workers.\n",
      "\n",
      "Though this seems like a simple problem, unfortunately, it’s fraught with pitfalls. To\n",
      "make  this  problem  completely  hypothetical  and  to  detach  it  from  the  complexities\n",
      "and problems of the real world, let’s say everyone in the working population belongs\n",
      "to one of two groups: Weequay or Togruta.\n",
      "\n",
      "For  this  hypothetical  example,  let’s  claim  that  a  far  larger  population  of  Weequay\n",
      "attend  university.  Off  the  bat,  there  would  be  an  initial  bias  in  favor  of  Weequay\n",
      "(amplified by the fact they would have been able to develop their skills through years\n",
      "of experience).\n",
      "\n",
      "As a result, there would not only be more Weequay than Togruta in the pool of appli‐\n",
      "cants, but Weequay applicants would tend to be more qualified. The employer has to\n",
      "hire 10 people during the month to come. What should it do?\n",
      "\n",
      "• As an equal opportunity employer, it should ensure the fairness of its recruitment\n",
      "process  as  it  controls  it.  That  means  in  mathematical  terms,  for  each  applicant\n",
      "and all things being equal, being hired (or not) should not depend on their group\n",
      "affiliation (in this case, Weequay or Togruta). However, this results in bias in and\n",
      "\n",
      "54 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "of itself, as Weequay are more qualified. Note that “all things being equal” can be\n",
      "interpreted in various ways, but the usual interpretation is that the organization\n",
      "is likely not considered accountable for processes it does not control.\n",
      "\n",
      "• The  employer  may  also  have  to  avoid  disparate  impact,  that  is,  practices  in\n",
      "employment that adversely affect one group of people of a protected characteris‐\n",
      "tic  more  than  another.  Disparate  impact  is  assessed  on  subpopulations  and  not\n",
      "on individuals; practically, it assesses whether proportionally speaking, the com‐\n",
      "pany has hired as many Weequay as Togruta. Once again, the target proportions\n",
      "may be those of the applicants or those of the general population, though the for‐\n",
      "mer  is  more  likely,  as  again,  the  organization  can’t  be  accountable  for  biases  in\n",
      "processes out of its control.\n",
      "\n",
      "The two objectives are mutually exclusive. In this scenario, equal opportunity would\n",
      "lead to hiring 60% (or more) Weequay and 40% (or fewer) Togruta. As a result, the\n",
      "process  has  a  disparate  impact  on  the  two  populations,  because  the  hiring  rates  are\n",
      "different.\n",
      "\n",
      "Conversely,  if  the  process  is  corrected  so  that  40%  of  people  hired  are  Togruta  to\n",
      "avoid  disparate  impact,  it  means  that  some  rejected  Weequay  applicants  will  have\n",
      "been predicted as more qualified than some accepted Togruta applicants (contradict‐\n",
      "ing the equal opportunity assertion).\n",
      "\n",
      "There needs to be a trade-off—the law sometimes referred to as the 80% rule. In this\n",
      "example, it would mean that the hiring rate of Togruta should be equal to or larger\n",
      "than 80% of the hiring rate of Weequay. In this example, it means that it would be OK\n",
      "to hire up to 65% Weequay.\n",
      "\n",
      "The  point  here  is  that  defining  these  objectives  cannot  be  a  decision  made  by  data\n",
      "scientists  alone.  But  even  once  the  objectives  are  defined,  the  implementation  itself\n",
      "may also be problematic:\n",
      "\n",
      "• Without any indications, data scientists naturally try to build equal opportunity\n",
      "models because they correspond to models of the world as it is. Most of the tools\n",
      "data scientists employ also try to achieve this because it is the most mathemati‐\n",
      "cally  sound  option.  Yet  some  ways  to  achieve  this  goal  may  be  unlawful.  For\n",
      "example,  the  data  scientist  may  choose  to  implement  two  independent  models:\n",
      "one for Weequay and one for Togruta. This could be a reasonable way to address\n",
      "the biases induced by a training dataset in which Weequay are overrepresented,\n",
      "but  it  would  induce  a  disparate  treatment  of  the  two  that  could  be  considered\n",
      "discriminatory.\n",
      "\n",
      "• To let data scientists use their tools in the way they were designed (i.e., to model\n",
      "the world as it is), they may decide to post-process the predictions so that they fit\n",
      "with  the  organization’s  vision  of  the  world  as  it  should  be.  The  simplest  way  of\n",
      "doing this is to choose a higher threshold for Weequay than for Togruta. The gap\n",
      "\n",
      "Evaluating and Comparing Models \n",
      "\n",
      "| \n",
      "\n",
      "55\n",
      "\n",
      "\f",
      "between  them  will  adjust  the  trade-off  between  “equal  opportunity”  and  “equal\n",
      "impact”; however, it may still be considered discriminatory because of the dispa‐\n",
      "rate treatment.\n",
      "\n",
      "Data  scientists  are  unlikely  to  be  able  to  sort  this  problem  out  alone  (see  “Key  Ele‐\n",
      "ments of Responsible AI” on page 113 for a broader view on the subject). This simple\n",
      "example illustrates the complexity of the subject, which may be even more complex\n",
      "given that there may be many protected attributes, and the fact that bias is as much a\n",
      "business question as a technical question.\n",
      "\n",
      "Consequently, the solution heavily depends on the context. For instance, this example\n",
      "of Weequay and Togruta is representative of processes that give access to privileges.\n",
      "The situation is different if the process has negative impacts on the user (like fraud\n",
      "prediction that leads to transaction rejection) or is neutral (like disease prediction).\n",
      "\n",
      "Version Management and Reproducibility\n",
      "Discussing the evaluation and comparison of models (for fairness as discussed imme‐\n",
      "diately before, but also a host of other factors) necessarily brings up the idea of ver‐\n",
      "sion control and the reproducibility of different model versions. With data scientists\n",
      "building, testing, and iterating on several versions of models, they need to be able to\n",
      "keep all the versions straight.\n",
      "\n",
      "Version management and reproducibility address two different needs:\n",
      "\n",
      "• During  the  experimentation  phase,  data  scientists  may  find  themselves  going\n",
      "back  and  forth  on  different  decisions,  trying  out  different  combinations,  and\n",
      "reverting  when  they  don’t  produce  the  desired  results.  That  means  having  the\n",
      "ability  to  go  back  to  different  “branches”  of  the  experiments—for  example,\n",
      "restoring a previous state of a project when the experimentation process led to a\n",
      "dead end.\n",
      "\n",
      "• Data scientists or others (auditors, managers, etc.) may need to be able to replay\n",
      "the computations that led to model deployment for an audit team several years\n",
      "after the experimentation was first done.\n",
      "\n",
      "Versioning has arguably been somewhat solved when everything is code-based, with\n",
      "source version control technology. Modern data processing platforms typically offer\n",
      "similar capabilities for data transformation pipelines, model configuration, etc. Merg‐\n",
      "ing several parts is, of course, less straightforward than merging code that diverged,\n",
      "but the basic need is to be able to go back to some specific experiment, if only to be\n",
      "able to copy its settings to replicate them in another branch.\n",
      "\n",
      "Another very important property of a model is reproducibility. After a lot of experi‐\n",
      "ments and tweaking, data scientists may arrive at a model that fits the bill. But after\n",
      "\n",
      "56 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "that, operationalization necessitates model reproduction not only in another environ‐\n",
      "ment,  but  also  possibly  from  a  different  starting  point.  Repeatability  also  makes\n",
      "debugging much easier (sometimes even simply possible). To this end, all facets of the\n",
      "model need to be documented and reusable, including:\n",
      "\n",
      "Assumptions\n",
      "\n",
      "When  a  data  scientist  makes  decisions  and  assumptions  about  the  problem  at\n",
      "hand, its scope, the data, etc., they should all be explicit and logged so that they\n",
      "can be checked against any new information down the line.\n",
      "\n",
      "Randomness\n",
      "\n",
      "A  lot  of  ML  algorithms  and  processes,  such  as  sampling,  make  use  of  pseudo-\n",
      "random numbers. Being able to precisely reproduce an experiment, such as for\n",
      "debugging, means to have control over that pseudo-randomness, most often by\n",
      "controlling  the  “seed”  of  the  generator  (i.e.,  the  same  generator  initialized  with\n",
      "the same seed would yield the same sequence of pseudo-random numbers).\n",
      "\n",
      "Data\n",
      "\n",
      "To  get  repeatability,  the  same  data  must  be  available.  This  can  sometimes  be\n",
      "tricky  because  the  storage  capacity  required  to  version  data  can  be  prohibitive\n",
      "depending on the rate of update and quantity. Also, branching on data does not\n",
      "yet have as rich an ecosystem of tools as branching on code.\n",
      "\n",
      "Settings\n",
      "\n",
      "This one is a given: all processing that has been done must be reproducible with\n",
      "the same settings.\n",
      "\n",
      "Results\n",
      "\n",
      "While developers use merging tools to compare and merge different text file ver‐\n",
      "sions,  data  scientists  need  to  be  able  to  compare  in-depth  analysis  of  models\n",
      "(from confusion matrices to partial dependence plots) to obtain models that sat‐\n",
      "isfy the requirements.\n",
      "\n",
      "Implementation\n",
      "\n",
      "Ever-so-slightly  different  implementations  of  the  same  model  can  actually  yield\n",
      "different models, enough to change the predictions on some close calls. And the\n",
      "more  sophisticated  the  model,  the  higher  the  chances  that  these  discrepancies\n",
      "happen. On the other hand, scoring a dataset in bulk with a model comes with\n",
      "different  constraints  than  scoring  a  single  record  live  in  an  API,  so  different\n",
      "implementations  may  sometimes  be  warranted  for  the  same  model.  But  when\n",
      "debugging and comparing, data scientists need to keep the possible differences in\n",
      "mind.\n",
      "\n",
      "Version Management and Reproducibility \n",
      "\n",
      "| \n",
      "\n",
      "57\n",
      "\n",
      "\f",
      "Environment\n",
      "\n",
      "Given  all  the  steps  covered  in  this  chapter,  it’s  clear  that  a  model  is  not  just  its\n",
      "algorithm and parameters. From the data preparation to the scoring implementa‐\n",
      "tion, including feature selection, feature encoding, enrichment, etc., the environ‐\n",
      "ment in which several of those steps run may be more or less implicitly tied to the\n",
      "results. For instance, a slightly different version of a Python package involved in\n",
      "one step may change the results in ways that can be hard to predict. Preferably,\n",
      "data scientists should make sure that the runtime environment is also repeatable.\n",
      "Given the pace at which ML is evolving, this might require techniques that freeze\n",
      "the computation environments.\n",
      "\n",
      "Fortunately,  part  of  the  underlying  documentation  tasks  associated  with  versioning\n",
      "and  reproducibility  can  be  automated,  and  the  use  of  an  integrated  platform  for\n",
      "design  and  deployment  can  greatly  decrease  the  reproducibility  costs  by  ensuring\n",
      "structured information transfer.\n",
      "\n",
      "Clearly, while maybe not the sexiest part of model development, version management\n",
      "and  reproducibility  are  critical  to  building  machine  learning  efforts  in  real-world\n",
      "organizational settings where governance—including audits—matters.\n",
      "\n",
      "Closing Thoughts\n",
      "Model  development  is  one  of  the  most  critical  and  consequential  steps  of  MLOps.\n",
      "The  many  technical  questions  that  are  necessarily  answered  during  this  phase  have\n",
      "big repercussions on all aspects of the MLOps process throughout the life of the mod‐\n",
      "els.  Therefore,  exposure,  transparency,  and  collaboration  are  crucial  to  long-term\n",
      "success.\n",
      "\n",
      "The model development stage is also the one that has been practiced the most by pro‐\n",
      "files like data scientists and, in the pre-MLOps world, often represents the whole ML\n",
      "effort,  yielding  a  model  that  will  then  be  used  as  is  (with  all  its  consequences  and\n",
      "limitations).\n",
      "\n",
      "58 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Developing Models\n",
      "\n",
      "\f",
      "CHAPTER 5\n",
      "Preparing for Production\n",
      "\n",
      "Joachim Zentici\n",
      "\n",
      "Confirming that something works in the laboratory has never been a sure sign it will\n",
      "work well in the real world, and machine learning models are no different. Not only\n",
      "is  the  production  environment  typically  very  different  from  the  development  envi‐\n",
      "ronment,  but  the  commercial  risks  associated  with  models  in  production  are  much\n",
      "greater.  It  is  important  that  the  complexities  of  the  transition  to  production  are\n",
      "understood and tested and that the potential risks have been adequately mitigated.\n",
      "\n",
      "This chapter explores the steps required to prepare for production (highlighted in the\n",
      "context of the entire life cycle in Figure 5-1). The goal is to illustrate, by extension, the\n",
      "elements that must be considered for robust MLOps systems.\n",
      "\n",
      "Figure 5-1. Preparing for production highlighted in the larger context of the ML project\n",
      "life cycle\n",
      "\n",
      "59\n",
      "\n",
      "\f",
      "Runtime Environments\n",
      "The first step in sending a model to production is making sure it’s technically possi‐\n",
      "ble. As discussed in Chapter 3, ideal MLOps systems favor rapid, automated deploy‐\n",
      "ment over labor-intensive processes, and runtime environments can have a big effect\n",
      "on which approach prevails.\n",
      "\n",
      "Production  environments  take  a  wide  variety  of  forms:  custom-built  services,  data\n",
      "science  platforms,  dedicated  services  like  TensorFlow  Serving,  low-level  infrastruc‐\n",
      "ture like Kubernetes clusters, JVMs on embedded systems, etc. To make things even\n",
      "more complex, consider that in some organizations, multiple heterogeneous produc‐\n",
      "tion environments coexist.\n",
      "\n",
      "Ideally, models running in the development environment would be validated and sent\n",
      "as is to production; this minimizes the amount of adaptation work and improves the\n",
      "chances that the model in production will behave as it did in development. Unfortu‐\n",
      "nately,  this  ideal  scenario  is  not  always  possible,  and  it’s  not  unheard  of  that  teams\n",
      "finish a long-term project only to realize it can’t be put in production.\n",
      "\n",
      "Adaptation from Development to Production Environments\n",
      "In terms of adaptation work, on one end of the spectrum, the development and pro‐\n",
      "duction platforms are from the same vendor or are otherwise interoperable, and the\n",
      "dev model can run without any modification in production. In this case, the technical\n",
      "steps required to push the model into production are reduced to a few clicks or com‐\n",
      "mands, and all efforts can be focused on validation.\n",
      "\n",
      "On the other end of the spectrum, there are cases where the model needs to be reim‐\n",
      "plemented  from  scratch—possibly  by  another  team,  and  possibly  in  another  pro‐\n",
      "gramming language. Given the resources and time required, there are few cases today\n",
      "where this approach makes sense. However, it’s still the reality in many organizations\n",
      "and is often a consequence of the lack of appropriate tooling and processes. The real‐\n",
      "ity is that handing over a model for another team to reimplement and adapt for the\n",
      "production  environment  means  that  model  won’t  reach  production  for  months\n",
      "(maybe years), if at all.\n",
      "\n",
      "Between  these  two  extreme  cases,  there  can  be  a  number  of  transformations  per‐\n",
      "formed on the model or the interactions with its environment to make it compatible\n",
      "with  production.  In  all  cases,  it  is  crucial  to  perform  validation  in  an  environment\n",
      "that  mimics  production  as  closely  as  possible,  rather  than  in  the  development\n",
      "environment.\n",
      "\n",
      "60 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "Tooling considerations\n",
      "\n",
      "The format required to send to production should be considered early, as it may have\n",
      "a large impact on the model itself and the quantity of work required to productional‐\n",
      "ize it. For example, when a model is developed using scikit-learn (Python) and pro‐\n",
      "duction  is  a  Java-based  environment  that  expects  PMML  or  ONNX  as  input,\n",
      "conversion is obviously required.\n",
      "\n",
      "In this case, teams should set up tooling while developing the model, ideally before\n",
      "the first version of the model is finished or even started. Failure to create this pipeline\n",
      "up  front  would  block  the  validation  process  (and,  of  course,  final  validation  should\n",
      "not  be  performed  on  the  scikit-learn  model,  as  it’s  not  the  one  that  will  be  put  into\n",
      "production).\n",
      "\n",
      "Performance considerations\n",
      "\n",
      "Another common reason conversion may be required is for performance. For exam‐\n",
      "ple, a Python model will typically have higher latency for scoring a single record than\n",
      "its  equivalent  converted  to  C++.  The  resulting  model  will  likely  be  dozens  of  times\n",
      "faster  (although  obviously  it  depends  on  many  factors,  and  the  result  can  also  be  a\n",
      "model that is dozens of times slower).\n",
      "\n",
      "Performance  also  comes  into  play  when  the  production  model  must  run  on  a  low-\n",
      "power device. In the specific case of deep neural networks, for example, trained mod‐\n",
      "els  can  become  extremely  large  with  billions  or  hundreds  of  billions  of  parameters.\n",
      "Running them on small devices is simply impossible, and running them on standard\n",
      "servers can be slow and expensive.\n",
      "\n",
      "For these models, an optimized runtime is not enough. To obtain better performance,\n",
      "the  model  definition  must  be  optimized.  One  solution  is  to  use  compression\n",
      "techniques:\n",
      "\n",
      "• With quantization, the model can be trained using 32-bit floating-point numbers\n",
      "and used for inference at a lower precision so that the model requires less mem‐\n",
      "ory and is faster while accuracy is mostly preserved.\n",
      "\n",
      "• With pruning, one simply removes weights (or even entire layers) from the neu‐\n",
      "ral  network.  This  is  a  rather  radical  approach,  but  some  methods  allow  for  the\n",
      "preservation of accuracy.\n",
      "\n",
      "• With distillation, a smaller “student” network is trained to mimic a bigger, more\n",
      "powerful  network.  Done  appropriately,  this  can  lead  to  better  models  (as  com‐\n",
      "pared to trying to train the smaller network directly from the data).\n",
      "\n",
      "These methods are efficient if the initial model is trained in a way that reduces infor‐\n",
      "mation loss while performing them, so these operations are not simply conversions of\n",
      "the  trained  model  post  hoc,  but  rather  orient  the  way  the  model  is  trained.  These\n",
      "\n",
      "Runtime Environments \n",
      "\n",
      "| \n",
      "\n",
      "61\n",
      "\n",
      "\f",
      "methods are still very recent and quite advanced but already commonly used in natu‐\n",
      "ral language processing (NLP) pretrained models.\n",
      "\n",
      "Data Access Before Validation and Launch to Production\n",
      "Another technical aspect that needs to be addressed before validation and launch to\n",
      "production is data access. For example, a model evaluating apartment prices may use\n",
      "the average market price in a zip code area; however, the user or the system request‐\n",
      "ing the scoring will probably not provide this average and would most likely provide\n",
      "simply the zip code, meaning a lookup is necessary to fetch the value of the average.\n",
      "\n",
      "In some cases, data can be frozen and bundled with the model. But when this is not\n",
      "possible (e.g., if the dataset is too large or the enrichment data needs to always be up\n",
      "to  date),  the  production  environment  should  access  a  database  and  thus  have  the\n",
      "appropriate network connectivity, libraries, or drivers required to communicate with\n",
      "the data storage installed, and authentication credentials stored in some form of pro‐\n",
      "duction configuration.\n",
      "\n",
      "Managing this setup and configuration can be quite complex in practice since, again,\n",
      "it requires appropriate tooling and collaboration (in particular to scale to more than a\n",
      "few dozen models). When using external data access, model validation in situations\n",
      "that closely match production is even more critical as technical connectivity is a com‐\n",
      "mon source of production malfunction.\n",
      "\n",
      "Final Thoughts on Runtime Environments\n",
      "Training a model is usually the most impressive computation, requiring a high level\n",
      "of software sophistication, massive data volumes, and high-end machines with pow‐\n",
      "erful GPUs. But in the whole life cycle of a model, there is a good chance that most of\n",
      "the compute is spent at inference time (even if this computation is orders of magni‐\n",
      "tude simpler and faster). This is because a model is trained once and can be used bil‐\n",
      "lions of times for inference.\n",
      "\n",
      "Scaling  inference  on  complex  models  can  be  expensive  and  have  significant  energy\n",
      "and  environmental  impact.  Lowering  the  complexity  of  models  or  compressing\n",
      "extremely  complex  models  can  lower  the  infrastructure  cost  of  operating  machine\n",
      "learning models.\n",
      "\n",
      "It’s important to remember that not all applications require deep learning, and in fact,\n",
      "not  all  applications  require  machine  learning  at  all.  A  valuable  practice  to  control\n",
      "complexity in production is to develop complex models only to provide a baseline for\n",
      "what  seems  achievable.  What  goes  into  production  can  then  be  a  much  simpler\n",
      "model, with the advantages of lowering the operating risk, increasing computational\n",
      "performance, and lowering power consumption. If the simple model is close enough\n",
      "\n",
      "62 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "to  the  high  complexity  baseline,  then  it  can  be  a  much  more  desirable  solution  for\n",
      "production.\n",
      "\n",
      "Model Risk Evaluation\n",
      "Before  exploring  how  validation  should  be  done  in  an  ideal  MLOps  system,  it’s\n",
      "important to consider the purpose of validation. As discussed in Chapter 4, models\n",
      "attempt to mimic reality, but they are imperfect; their implementation can have bugs,\n",
      "as  can  the  environment  they  are  executing  in.  The  indirect,  real-world  impact  a\n",
      "model in production can have is never certain, and the malfunctioning of a seemingly\n",
      "insignificant cog can have tremendous consequences in a complex system.\n",
      "\n",
      "The Purpose of Model Validation\n",
      "It is, to some extent, possible (not to mention absolutely necessary) to anticipate the\n",
      "risks of models in production and thus design and validate so as to minimize these\n",
      "risks. As organizations become more and more complex, it is essential to understand\n",
      "that involuntary malfunctions or malicious attacks are potentially threatening in most\n",
      "uses  of  machine  learning  in  the  enterprise,  not  only  in  financial  or  safety-related\n",
      "applications.\n",
      "\n",
      "Before putting a model in production (and in fact constantly from the beginning of\n",
      "the machine learning project), teams should ask the uncomfortable questions:\n",
      "\n",
      "• What if the model acts in the worst imaginable way?\n",
      "\n",
      "• What  if  a  user  manages  to  extract  the  training  data  or  the  internal  logic  of  the\n",
      "\n",
      "model?\n",
      "\n",
      "• What are the financial, business, legal, safety, and reputational risks?\n",
      "\n",
      "For  high-risk  applications,  it  is  essential  that  the  whole  team  (and  in  particular  the\n",
      "engineers in charge of validation) be fully aware of these risks so that they can design\n",
      "the  validation  process  appropriately  and  apply  the  strictness  and  complexity  appro‐\n",
      "priate for the magnitude of the risks.\n",
      "\n",
      "In  many  ways,  machine  learning  risk  management  covers  model  risk  management\n",
      "practices that are well established in many industries, such as banking and insurance.\n",
      "However, machine learning introduces new types of risks and liabilities, and as data\n",
      "science gets democratized, it involves many new organizations or teams that have no\n",
      "experience with more traditional model risk management.\n",
      "\n",
      "Model Risk Evaluation \n",
      "\n",
      "| \n",
      "\n",
      "63\n",
      "\n",
      "\f",
      "The Origins of ML Model Risk\n",
      "The magnitude of risk ML models can bring is hard to model for mathematical rea‐\n",
      "sons,  but  also  because  the  materialization  of  risks  arises  through  real-world  conse‐\n",
      "quences. The ML metrics, and in particular the cost matrix, allow teams to evaluate\n",
      "the  average  cost  of  operating  a  model  in  its  “nominal”  case,  meaning  on  its  cross-\n",
      "validation data, compared to operating a perfect magical model.\n",
      "\n",
      "But while computing this expected cost can be very important, a wide range of things\n",
      "can  go  wrong  well  beyond  expected  cost.  In  some  applications,  the  risk  can  be  a\n",
      "financially unbounded liability, a safety issue for individuals, or an existential threat\n",
      "for the organization. ML model risk originates essentially from:\n",
      "\n",
      "• Bugs, errors in designing, training, or evaluating the model (including data prep)\n",
      "\n",
      "• Bugs in the runtime framework, bugs in the model post-processing/conversion,\n",
      "\n",
      "or hidden incompatibilities between the model and its runtime\n",
      "\n",
      "• Low quality of training data\n",
      "\n",
      "• High difference between production data and training data\n",
      "\n",
      "• Expected  error  rates,  but  with  failures  that  have  higher  consequences  than\n",
      "\n",
      "expected\n",
      "\n",
      "• Misuse of the model or misinterpretation of its outputs\n",
      "\n",
      "• Adversarial attacks\n",
      "\n",
      "• Legal  risk  originating  in  particular  from  copyright  infringement  or  liability  for\n",
      "\n",
      "the model output\n",
      "\n",
      "• Reputational risk due to bias, unethical use of machine learning, etc.\n",
      "\n",
      "The probability of materialization of the risk and its magnitude can be amplified by:\n",
      "\n",
      "• Broad use of the model\n",
      "\n",
      "• A rapidly changing environment\n",
      "\n",
      "• Complex interactions between models\n",
      "\n",
      "The  following  sections  provide  more  details  on  these  threats  and  how  to  mitigate\n",
      "them,  which  should  ultimately  be  the  goal  of  any  MLOps  system  the  organization\n",
      "puts in place.\n",
      "\n",
      "Quality Assurance for Machine Learning\n",
      "Software engineering has developed a mature set of tools and methodologies for qual‐\n",
      "ity assurance (QA), but the equivalent for data and models is still in its infancy, which\n",
      "makes it challenging to incorporate into MLOps processes. The statistical methods as\n",
      "\n",
      "64 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "well as documentation best practices are well known, but implementing them at scale\n",
      "is not common.\n",
      "\n",
      "Though it’s being covered as a part of this chapter on preparing for production, to be\n",
      "clear,  QA  for  machine  learning  does  not  occur  only  at  the  final  validation  stage;\n",
      "rather, it should accompany all stages of model development. Its purpose is to ensure\n",
      "compliance  with  processes  as  well  as  ML  and  computational  performance  require‐\n",
      "ments, with a level of detail that is proportionate to the level of risk.\n",
      "\n",
      "In the case where the people in charge of validation are not the ones who developed\n",
      "the  model,  it  is  essential  that  they  have  enough  training  in  machine  learning  and\n",
      "understand the risks so that they can design appropriate validation or detect breaches\n",
      "in  the  validation  proposed  by  the  development  team.  It  is  also  essential  that  the\n",
      "organization’s  structure  and  culture  give  them  the  authority  to  appropriately  report\n",
      "issues and contribute to continuous improvement or block passage to production if\n",
      "the level of risk justifies it.\n",
      "\n",
      "Robust MLOps practices dictate that performing QA before sending to production is\n",
      "not  only  about  technical  validation.  It  is  also  the  occasion  to  create  documentation\n",
      "and validate the model against organizational guidelines. In particular, this means the\n",
      "origin  of  all  input  datasets,  pretrained  models,  or  other  assets  should  be  known,  as\n",
      "they could be subject to regulations or copyrights. For this reason (and for computer\n",
      "security  reasons  in  particular),  some  organizations  choose  to  allow  only  whitelisted\n",
      "dependencies.  While  this  can  significantly  impact  the  ability  of  data  scientists  to\n",
      "innovate quickly, though the list of dependencies can be reported and checked partly\n",
      "automatically, it can also provide additional safety.\n",
      "\n",
      "Key Testing Considerations\n",
      "Obviously, model testing will consist of applying the model to carefully curated data\n",
      "and validating measurements against requirements. How the data is selected or gen‐\n",
      "erated as well as how much data is required is crucial, but it will depend on the prob‐\n",
      "lem tackled by the model.\n",
      "\n",
      "There are some scenarios in which the test data should not always match “real-world”\n",
      "data. For example, it can be a good idea to prepare a certain number of scenarios, and\n",
      "while some of them should match realistic situations, other data should be specifically\n",
      "generated in ways that could be problematic (e.g., extreme values, missing values).\n",
      "\n",
      "Metrics must be collected on both statistical (accuracy, precision, recall, etc.) as well\n",
      "as computational (average latency, 95th latency percentile, etc.) aspects, and the test\n",
      "scenarios should fail if some assumptions on them are not verified. For example, the\n",
      "test  should  fail  if  the  accuracy  of  the  model  falls  below  90%,  the  average  inference\n",
      "time goes above 100 milliseconds, or more than 5% of inferences take more than 200\n",
      "\n",
      "Key Testing Considerations \n",
      "\n",
      "| \n",
      "\n",
      "65\n",
      "\n",
      "\f",
      "milliseconds. These assumptions can also be called expectations, checks, or assertions,\n",
      "as in traditional software engineering.\n",
      "\n",
      "Statistical tests on results can also be performed but are typically used for subpopula‐\n",
      "tions. It is also important to be able to compare the model with its previous version. It\n",
      "can  allow  putting  in  place  a  champion/challenger  approach  (described  in  detail  in\n",
      "“Champion/Challenger”  on  page  100)  or  checking  that  a  metric  does  not  suddenly\n",
      "drop.\n",
      "\n",
      "Subpopulation Analysis and Model Fairness\n",
      "It can be useful to design test scenarios by splitting data into subpopulations based on\n",
      "a “sensitive” variable (that may or may not be used as a feature of the model). This is\n",
      "how fairness (typically between genders) is evaluated.\n",
      "\n",
      "Virtually all models that apply to people should be analyzed for fairness. Increasingly,\n",
      "failure to assess model fairness will have business, regulatory, and reputational impli‐\n",
      "cations  for  organizations.  For  details  about  biases  and  fairness,  refer  to  “Impact  of\n",
      "Responsible AI on Modeling” on page 53 and “Key Elements of Responsible AI” on\n",
      "page 113.\n",
      "\n",
      "In addition to validating the ML and computational performance metrics, model sta‐\n",
      "bility  is  an  important  testing  property  to  consider.  When  changing  one  feature\n",
      "slightly, one expects small changes in the outcome. While this cannot be always true,\n",
      "it is generally a desirable model property. A very unstable model introduces a lot of\n",
      "complexity  and  loopholes  in  addition  to  delivering  a  frustrating  experience,  as  the\n",
      "model can feel unreliable even if it has decent performance. There is no single answer\n",
      "to  model  stability,  but  generally  speaking,  simpler  models  or  more  regularized  ones\n",
      "show better stability.\n",
      "\n",
      "Reproducibility and Auditability\n",
      "Reproducibility  in  MLOps  does  not  have  the  same  meaning  as  in  academia.  In  the\n",
      "academic world, reproducibility essentially means that the findings of an experiment\n",
      "are  described  well  enough  that  another  competent  person  can  replicate  the  experi‐\n",
      "ment using the explanations alone, and if the person doesn’t make any mistakes, they\n",
      "will arrive at the same conclusion.\n",
      "\n",
      "In general, reproducibility in MLOps also involves the ability to easily rerun the exact\n",
      "same experiment. It implies that the model comes with detailed documentation, the\n",
      "data used for training and testing, and with an artifact that bundles the implementa‐\n",
      "tion  of  the  model  plus  the  full  specification  of  the  environment  it  was  run  in  (see\n",
      "\n",
      "66 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "“Version Management and Reproducibility” on page 56). Reproducibility is essential\n",
      "to prove model findings, but also to debug or build on a previous experiment.\n",
      "\n",
      "Auditability is related to reproducibility, but it adds some requirements. For a model\n",
      "to be auditable, it must be possible to access the full history of the ML pipeline from a\n",
      "central  and  reliable  storage  and  to  easily  fetch  metadata  on  all  model  versions\n",
      "including:\n",
      "\n",
      "• The full documentation\n",
      "\n",
      "• An artifact that allows running the model with its exact initial environment\n",
      "\n",
      "• Test results, including model explanations and fairness reports\n",
      "\n",
      "• Detailed model logs and monitoring metadata\n",
      "\n",
      "Auditability  can  be  an  obligation  in  some  highly  regulated  applications,  but  it  has\n",
      "benefits  for  all  organizations  because  it  can  facilitate  model  debugging,  continuous\n",
      "improvement, and keeping track of actions and responsibilities (which is an essential\n",
      "part of governance for responsible applications of ML, as discussed at length in Chap‐\n",
      "ter  8).  A  full  QA  toolchain  for  machine  learning—and,  thus,  MLOps  processes—\n",
      "should provide a clear view of model performance with regard to requirements while\n",
      "also facilitating auditability. \n",
      "\n",
      "Even when MLOps frameworks allow data scientists (or others) to find a model with\n",
      "all its metadata, understanding the model itself can still be challenging (see “Impact\n",
      "of Responsible AI on Modeling” on page 53 for a detailed discussion).\n",
      "\n",
      "To have a strong practical impact, auditability must allow for intuitive human under‐\n",
      "standing of all the parts of the system and their version histories. This doesn’t change\n",
      "the fact that understanding a machine learning model (even a relatively simple one)\n",
      "requires  appropriate  training,  but  depending  on  the  criticality  of  the  application,  a\n",
      "wider  audience  may  need  to  be  able  to  understand  the  details  of  the  model.  As  a\n",
      "result, full auditability comes at a cost that should be balanced with the criticality of\n",
      "the model itself.\n",
      "\n",
      "Machine Learning Security\n",
      "As  a  piece  of  software,  a  deployed  model  running  in  its  serving  framework  can\n",
      "present multiple security issues that range from low-level glitches to social engineer‐\n",
      "ing. Machine learning introduces a new range of potential threats where an attacker\n",
      "provides malicious data designed to cause the model to make a mistake.\n",
      "\n",
      "There are numerous cases of potential attacks. For example, spam filters were an early\n",
      "application of machine learning essentially based on scoring words that were in a dic‐\n",
      "tionary.  One  way  for  spam  creators  to  avoid  detection  was  to  avoid  writing  these\n",
      "exact  words  while  still  making  their  message  easily  understandable  by  a  human\n",
      "\n",
      "Machine Learning Security \n",
      "\n",
      "| \n",
      "\n",
      "67\n",
      "\n",
      "\f",
      "reader (e.g., using exotic Unicode characters, voluntarily introducing typos, or using\n",
      "images).\n",
      "\n",
      "Adversarial Attacks\n",
      "A more modern but quite analogous example of a machine learning model security\n",
      "issue is an adversarial attack for deep neural networks in which an image modifica‐\n",
      "tion that can seem minor or even impossible for a human eye to notice can cause the\n",
      "model to drastically change its prediction. The core idea is mathematically relatively\n",
      "simple:  since  deep  learning  inference  is  essentially  matrix  multiplication,  carefully\n",
      "chosen  small  perturbations  to  coefficients  can  cause  a  large  change  in  the  output\n",
      "numbers.\n",
      "\n",
      "One example of this is that small stickers glued to road signs can confuse an autono‐\n",
      "mous car’s computer vision system, rendering signs invisible or incorrectly classified\n",
      "by  the  system,  while  remaining  fully  visible  and  understandable  to  a  human  being.\n",
      "The more the attacker knows about the system, the more likely they are to find exam‐\n",
      "ples that will confuse it.\n",
      "\n",
      "A  human  can  use  reason  to  find  these  examples  (in  particular  for  simple  models).\n",
      "However,  for  more  complex  models  like  deep  learning,  the  attacker  will  probably\n",
      "need  to  perform  many  queries  and  either  use  brute  force  to  test  as  many  combina‐\n",
      "tions as possible or use a model to search for problematic examples. The difficulty of\n",
      "countermeasures  is  increasing  with  the  complexity  of  models  and  their  availability.\n",
      "Simple  models  such  as  logistic  regressions  are  essentially  immune,  while  an  open\n",
      "source pretrained deep neural network will basically always be vulnerable, even with\n",
      "advanced, built-in attack detectors.\n",
      "\n",
      "Adversarial attacks don’t necessarily happen at inference time. If an attacker can get\n",
      "access to the training data, even partially, then they get control over the system. This\n",
      "kind of attack is traditionally known as a poisoning attack in computer security.\n",
      "\n",
      "One famous example is the Twitter chatbot released by Microsoft in 2016. Just a few\n",
      "hours after launch, the bot started to generate very offensive tweets. This was caused\n",
      "by  the  bot  adapting  to  its  input;  when  realizing  that  some  users  submitted  a  large\n",
      "amount of offensive content, the bot started to replicate. In theory, a poisoning attack\n",
      "can  occur  as  a  result  of  an  intrusion  or  even,  in  a  more  sophisticated  way,  through\n",
      "pretrained models. But in practice, one should mostly care about data collected from\n",
      "easily  manipulated  data  sources.  Tweets  sent  to  a  specific  account  are  a  particularly\n",
      "clear example.\n",
      "\n",
      "Other Vulnerabilities\n",
      "Some patterns do not exploit machine learning vulnerabilities per se, but they do use\n",
      "the machine learning model in ways that lead to undesirable situations. One example\n",
      "\n",
      "68 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "is in credit scoring: for a given amount of money, borrowers with less flexibility tend\n",
      "to choose a longer period to lower the payments, while borrowers who are not con‐\n",
      "cerned about their ability to pay may choose a shorter period to lower the total cost of\n",
      "credit. Salespeople may advise those who do not have a good enough score to shorten\n",
      "their  payments.  This  increases  the  risk  for  the  borrower  and  the  bank  and  is  not  a\n",
      "meaningful course of action. Correlation is not causality!\n",
      "\n",
      "Models can also leak data in many ways. Since the machine learning models can fun‐\n",
      "damentally be considered a summary of the data they have been trained on, they can\n",
      "leak more or less precise information on the training data, up to the full training set\n",
      "in  some  cases.  Imagine,  for  example,  that  a  model  predicts  how  much  someone  is\n",
      "paid using the nearest neighbor algorithm. If one knows the zip code, age, and pro‐\n",
      "fession of a certain person registered on the service, it’s pretty easy to obtain that per‐\n",
      "son’s  exact  income.  There  are  a  wide  range  of  attacks  that  can  extract  information\n",
      "from models in this way.\n",
      "\n",
      "In addition to technical hardening and audit, governance plays a critical role in secu‐\n",
      "rity. Responsibilities must be assigned clearly and in a way that ensures an appropri‐\n",
      "ate balance between security and capacity of execution. It is also important to put in\n",
      "place feedback mechanisms, and employees and users should have an easy channel to\n",
      "communicate  breaches  (including,  potentially,  “bug  bounty  programs”  that  reward\n",
      "reporting  vulnerabilities).  It  is  also  possible,  and  necessary,  to  build  safety  nets\n",
      "around the system to mitigate the risks.\n",
      "\n",
      "Machine learning security shares many common traits with general computer system\n",
      "security,  one  of  the  main  ideas  being  that  security  is  not  an  additional  independent\n",
      "feature  of  the  system;  that  is,  generally  you  cannot  secure  a  system  that  is  not\n",
      "designed  to  be  secure,  and  the  organization  processes  must  take  into  account  the\n",
      "nature of the threat from the beginning. Strong MLOps processes, including all of the\n",
      "steps  in  preparing  for  production  described  in  this  chapter,  can  help  make  this\n",
      "approach a reality.\n",
      "\n",
      "Model Risk Mitigation\n",
      "Generally speaking, as discussed in detail in Chapter 1, the broader the model deploy‐\n",
      "ment, the greater the risk. When risk impact is high enough, it is essential to control\n",
      "the deployment of new versions, which is where tightly controlled MLOps processes\n",
      "come  into  play  in  particular.  Progressive  or  canary  rollouts  should  be  a  common\n",
      "practice, with new versions of models being served to a small proportion of the orga‐\n",
      "nization or customer base first and slowly increasing that proportion, while monitor‐\n",
      "ing behavior and getting human feedback if appropriate.\n",
      "\n",
      "Model Risk Mitigation \n",
      "\n",
      "| \n",
      "\n",
      "69\n",
      "\n",
      "\f",
      "Changing Environments\n",
      "Rapidly changing environments also multiply risk, as mentioned earlier in this chap‐\n",
      "ter. Changes in inputs is a related and also well-identified risk, and Chapter 7 dives\n",
      "into these challenges and how to address them in more detail. But what’s important to\n",
      "note  is  that  the  speed  of  change  can  amplify  the  risk  depending  on  the  application.\n",
      "Changes may be so fast that they have consequences even before the monitoring sys‐\n",
      "tem sends alerts. That is to say, even with an efficient monitoring system and a proce‐\n",
      "dure  to  retrain  models,  the  time  necessary  to  remediate  may  be  a  critical  threat,\n",
      "especially  if  simply  retraining  the  model  on  new  data  is  not  sufficient  and  a  new\n",
      "model must be developed. During this time, the production systems misbehaving can\n",
      "cause large losses for the organization.\n",
      "\n",
      "To  control  this  risk,  monitoring  via  MLOps  should  be  reactive  enough  (typically,\n",
      "alerting on distributions computed every week might not be enough), and the proce‐\n",
      "dure should consider the period necessary for remediation. For example, in addition\n",
      "to  retraining  or  rollout  strategies,  the  procedure  may  define  thresholds  that  would\n",
      "trigger  a  degraded  mode  for  the  system.  A  degraded  mode  may  simply  consist  of  a\n",
      "warning message displayed for end users, but could be as drastic as shutting down the\n",
      "dysfunctional system to avoid harm until a stable solution can be deployed.\n",
      "\n",
      "Less dramatic issues that are frequent enough can also do harm that quickly becomes\n",
      "difficult  to  control.  If  the  environment  changes  often,  even  if  remediation  never\n",
      "seems urgent, a model can always be slightly off, never operating within its nominal\n",
      "case, and the operating cost can be challenging to evaluate. This can only be detected\n",
      "through dedicated MLOps, including relatively long-term monitoring and reevaluat‐\n",
      "ing the cost of operating the model.\n",
      "\n",
      "In  many  cases,  retraining  the  model  on  more  data  will  increasingly  improve  the\n",
      "model, and this problem will eventually disappear, but this can take time. Before this\n",
      "convergence, a solution might be to use a less complex model that may have a lower\n",
      "evaluated  performance  and  may  be  more  consistent  in  a  frequently  changing\n",
      "environment.\n",
      "\n",
      "Interactions Between Models\n",
      "Complex  interactions  between  models  is  probably  the  most  challenging  source  of\n",
      "risk. This class of issue will be a growing concern as ML models become pervasive,\n",
      "and it’s an important potential area of focus for MLOps systems. Obviously, adding\n",
      "models  will  often  add  complexity  to  an  organization,  but  the  complexity  does  not\n",
      "necessarily grow linearly in proportion to the number of models; having two models\n",
      "is more complicated to understand than the sum since there are potential interactions\n",
      "between them.\n",
      "\n",
      "70 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "Moreover,  the  total  complexity  is  heavily  determined  by  how  the  interactions  with\n",
      "models  are  designed  at  a  local  scale  and  governed  at  an  organizational  scale.  Using\n",
      "models in chains (where a model uses inputs from another model) can create signifi‐\n",
      "cant additional complexity as well as totally unexpected results, whereas using models\n",
      "in independent parallel processing chains, which are each as short and explainable as\n",
      "possible,  is  a  much  more  sustainable  way  to  design  large-scale  deployment  of\n",
      "machine learning.\n",
      "\n",
      "First, the absence of obvious interactions between models makes the complexity grow\n",
      "closer  to  linearly  (though  note  that,  in  practice,  it  is  rarely  the  case,  as  there  can\n",
      "always be interactions in the real world even if models are not connected). Also, mod‐\n",
      "els used in redundant chains of processing can avoid errors—that is, if a decision is\n",
      "based on several independent chains of processing with methods as different as possi‐\n",
      "ble, it can be more robust.\n",
      "\n",
      "Finally, generally speaking, the more complex the model, the more complex its inter‐\n",
      "actions with other systems may be, as it may have many edge cases, be less stability in\n",
      "some domains, overreact to the changes of an upstream model, or confuse a sensitive\n",
      "downstream model, etc. Here again, we see that model complexity has a cost, and a\n",
      "potentially highly unpredictable one at that.\n",
      "\n",
      "Model Misbehavior\n",
      "A  number  of  measures  can  be  implemented  to  avoid  model  misbehavior,  including\n",
      "examining its inputs and outputs in real time. While training a model, it is possible to\n",
      "characterize  its  domain  of  applicability  by  examining  the  intervals  on  which  the\n",
      "model was trained and validated. If the value of a feature at inference time is out of\n",
      "bounds,  the  system  can  trigger  appropriate  measures  (e.g.,  rejecting  the  sample  or\n",
      "dispatching a warning message).\n",
      "\n",
      "Controlling feature-value intervals is a useful and simple technique, but it might be\n",
      "insufficient. For example, when training an algorithm to evaluate car prices, the data\n",
      "may  have  provided  examples  of  recent  light  cars  and  old  heavy  cars,  but  no  recent\n",
      "heavy  cars.  The  performance  of  a  complex  model  for  these  is  unpredictable.  When\n",
      "the  number  of  features  is  large,  this  issue  becomes  unavoidable  due  to  the  curse  of\n",
      "dimensionality—i.e., the number of combinations is exponential relative to the num‐\n",
      "ber of features.\n",
      "\n",
      "In  these  situations,  more  sophisticated  methods  can  be  used,  including  anomaly\n",
      "detection  to  identify  records  where  the  model  is  used  outside  of  its  application\n",
      "domain. After scoring, the outputs of the model can be examined before confirming\n",
      "the inference. In the case of classification, many algorithms provide certainty scores\n",
      "in  addition  to  their  prediction,  and  a  threshold  can  be  fixed  to  accept  an  inference\n",
      "output.  Note  that  these  certainty  scores  do  not  typically  translate  into  probabilities,\n",
      "even if they are named this way in the model.\n",
      "\n",
      "Model Risk Mitigation \n",
      "\n",
      "| \n",
      "\n",
      "71\n",
      "\n",
      "\f",
      "Conformal prediction is a set of techniques that helps calibrate these scores to obtain\n",
      "an accurate estimation of the probability of correctness. For regression, the value can\n",
      "be checked against a predetermined interval. For example, if the model predicts a car\n",
      "costs $50 or $500,000, you may not want to commit any business on this prediction.\n",
      "The  complexity  of  the  implemented  techniques  should  be  relevant  for  the  level  of\n",
      "risk: a highly complex, highly critical model will require more thorough safeguards.\n",
      "\n",
      "Closing Thoughts\n",
      "In practice, preparing models for production starts from the beginning at the devel‐\n",
      "opment  phase;  that  is  to  say,  the  requirements  of  production  deployments,  security\n",
      "implications, and risk mitigation aspects should be considered when developing the\n",
      "models. MLOps includes having a clear validation step before sending models to pro‐\n",
      "duction, and the key ideas to successfully prepare models for productions are:\n",
      "\n",
      "• Clearly identifying the nature of the risks and their magnitudes\n",
      "\n",
      "• Understanding  model  complexity  and  its  impact  at  multiple  levels,  including\n",
      "increased  latency,  increased  memory  and  power  consumption,  lower  ability  to\n",
      "interpret inference in production, and a harder-to-control risk\n",
      "\n",
      "• Providing a simple but clear standard of quality, making sure the team is appro‐\n",
      "priately trained and the organization structure allows for fast and reliable valida‐\n",
      "tion processes\n",
      "\n",
      "• Automating all the validation that can be automated to ensure it is properly and\n",
      "\n",
      "consistently performed while maintaining the ability to deploy quickly\n",
      "\n",
      "72 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Preparing for Production\n",
      "\n",
      "\f",
      "CHAPTER 6\n",
      "Deploying to Production\n",
      "\n",
      "Joachim Zentici\n",
      "\n",
      "Business leaders view the rapid deployment of new systems into production as key to\n",
      "maximizing business value. But this is only true if deployment can be done smoothly\n",
      "and  at  low  risk  (software  deployment  processes  have  become  more  automated  and\n",
      "rigorous in recent years to address this inherent conflict). This chapter dives into the\n",
      "concepts and considerations when deploying machine learning models to production\n",
      "that  impact—and  indeed,  drive—the  way  MLOps  deployment  processes  are  built\n",
      "(Figure 6-1 presents this phase in the context of the larger life cycle).\n",
      "\n",
      "Figure 6-1. Deployment to production highlighted in the larger context of the ML project\n",
      "life cycle\n",
      "\n",
      "CI/CD Pipelines\n",
      "CI/CD is a common acronym for continuous integration and continuous delivery (or\n",
      "put more simply, deployment). The two form a modern philosophy of agile software\n",
      "development and a set of practices and tools to release applications more often and\n",
      "faster, while also better controlling quality and risk.\n",
      "\n",
      "73\n",
      "\n",
      "\f",
      "While  these  ideas  are  decades  old  and  already  used  to  various  extents  by  software\n",
      "engineers, different people and organizations use certain terms in very different ways.\n",
      "Before digging into how CI/CD applies to machine learning workflows, it is essential\n",
      "to keep in mind that these concepts should be tools to serve the purpose of delivering\n",
      "quality  fast,  and  the  first  step  is  always  to  identify  the  specific  risks  present  at  the\n",
      "organization.  In  other  words,  as  always,  CI/CD  methodology  should  be  adapted\n",
      "based on the needs of the team and the nature of the business.\n",
      "\n",
      "CI/CD concepts apply to traditional software engineering, but they apply just as well\n",
      "to machine learning systems and are a critical part of MLOps strategy. After success‐\n",
      "fully developing a model, a data scientist should push the code, metadata, and docu‐\n",
      "mentation to a central repository and trigger a CI/CD pipeline. An example of such\n",
      "pipeline could be:\n",
      "\n",
      "1. Build the model\n",
      "\n",
      "a. Build the model artifacts\n",
      "\n",
      "b. Send the artifacts to long-term storage\n",
      "\n",
      "c. Run basic checks (smoke tests/sanity checks)\n",
      "\n",
      "d. Generate fairness and explainability reports\n",
      "\n",
      "2. Deploy to a test environment\n",
      "\n",
      "a. Run tests to validate ML performance, computational performance\n",
      "\n",
      "b. Validate manually\n",
      "\n",
      "3. Deploy to production environment\n",
      "\n",
      "a. Deploy the model as canary\n",
      "\n",
      "b. Fully deploy the model\n",
      "\n",
      "Many scenarios are possible and depend on the application, the risks from which the\n",
      "system should be protected, and the way the organization chooses to operate. Gener‐\n",
      "ally  speaking,  an  incremental  approach  to  building  a  CI/CD  pipeline  is  preferred:  a\n",
      "simple or even naïve workflow on which a team can iterate is often much better than\n",
      "starting with complex infrastructure from scratch.\n",
      "\n",
      "A starting project does not have the infrastructure requirements of a tech giant, and it\n",
      "can be hard to know up front which challenges deployments will present. There are\n",
      "common tools and best practices, but there is no one-size-fits-all CI/CD methodol‐\n",
      "ogy. This means the best path forward is starting from a simple (but fully functional)\n",
      "CI/CD  workflow  and  introducing  additional  or  more  sophisticated  steps  along  the\n",
      "way as quality or scaling challenges appear.\n",
      "\n",
      "74 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "Building ML Artifacts\n",
      "The goal of a continuous integration pipeline is to avoid unnecessary effort in merg‐\n",
      "ing the work from several contributors as well as to detect bugs or development con‐\n",
      "flicts  as  soon  as  possible.  The  very  first  step  is  using  centralized  version  control\n",
      "systems  (unfortunately,  working  for  weeks  on  code  stored  only  on  a  laptop  is  still\n",
      "quite common).\n",
      "\n",
      "The  most  common  version  control  system  is  Git,  an  open  source  software  initially\n",
      "developed to manage the source code for the Linux kernel. The majority of software\n",
      "engineers across the world already use Git, and it is increasingly being adopted in sci‐\n",
      "entific  computing  and  data  science.  It  allows  for  maintaining  a  clear  history  of\n",
      "changes, safe rollback to a previous version of the code, multiple contributors to work\n",
      "on their own branches of the project before merging to the main branch, etc.\n",
      "\n",
      "While Git is appropriate for code, it was not designed to store other types of assets\n",
      "common  in  data  science  workflows,  such  as  large  binary  files  (for  example,  trained\n",
      "model weights), or to version the data itself. Data versioning is a more complex topic\n",
      "with numerous solutions, including Git extensions, file formats, databases, etc.\n",
      "\n",
      "What’s in an ML Artifact?\n",
      "Once the code and data is in a centralized repository, a testable and deployable bun‐\n",
      "dle of the project must be built. These bundles are usually called artifacts in the con‐\n",
      "text  of  CI/CD.  Each  of  the  following  elements  needs  to  be  bundled  into  an  artifact\n",
      "that  goes  through  a  testing  pipeline  and  is  made  available  for  deployment  to\n",
      "production:\n",
      "\n",
      "• Code for the model and its preprocessing\n",
      "\n",
      "• Hyperparameters and configuration\n",
      "\n",
      "• Training and validation data\n",
      "\n",
      "• Trained model in its runnable form\n",
      "\n",
      "• An  environment  including  libraries  with  specific  versions,  environment  vari‐\n",
      "\n",
      "ables, etc.\n",
      "\n",
      "• Documentation\n",
      "\n",
      "• Code and data for testing scenarios\n",
      "\n",
      "The Testing Pipeline\n",
      "As touched on in Chapter 5, the testing pipeline can validate a wide variety of proper‐\n",
      "ties of the model contained in the artifact. One of the important operational aspects\n",
      "\n",
      "Building ML Artifacts \n",
      "\n",
      "| \n",
      "\n",
      "75\n",
      "\n",
      "\f",
      "of  testing  is  that,  in  addition  to  verifying  compliance  with  requirements,  good  tests\n",
      "should make it as easy as possible to diagnose the source issue when they fail.\n",
      "\n",
      "For that purpose, naming the tests is extremely important, and carefully choosing a\n",
      "number of datasets to validate the model against can be valuable. For example:\n",
      "\n",
      "• A test on a fixed (not automatically updated) dataset with simple data and not-\n",
      "too-restrictive  performance  thresholds  can  be  executed  first  and  called  “base\n",
      "case.” If the test reports show that this test failed, there is a strong possibility that\n",
      "the model is way off, and the cause may be a programming error or a misuse of\n",
      "the model, for example.\n",
      "\n",
      "• Then,  a  number  of  datasets  that  each  have  one  specific  oddity  (missing  values,\n",
      "extreme values, etc.) could be used with tests appropriately named so that the test\n",
      "report immediately shows the kind of data that is likely to make the model fail.\n",
      "These datasets can represent realistic yet remarkable cases, but it may also be use‐\n",
      "ful to generate synthetic data that is not expected in production. This could pos‐\n",
      "sibly  protect  the  model  from  new  situations  not  yet  encountered,  but  most\n",
      "importantly,  this  could  protect  the  model  from  malfunctions  in  the  system\n",
      "querying or from adversarial examples (as discussed in “Machine Learning Secu‐\n",
      "rity” on page 67).\n",
      "\n",
      "• Then, an essential part of model validation is testing on recent production data.\n",
      "One or several datasets should be used, extracted from several time windows and\n",
      "named appropriately. This category of tests should be performed and automati‐\n",
      "cally analyzed when the model is already deployed to production. Chapter 7 pro‐\n",
      "vides more specific details on how to do that.\n",
      "\n",
      "Automating these tests as much as possible is essential and, indeed, is a key compo‐\n",
      "nent  of  efficient  MLOps.  A  lack  of  automation  or  speed  wastes  time,  but,  more\n",
      "importantly, it discourages the development team from testing and deploying often,\n",
      "which  can  delay  the  discovery  of  bugs  or  design  choices  that  make  it  impossible  to\n",
      "deploy to production.\n",
      "\n",
      "In  extreme  cases,  a  development  team  can  hand  over  a  monthslong  project  to  a\n",
      "deployment team that will simply reject it because it does not satisfy requirements for\n",
      "the  production  infrastructure.  Also,  less  frequent  deployments  imply  larger  incre‐\n",
      "ments that are harder to manage; when many changes are deployed at once and the\n",
      "system is not behaving in the desired way, isolating the origin of an issue is more time\n",
      "consuming.\n",
      "\n",
      "The most widespread tool for software engineering continuous integration is Jenkins,\n",
      "a very flexible build system that allows for the building of CI/CD pipelines regardless\n",
      "of the programming language, testing framework, etc. Jenkins can be used in data sci‐\n",
      "ence to orchestrate CI/CD pipelines, although there are many other options.\n",
      "\n",
      "76 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "Deployment Strategies\n",
      "To  understand  the  details  of  a  deployment  pipeline,  it  is  important  to  distinguish\n",
      "among concepts often used inconsistently or interchangeably.\n",
      "\n",
      "Integration\n",
      "\n",
      "The process of merging a contribution to a central repository (typically merging\n",
      "a Git feature branch to the main branch) and performing more or less complex\n",
      "tests.\n",
      "\n",
      "Delivery\n",
      "\n",
      "As used in the continuous delivery (CD) part of CI/CD, the process of building a\n",
      "fully  packaged  and  validated  version  of  the  model  ready  to  be  deployed  to\n",
      "production.\n",
      "\n",
      "Deployment\n",
      "\n",
      "The  process  of  running  a  new  model  version  on  a  target  infrastructure.  Fully\n",
      "automated deployment is not always practical or desirable and is a business deci‐\n",
      "sion as much as a technical decision, whereas continuous delivery is a tool for the\n",
      "development  team  to  improve  productivity  and  quality  as  well  as  measure  pro‐\n",
      "gress more reliably. Continuous delivery is required for continuous deployment,\n",
      "but it also provides enormous value without.\n",
      "\n",
      "Release\n",
      "\n",
      "In principle, release is yet another step, as deploying a model version (even to the\n",
      "production infrastructure) does not necessarily mean that the production work‐\n",
      "load is directed to the new version. As we will see, multiple versions of a model\n",
      "can run at the same time on the production infrastructure.\n",
      "\n",
      "Getting everyone in the MLOps process on the same page about what these concepts\n",
      "mean and how they apply will allow for smoother processes on both the technical and\n",
      "business sides.\n",
      "\n",
      "Categories of Model Deployment\n",
      "In addition to different deployment strategies, there are two ways to approach model\n",
      "deployment:\n",
      "\n",
      "• Batch scoring, where whole datasets are processed using a model, such as in daily\n",
      "\n",
      "scheduled jobs.\n",
      "\n",
      "• Real-time  scoring,  where  one  or  a  small  number  of  records  are  scored,  such  as\n",
      "when an ad is displayed on a website and a user session is scored by models to\n",
      "decide what to display.\n",
      "\n",
      "Deployment Strategies \n",
      "\n",
      "| \n",
      "\n",
      "77\n",
      "\n",
      "\f",
      "There  is  a  continuum  between  these  two  approaches,  and  in  fact,  in  some  systems,\n",
      "scoring  on  one  record  is  technically  identical  to  requesting  a  batch  of  one.  In  both\n",
      "cases,  multiple  instances  of  the  model  can  be  deployed  to  increase  throughput  and\n",
      "potentially lower latency.\n",
      "\n",
      "Deploying many real-time scoring systems is conceptually simpler since the records\n",
      "to be scored can be dispatched between several machines (e.g., using a load balancer). \n",
      "Batch scoring can also be parallelized, for example by using a parallel processing run‐\n",
      "time  like  Apache  Spark,  but  also  by  splitting  datasets  (which  is  usually  called  parti‐\n",
      "tioning  or  sharding)  and  scoring  the  partitions  independently.  Note  that  these  two\n",
      "concepts of splitting the data and computation can be combined, as they can address\n",
      "different problems.\n",
      "\n",
      "Considerations When Sending Models to Production\n",
      "When sending a new model version to production, the first consideration is often to\n",
      "avoid downtime, in particular for real-time scoring. The basic idea is that rather than\n",
      "shutting down the system, upgrading it, and then putting it back online, a new system\n",
      "can  be  set  up  next  to  the  stable  one,  and  when  it’s  functional,  the  workload  can  be\n",
      "directed to the newly deployed version (and if it remains healthy, the old one is shut\n",
      "down).  This  deployment  strategy  is  called  blue-green—or  sometimes  red-black—\n",
      "deployment. There are many variations and frameworks (like Kubernetes) to handle\n",
      "this natively.\n",
      "\n",
      "Another more advanced solution to mitigate the risk is to have canary releases (also\n",
      "called canary deployments). The idea is that the stable version of the model is kept in\n",
      "production, but a certain percentage of the workload is redirected to the new model,\n",
      "and results are monitored. This strategy is usually implemented for real-time scoring,\n",
      "but a version of it could also be considered for batch.\n",
      "\n",
      "A  number  of  computational  performance  and  statistical  tests  can  be  performed  to\n",
      "decide whether to fully switch to the new model, potentially in several workload per‐\n",
      "centage increments. This way, a malfunction would likely impact only a small portion\n",
      "of the workload.\n",
      "\n",
      "Canary releases apply to production systems, so any malfunction is an incident, but\n",
      "the idea here is to limit the blast radius. Note that scoring queries that are handled by\n",
      "the canary model should be carefully picked, because some issues may go unnoticed\n",
      "otherwise. For example, if the canary model is serving a small percentage of a region\n",
      "or  country  before  the  model  is  fully  released  globally,  it  could  be  the  case  that  (for\n",
      "machine learning or infrastructure reasons) the model does not perform as expected\n",
      "in other regions.\n",
      "\n",
      "78 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "A more robust approach is to pick the portion of users served by the new model at\n",
      "random,  but  then  it  is  often  desirable  for  user  experience  to  implement  an  affinity\n",
      "mechanism so that the same user always uses the same version of the model.\n",
      "\n",
      "Canary testing can be used to carry out A/B testing, which is a process to compare\n",
      "two  versions  of  an  application  in  terms  of  a  business  performance  metric.  The  two\n",
      "concepts  are  related  but  not  the  same,  as  they  don’t  operate  at  the  same  level  of\n",
      "abstraction. A/B testing can be made possible through a canary release, but it could\n",
      "also  be  implemented  as  logic  directly  coded  into  a  single  version  of  an  application.\n",
      "Chapter 7 provides more details on the statistical aspects of setting up A/B testing.\n",
      "\n",
      "Overall,  canary  releases  are  a  powerful  tool,  but  they  require  somewhat  advanced\n",
      "tooling to manage the deployment, gather the metrics, specify and run computations\n",
      "on them, display the results, and dispatch and process alerts.\n",
      "\n",
      "Maintenance in Production\n",
      "Once a model is released, it must be maintained. At a high level, there are three main‐\n",
      "tenance measures:\n",
      "\n",
      "Resource monitoring\n",
      "\n",
      "Just as for any application running on a server, collecting IT metrics such as CPU,\n",
      "memory, disk, or network usage can be useful to detect and troubleshoot issues.\n",
      "\n",
      "Health check\n",
      "\n",
      "To check if the model is indeed online and to analyze its latency, it is common to\n",
      "implement  a  health  check  mechanism  that  simply  queries  the  model  at  a  fixed\n",
      "interval (on the order of one minute) and logs the results.\n",
      "\n",
      "ML metrics monitoring\n",
      "\n",
      "This  is  about  analyzing  the  accuracy  of  the  model  and  comparing  it  to  another\n",
      "version or detecting when it is going stale. Since it may require heavy computa‐\n",
      "tion, this is typically lower frequency, but as always, will depend on the applica‐\n",
      "tion;  it  is  typically  done  once  a  week.  Chapter  7  details  how  to  implement  this\n",
      "feedback loop.\n",
      "\n",
      "Finally, when a malfunction is detected, a rollback to a previous version may be nec‐\n",
      "essary. It is critical to have the rollback procedure ready and as automated as possible;\n",
      "testing it regularly can make sure it is indeed functional.\n",
      "\n",
      "Containerization\n",
      "As described earlier, managing the versions of a model is much more than just saving\n",
      "its code into a version control system. In particular, it is necessary to provide an exact\n",
      "description of the environment (including, for example, all the Python libraries used\n",
      "as well as their versions, the system dependencies that need to be installed, etc.).\n",
      "\n",
      "Containerization \n",
      "\n",
      "| \n",
      "\n",
      "79\n",
      "\n",
      "\f",
      "But storing this metadata is not enough. Deploying to production should automati‐\n",
      "cally  and  reliably  rebuild  this  environment  on  the  target  machine.  In  addition,  the\n",
      "target  machine  will  typically  run  multiple  models  simultaneously,  and  two  models\n",
      "may have incompatible dependency versions. Finally, several models running on the\n",
      "same machine could compete for resources, and one misbehaving model could hurt\n",
      "the performance of multiple cohosted models.\n",
      "\n",
      "Containerization  technology  is  increasingly  used  to  tackle  these  challenges.  These\n",
      "tools bundle an application together with all of its related configuration files, libraries,\n",
      "and dependencies that are required for it to run across different operating environ‐\n",
      "ments.  Unlike  virtual  machines  (VMs),  containers  do  not  duplicate  the  complete\n",
      "operating  system;  multiple  containers  share  a  common  operating  system  and  are\n",
      "therefore far more resource efficient. \n",
      "\n",
      "The  most  well-known  containerization  technology  is  the  open  source  platform\n",
      "Docker. Released in 2014, it has become the de facto standard. It allows an application\n",
      "to be packaged, sent to a server (the Docker host), and run with all its dependencies\n",
      "in isolation from other applications.\n",
      "\n",
      "Building  the  basis  of  a  model-serving  environment  that  can  accommodate  many\n",
      "models, each of which may run multiple copies, may require multiple Docker hosts.\n",
      "When deploying a model, the framework should solve a number of issues:\n",
      "\n",
      "• Which Docker host(s) should receive the container?\n",
      "\n",
      "• When a model is deployed in several copies, how can the workload be balanced?\n",
      "\n",
      "• What happens if the model becomes unresponsive, for example, if the machine\n",
      "\n",
      "hosting it fails? How can that be detected and a container reprovisioned?\n",
      "\n",
      "• How  can  a  model  running  on  multiple  machines  be  upgraded,  with  assurances\n",
      "that old and new versions are switched on and off, and that the load balancer is\n",
      "updated with a correct sequence?\n",
      "\n",
      "Kubernetes, an open source platform that has gained a lot of traction in the past few\n",
      "years  and  is  becoming  the  standard  for  container  orchestration,  greatly  simplifies\n",
      "these issues and many others. It provides a powerful declarative API to run applica‐\n",
      "tions  in  a  group  of  Docker  hosts,  called  a  Kubernetes  cluster.  The  word  declarative\n",
      "means that rather than trying to express in code the steps to set up, monitor, upgrade,\n",
      "stop, and connect the container (which can be complex and error prone), users spec‐\n",
      "ify in a configuration file the desired state, and Kubernetes makes it happen and then\n",
      "maintains it.\n",
      "\n",
      "For example, users need only specify to Kubernetes “make sure four instances of this\n",
      "container run at all times,” and Kubernetes will allocate the hosts, start the containers,\n",
      "monitor them, and start a new instance if one of them fails. Finally, the major cloud\n",
      "providers all provide managed Kubernetes services; users do not even have to install\n",
      "\n",
      "80 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "and maintain Kubernetes itself. If an application or a model is packaged as a Docker\n",
      "container,  users  can  directly  submit  it,  and  the  service  will  provision  the  required\n",
      "machines to run one or several instances of the container inside Kubernetes.\n",
      "\n",
      "Docker  with  Kubernetes  can  provide  a  powerful  infrastructure  to  host  applications,\n",
      "including ML models. Leveraging these products greatly simplifies the implementa‐\n",
      "tion of the deployment strategies—like blue-green deployments or canary releases—\n",
      "although they are not aware of the nature of the deployed applications and thus can’t\n",
      "natively manage the ML performance analysis. Another major advantage of this type\n",
      "of infrastructure is the ability to easily scale the model’s deployment.\n",
      "\n",
      "Scaling Deployments\n",
      "As ML adoption grows, organizations face two types of growth challenges:\n",
      "\n",
      "• The ability to use a model in production with high-scale data\n",
      "\n",
      "• The ability to train larger and larger numbers of models\n",
      "\n",
      "Handling more data for real-time scoring is made much easier by frameworks such as\n",
      "Kubernetes. Since most of the time trained models are essentially formulas, they can\n",
      "be replicated in the cluster in as many copies as necessary. With the auto-scaling fea‐\n",
      "tures  in  Kubernetes,  both  provisioning  new  machines  and  load  balancing  are  fully\n",
      "handled by the framework, and setting up a system with huge scaling capabilities is\n",
      "now relatively simple. The major difficulty can then be to process the large amount of\n",
      "monitoring data; Chapter 7 provides some details on this challenge.\n",
      "\n",
      "Scalable and Elastic Systems\n",
      "A computational system is said to be horizontally scalable (or just scalable) if it is pos‐\n",
      "sible to incrementally add more computers to expand its processing power. For exam‐\n",
      "ple,  a  Kubernetes  cluster  can  be  expanded  to  hundreds  of  machines.  However,  if  a\n",
      "system includes only one machine, it may be challenging to incrementally upgrade it\n",
      "significantly,  and  at  some  point,  a  migration  to  a  bigger  machine  or  a  horizontally\n",
      "scalable system will be required (and may be very expensive and require interruption\n",
      "of service).\n",
      "\n",
      "An elastic system allows, in addition to being scalable, easy addition and removal of\n",
      "resources to match the compute requirements. For example, a Kubernetes cluster in\n",
      "the cloud can have an auto-scaling capability that automatically adds machines when\n",
      "the cluster usage metrics are high and removes them when they are low. In principle,\n",
      "elastic  systems  can  optimize  the  usage  of  resources;  they  automatically  adapt  to  an\n",
      "increase in usage without the need to permanently provision resources that are rarely\n",
      "required.\n",
      "\n",
      "Scaling Deployments \n",
      "\n",
      "| \n",
      "\n",
      "81\n",
      "\n",
      "\f",
      "For  batch  scoring,  the  situation  can  be  more  complex.  When  the  volume  of  data\n",
      "becomes  too  large,  there  are  essentially  two  types  of  strategies  to  distribute  the\n",
      "computation:\n",
      "\n",
      "• Using  a  framework  that  handles  distributed  computation  natively,  in  particular\n",
      "Spark. Spark is an open source distributed computation framework. It is useful to\n",
      "understand that Spark and Kubernetes do not play similar roles and can be com‐\n",
      "bined.  Kubernetes  orchestrates  containers,  but  Kubernetes  is  not  aware  of  what\n",
      "the containers are actually doing; as far as Kubernetes is concerned, they are just\n",
      "containers that run an application on one specific host. (In particular, Kubernetes\n",
      "has no concept of data processing, as it can be used to run any kind of applica‐\n",
      "tion.) Spark is a computation framework that can split the data and the computa‐\n",
      "tion among its nodes. A modern way to use Spark is through Kubernetes. To run\n",
      "a  Spark  job,  the  desired  number  of  Spark  containers  are  started  by  Kubernetes;\n",
      "once they are started, they can communicate to complete the computation, after\n",
      "which  the  containers  are  destroyed  and  the  resources  are  available  for  other\n",
      "applications, including other Spark jobs that may have different Spark versions or\n",
      "dependencies.\n",
      "\n",
      "• Another  way  to  distribute  batch  processing  is  to  partition  the  data.  There  are\n",
      "many ways to achieve this, but the general idea is that scoring is typically a row-\n",
      "by-row  operation  (each  row  is  scored  one  by  one),  and  the  data  can  be  split  in\n",
      "some way so that several machines can each read a subset of the data and score a\n",
      "subset of the rows.\n",
      "\n",
      "In terms of computation, scaling the number of models is somewhat simpler. The key\n",
      "is to add more computing power and to make sure the monitoring infrastructure can\n",
      "handle the workload. But in terms of governance and processes, this is the most chal‐\n",
      "lenging situation.\n",
      "\n",
      "In particular, scaling the number of models means that the CI/CD pipeline must be\n",
      "able  to  handle  large  numbers  of  deployments.  As  the  number  of  models  grows,  the\n",
      "need for automation and governance grows, as human verification cannot necessarily\n",
      "be systematic or consistent.\n",
      "\n",
      "In some applications, it is possible to rely on fully automated continuous deployment\n",
      "if the risks are well controlled by automated validation, canary releases, and automa‐\n",
      "ted canary analysis. There can be numerous infrastructure challenges since training,\n",
      "building  models,  validating  on  test  data,  etc.,  all  need  to  be  performed  on  clusters\n",
      "rather than on a single machine. Also, with a higher number of models, the CI/CD\n",
      "pipeline of each model can vary widely, and if nothing is done, each team will have to\n",
      "develop its own CI/CD pipeline for each model.\n",
      "\n",
      "This is suboptimal from efficiency and governance perspectives. While some models\n",
      "may need highly specific validation pipelines, most projects can probably use a small\n",
      "\n",
      "82 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "number of common patterns. In addition, maintenance is made much more complex\n",
      "as  it  may  become  impractical  to  implement  a  new  systematic  validation  step,  for\n",
      "example,  since  the  pipelines  would  not  necessarily  share  a  common  structure  and\n",
      "would then be impossible to update safely, even programmatically. Sharing practices\n",
      "and  standardized  pipelines  can  help  limit  complexity.  A  dedicated  tool  to  manage\n",
      "large numbers of pipelines can also be used; for example, Netflix released Spinnaker,\n",
      "an open source continuous deployment and infrastructure management platform.\n",
      "\n",
      "Requirements and Challenges\n",
      "When deploying a model, there are several possible scenarios:\n",
      "\n",
      "• One model deployed on one server\n",
      "\n",
      "• One model deployed on multiple servers\n",
      "\n",
      "• Multiple versions of a model deployed on one server\n",
      "\n",
      "• Multiple versions of a model deployed on multiple servers\n",
      "\n",
      "• Multiple versions of multiple models deployed on multiple servers\n",
      "\n",
      "An effective logging system should be able to generate centralized datasets that can be\n",
      "exploited by the model designer or the ML engineer, usually outside of the produc‐\n",
      "tion environment. More specifically, it should cover all of the following situations:\n",
      "\n",
      "• The system can access and retrieve scoring logs from multiple servers, either in a\n",
      "\n",
      "real-time scoring use case or in a batch scoring use case.\n",
      "\n",
      "• When a model is deployed on multiple servers, the system can handle the map‐\n",
      "\n",
      "ping and aggregation of all information per model across servers.\n",
      "\n",
      "• When  different  versions  of  a  model  are  deployed,  the  system  can  handle  the\n",
      "mapping  and  aggregation  of  all  information  per  version  of  the  model  across\n",
      "servers.\n",
      "\n",
      "In terms of challenges, for large-scale machine learning applications, the number of\n",
      "raw event logs generated can be an issue if there are no preprocessing steps in place to\n",
      "filter  and  aggregate  data.  For  real-time  scoring  use  cases,  logging  streaming  data\n",
      "requires  setting  up  a  whole  new  set  of  tooling  that  entails  a  significant  engineering\n",
      "effort to maintain. However, in both cases, because the goal of monitoring is usually\n",
      "to  estimate  aggregate  metrics,  saving  only  a  subset  of  the  predictions  may  be\n",
      "acceptable.\n",
      "\n",
      "Requirements and Challenges \n",
      "\n",
      "| \n",
      "\n",
      "83\n",
      "\n",
      "\f",
      "Closing Thoughts\n",
      "Deploying  to  production  is  a  key  component  of  MLOps,  and  as  dissected  in  this\n",
      "chapter,  having  the  right  processes  and  tools  in  place  can  ensure  that  it  happens\n",
      "quickly. The good news is that many of the elements of success, particularly CI/CD\n",
      "best  practices,  are  not  new.  Once  teams  understand  how  they  can  be  applied  to\n",
      "machine learning models, the organization will have a good foundation on which to\n",
      "expand as MLOps scales with the business.\n",
      "\n",
      "84 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Deploying to Production\n",
      "\n",
      "\f",
      "CHAPTER 7\n",
      "Monitoring and Feedback Loop\n",
      "\n",
      "Du Phan\n",
      "\n",
      "When a machine learning model is deployed in production, it can start degrading in\n",
      "quality fast—and without warning—until it’s too late (i.e., it’s had a potentially nega‐\n",
      "tive impact on the business). That’s why model monitoring is a crucial step in the ML\n",
      "model life cycle and a critical piece of MLOps (illustrated in Figure 7-1 as a part of\n",
      "the overall life cycle).\n",
      "\n",
      "Figure 7-1. Monitoring and feedback loop highlighted in the larger context of the ML\n",
      "project life cycle\n",
      "\n",
      "Machine learning models need to be monitored at two levels:\n",
      "\n",
      "• At  the  resource  level,  including  ensuring  the  model  is  running  correctly  in  the\n",
      "production  environment.  Key  questions  include:  Is  the  system  alive?  Are  the\n",
      "CPU, RAM, network usage, and disk space as expected? Are requests being pro‐\n",
      "cessed at the expected rate?\n",
      "\n",
      "85\n",
      "\n",
      "\f",
      "• At the performance level, meaning monitoring the pertinence of the model over\n",
      "time. Key questions include: Is the model still an accurate representation of the\n",
      "pattern of new incoming data? Is it performing as well as it did during the design\n",
      "phase?\n",
      "\n",
      "The first level is a traditional DevOps topic that has been extensively addressed in the\n",
      "literature (and has been covered in Chapter 6). However, the latter is more compli‐\n",
      "cated.  Why?  Because  how  well  a  model  performs  is  a  reflection  of  the  data  used  to\n",
      "train it; in particular, how representative that training data is of the live request data.\n",
      "As the world is constantly changing, a static model cannot catch up with new patterns\n",
      "that  are  emerging  and  evolving  without  a  constant  source  of  new  data.  While  it  is\n",
      "possible to detect large deviations on single predictions (see Chapter 5), smaller but\n",
      "still significant deviations have to be detected statistically on datasets of scored rows,\n",
      "with or without ground truth.\n",
      "\n",
      "Model performance monitoring attempts to track this degradation, and, at an appro‐\n",
      "priate time, it will also trigger the retraining of the model with more representative\n",
      "data. This chapter delves into detail on how data teams should handle both monitor‐\n",
      "ing and subsequent retraining.\n",
      "\n",
      "How Often Should Models Be Retrained?\n",
      "One  of  the  key  questions  teams  have  regarding  monitoring  and  retraining  is:  how\n",
      "often  should  models  be  retrained?  Unfortunately,  there  is  no  easy  answer,  as  this\n",
      "question depends on many factors, including:\n",
      "\n",
      "The domain\n",
      "\n",
      "Models in areas like cybersecurity or real-time trading need to be updated regu‐\n",
      "larly to keep up with the constant changes inherent in these fields. Physical mod‐\n",
      "els,  like  voice  recognition,  are  generally  more  stable,  because  the  patterns  don’t\n",
      "often abruptly change. However, even more stable physical models need to adapt\n",
      "to change: what happens to a voice recognition model if the person has a cough\n",
      "and the tone of their voice changes?\n",
      "\n",
      "The cost\n",
      "\n",
      "Organizations  need  to  consider  whether  the  cost  of  retraining  is  worth  the\n",
      "improvement in performance. For example, if it takes one week to run the whole\n",
      "data pipeline and retrain the model, is it worth a 1% improvement?\n",
      "\n",
      "The model performance\n",
      "\n",
      "In some situations, the model performance is restrained by the limited number of\n",
      "training examples, and thus the decision to retrain hinges on collecting enough\n",
      "new data.\n",
      "\n",
      "86 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "Whatever the domain, the delay to obtain the ground truth is key to defining a lower\n",
      "bound to the retraining period. It is very risky to use a prediction model when there\n",
      "is  a  possibility  that  it  drifts  faster  than  the  lag  between  prediction  time  and  ground\n",
      "truth obtention time. In this scenario, the model can start giving bad results without\n",
      "any  recourse  other  than  to  withdraw  the  model  if  the  drift  is  too  significant.  What\n",
      "this means in practice is that it is unlikely a model with a lag of one year is retrained\n",
      "more than a few times a year.\n",
      "\n",
      "For the same reason, it is unlikely that a model is trained on data collected during a\n",
      "period  smaller  than  this  lag.  Retraining  will  not  be  performed  in  a  shorter  period,\n",
      "either.  In  other  words,  if  the  model  retraining  occurs  way  more  often  than  the  lag,\n",
      "there will be almost no impact of the retraining on the performance of the model.\n",
      "\n",
      "There  are  also  two  organizational  bounds  to  consider  when  it  comes  to  retraining\n",
      "frequency:\n",
      "\n",
      "An upper bound\n",
      "\n",
      "It  is  better  to  perform  retraining  once  every  year  to  ensure  that  the  team  in\n",
      "charge has the skills to do it (despite potential turnover—i.e., the possibility that\n",
      "the people retraining the model were not the ones who built it) and that the com‐\n",
      "puting toolchain is still up.\n",
      "\n",
      "A lower bound\n",
      "\n",
      "Take, for example, a model with near-instantaneous feedback, such as a recom‐\n",
      "mendation engine where the user clicks on the product offerings within seconds\n",
      "after the prediction. Advanced deployment schemes will involve shadow testing\n",
      "or A/B testing to make sure that the model performs as anticipated. Because it is\n",
      "a statistical validation, it takes some time to gather the required information. This\n",
      "necessarily  sets  a  lower  bound  to  the  retraining  period.  Even  with  a  simple\n",
      "deployment,  the  process  will  probably  allow  for  some  human  validation  or  for\n",
      "the  possibility  of  manual  rollback,  which  means  it’s  unlikely  that  the  retraining\n",
      "will occur more than once a day.\n",
      "\n",
      "Therefore, it is very likely that retraining will be done between once a day and once a\n",
      "year. The simplest solution that consists of retraining the model in the same way and\n",
      "in the same environment it was trained in originally is acceptable. Some critical cases\n",
      "may require retraining in a production environment, even though the initial training\n",
      "was done in a design environment, but the retraining method is usually identical to\n",
      "the training method so that the overall complexity is limited. As always, there is an\n",
      "exception to this rule: online learning.\n",
      "\n",
      "How Often Should Models Be Retrained? \n",
      "\n",
      "| \n",
      "\n",
      "87\n",
      "\n",
      "\f",
      "Online Learning\n",
      "Sometimes, the use case requires teams to go further than the automation of the exist‐\n",
      "ing manual ML pipeline by using dedicated algorithms that can train themselves iter‐\n",
      "atively. (Standard algorithms, by contrast, are retrained from scratch most of the time,\n",
      "with the exception of deep learning algorithms.)\n",
      "\n",
      "While  conceptually  attractive,  these  algorithms  are  more  costly  to  set  up.  The\n",
      "designer has to not only test the performance of the model on a test dataset, but also\n",
      "qualify its behavior when data changes. (The latter is required because it’s difficult to\n",
      "mitigate bad learning once the algorithm is deployed, and it’s hard to reproduce the\n",
      "behavior when each training recursively relies on the previous one because one needs\n",
      "to replay all the steps to understand the bad behavior). In addition, these algorithms\n",
      "are not stateless: running them twice on the same data will not give the same result\n",
      "because they have learned from the first run.\n",
      "\n",
      "There  is  no  standard  way—similar  to  cross-validation—to  do  this  process,  so  the\n",
      "design costs will be higher. Online machine learning is a vivid branch of research with\n",
      "some  mature  technologies  like  state-space  models,  though  they  require  significant\n",
      "skills  to  be  used  effectively.  Online  learning  is  typically  appealing  in  streaming  use\n",
      "cases, though mini batches may be more than enough to handle it.\n",
      "\n",
      "In any case, some level of model retraining is definitely necessary—it’s not a question\n",
      "of  if,  but  of  when.  Deploying  ML  models  without  considering  retraining  would  be\n",
      "like launching an unmanned aircraft from Paris in the exact right direction and hop‐\n",
      "ing it will land safely in New York City without further control.\n",
      "\n",
      "The good news is that if it was possible to gather enough data to train the model the\n",
      "first  time,  then  most  of  the  solutions  for  retraining  are  already  available  (with  the\n",
      "possible  exception  of  cross-trained  models  that  are  used  in  a  different  context—for\n",
      "example, trained with data from one country but used in another). It is therefore crit‐\n",
      "ical for organizations to have a clear idea of deployed models’ drift and accuracy by\n",
      "setting up a process that allows for easy monitoring and notifications. An ideal sce‐\n",
      "nario would be a pipeline that automatically triggers checks for degradation of model\n",
      "performance.\n",
      "\n",
      "It’s  important  to  note  that  the  goal  of  notifications  is  not  necessarily  to  kick  off  an\n",
      "automated process of retraining, validation, and deployment. Model performance can\n",
      "change  for  a  variety  of  reasons,  and  retraining  may  not  always  be  the  answer.  The\n",
      "point  is  to  alert  the  data  scientist  of  the  change;  that  person  can  then  diagnose  the\n",
      "issue and evaluate the next course of action.\n",
      "\n",
      "It is therefore critical that as part of MLOps and the ML model life cycle, data scien‐\n",
      "tists  and  their  managers  and  the  organization  as  a  whole  (which  is  ultimately  the\n",
      "\n",
      "88 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "entity that has to deal with the business consequences of degrading model perform‐\n",
      "ances and any subsequent changes) understand model degradation. Practically, every\n",
      "deployed  model  should  come  with  monitoring  metrics  and  corresponding  warning\n",
      "thresholds  to  detect  meaningful  business  performance  drops  as  quickly  as  possible.\n",
      "The  following  sections  focus  on  understanding  these  metrics  to  be  able  to  define\n",
      "them for a particular model.\n",
      "\n",
      "Understanding Model Degradation\n",
      "Once a machine learning model is trained and deployed in production, there are two\n",
      "approaches  to  monitor  its  performance  degradation:  ground  truth  evaluation  and\n",
      "input  drift  detection.  Understanding  the  theory  behind  and  limitations  of  these\n",
      "approaches is critical to determining the best strategy.\n",
      "\n",
      "Ground Truth Evaluation\n",
      "Ground truth retraining requires waiting for the label event. For example, in a fraud\n",
      "detection model, the ground truth would be whether or not a specific transaction was\n",
      "actually  fraudulent.  For  a  recommendation  engine,  it  would  be  whether  or  not  the\n",
      "customer clicked on—or ultimately bought—one of the recommended products.\n",
      "\n",
      "With the new ground truth collected, the next step is to compute the performance of\n",
      "the model based on ground truth and compare it with registered metrics in the train‐\n",
      "ing  phase.  When  the  difference  surpasses  a  threshold,  the  model  can  be  deemed  as\n",
      "outdated, and it should be retrained.\n",
      "\n",
      "The metrics to be monitored can be of two varieties:\n",
      "\n",
      "• Statistical metrics like accuracy, ROC AUC, log loss, etc. As the model designer\n",
      "has probably already chosen one of these metrics to pick the best model, it is a\n",
      "first-choice candidate for monitoring. For more complex models, where the aver‐\n",
      "age performance is not enough, it may be necessary to look at metrics computed\n",
      "by subpopulations.\n",
      "\n",
      "• Business  metrics,  like  cost-benefit  assessment.  For  example,  the  credit  scoring\n",
      "\n",
      "business has developed its own specific metrics.\n",
      "\n",
      "The  main  advantage  of  the  first  kind  of  metric  is  that  it  is  domain  agnostic,  so  the\n",
      "data  scientist  likely  feels  comfortable  setting  thresholds.  So  as  to  have  the  earliest\n",
      "meaningful warning, it is even possible to compute p-values to assess the probability\n",
      "that the observed drop is not due to random fluctuations.\n",
      "\n",
      "Understanding Model Degradation \n",
      "\n",
      "| \n",
      "\n",
      "89\n",
      "\n",
      "\f",
      "A Stats Primer: From Null Hypothesis to p-Values\n",
      "The null hypothesis says that there is no relationship between the variables being com‐\n",
      "pared; any results are due to sheer chance.\n",
      "\n",
      "The alternative hypothesis says that the variables being compared are related, and the\n",
      "results  are  significant  in  supporting  the  theory  being  considered,  and  not  due  to\n",
      "chance.\n",
      "\n",
      "The  level  of  statistical  significance  is  often  expressed  as  a  p-value  between  0  and  1.\n",
      "The  smaller  the  p-value,  the  stronger  the  evidence  that  one  should  reject  the  null\n",
      "hypothesis.\n",
      "\n",
      "The  drawback  is  that  the  drop  may  be  statistically  significant  without  having  any\n",
      "noticeable  impact.  Or  worse,  the  cost  of  retraining  and  the  risk  associated  with  a\n",
      "redeployment  may  be  higher  than  the  expected  benefits.  Business  metrics  are  far\n",
      "more  interesting  because  they  ordinarily  have  a  monetary  value,  enabling  subject\n",
      "matter experts to better handle the cost-benefit trade-off of the retraining decision.\n",
      "\n",
      "When  available,  ground  truth  monitoring  is  the  best  solution.  However,  it  may  be\n",
      "problematic. There are three main challenges:\n",
      "\n",
      "• Ground truth is not always immediately, or even imminently, available. For some\n",
      "types of models, teams need to wait months (or longer) for ground truth labels to\n",
      "be available, which can mean significant economic loss if the model is degrading\n",
      "quickly. As said before, deploying a model for which the drift is faster than the\n",
      "lag  is  risky.  However,  by  definition,  drifts  are  not  forecastable,  so  models  with\n",
      "long lags need mitigation measures.\n",
      "\n",
      "• Ground truth and prediction are decoupled. To compute the performance of the\n",
      "deployed model on new data, it’s necessary to be able to match ground truth with\n",
      "the corresponding observation. In many production environments, this is a chal‐\n",
      "lenging task because these two pieces of information are generated and stored in\n",
      "different systems and at different timestamps. For low-cost or short-lived models,\n",
      "it might not be worth automated ground truth collection. Note that this is rather\n",
      "short-sighted, because sooner or later, the model will need to be retrained.\n",
      "\n",
      "• Ground truth is only partially available. In some situations, it is extremely expen‐\n",
      "sive to retrieve the ground truth for all the observations, which means choosing\n",
      "which samples to label and thus inadvertently introducing bias into the system.\n",
      "\n",
      "For the last challenge, fraud detection presents a clear use case. Given that each trans‐\n",
      "action needs to be examined manually and the process takes a long time, does it make\n",
      "sense  to  establish  ground  truth  for  only  suspect  cases  (i.e.,  cases  where  the  model\n",
      "\n",
      "90 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "gives  a  high  probability  of  fraud)?  At  first  glance,  the  approach  seems  reasonable;\n",
      "however,  a  critical  mind  understands  that  this  creates  a  feedback  loop  that  will\n",
      "amplify the flaws of the model. Fraud patterns that were never captured by the model\n",
      "(i.e.,  those  that  have  a  low  fraud  probability  according  to  the  model)  will  never  be\n",
      "taken into account in the retraining process.\n",
      "\n",
      "One  solution  to  this  challenge  might  be  to  randomly  label,  establishing  a  ground\n",
      "truth  for  just  a  subsample  of  transactions  in  addition  to  those  that  were  flagged  as\n",
      "suspicious. Another solution might be to reweight the biased sample so that its char‐\n",
      "acteristics  match  the  general  population  more  closely.  For  example,  if  the  system\n",
      "awarded  little  credit  to  people  with  low  income,  the  model  should  reweight  them\n",
      "according to their importance in the applicant, or even in the general, population.\n",
      "\n",
      "The  bottom  line  is  that  whatever  the  mitigation  measure,  the  labeled  sample  subset\n",
      "must cover all possible future predictions so that the trained model makes good pre‐\n",
      "dictions whatever the sample; this will sometimes mean making suboptimal decisions\n",
      "for the sake of checking that the model continues to generalize well.\n",
      "\n",
      "Once this problem is solved for retraining, the solution (reweighting, random sam‐\n",
      "pling) can be used for monitoring. Input drift detection complements this approach,\n",
      "as it is needed to make sure that ground truth covering new, unexplored domains is\n",
      "made available to retrain the model.\n",
      "\n",
      "Input Drift Detection\n",
      "Given the challenges and limitations of ground truth retraining presented in the pre‐\n",
      "vious section, a more practical approach might be input drift detection. This section\n",
      "takes a brief but deep dive into the underlying logic behind drift and presents differ‐\n",
      "ent scenarios that can cause models and data to drift.\n",
      "\n",
      "Say the goal is to predict the quality of Bordeaux wines using as training data the UCI\n",
      "Wine Quality dataset, which contains information about red and white variants of the\n",
      "Portuguese wine vinho verde along with a quality score varying between 0 and 10.\n",
      "\n",
      "The following features are provided for each wine: type, fixed acidity, volatile acidity,\n",
      "citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density,\n",
      "pH, sulphates, and alcohol rate.\n",
      "\n",
      "To simplify the modeling problem, say that a good wine is one with a quality score\n",
      "equal to or greater than 7. The goal is thus to build a binary model that predicts this\n",
      "label from the wine’s attributes.\n",
      "\n",
      "Understanding Model Degradation \n",
      "\n",
      "| \n",
      "\n",
      "91\n",
      "\n",
      "\f",
      "To demonstrate data drift, we explicitly split the original dataset into two:\n",
      "\n",
      "• wine_alcohol_above_11,  which  contains  all  wines  with  an  alcohol  rate  of  11%\n",
      "\n",
      "and above\n",
      "\n",
      "• wine_alcohol_below_11,  which  contains  all  wines  with  an  alcohol  rate  below\n",
      "\n",
      "11%\n",
      "\n",
      "We split wine_alcohol_above_11 to train and score our model, and the second data‐\n",
      "set, wine_alcohol_below_11, will be considered as new incoming data that needs to\n",
      "be scored once the model has been deployed.\n",
      "\n",
      "We have artificially created a big problem: it is very unlikely that the quality of wine is\n",
      "independent from the alcohol level. Worse, the alcohol level is likely to be correlated\n",
      "differently with the other features in the two datasets. As a result, what is learned on\n",
      "one dataset (“if the residual sugar is low and the pH is high, then the probability that\n",
      "the wine is good is high”) may be wrong on the other one because, for example, the\n",
      "residual sugar is not important anymore when the alcohol level is high.\n",
      "\n",
      "Mathematically speaking, the samples of each dataset cannot be assumed to be drawn\n",
      "from  the  same  distribution  (i.e.,  they  are  not  “identically  distributed”).  Another\n",
      "mathematical  property  is  necessary  to  ensure  that  ML  algorithms  perform  as\n",
      "expected:  independence.  This  property  is  broken  if  samples  are  duplicated  in  the\n",
      "dataset  or  if  it  is  possible  to  forecast  the  “next”  sample  given  the  previous  one,  for\n",
      "example.\n",
      "\n",
      "Let’s  assume  that  despite  the  obvious  problems,  we  train  the  algorithm  on  the  first\n",
      "dataset and then deploy it on the second one. The resulting distribution shift is called\n",
      "a drift. It will be called a feature drift if the alcohol level is one of the features used by\n",
      "the  ML  model  (or  if  the  alcohol  level  is  correlated  with  other  features  used  by  the\n",
      "model) and a concept drift if it is not.\n",
      "\n",
      "Drift Detection in Practice\n",
      "As  explained  previously,  to  be  able  to  react  in  a  timely  manner,  model  behavior\n",
      "should be monitored solely based on the feature values of the incoming data, without\n",
      "waiting for the ground truth to be available.\n",
      "\n",
      "The logic is that if the data distribution (e.g., mean, standard deviation, correlations\n",
      "between features) diverges between the training and testing phases1 on one side and\n",
      "the development phase on the other, it is a strong signal that the model’s performance\n",
      "won’t  be  the  same.  It  is  not  the  perfect  mitigation  measure,  as  retraining  on  the\n",
      "\n",
      "1 It is also advisable to assess the drift between the training and the test dataset, especially when the test dataset\n",
      "\n",
      "is posterior to the training dataset. See “Choosing Evaluation Metrics” on page 51 for details.\n",
      "\n",
      "92 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "drifted dataset will not be an option, but it can be part of mitigation measures (e.g.,\n",
      "reverting to a simpler model, reweighting).\n",
      "\n",
      "Example Causes of Data Drift\n",
      "There are two frequent root causes of data drift:\n",
      "\n",
      "• Sample selection bias, where the training sample is not representative of the pop‐\n",
      "ulation.  For  instance,  building  a  model  to  assess  the  effectiveness  of  a  discount\n",
      "program  will  be  biased  if  the  best  discounts  are  proposed  for  the  best  clients.\n",
      "  Selection  bias  often  stems  from  the  data  collection  pipeline  itself.  In  the  wine\n",
      "example,  the  original  dataset  sample  with  alcohol  levels  above  11%  surely  does\n",
      "not represent the whole population of wines—this is sample selection at its best.\n",
      "It could have been mitigated if a few samples of wine with an alcohol level above\n",
      "11% had been kept and reweighted according to the expected proportion in the\n",
      "population of wines to be seen by the deployed model. Note that this task is eas‐\n",
      "ier said than done in real life, as the problematic features are often unknown or\n",
      "maybe even not available.\n",
      "\n",
      "• Non-stationary environment, where training data collected from the source pop‐\n",
      "ulation  does  not  represent  the  target  population.  This  often  happens  for  time-\n",
      "dependent tasks—such as forecasting use cases—with strong seasonality effects,\n",
      "where learning a model over a given month won’t generalize to another month.\n",
      "Back to the wine example: one can imagine a case where the original dataset sam‐\n",
      "ple only includes wines from a specific year, which might represent a particularly\n",
      "good (or bad) vintage. A model trained on this data may not generalize to other\n",
      "years.\n",
      "\n",
      "Input Drift Detection Techniques\n",
      "After understanding the possible situations that can cause different types of drift, the\n",
      "next  logical  question  is:  how  can  drift  be  detected?  This  section  presents  two  com‐\n",
      "mon  approaches.  The  choice  between  them  depends  on  the  expected  level  of\n",
      "interpretability.\n",
      "\n",
      "Organizations  that  need  proven  and  explainable  methods  should  prefer  univariate\n",
      "statistical tests. If complex drift involving several features simultaneously is expected,\n",
      "or if the data scientists want to reuse what they already know and assuming the orga‐\n",
      "nization doesn’t dread the black box effect, the domain classifier approach may be a\n",
      "good option, too.\n",
      "\n",
      "Drift Detection in Practice \n",
      "\n",
      "| \n",
      "\n",
      "93\n",
      "\n",
      "\f",
      "Univariate statistical tests\n",
      "\n",
      "This method requires applying a statistical test on data from the source distribution\n",
      "and the target distribution for each feature. A warning will be raised when the results\n",
      "of those tests are significant.\n",
      "\n",
      "The choice of hypothesis tests have been extensively studied in the literature, but the\n",
      "basic approaches rely on these two tests:\n",
      "\n",
      "• For  continuous  features,  the  Kolmogorov-Smirnov  test  is  a  nonparametric\n",
      "hypothesis  test  that  is  used  to  check  whether  two  samples  come  from  the  same\n",
      "distribution. It measures a distance between the empirical distribution functions.\n",
      "\n",
      "• For  categorical  features,  the  Chi-squared  test  is  a  practical  choice  that  checks\n",
      "whether  the  observed  frequencies  for  a  categorical  feature  in  the  target  data\n",
      "match the expected frequencies seen from the source data.\n",
      "\n",
      "The  main  advantage  of  p-values  is  that  they  help  detect  drift  as  quickly  as  possible.\n",
      "The main drawback is that they detect an effect, but they do not quantify the level of\n",
      "the effect (i.e., on large datasets, they detect very small changes, which may be com‐\n",
      "pletely without impact). As a result, if development datasets are very large, it is neces‐\n",
      "sary  to  complement  p-values  with  business-significant  metrics.  For  example,  on  a\n",
      "sufficiently large dataset, the average age may have significantly drifted from a statisti‐\n",
      "cal perspective, but if the drift is only a few months, this is probably an insignificant\n",
      "value for many business use cases.\n",
      "\n",
      "Domain classifier\n",
      "\n",
      "In this approach, data scientists train a model that tries to discriminate between the\n",
      "original dataset (input features and, optionally, predicted target) and the development\n",
      "dataset. In other words, they stack the two datasets and train a classifier that aims at\n",
      "predicting the data’s origin. The performance of the model (its accuracy, for example)\n",
      "can then be considered as a metric for the drift level.\n",
      "\n",
      "If this model is successful in its task, and thus has a high drift score, it implies that the\n",
      "data used at training time and the new data can be distinguished, so it’s fair to say that\n",
      "the new data has drifted. To gain more insights, in particular to identify the features\n",
      "that  are  responsible  for  the  drift,  one  can  use  the  feature  importance  of  the  trained\n",
      "model.\n",
      "\n",
      "Interpretation of results\n",
      "\n",
      "Both domain classifier and univariate statistical tests point to the importance of fea‐\n",
      "tures  or  of  the  target  to  explain  drift.  Drift  attributed  to  the  target  is  important  to\n",
      "identify because it often directly impacts the bottom line of the business. (Think, for\n",
      "example, of credit scores: if the scores are lower overall, the number of awarded loans\n",
      "\n",
      "94 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "is likely to be lower, and therefore revenue will be lower.) Drift attributed to features\n",
      "is useful to mitigate the impact of drift, as it may hint at the need for:\n",
      "\n",
      "• Reweighting according to this feature (e.g., if customers above 60 now represent\n",
      "60% of users but were only 30% in the training set, then double their weight and\n",
      "retrain the model)\n",
      "\n",
      "• Removing the feature and training a new model without it\n",
      "\n",
      "In all cases, it is very unlikely that automatic actions exist if drift is detected. It could\n",
      "happen  if  it  is  costly  to  deploy  retrained  models:  the  model  would  be  retrained  on\n",
      "new data only if performance based on ground truth had dropped or significant drift\n",
      "was detected. In this peculiar case, new data is indeed available to mitigate the drift.\n",
      "\n",
      "The Feedback Loop\n",
      "All effective machine learning projects implement a form of data feedback loop; that\n",
      "is, information from the production environment flows back to the model prototyp‐\n",
      "ing environment for further improvement.\n",
      "\n",
      "One can see in Figure 7-2 that data collected in the monitoring and feedback loop is\n",
      "sent  to  the  model  development  phase  (details  about  this  data  are  covered  in  Chap‐\n",
      "ter 6). From there, the system analyzes whether the model is working as expected. If it\n",
      "is, no action is required. If the model’s performance is degrading, an update will be\n",
      "triggered, either automatically or manually by the data scientist. In practice, as seen at\n",
      "the beginning of this chapter, this usually means either retraining the model with new\n",
      "labeled data or developing a new model with additional features.\n",
      "\n",
      "Figure 7-2. Continuous delivery for end-to-end machine learning process\n",
      "\n",
      "The Feedback Loop \n",
      "\n",
      "| \n",
      "\n",
      "95\n",
      "\n",
      "\f",
      "In either case, the goal is to be able to capture the emerging patterns and make sure\n",
      "that the business is not negatively impacted. This infrastructure is comprised of three\n",
      "main components, which in addition to the concepts discussed in the first part of this\n",
      "chapter, are critical to robust MLOps capabilities:\n",
      "\n",
      "• A logging system that collects data from several production servers\n",
      "\n",
      "• A model evaluation store that does versioning and evaluation between different\n",
      "\n",
      "model versions\n",
      "\n",
      "• An  online  system  that  does  model  comparison  on  production  environments,\n",
      "either with the shadow scoring (champion/challenger) setup or with A/B testing\n",
      "\n",
      "The following sections address each of these components individually, including their\n",
      "purpose, key features, and challenges.\n",
      "\n",
      "Logging\n",
      "Monitoring a live system, with or without machine learning components, means col‐\n",
      "lecting and aggregating data about its states. Nowadays, as production infrastructures\n",
      "are  getting  more  and  more  complex,  with  several  models  deployed  simultaneously\n",
      "across several servers, an effective logging system is more important than ever.\n",
      "\n",
      "Data from these environments needs to be centralized to be analyzed and monitored,\n",
      "either automatically or manually. This will enable continuous improvement of the ML\n",
      "system. An event log of a machine learning system is a record with a timestamp and\n",
      "the following information.\n",
      "\n",
      "Model metadata\n",
      "\n",
      "Identification of the model and the version.\n",
      "\n",
      "Model inputs\n",
      "\n",
      "Feature  values  of  new  observations,  which  allow  for  verification  of  whether  the\n",
      "new incoming data is what the model was expecting and thus allowing for detec‐\n",
      "tion of data drift (as explained in the previous section).\n",
      "\n",
      "Model outputs\n",
      "\n",
      "Predictions made by the model that, along with the ground truth collected later\n",
      "on,  give  a  concrete  idea  about  the  model  performance  in  a  production\n",
      "environment.\n",
      "\n",
      "System action\n",
      "\n",
      "It’s rare that the model prediction is the end product of a machine learning appli‐\n",
      "cation; the more common situation is that the system will take an action based on\n",
      "this prediction. For example, in a fraud detection use case, when the model gives\n",
      "high probability, the system can either block the transaction or send a warning to\n",
      "\n",
      "96 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "the bank. This type of information is important because it affects the user reac‐\n",
      "tion and thus indirectly affects the feedback data.\n",
      "\n",
      "Model explanation\n",
      "\n",
      "In  some  highly  regulated  domains  such  as  finance  or  healthcare,  predictions\n",
      "must come with an explanation (i.e., which features have the most influence on\n",
      "the  prediction).  This  kind  of  information  is  usually  computed  with  techniques\n",
      "such  as  Shapley  value  computation  and  should  be  logged  to  identify  potential\n",
      "issues with the model (e.g., bias, overfitting).\n",
      "\n",
      "Model Evaluation\n",
      "Once the logging system is in place, it periodically fetches data from the production\n",
      "environment for monitoring. Everything goes well until one day the data drift alert is\n",
      "triggered: the incoming data distribution is drifting away from the training data dis‐\n",
      "tribution. It’s possible that the model performance is degrading.\n",
      "\n",
      "After  review,  data  scientists  decide  to  improve  the  model  by  retraining  it,  using  the\n",
      "techniques  described  earlier  in  this  chapter.  With  several  trained  candidate  models,\n",
      "the  next  step  is  to  compare  them  with  the  deployed  model.  In  practice,  this  means\n",
      "evaluating all the models (the candidates as well as the deployed model) on the same\n",
      "dataset.  If  one  of  the  candidate  models  outperforms  the  deployed  model,  there  are\n",
      "two  ways  to  proceed:  either  update  the  model  on  the  production  environment  or\n",
      "move to an online evaluation via a champion/challenger or A/B testing setup.\n",
      "\n",
      "In a nutshell, this is the notion of model store. It is a structure that allows data scien‐\n",
      "tists to:\n",
      "\n",
      "• Compare  multiple,  newly  trained  model  versions  against  existing  deployed\n",
      "\n",
      "versions\n",
      "\n",
      "• Compare  completely  new  models  against  versions  of  other  models  on  labeled\n",
      "\n",
      "data\n",
      "\n",
      "• Track model performance over time\n",
      "\n",
      "Formally,  the  model  evaluation  store  serves  as  a  structure  that  centralizes  the  data\n",
      "related to model life cycle to allow comparisons (though note that comparing models\n",
      "makes sense only if they address the same problem). By definition, all these compari‐\n",
      "sons are grouped under the umbrella of a logical model.\n",
      "\n",
      "Logical model\n",
      "\n",
      "Building  a  machine  learning  application  is  an  iterative  process,  from  deploying  to\n",
      "production,  monitoring  performance,  retrieving  data,  and  looking  for  ways  to\n",
      "improve how the system addresses the target problem. There are many ways to iter‐\n",
      "ate, some of which have already been discussed in this chapter, including:\n",
      "\n",
      "The Feedback Loop \n",
      "\n",
      "| \n",
      "\n",
      "97\n",
      "\n",
      "\f",
      "• Retraining the same model on new data\n",
      "\n",
      "• Adding new features to the model\n",
      "\n",
      "• Developing new algorithms\n",
      "\n",
      "For  those  reasons,  the  machine  learning  model  itself  is  not  a  static  object;  it  con‐\n",
      "stantly changes with time. It is therefore helpful to have a higher abstraction level to\n",
      "reason about machine learning applications, which is referred to as a logical model.\n",
      "\n",
      "A  logical  model  is  a  collection  of  model  templates  and  their  versions  that  aims  to\n",
      "solve a business problem. A model version is obtained by training a model template\n",
      "on a given dataset. All versions of model templates of the same logical model can usu‐\n",
      "ally be evaluated on the same kinds of datasets (i.e., on datasets with the same feature\n",
      "definition and/or schema); however, this may not be the case if the problem did not\n",
      "change  but  the  features  available  to  solve  it  did.  Model  versions  could  be  imple‐\n",
      "mented  using  completely  different  technologies,  and  there  could  even  be  several\n",
      "implementations of the same model version (Python, SQL, Java, etc.); regardless, they\n",
      "are supposed to give the same prediction if given the same input.\n",
      "\n",
      "Let’s  get  back  to  the  wine  example  introduced  earlier  in  this  chapter.  Three  months\n",
      "after  deployment,  there  is  new  data  about  less  alcoholic  wine.  We  can  retrain  our\n",
      "model  on  the  new  data,  thus  obtaining  a  new  model  version  using  the  same  model\n",
      "template. While investigating the result, we discover new patterns are emerging. We\n",
      "may  decide  to  create  new  features  that  capture  this  information  and  add  it  to  the\n",
      "model, or we may decide to use another ML algorithm (like deep learning) instead of\n",
      "XGBoost. This would result in a new model template.\n",
      "\n",
      "As a result, our model has two model templates and three versions:\n",
      "\n",
      "• The first version is live in production, based on the original model template.\n",
      "\n",
      "• The second version is based on the original template, but trained on new data.\n",
      "\n",
      "• The third version uses the deep learning–based template with additional features\n",
      "\n",
      "and is trained on the same data as the second version.\n",
      "\n",
      "The information about the evaluation of these versions on various datasets (both the\n",
      "test datasets used at training time and the development datasets that may be scored\n",
      "after training) is then stored in the model evaluation store.\n",
      "\n",
      "Model evaluation store\n",
      "\n",
      "As a reminder, model evaluation stores are structures that centralize the data related\n",
      "to model life cycles to allow comparisons. The two main tasks of a model evaluation\n",
      "store are:\n",
      "\n",
      "98 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "• Versioning the evolution of a logical model through time. Each logged version of\n",
      "the  logical  model  must  come  with  all  the  essential  information  concerning  its\n",
      "training phase, including:\n",
      "\n",
      "— The list of features used\n",
      "\n",
      "— The preprocessing techniques that are applied to each feature\n",
      "\n",
      "— The algorithm used, along with the chosen hyperparameters\n",
      "\n",
      "— The training dataset\n",
      "\n",
      "— The test dataset used to evaluate the trained model (this is necessary for the\n",
      "\n",
      "version comparison phase)\n",
      "\n",
      "— Evaluation metrics\n",
      "\n",
      "• Comparing  the  performance  between  different  versions  of  a  logical  model.  To\n",
      "decide which version of a logical model to deploy, all of them (the candidates and\n",
      "the deployed one) must be evaluated on the same dataset.\n",
      "\n",
      "The choice of dataset to evaluate is crucial. If there is enough new labeled data to give\n",
      "a reliable estimation of the model performance, this is the preferred choice because it\n",
      "is closest to what we are expecting to receive in the production environment. Other‐\n",
      "wise, we can use the original test dataset of the deployed model. Assuming that the\n",
      "data has not drifted, this gives us a concrete idea about the performance of the candi‐\n",
      "date models compared to the original model.\n",
      "\n",
      "After identifying the best candidate model, the job is not yet done. In practice, there\n",
      "is often a substantial discrepancy between the offline and online performance of the\n",
      "models. Therefore, it’s critical to take the testing to the production environment. This\n",
      "online  evaluation  gives  the  most  truthful  feedback  about  the  behavior  of  the  candi‐\n",
      "date model when facing real data.\n",
      "\n",
      "Online Evaluation\n",
      "Online evaluation of models in production is critical from a business perspective, but\n",
      "can  be  challenging  from  a  technical  perspective.  There  two  main  modes  of  online\n",
      "evaluation:\n",
      "\n",
      "• Champion/challenger (otherwise known as shadow testing), where the candidate\n",
      "\n",
      "model shadows the deployed model and scores the same live requests\n",
      "\n",
      "• A/B testing, where the candidate model scores a portion of the live requests and\n",
      "\n",
      "the deployed model scores the others\n",
      "\n",
      "Both  cases  require  ground  truth,  so  the  evaluation  will  necessarily  take  longer  than\n",
      "the  lag  between  prediction  and  ground  truth  obtention.  In  addition,  whenever\n",
      "\n",
      "The Feedback Loop \n",
      "\n",
      "| \n",
      "\n",
      "99\n",
      "\n",
      "\f",
      "shadow testing is possible, it should be used over A/B testing because it is far simpler\n",
      "to understand and set up, and it detects differences more quickly.\n",
      "\n",
      "Champion/Challenger\n",
      "\n",
      "Champion/challenger involves deploying one or several additional models (the chal‐\n",
      "lengers)  to  the  production  environment.  These  models  receive  and  score  the  same\n",
      "incoming requests as the active model (the champion). However, they do not return\n",
      "any  response  or  prediction  to  the  system:  that’s  still  the  job  of  the  old  model.  The\n",
      "predictions  are  simply  logged  for  further  analysis.  That’s  why  this  method  is  also\n",
      "called “shadow testing” or “dark launch.”\n",
      "\n",
      "This setup allows for two things:\n",
      "\n",
      "• Verification that the performance of the new models is better than, or at least as\n",
      "good  as,  the  old  model.  Because  the  two  models  are  scoring  on  the  same  data,\n",
      "there  is  a  direct  comparison  of  their  accuracy  in  the  production  environment.\n",
      "Note that this could also be done offline by using the new models on the dataset\n",
      "made of new requests scored by the champion model.\n",
      "\n",
      "• Measurement  of  how  the  new  models  handle  realistic  load.  Because  the  new\n",
      "models  can  have  new  features,  new  preprocessing  techniques,  or  even  a  new\n",
      "algorithm, the prediction time for a request won’t be the same as that of the origi‐\n",
      "nal model, and it is important to have a concrete idea of this change. Of course,\n",
      "this is the main advantage of doing it online.\n",
      "\n",
      "The other advantage of this deployment scheme is that the data scientist or the ML\n",
      "engineer  is  giving  visibility  to  other  stakeholders  on  the  future  champion  model:\n",
      "instead of being locked in the data science environment, the challenger model results\n",
      "are exposed to the business leaders, which decreases the perceived risk to switch to a\n",
      "new model.\n",
      "\n",
      "To be able to compare the champion and the challenger models, the same informa‐\n",
      "tion must be logged for both, including input data, output data, processing time, etc.\n",
      "This means updating the logging system so that it can differentiate between the two\n",
      "sources of data.\n",
      "\n",
      "How long should both models be deployed before it’s clear that one is better than the\n",
      "other?  Long  enough  that  the  metric  fluctuations  due  to  randomness  are  dampened\n",
      "because  enough  predictions  have  been  made.  This  can  be  assessed  graphically  by\n",
      "checking  that  the  metric  estimations  are  not  fluctuating  anymore  or  by  doing  a\n",
      "proper statistical test (as most metrics are averages of row-wise scores, the most usual\n",
      "test is a paired sample T-test) that yields the probability that the observation that one\n",
      "metric  is  higher  than  the  other  is  due  to  these  random  fluctuations.  The  wider  the\n",
      "metric  difference,  the  fewer  predictions  necessary  to  ensure  that  the  difference  is\n",
      "significant.\n",
      "\n",
      "100 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "Depending on the use case and the implementation of the champion/challenger sys‐\n",
      "tem,  server  performance  can  be  a  concern.  If  two  memory-intensive  models  are\n",
      "called synchronously, they can slow the system down. This will not only have a nega‐\n",
      "tive impact on the user experience but also corrupt the data collected about the func‐\n",
      "tioning of the models.\n",
      "\n",
      "Another concern is communication with the external system. If the two models use\n",
      "an external API to enrich their features, that doubles the number of requests to these\n",
      "services, thus doubling costs. If that API has a caching system in place, then the sec‐\n",
      "ond  request  will  be  processed  much  faster  than  the  first,  which  can  bias  the  result\n",
      "when comparing the total prediction time of the two models. Note that the challenger\n",
      "may be used only for a random subset of the incoming requests, which will alleviate\n",
      "the load at the expense of increased time before a conclusion can be drawn.\n",
      "\n",
      "Finally,  when  implementing  a  challenger  model,  it’s  important  to  ensure  it  doesn’t\n",
      "have any influence on the system’s actions. This implies two scenarios:\n",
      "\n",
      "• When  the  challenger  model  encounters  an  unexpected  issue  and  fails,  the  pro‐\n",
      "duction environment will not experience any discontinuation or degradation in\n",
      "terms of response time.\n",
      "\n",
      "• Actions  taken  by  the  system  depend  only  on  the  prediction  of  the  champion\n",
      "model,  and  they  happen  only  once.  For  example,  in  a  fraud  detection  use  case,\n",
      "imagine that by mistake the challenger model is plugged directly into the system,\n",
      "charging each transaction twice—a catastrophic scenario.\n",
      "\n",
      "In general, some effort needs to be spent on the logging, monitoring, and serving sys‐\n",
      "tem to ensure the production environment functions as usual and is not impacted by\n",
      "any issues coming from the challenger model.\n",
      "\n",
      "A/B testing\n",
      "\n",
      "A/B testing (a randomized experiment testing two variants, A and B) is a widely used\n",
      "technique  in  website  optimization.  For  ML  models,  it  should  be  used  only  when\n",
      "champion/challenger is not possible. This might happen when:\n",
      "\n",
      "• The ground truth cannot be evaluated for both models. For example, for a rec‐\n",
      "ommendation engine, the prediction gives a list of items on which a given cus‐\n",
      "tomer is likely to click if they are presented. Therefore, it is impossible to know if\n",
      "the customer would have clicked if an item was not presented. In this case, some\n",
      "kind of A/B testing will have to be done, in which some customers will be shown\n",
      "the recommendations of model A, and some the recommendations of model B.\n",
      "Similarly,  for  a  fraud  detection  model,  because  heavy  work  is  needed  to  obtain\n",
      "the ground truth, it may not be possible to do so for the positive predictions of\n",
      "two models; it would increase the workload too much, because some frauds are\n",
      "\n",
      "The Feedback Loop \n",
      "\n",
      "| \n",
      "\n",
      "101\n",
      "\n",
      "\f",
      "detected by only one model. As a result, randomly applying only the B model to a\n",
      "small fraction of the requests will allow the workload to remain constant.\n",
      "\n",
      "• The objective to optimize is only indirectly related to the performance of the pre‐\n",
      "diction. Imagine an ad engine based on an ML model that predicts if a user will\n",
      "click on the ad. Now imagine that it is evaluated on the buy rate, i.e., whether the\n",
      "user  bought  the  product  or  service.  Once  again,  it  is  not  possible  to  record  the\n",
      "reaction  of  the  user  for  two  different  models,  so  in  this  case,  A/B  testing  is  the\n",
      "only way.\n",
      "\n",
      "Entire books are dedicated to A/B testing, so this section presents only its main idea\n",
      "and  a  simple  walkthrough.  Unlike  the  champion/challenger  framework,  with  A/B\n",
      "testing, the candidate model returns predictions for certain requests, and the original\n",
      "model handles the other requests. Once the test period is over, statistical tests com‐\n",
      "pare the performance of the two models, and teams can make a decision based on the\n",
      "statistical significance of those tests.\n",
      "\n",
      "In an MLOps context, some considerations need to be made. A walkthrough of these\n",
      "considerations is presented in Table 7-1.\n",
      "\n",
      "Table 7-1. Considerations for A/B testing in MLOps\n",
      "\n",
      "Stage\n",
      "Before the\n",
      "A/B test\n",
      "\n",
      "MLOps consideration\n",
      "Define a clear goal: A quantitative business metric that needs to be optimized, such as click-through rate.\n",
      "Define a precise population: Carefully choose a segment for the test along with a splitting strategy that assures\n",
      "no bias between groups. (This is the so-called experimental design or randomized control trial that’s been\n",
      "popularized by drug studies.) This may be a random split, or it may be more complex. For example, the situation\n",
      "might dictate that all the requests of a particular customer are handled by the same model.\n",
      "Define the statistical protocol: The resulting metrics are compared using statistical tests, and the null hypothesis\n",
      "is either rejected or retained. To make the conclusion robust, teams need to define beforehand the sample size\n",
      "for the desired minimum effect size, which is the minimum difference between the two models’ performance\n",
      "metrics. Teams must also fix a test duration (or alternatively have a method to handle multiple tests). Note that\n",
      "with similar sample sizes, the power to detect meaningful differences will be lower than with champion/\n",
      "challenger because unpaired sample tests have to be used. (It is usually impossible to match each request scored\n",
      "with model B to a request scored with model A, whereas with champion/challenger, this is trivial.)\n",
      "\n",
      "During the\n",
      "A/B test\n",
      "\n",
      "After the\n",
      "A/B test\n",
      "\n",
      "It is important not to stop the experiment before the test duration is over, even if the statistical test starts to\n",
      "return a significant metric difference. This practice (also called p-hacking) produces unreliable and biased results\n",
      "due to cherry-picking the desired outcome.\n",
      "Once the test duration is over, check the collected data to make sure the quality is good. From there, run the\n",
      "statistical tests; if the metric difference is statistically significant in favor of the candidate model, the original\n",
      "model can be replaced with the new version.\n",
      "\n",
      "102 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Monitoring and Feedback Loop\n",
      "\n",
      "\f",
      "Closing Thoughts\n",
      "Ordinary software is built to satisfy specifications. Once an application is deployed,\n",
      "its ability to fulfill its objective does not degrade. ML models, by contrast, have objec‐\n",
      "tives  statistically  defined  by  their  performance  on  a  given  dataset.  As  a  result,  their\n",
      "performance changes, usually for the worse, when the statistical properties of the data\n",
      "change.\n",
      "\n",
      "In  addition  to  ordinary  software  maintenance  needs  (bug  correction,  release\n",
      "upgrades,  etc.),  this  performance  drift  has  to  be  carefully  monitored.  We  have  seen\n",
      "that  performance  monitoring  based  on  the  ground  truth  is  the  cornerstone,  while\n",
      "drift monitoring can provide early warning signals. Among possible drift mitigation\n",
      "measures, the workhorse is definitely retraining on new data, while model modifica‐\n",
      "tion remains an option. Once a new model is ready to be deployed, its improved per‐\n",
      "formance  can  be  validated  thanks  to  shadow  scoring  or,  as  a  second  choice,  A/B\n",
      "testing.  This  enables  proving  that  the  new  model  is  better  in  order  to  improve  the\n",
      "performance of the system. \n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "103\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 8\n",
      "Model Governance\n",
      "\n",
      "Mark Treveil\n",
      "\n",
      "We explored the idea of governance as a set of controls placed on a business in Chap‐\n",
      "ter 3. These goals aim to ensure that the business delivers on its responsibilities to all\n",
      "stakeholders,  from  shareholders  and  employees  to  the  public  and  national  govern‐\n",
      "ments. The responsibilities include financial, legal, and ethical, and are all underpin‐\n",
      "ned by the desire for fairness.\n",
      "\n",
      "This chapter goes even more in depth on these topics, shifting from why they matter\n",
      "to how organizations can incorporate them as a part of their MLOps strategy.\n",
      "\n",
      "Who Decides What Governance the Organization Needs?\n",
      "National regulations are a key part of a society’s framework for safeguarding fairness. \n",
      "But  these  take  considerable  time  to  be  agreed  upon  and  implemented;  they  always\n",
      "reflect a slightly historical understanding of fairness and the challenges to it. Just as\n",
      "with  ML  models,  the  past  cannot  always  anticipate  the  evolving  problems  of  the\n",
      "future.\n",
      "\n",
      "What most businesses want from governance is to safeguard shareholder investment\n",
      "and to help ensure a suitable ROI, both now and in the future. That means the busi‐\n",
      "ness has to perform effectively, profitability, and sustainably. The shareholders need\n",
      "clear visibility that customers, employees, and regulatory bodies are happy, and they\n",
      "want reassurances that appropriate measures are in place to detect and manage any\n",
      "difficulties that could occur in the future.\n",
      "\n",
      "None of this is news, of course, nor specific to MLOps. What is different with ML is\n",
      "that it is a new and often opaque technology that carries many risks, but it is rapidly\n",
      "being  embedded  in  decision-making  systems  that  impact  every  aspect  of  our  lives.\n",
      "ML  systems  invent  their  own  statistically  driven  decision-making  processes,  often\n",
      "\n",
      "105\n",
      "\n",
      "\f",
      "extremely difficult to understand, based on large volumes of data that is thought to\n",
      "represent the real world. It’s not hard to see what could go wrong!\n",
      "\n",
      "Perhaps  the  most  surprising  influence  on  the  direction  of  ML  governance  is  public\n",
      "opinion, which evolves much faster than formal regulation. It follows no formal pro‐\n",
      "cess or etiquette. It doesn’t have to be based on fact or reason. Public opinion deter‐\n",
      "mines what products people buy, where they invest their money, and what rules and\n",
      "regulations governments make. Public opinion decides what is fair and what is not.\n",
      "\n",
      "For  example,  the  agricultural  biotechnology  companies  that  developed  genetically\n",
      "modified  crops  felt  the  power  of  public  opinion  painfully  in  the  1990s.  While  the\n",
      "arguments rage back and forth about whether there was, or was not, a risk to health,\n",
      "public opinion in Europe swung against genetic modification, and these crops were\n",
      "banned in many European countries. The parallels with ML are clear: ML offers ben‐\n",
      "efits  to  all  and  yet  brings  risks  that  need  to  be  managed  if  the  public  is  to  trust  it.\n",
      "Without public trust, the benefits will not fully materialize.\n",
      "\n",
      "The general public needs to be reassured that ML is fair. What is considered “fair” is\n",
      "not defined in a rule book, and it is not fixed; it will fluctuate based on events, and it\n",
      "will not always be the same across the world. Right now, opinion on ML is in the bal‐\n",
      "ance. Most people prefer getting sensibly targeted ads, they like their cars being able\n",
      "to  read  speed-limit  signs,  and  improving  fraud  detection  ultimately  saves  them\n",
      "money.\n",
      "\n",
      "But  there  have  also  been  well-publicized  scandals  that  have  rocked  the  public’s\n",
      "acceptance  of  this  technology.  The  Facebook-Cambridge  Analytica  affair,  where  the\n",
      "companies  used  the  power  of  ML  to  manipulate  public  opinion  on  social  media,\n",
      "shocked the world. This looked like ML with explicitly malicious intent. Equally wor‐\n",
      "rying have been instances of entirely unintentional harm, where ML black box judg‐\n",
      "ments  proved  to  be  unacceptably  and  illegally  biased  on  criteria  such  as  race  or\n",
      "gender, for example in criminal assessment systems and in recruitment tools.\n",
      "\n",
      "If  businesses  and  governments  want  to  reap  the  benefits  of  ML,  they  have  to  safe‐\n",
      "guard the public trust in it as well as proactively address the risks. For businesses, this\n",
      "means developing strong governance of their MLOps process. They must assess the\n",
      "risks,  determine  their  own  set  of  fairness  values,  and  then  implement  the  necessary\n",
      "process  to  manage  them.  Much  of  this  is  simply  about  good  housekeeping  with  an\n",
      "added  focus  on  mitigating  the  inherent  risks  of  ML,  addressing  topics  such  as  data\n",
      "provenance, transparency, bias, performance management, and reproducibility.\n",
      "\n",
      "106 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Matching Governance with Risk Level\n",
      "Governance is not a free lunch; it takes effort, discipline, and time.\n",
      "\n",
      "From  the  business  stakeholders’  perspective,  governance  is  likely  to  slow  down  the\n",
      "delivery of new models, which may cost the business money. For data scientists, it can\n",
      "look like a lot of bureaucracy that erodes their ability to get things done. In contrast,\n",
      "those  responsible  for  managing  risk  and  the  DevOps  team  managing  deployment\n",
      "would argue that strict governance across the board should be mandatory.\n",
      "\n",
      "Those  responsible  for  MLOps  must  manage  the  inherent  tension  between  different\n",
      "user profiles, striking a balance between getting the job done efficiently and protect‐\n",
      "ing  against  all  possible  threats.  This  balance  can  be  found  by  assessing  the  specific\n",
      "risk of each project and matching the governance process to that risk level. There are\n",
      "several dimensions to consider when assessing risk, including:\n",
      "\n",
      "• The audience for the model\n",
      "\n",
      "• The lifetime of the model and its outcomes\n",
      "\n",
      "• The impact of the outcomes\n",
      "\n",
      "This  assessment  should  not  only  determine  the  governance  measures  applied,  but\n",
      "also drive the complete MLOps development and deployment tool chain.\n",
      "\n",
      "For example, a self-service analytics (SSA) project (one consumed by a small internal-\n",
      "only audience and often built by business analysts) calls for relatively lightweight gov‐\n",
      "ernance.  Conversely,  a  model  deployed  to  a  public-facing  website  making  decisions\n",
      "that impact people’s lives or company finances requires a very thorough process. This\n",
      "process would consider the type of KPIs chosen by the business, the type of model-\n",
      "building algorithm used for the required level of explainability, the coding tools used,\n",
      "the  level  of  documentation  and  reproducibility,  the  level  of  automated  testing,  the\n",
      "resilience of the hardware platform, and the type of monitoring implemented.\n",
      "\n",
      "But the business risk is not always so clear cut. An SSA project that makes a decision\n",
      "that has a long-term impact can also be high risk and can justify stronger governance\n",
      "measures.  That’s  why  across  the  board,  teams  need  well  thought  out,  regularly\n",
      "reviewed  strategies  for  MLOps  risk  assessment  (see  Figure  8-1  for  a  breakdown  of\n",
      "project criticality and operationalization approaches).\n",
      "\n",
      "Matching Governance with Risk Level \n",
      "\n",
      "| \n",
      "\n",
      "107\n",
      "\n",
      "\f",
      "Figure 8-1. Choosing the right kind of operationalization model and MLOps features\n",
      "depending on the project’s criticality\n",
      "\n",
      "Current Regulations Driving MLOps Governance\n",
      "There  is  little  regulation  around  the  world  today  specifically  aimed  at  ML  and  AI.\n",
      "Many existing regulations do, however, have a significant impact on ML governance.\n",
      "These take two forms:\n",
      "\n",
      "• Industry-specific  regulation.  This  is  particularly  significant  in  the  finance  and\n",
      "\n",
      "pharmaceutical sectors.\n",
      "\n",
      "• Broad-spectrum regulation, particularly addressing data privacy.\n",
      "\n",
      "A few of the most pertinent regulations are outlined in the following sections. Their\n",
      "relevance  to  the  challenges  of  MLOps  governance  is  striking,  and  these  regulations\n",
      "give  a  good  indication  of  what  governance  measures  will  be  needed  broadly  across\n",
      "the industry to establish and maintain trust in ML.\n",
      "\n",
      "Even for those working in industries that don’t have specific regulations, the follow‐\n",
      "ing  sections  can  give  a  brief  idea  of  what  organizations  worldwide,  regardless  of\n",
      "industry,  might  face  in  the  future  in  terms  of  the  level  of  specificity  of  control  with\n",
      "regards to machine learning.\n",
      "\n",
      "108 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Pharmaceutical Regulation in the US: GxP\n",
      "GxP is a collection of quality guidelines (such as the Good Clinical Practice, or GCP,\n",
      "guidelines)  and  regulations  established  by  the  U.S.  Food  and  Drug  Administration\n",
      "(FDA), which aim to ensure that bio and pharmaceutical products are safe.\n",
      "\n",
      "GxP’s guidelines focus on:\n",
      "\n",
      "• Traceability, or the ability to re-create the development history of a drug or medi‐\n",
      "\n",
      "cal device.\n",
      "\n",
      "• Accountability, meaning who has contributed what to the development of a drug\n",
      "\n",
      "and when.\n",
      "\n",
      "• Data  Integrity  (DI),  or  the  reliability  of  data  used  in  development  and  testing.\n",
      "This  is  based  on  the  ALCOA  principle:  attributable,  legible,  contemporaneous,\n",
      "original,  and  accurate,  and  considerations  include  identifying  risks  and  mitiga‐\n",
      "tion strategies.\n",
      "\n",
      "Financial Model Risk Management Regulation\n",
      "In finance, model risk is the risk of incurring losses when the models used for making\n",
      "decisions  about  tradable  assets  prove  to  be  inaccurate.  These  models,  such  as  the\n",
      "Black–Scholes model, existed long before the arrival of ML.\n",
      "\n",
      "Model risk management (MRM) regulation has been driven by the experience of the\n",
      "impact of extraordinary events, such as financial crashes, and the resulting harm to\n",
      "the public and the wider economy if severe losses are incurred. Since the financial cri‐\n",
      "sis  of  2007–2008,  a  large  amount  of  additional  regulation  has  been  introduced  to\n",
      "force good MRM practices (see Figure 8-2).\n",
      "\n",
      "The  UK  Prudential  Regulation  Authority’s  (PRA)  regulation,  for  example,  defines\n",
      "four principles for good MRM:\n",
      "\n",
      "Model definition\n",
      "\n",
      "Define a model and record such models in inventory.\n",
      "\n",
      "Risk governance\n",
      "\n",
      "Establish model risk governance framework, policies, procedures, and controls.\n",
      "\n",
      "Life cycle management\n",
      "\n",
      "Create robust model development, implementation, and usage processes.\n",
      "\n",
      "Effective challenge\n",
      "\n",
      "Undertake appropriate model validation and independent review.\n",
      "\n",
      "Current Regulations Driving MLOps Governance \n",
      "\n",
      "| \n",
      "\n",
      "109\n",
      "\n",
      "\f",
      "Figure 8-2. The history of model risk management (MRM) regulation\n",
      "\n",
      "GDPR and CCPA Data Privacy Regulations\n",
      "The EU General Data Protection Regulation (GDPR) was first implemented in 2018,\n",
      "setting  guidelines  for  the  collection  and  processing  of  personal  information  from\n",
      "individuals  who  live  in  the  European  Union.  However,  it  was  developed  with  the\n",
      "internet age in mind, so it actually applies for EU visitors to any website, regardless of\n",
      "where  that  website  is  based.  Since  few  websites  want  to  exclude  EU  visitors,  sites\n",
      "across the world have been forced to meet the requirements, making GDPR a de facto\n",
      "standard for data protection. The regulations aim to give people control of their per‐\n",
      "sonal data that IT systems have collected, including the rights to:\n",
      "\n",
      "• Be informed about data collected or processed\n",
      "\n",
      "• Access collected data and understand its processing\n",
      "\n",
      "• Correct inaccurate data\n",
      "\n",
      "• Be forgotten (i.e., to have data removed)\n",
      "\n",
      "• Restrict processing of personal data\n",
      "\n",
      "• Obtain collected data and reuse it elsewhere\n",
      "\n",
      "• Object to automated decision-making\n",
      "\n",
      "110 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "The California Consumer Privacy Act (CCPA) is quite similar to GDPR in terms of\n",
      "who and what is protected, although the scope, territorial reach, and financial penal‐\n",
      "ties are all more limited.\n",
      "\n",
      "The New Wave of AI-Specific Regulations\n",
      "Around the world, a new wave of regulations and guidelines specifically targeting AI\n",
      "applications (and thus all ML applications) is emerging. The European Union is lead‐\n",
      "ing the way with an attempt to establish a framework for trustworthy AI.\n",
      "\n",
      "In a white paper on artificial intelligence, the EU emphasizes the potential benefits of\n",
      "AI for all walks of life. Equally, it highlights that scandals surrounding the misuse of\n",
      "AI  and  warnings  of  the  dangers  of  potential  advances  in  the  power  of  AI  have  not\n",
      "gone  unnoticed.  The  EU  considers  that  regulatory  framework  based  on  its  funda‐\n",
      "mental values “will enable it to become a global leader in innovation in the data econ‐\n",
      "omy and its applications.”\n",
      "\n",
      "The  EU  identifies  seven  key  requirements  that  AI  applications  should  respect  to  be\n",
      "considered trustworthy:\n",
      "\n",
      "• Human agency and oversight\n",
      "\n",
      "• Technical robustness and safety\n",
      "\n",
      "• Privacy and data governance\n",
      "\n",
      "• Transparency\n",
      "\n",
      "• Diversity, non-discrimination, and fairness\n",
      "\n",
      "• Societal and environmental well-being\n",
      "\n",
      "• Accountability\n",
      "\n",
      "The  EU  approach  is  not  one-size-fits-all:  it  will  primarily  impact  specific  high-risk\n",
      "sectors,  including  healthcare,  transportation,  energy,  and  parts  of  the  public  sector.\n",
      "The regulations are expected to be optional for other sectors.\n",
      "\n",
      "As  with  GDPR,  the  EU  approach  is  likely  to  have  a  worldwide  influence.  It  is  also\n",
      "probable that many large organizations will decide to opt in considering the impor‐\n",
      "tance to their businesses of public trust in the use of AI. Even for those not opting in,\n",
      "the framework is likely to establish a way of thinking about governance in AI and will\n",
      "influence their approach.\n",
      "\n",
      "Table 8-1 outlines some of the statuses of AI governance initiatives across the world.\n",
      "All are following an unmistakably similar route, even if the level of prescriptiveness\n",
      "reflects their traditionally distinct approaches to regulation.\n",
      "\n",
      "The New Wave of AI-Specific Regulations \n",
      "\n",
      "| \n",
      "\n",
      "111\n",
      "\n",
      "\f",
      "Table 8-1. Status of AI governance initiatives across the world\n",
      "\n",
      "Stage\n",
      "\n",
      "Focus\n",
      "\n",
      "Coming next\n",
      "\n",
      "Regions &\n",
      "organizations\n",
      "OECD\n",
      "\n",
      "Guidance\n",
      "\n",
      "• 42 signatories\n",
      "• 5 principles for responsible stewardship of trustworthy AI:\n",
      "\n",
      "inclusive growth, human-centered and fairness,\n",
      "transparency and explainability, robustness, and\n",
      "accountability\n",
      "\n",
      "• Recommendations for national policies\n",
      "\n",
      "• Binding for high-risk activities (Sector X impact), optional\n",
      "\n",
      "with possibility for label for others\n",
      "\n",
      "• Specifically targeting model fairness, robustness, and\n",
      "auditability, mixing policies and controls, integrating\n",
      "strong ethical considerations on environmental and social\n",
      "impacts\n",
      "\n",
      "• Positive, nonsanctioned-based approach focusing on\n",
      "\n",
      "practical steps to implementation AI governance at an\n",
      "organization level\n",
      "\n",
      "• Best practice center, supporting AI governance work at\n",
      "\n",
      "Economic Forum level\n",
      "\n",
      "• Federal guidelines issued to prepare ground for industry-\n",
      "\n",
      "specific guidelines or regulation\n",
      "\n",
      "• Focus on public trust and fairness; no broader ethics\n",
      "\n",
      "considerations\n",
      "\n",
      "High-level guidelines only; nonbinding and broad in coverage\n",
      "Detailed guidelines issued, integrating ethical and a strong\n",
      "focus on end-consumer protection\n",
      "\n",
      "• Directive by end\n",
      "2020/early 2021\n",
      "• To be translated\n",
      "into national\n",
      "regime\n",
      "\n",
      "• Regulation by\n",
      "end 2020/early\n",
      "2021\n",
      "\n",
      "EU\n",
      "\n",
      "Guidance,\n",
      "communication,\n",
      "direction, and\n",
      "regulation\n",
      "\n",
      "Singapore\n",
      "\n",
      "Guidance\n",
      "\n",
      "US\n",
      "\n",
      "Guidance,\n",
      "communication,\n",
      "and regulation\n",
      "\n",
      "UK\n",
      "Australia\n",
      "\n",
      "Guidance\n",
      "Guidance\n",
      "\n",
      "The Emergence of Responsible AI\n",
      "As the adoption of data science, machine learning, and AI has accelerated worldwide,\n",
      "a  loose  consensus  among  AI  thinkers  has  emerged.  The  most  common  banner  for\n",
      "this  consensus  is  Responsible  AI:  the  idea  of  developing  machine  learning  systems\n",
      "that  are  accountable,  sustainable,  and  governable.  In  essence,  AI  systems  should  do\n",
      "what they are supposed to, remain reliable over time, and be well controlled as well as\n",
      "auditable. \n",
      "\n",
      "There is no strict definition of Responsible AI or the terms used to frame it, but there\n",
      "is agreement about the overarching considerations and largely about what is needed\n",
      "to  deliver  it  (see  Table  8-2).  Despite  the  lack  of  any  single  body  driving  the  move‐\n",
      "ment, Responsible AI has already had a significant influence on collective thinking,\n",
      "and especially on the EU’s trustworthy AI regulators.\n",
      "\n",
      "112 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f",
      "Table 8-2. Components of Responsible AI, an increasingly critical part of MLOps\n",
      "\n",
      "Intentionality\n",
      "Must have:\n",
      "\n",
      "Accountability\n",
      "Must have:\n",
      "\n",
      "• Assurance that models are designed and behave in\n",
      "\n",
      "• Central control, management, and the ability to audit the\n",
      "\n",
      "ways aligned with their purpose\n",
      "\n",
      "enterprise AI effort (no shadow IT!)\n",
      "\n",
      "• Assurance that data used for AI projects comes from\n",
      "compliant and unbiased sources plus a collaborative\n",
      "approach to AI projects that ensures multiple checks\n",
      "and balances on potential model bias\n",
      "\n",
      "• Intentionality also includes explainability, meaning\n",
      "the result of AI systems should be explainable by\n",
      "humans (ideally not just the humans that created the\n",
      "system)\n",
      "\n",
      "• An overall view of which teams are using what data, how,\n",
      "\n",
      "and in which models\n",
      "\n",
      "• Trust that data is reliable and being collected in accordance\n",
      "with regulation as well as a centralized understanding of\n",
      "which models are being used for which business process.\n",
      "This is closely tied to traceability—if something goes\n",
      "wrong, is it easy to find where in the pipeline it happened?\n",
      "\n",
      "Human-centered approach\n",
      "Providing people with the tools and training to be aware of and then execute on both components  \n",
      "\n",
      "Key Elements of Responsible AI\n",
      "Responsible  AI  is  about  the  responsibility  of  data  practitioners,  not  about  AI  itself\n",
      "being responsible: this is a very important distinction.  Another important distinction\n",
      "is that, according to Kurt Muemel of Dataiku, “It is not necessarily about intentional\n",
      "harm, but accidental harm.”\n",
      "\n",
      "This section presents five key elements that figure in Responsible AI thinking—data,\n",
      "bias, inclusiveness, model management at scale, and governance—as well as MLOps\n",
      "considerations for each element.\n",
      "\n",
      "Element 1: Data\n",
      "The dependence on data is a fundamental differentiator between ML and traditional\n",
      "software development. The quality of the data used will make the biggest impact on\n",
      "the accuracy of the model. Some real-world considerations are as follows:\n",
      "\n",
      "• Provenance is king. Understand how the data was collected and its journey to the\n",
      "\n",
      "point of use.\n",
      "\n",
      "• Get the data off of desktops. Data must be manageable, securable, and traceable.\n",
      "\n",
      "Personal data must be strictly managed.\n",
      "\n",
      "• The quality of data over time: consistency, completeness, and ownership.\n",
      "\n",
      "• Bias in, bias out. Biased input data can occur easily and unintentionally.\n",
      "\n",
      "Key Elements of Responsible AI \n",
      "\n",
      "| \n",
      "\n",
      "113\n",
      "\n",
      " \n",
      " \n",
      "\f",
      "Element 2: Bias\n",
      "ML predictive modeling is about building a system to recognize and exploit tenden‐\n",
      "cies in the real world. Certain types of cars, driven by certain types of people, in cer‐\n",
      "tain places are more likely to be costlier to insurance companies than others. But is\n",
      "matching  a  pattern  always  considered  ethical?  When  is  such  pattern-matching  pro‐\n",
      "portionate, and when is it an unfair bias?\n",
      "\n",
      "Establishing what is fair is not clear-cut. Even using a churn model to give rebates to\n",
      "the customers who are more likely to leave might be considered as unfair against dor‐\n",
      "mant customers who will pay more for the same product. Regulations are a place to\n",
      "start looking, but as already discussed, opinion is not universal and is not fixed. Even\n",
      "with a clear understanding of the fairness constraints to work toward, achieving them\n",
      "is not simple. When the developers of the recruitment system that was biased against\n",
      "women’s  schools  adapted  the  model  to  ignore  the  words  like  “women’s,”  they  found\n",
      "that even the tone of the language in a resume reflected the gender of the author and\n",
      "created unwanted bias against women. Addressing these biases has deep implications\n",
      "on the ML model to be built (see “Impact of Responsible AI on Modeling” on page 53\n",
      "for a detailed example).\n",
      "\n",
      "Taking a step back, these bias problems are not new; for example, hiring discrimina‐\n",
      "tion has always been an issue. What is new is that, thanks to the IT revolution, data to\n",
      "assess biases is more available. On top of that, thanks to the automation of decision\n",
      "making with machine learning, it is possible to change the behavior without having to\n",
      "go through the filter of individuals making subjective decisions.\n",
      "\n",
      "The bottom line is that biases are not only statistical. Bias checks should be integrated\n",
      "in governance frameworks so that issues are identified as early as possible, since they\n",
      "do have the potential to derail data science and machine learning projects.\n",
      "\n",
      "It’s  not  all  bad  news:  there  are  many  potential  sources  of  statistical  bias  (i.e.,  of  the\n",
      "world as it was) that can be addressed by data scientists:\n",
      "\n",
      "• Is bias encoded into the training data? Is the raw material biased? Has data prepa‐\n",
      "\n",
      "ration, sampling, or splitting introduced bias?\n",
      "\n",
      "• Is the problem framed properly?\n",
      "\n",
      "• Do we have the right target for all subpopulations? Beware that many variables\n",
      "\n",
      "may be highly correlated.\n",
      "\n",
      "• Is feedback-loop data biased through factors such as the order in which choices\n",
      "\n",
      "are presented in the UI?\n",
      "\n",
      "It  is  so  complex  to  prevent  the  problems  caused  by  bias  that  much  of  the  current\n",
      "focus  is  on  detecting  bias  before  it  causes  harm.  ML  interpretability  is  the  current\n",
      "\n",
      "114 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "mainstay  of  bias  detection,  bringing  understanding  to  ML  models  through  a  set  of\n",
      "technical tools to analyze models including:\n",
      "\n",
      "• Prediction understanding: Why did a model make a specific prediction?\n",
      "\n",
      "• Subpopulation analysis: Is there bias among subpopulations?\n",
      "\n",
      "• Dependency understanding: What contributions are individual features making?\n",
      "\n",
      "A  very  different,  but  complementary,  approach  to  addressing  bias  is  to  leverage  as\n",
      "broad a range of human expertise as possible in the development process. This is one\n",
      "aspect of the idea of inclusiveness in Responsible AI.\n",
      "\n",
      "Element 3: Inclusiveness\n",
      "The human-in-the-loop (HITL) approach aims to combine the best of human intelli‐\n",
      "gence with the best of machine intelligence. Machines are great at making smart deci‐\n",
      "sions  from  vast  datasets,  whereas  people  are  much  better  at  making  decisions  with\n",
      "less  information.  Human  judgment  is  particularly  effective  for  making  ethical  and\n",
      "harm-related judgments.\n",
      "\n",
      "This concept can be applied to the way models are used in production, but it can be\n",
      "equally important in the way models are built. Formalizing human responsibility in\n",
      "the  MLOps  loop,  for  example  through  sign-off  processes,  can  be  simple  to  do,  but\n",
      "highly effective.\n",
      "\n",
      "The  principle  of  inclusiveness  takes  the  idea  of  human-AI  collaboration  further:\n",
      "bringing as diverse a set of human expertise to the ML life cycle as possible reduces\n",
      "the  risk  of  serious  blind  spots  and  omissions.  The  less  inclusive  the  group  building\n",
      "the ML, the greater the risk.\n",
      "\n",
      "The perspectives of the business analyst, the subject matter expert, the data scientist,\n",
      "the data engineer, the risk manager, and the technical architect are all different. All of\n",
      "these perspectives together bring far greater clarity to managing model development\n",
      "and deployment than relying on any single user profile, and enabling these user pro‐\n",
      "files to collaborate effectively is a key factor in reducing risk and increasing the per‐\n",
      "formance  of  MLOps  in  any  organization.  Refer  to  Chapter  2  for  clear  examples  of\n",
      "collaboration among different profiles for better MLOps performance.\n",
      "\n",
      "Full  inclusiveness  may  even  bring  the  consumer  into  the  process,  perhaps  through\n",
      "focus group testing. The objective of inclusiveness is to bring the appropriate human\n",
      "expertise  into  the  process,  regardless  of  source.  Leaving  ML  to  data  scientists  is  not\n",
      "the answer to managing risk.\n",
      "\n",
      "Key Elements of Responsible AI \n",
      "\n",
      "| \n",
      "\n",
      "115\n",
      "\n",
      "\f",
      "Element 4: Model Management at Scale\n",
      "Managing the risk associated with ML when there are a handful of models in produc‐\n",
      "tion  can  afford  to  be  largely  manual.  But  as  the  volume  of  deployments  grows,  the\n",
      "challenges  multiply  rapidly.  Here  are  some  key  considerations  for  managing  ML  at\n",
      "scale:\n",
      "\n",
      "• A scalable model life cycle needs to be largely automated as well as streamlined.\n",
      "\n",
      "• Errors, for example in a subset of a dataset, will propagate out rapidly and widely.\n",
      "\n",
      "• Existing software engineering techniques can assist ML at scale.\n",
      "\n",
      "• Decisions must be explainable, auditable, and traceable.\n",
      "\n",
      "• Reproducibility  is  key  to  understanding  what  went  wrong,  who  or  what  was\n",
      "\n",
      "responsible, and who should ensure it is corrected.\n",
      "\n",
      "• Model  performance  will  degrade  over  time:  monitoring,  drift  management,\n",
      "\n",
      "retraining, and remodeling must be built into the process.\n",
      "\n",
      "• Technology  is  evolving  rapidly;  an  approach  to  integrating  new  technologies  is\n",
      "\n",
      "required.\n",
      "\n",
      "Element 5: Governance\n",
      "Responsible AI sees strong governance as the key to achieving fairness and trustwor‐\n",
      "thiness. The approach builds on traditional governance techniques:\n",
      "\n",
      "• Determine intentions at the beginning of the process\n",
      "\n",
      "• Formalize bringing humans in the loop\n",
      "\n",
      "• Clearly identify responsibilities (Figure 8-3)\n",
      "\n",
      "• Integrate goals that define and structure the process\n",
      "\n",
      "• Establish and communicate a process and rules\n",
      "\n",
      "• Define measurable metrics and monitor for deviation\n",
      "\n",
      "• Build multiple checks into the MLOps pipeline aligned with overall goals\n",
      "\n",
      "• Empower people through education\n",
      "\n",
      "• Teach builders as well as decision makers how to prevent harm\n",
      "\n",
      "Governance  is,  therefore,  both  the  foundation  and  the  glue  of  MLOps  initiatives.\n",
      "However,  it’s  important  to  recognize  that  it  goes  beyond  the  borders  of  traditional\n",
      "data governance.\n",
      "\n",
      "116 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Figure 8-3. A representation of who is responsible at different levels of the organization\n",
      "for different parts of the Responsible AI process\n",
      "\n",
      "A Template for MLOps Governance\n",
      "Having  explored  the  key  themes  to  be  addressed  by  an  MLOps  governance,  both\n",
      "through regulatory measures and the Responsible AI movement, it is time to map out\n",
      "how to implement a robust governance framework of MLOps.\n",
      "\n",
      "There is no one-size-fits-all solution across businesses, and different use cases within\n",
      "a business justify different levels of management, but the step-by-step approach out‐\n",
      "lined can be applied in any organization to guide the implementation process.\n",
      "\n",
      "The process has eight steps:\n",
      "\n",
      "1. Understand and classify the analytics use cases.\n",
      "\n",
      "2. Establish an ethical position.\n",
      "\n",
      "3. Establish responsibilities.\n",
      "\n",
      "4. Determine governance policies.\n",
      "\n",
      "5. Integrate policies into the MLOps process.\n",
      "\n",
      "6. Select the tools for centralized governance management.\n",
      "\n",
      "7. Engage and educate.\n",
      "\n",
      "8. Monitor and refine.\n",
      "\n",
      "This section will go through each of the steps in detail, including a simple definition\n",
      "and the “how” of actually implementing the step.\n",
      "\n",
      "A Template for MLOps Governance \n",
      "\n",
      "| \n",
      "\n",
      "117\n",
      "\n",
      "\f",
      "Step 1: Understand and Classify the Analytics Use Cases\n",
      "This step entails defining what the different classes of analytics use cases are and, sub‐\n",
      "sequently, what the governance needs are for each.\n",
      "\n",
      "Consider the answers to the following questions for a representative cross-section of\n",
      "analytics use cases. Identify the key distinguishing features of the different use cases\n",
      "and categorize these features. Conflate categories where appropriate. Typically, it will\n",
      "be necessary to associate several categories to each use case to fully describe it.\n",
      "\n",
      "• What  regulations  is  each  use  case  subject  to,  and  what  are  the  implications?\n",
      "\n",
      "Sector-specific regulations, regional, PII?\n",
      "\n",
      "• Who  consumes  the  results  of  the  model?  The  public?  One  of  many  internal\n",
      "\n",
      "users?\n",
      "\n",
      "• What  are  the  availability  requirements  for  the  deployed  model?  24/7  real-time\n",
      "\n",
      "scoring, scheduled batch scoring, ad-hoc runs (self-service analytics)?\n",
      "\n",
      "• What is the impact of any errors and deficiencies? Legal, financial, personal, pub‐\n",
      "\n",
      "lic trust?\n",
      "\n",
      "• What is the cadence and urgency of releases?\n",
      "\n",
      "• What is the lifetime of the model and the lifetime of the impact of its decision?\n",
      "\n",
      "• What is the likely rate of model quality decay?\n",
      "\n",
      "• What is the need for explainability and transparency?\n",
      "\n",
      "Step 2: Establish an Ethical Position\n",
      "We established that fairness and ethical considerations are important motivating fac‐\n",
      "tors  for  effective  governance,  that  businesses  have  a  choice  on  their  ethical  stance,\n",
      "and that this impacts public perception and trust. The position a business takes is a\n",
      "trade-off between the cost to implement the position and public perception. Respon‐\n",
      "sible stances rarely come at zero short-term financial cost even if the long-term ROI\n",
      "may be positive.\n",
      "\n",
      "Any MLOps governance framework needs to reflect the ethical position of the com‐\n",
      "pany. While the position typically impacts what a model does and how it does it, the\n",
      "MLOps governance process needs to ensure that deployed models match the chosen\n",
      "ethical stance. This stance is likely to influence the governance process more widely,\n",
      "including the selection and verification of new models and the acceptable likelihood\n",
      "of accidental harm.\n",
      "\n",
      "118 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Consider the following ethical questions:\n",
      "\n",
      "• What aspects of well-being in society matter? E.g., equality, privacy, human rights\n",
      "\n",
      "and dignity, employment, democracy, bias\n",
      "\n",
      "• Is  the  potential  impact  on  human  psychology  to  be  considered?  E.g.,  human-\n",
      "\n",
      "human or human-AI relationships, deception, manipulation, exploitation\n",
      "\n",
      "• Is a stance on the financial impact required? E.g., market manipulation\n",
      "\n",
      "• How transparent should the decision making be?\n",
      "\n",
      "• What  level  of  accountability  for  AI-driven  mistakes  does  the  business  want  to\n",
      "\n",
      "accept?\n",
      "\n",
      "Step 3: Establish Responsibilities\n",
      "Identify the groups of people responsible for overseeing MLOps governance as well\n",
      "as their roles.\n",
      "\n",
      "• Engage  the  whole  organization,  across  departments,  from  top  to  bottom  of  the\n",
      "\n",
      "management hierarchy.\n",
      "\n",
      "• Peter  Drucker’s  famous  line  “Culture  eats  strategy  for  breakfast”  highlights  the\n",
      "\n",
      "power of broad engagement and shared beliefs.\n",
      "\n",
      "• Avoid  creating  all-new  governance  structures.  Look  at  what  structures  exist\n",
      "\n",
      "already and try to incorporate MLOps governance into them.\n",
      "\n",
      "• Get senior management sponsorship for the governance process.\n",
      "\n",
      "• Think in terms of separate levels of responsibility:\n",
      "\n",
      "— Strategic: set out the vision\n",
      "\n",
      "— Tactical: implement and enforce the vision\n",
      "\n",
      "— Operational: execute on a daily basis\n",
      "\n",
      "• Consider  building  a  RACI  matrix  for  the  complete  MLOps  process  (see\n",
      "Figure 8-4). RACI stands for responsible, accountable, consulted, informed, and it\n",
      "highlights the roles of different stakeholders in the overall MLOps process. It is\n",
      "quite likely that any matrix you create at this stage will need to be refined later on\n",
      "in the process.\n",
      "\n",
      "A Template for MLOps Governance \n",
      "\n",
      "| \n",
      "\n",
      "119\n",
      "\n",
      "\f",
      "Figure 8-4. A typical RACI matrix for MLOps\n",
      "\n",
      "Step 4: Determine Governance Policies\n",
      "With an understanding of the scope and objectives for governance now established,\n",
      "and the engagement of the responsible governance leaders, it is time to consider the\n",
      "core  policies  for  the  MLOps  process.  This  is  no  small  task,  and  it  is  unlikely  to  be\n",
      "achieved in one iteration. Focus on establishing the broad areas of policy and accept\n",
      "that experience will help to evolve the details.\n",
      "\n",
      "Consider the classification of initiatives from Step 1. What governance measures do\n",
      "the team or organization need in each case?\n",
      "\n",
      "In  initiatives  where  there  is  less  concern  about  the  risk  or  regulatory  compliance,\n",
      "lighter-weight, cheaper measures may be appropriate. For example, “what if ” calcula‐\n",
      "tions to determine the number of in-flight meals of different types has relatively little\n",
      "impact—after  all,  the  mix  was  never  right  even  before  the  introduction  of  machine\n",
      "learning. Even such a seemingly insignificant use case may have ethical implications\n",
      "as meal choices are likely to be correlated to religion or gender, which are protected\n",
      "attributes in many countries. On the other hand, the implications of calculations to\n",
      "determine the level of fueling of planes carry substantially greater risk.\n",
      "\n",
      "120 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Governance considerations can be broadly grouped under the headings in Table 8-3.\n",
      "For each heading, there are a range of measures to consider for each class.\n",
      "\n",
      "Table 8-3. MLOps governance considerations\n",
      "\n",
      "Governance\n",
      "consideration\n",
      "Reproducibility and\n",
      "traceability\n",
      "Audit and\n",
      "documentation\n",
      "Human-in-the-loop\n",
      "sign-off\n",
      "Preproduction\n",
      "verification\n",
      "\n",
      "Transparency and\n",
      "explainability\n",
      "Bias and harm testing\n",
      "\n",
      "Production deployment\n",
      "modes\n",
      "Production monitoring\n",
      "\n",
      "Data quality and\n",
      "compliance\n",
      "\n",
      "Example measures\n",
      "\n",
      "Full VM and data snapshot for precise and rapid model re-instantiation, or ability to re-create the\n",
      "environment and retrain with a data sample, or only record metrics of models deployed?\n",
      "Full log of all changes during development including experiments run and reasons for choices made\n",
      "or automated documentation of deployed model only or no documentation at all\n",
      "Multiple sign-offs for every environment move (development, QA, preproduction, production)\n",
      "\n",
      "Verify model documentation by hand-coding the model and comparing results or full automated\n",
      "test pipeline re-creating in production-like environment with extensive unit and end-to-end test\n",
      "cases or automated checks on database, software version, and naming standards only\n",
      "Use manually-coded decision tree for maximum explainability or use regression algorithms’\n",
      "explainability tools such as Shapely values or accept opaque algorithms such as neural networks\n",
      "“Red team” adversarial manual testing using multiple tools and attack vectors or automated bias\n",
      "checking on specific subpopulations\n",
      "Containerized deployment to elastic scalable high-availability, multi-node configuration with\n",
      "automated stress/load testing prior to deployment or a single production server\n",
      "Real-time alerting of errors, dynamic multi-armed bandit model balancing, automated nightly\n",
      "retraining, model evaluation, and redeployment or weekly input drift monitoring and manual\n",
      "retraining or basic infrastructure alerts, no monitoring, no feedback-based retraining\n",
      "PII considerations including anonymization and documented and reviewed column-level lineage to\n",
      "understand the source, quality, and appropriateness of the data and automated data quality checks\n",
      "for anomalies\n",
      "\n",
      "The finalized governance policies should provide:\n",
      "\n",
      "• A process for determining the classification of any analytics initiative. This could\n",
      "\n",
      "be implemented as a checklist or a risk assessment application.\n",
      "\n",
      "• A matrix of initiative classification against governance consideration, where each\n",
      "\n",
      "cell identifies the measures required.\n",
      "\n",
      "Step 5: Integrate Policies into the MLOps Process\n",
      "Once the governance policies for the different classes of initiatives have been identi‐\n",
      "fied, measures to implement them need to be incorporated into the MLOps process\n",
      "and responsibilities for actioning the measures assigned.\n",
      "\n",
      "While most businesses will have an existing MLOps process, it is quite likely that this\n",
      "has  not  been  defined  explicitly,  but  rather  has  evolved  in  response  to  individual\n",
      "needs.  Now  is  the  time  to  revisit,  enhance,  and  document  the  process.  Successful\n",
      "\n",
      "A Template for MLOps Governance \n",
      "\n",
      "| \n",
      "\n",
      "121\n",
      "\n",
      "\f",
      "adoption  of  the  governance  process  can  only  happen  if  it  is  communicated  clearly\n",
      "and buy-in is sought from each stakeholder group.\n",
      "\n",
      "Understand all of the steps in the existing process by interviewing those responsible.\n",
      "Where there is no existing formal process, this is often harder than it sounds because\n",
      "the process steps are often not explicitly defined, and ownership is unclear.\n",
      "\n",
      "Attempting to map the policy-driven governance measures into the understanding of\n",
      "the process will identify issues in the process very quickly. Within one business there\n",
      "may be a range of different styles of project and governance needs, such as:\n",
      "\n",
      "• One-off self-service analytics\n",
      "\n",
      "• Internally consumed models\n",
      "\n",
      "• Models embedded in public websites\n",
      "\n",
      "• Models deployed to Internet of Things devices\n",
      "\n",
      "In  these  cases,  the  differences  between  some  processes  may  be  so  great  it  is  best  to\n",
      "think in terms of several parallel processes. Ultimately, every governance measure for\n",
      "each  use  case  should  be  associated  with  a  process  step  and  with  a  team  that  is  ulti‐\n",
      "mately responsible, as presented here:\n",
      "\n",
      "Process step\n",
      "Business scoping\n",
      "Ideation\n",
      "\n",
      "Development\n",
      "\n",
      "Preproduction\n",
      "\n",
      "Deployment\n",
      "\n",
      "Example activities and governance considerations\n",
      "Record objectives, define KPIs, and record sign-off: for internal governance considerations\n",
      "\n",
      "Data discovery: data quality and regulatory compliance constraints\n",
      "Algorithm choice: impacted by explainability requirements\n",
      "\n",
      "Data preparation: consider PII compliance, separation of legal regional scopes, avoid input bias\n",
      "Model development: consider model reproducibility and auditabilityModel testing and verification:\n",
      "bias and harm testing, explainability\n",
      "\n",
      "Verify performance/bias with production data\n",
      "Production-ready testing: verify scalability\n",
      "\n",
      "Deployment strategy: driven by the level of operationalization\n",
      "Deployment verification testsUse of shadow challenger or A/B test techniques for in-production\n",
      "verification\n",
      "\n",
      "Monitoring and\n",
      "feedback\n",
      "\n",
      "Performance metrics and alerting\n",
      "Prediction log analysis for input drift with alerting\n",
      "\n",
      "Step 6: Select the Tools for Centralized Governance Management\n",
      "The MLOps governance process impacts both the complete ML life cycle and many\n",
      "teams across the organization. Each step requires a specific sequence of actions and\n",
      "checks to be executed. Traceability of both the development of the model and the exe‐\n",
      "cution of governance activities is a complex challenge.\n",
      "\n",
      "122 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "Most organizations still have a “paper form” mindset for process management, where\n",
      "forms  are  filled  in,  circulated,  signed,  and  filed.  The  forms  may  be  text  documents,\n",
      "circulated via email, and filed electronically, but the limitations of paper remain. It is\n",
      "hard to track progress, associate artifacts, review many projects at once, prompt for\n",
      "action,  and  remind  teams  of  responsibilities.  The  complete  record  of  events  is  typi‐\n",
      "cally spread across multiple systems and owned by individual teams, making a simple\n",
      "overview of analytics projects effectively impossible.\n",
      "\n",
      "While teams will always have tools specific to their roles, MLOps governance is much\n",
      "more  effective  if  the  overarching  process  is  managed  and  tracked  from  one  system.\n",
      "This system should:\n",
      "\n",
      "• Centralize the definition of the governance process flows for each class of analyt‐\n",
      "\n",
      "ics use cases\n",
      "\n",
      "• Enable tracking and enforcement of the complete governance process\n",
      "\n",
      "• Provide a single point of reference for the discovery of analytics projects\n",
      "\n",
      "• Enable collaboration between teams, in particular, the transfer of work between\n",
      "\n",
      "teams\n",
      "\n",
      "• Integrate with existing tools used for project execution\n",
      "\n",
      "The current workflow, project management, and MLOps tools can only partially sup‐\n",
      "port these objectives. A new category of ML governance tools is emerging to support\n",
      "this need directly and more fully. These new tools focus on the specific challenges of\n",
      "ML governance, including:\n",
      "\n",
      "• A single view of the status of all models (otherwise known as a model registry)\n",
      "\n",
      "• Process gates with a sign-off mechanism to allow ready traceability of the history\n",
      "\n",
      "of decision making\n",
      "\n",
      "• Ability to track all versions of a model\n",
      "\n",
      "• Ability to link to artifact stores, metrics snapshots, and documentation\n",
      "\n",
      "• Ability to tailor processes specifically for each class of analytics use cases\n",
      "\n",
      "• Ability to integrate health monitoring from production systems and to track the\n",
      "\n",
      "performance of models against the original business KPIs\n",
      "\n",
      "Step 7: Engage and Educate\n",
      "Without a program of engagement and training for the groups involved in overseeing\n",
      "and executing the governance process, the chances of it being even partially adopted\n",
      "are slim. It is essential that the importance of MLOps governance to the business, and\n",
      "the  necessity  of  each  team’s  contribution,  is  communicated.  Building  on  this\n",
      "understanding,  every  individual  needs  to  learn  what  they  must  do,  when,  and  how.\n",
      "\n",
      "A Template for MLOps Governance \n",
      "\n",
      "| \n",
      "\n",
      "123\n",
      "\n",
      "\f",
      "This  exercise  will  require  considerable  documentation,  training,  and,  most  of  all,\n",
      "time.\n",
      "\n",
      "Start  by  communicating  the  broad  vision  for  MLOps  governance  in  the  business.\n",
      "Highlight the dangers of the status quo, outline a process, and detail how it is tailored\n",
      "to the range of use cases.\n",
      "\n",
      "Engage directly with each team involved and build a training program directly with\n",
      "them. Do not be afraid to leverage their experience to shape not only the training, but\n",
      "also the detailed implementation of their governance responsibilities. The result will\n",
      "be much stronger buy-in and more effective governance.\n",
      "\n",
      "Step 8: Monitor and Refine\n",
      "Is the newly implemented governance working? Are the prescribed steps being imple‐\n",
      "mented, and are the objectives being met? What actions should be taken if things are\n",
      "going  poorly?  How  do  we  measure  the  gap  between  today’s  reality  and  where  the\n",
      "business needs to be?\n",
      "\n",
      "Measuring success requires metrics and checks. It requires people to be tasked with\n",
      "monitoring and a way to address problems. The governance process and the way it is\n",
      "implemented will need to be refined over time, based both on lessons learned and on\n",
      "evolving requirements (including, as discussed earlier in this chapter, evolving regula‐\n",
      "tory requirements).\n",
      "\n",
      "A  big  factor  in  the  success  of  the  process  will  be  the  diligence  of  the  individuals\n",
      "responsible for the individual measures in the process, and incentivizing them is key.\n",
      "\n",
      "Monitoring the governance process starts with a clear understanding of the key per‐\n",
      "formance  metrics  and  targets—KPIs  for  governance.  These  should  aim  to  measure\n",
      "both whether the process is being enacted and whether objectives are being achieved.\n",
      "Monitoring and auditing can be time consuming, so look to automate metrics as far\n",
      "as  possible  and  encourage  individual  teams  to  own  the  monitoring  of  metrics  that\n",
      "relate to their area of responsibility.\n",
      "\n",
      "It’s  hard  to  make  people  carry  out  tasks  that  seemingly  deliver  nothing  concrete  to\n",
      "those doing the work. One popular tactic to address this is gamification. This is not\n",
      "about making everything look like a video game, but about introducing incentives for\n",
      "people to carry out tasks where the main benefit is derived by others.\n",
      "\n",
      "Look to gamify the governance process in simple ways: publishing KPI results widely\n",
      "is  the  simplest  place  to  start.  Just  being  able  to  see  targets  being  met  is  a  source  of\n",
      "satisfaction  and  motivation.  Leaderboards,  whether  at  the  team  or  individual  level,\n",
      "can add some constructive element of competition. For example, people whose work\n",
      "consistently  passes  compliance  checks  the  first  time,  or  meets  deadlines,  should  be\n",
      "able to feel their efforts are visible.\n",
      "\n",
      "124 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Model Governance\n",
      "\n",
      "\f",
      "However, excessive competition can be disruptive and demotivating. A balance must\n",
      "be struck, and this is best achieved by building up gamification elements slowly over\n",
      "time.  Start  with  the  least  competition-oriented  and  add  new  elements  one  by  one,\n",
      "measuring their effectiveness before adding the next.\n",
      "\n",
      "Monitoring  changes  in  the  governance  landscape  is  essential.  This  might  be  regula‐\n",
      "tory, or it might be about public opinion. Those with responsibility for the strategic\n",
      "vision  must  continue  to  monitor  it  as  well  as  have  a  process  to  evaluate  potential\n",
      "changes.\n",
      "\n",
      "Finally,  all  monitoring  of  the  process  is  only  worthwhile  if  issues  are  acted  upon.\n",
      "Establish  a  process  for  agreeing  on  change  and  for  enacting  it.  This  may  result  in\n",
      "revisiting  policies,  processes,  tools,  responsibilities,  education,  and  monitoring!  It’s\n",
      "necessary to iterate and refine, but the balance between efficiency and effectiveness is\n",
      "hard to find; many lessons can only be learned the hard way. Build a culture where\n",
      "people see iteration and refinement as a measure of a successful process, not a failed\n",
      "one.\n",
      "\n",
      "Closing Thoughts\n",
      "It’s hard to separate MLOps from its governance. It’s not possible to successfully man‐\n",
      "age the model life cycle, mitigate the risks, and deliver value at scale without gover‐\n",
      "nance. Governance impacts everything from how the business can acceptably exploit\n",
      "ML,  to  the  data  and  algorithms  that  can  be  used,  to  the  style  of  operationalization,\n",
      "monitoring, and retraining.\n",
      "\n",
      "MLOps at scale is in its infancy. Few businesses are doing it, and even fewer are doing\n",
      "it well. While governance is the key to improving the effectiveness of MLOps, there\n",
      "are  few  tools  today  that  directly  address  this  challenge,  and  there  is  only  piecemeal\n",
      "advice.\n",
      "\n",
      "Public trust in ML is at risk. Even slow-moving organizations like the EU understand\n",
      "this. If trust is lost, then so too will be many of the benefits to be derived from ML.\n",
      "Additional  legislation  is  being  prepared,  but  even  without  this,  businesses  need  to\n",
      "worry  about  the  potential  damage  to  their  public  image  that  can  be  caused  by  an\n",
      "inadvertently harmful model.\n",
      "\n",
      "When planning to scale MLOps, start with governance and use it to drive the process.\n",
      "Don’t bolt it on at the end. Think through the policies; think about using tooling to\n",
      "give a centralized view; engage across the organization. It will take time and iteration,\n",
      "but  ultimately  the  business  will  be  able  to  look  back  and  be  proud  that  it  took  its\n",
      "responsibilities seriously.\n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "125\n",
      "\n",
      "\f",
      "\f",
      "PART III\n",
      "MLOps: Real-World Examples\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 9\n",
      "MLOps in Practice: Consumer\n",
      "Credit Risk Management\n",
      "\n",
      "In  the  final  chapters  of  this  book,  we  explore  three  examples  of  how  MLOps  might\n",
      "look in practice. We explicitly chose these three examples because they represent fun‐\n",
      "damentally different use cases for machine learning and illustrate how MLOps meth‐\n",
      "odology  might  differ  to  suit  the  needs  of  the  business  and  its  ML  model  life  cycle\n",
      "practices.\n",
      "\n",
      "Background: The Business Use Case\n",
      "When  a  consumer  asks  for  a  loan,  the  credit  institution  has  to  make  a  decision  on\n",
      "whether or not to grant it. Depending on the case, the amount of automation in the\n",
      "process  may  vary.  However,  it  is  very  likely  that  the  decision  will  be  informed  by\n",
      "scores that estimate the probability that the loan will or will not be repaid as expected.\n",
      "\n",
      "Scores are routinely used at different stages of the process:\n",
      "\n",
      "• At the prescreen stage, a score computed with a small number of features allows\n",
      "\n",
      "the institution to quickly discard some applications.\n",
      "\n",
      "• At  the  underwriting  stage,  a  score  computed  with  all  the  required  information\n",
      "\n",
      "gives a more precise basis for the decision.\n",
      "\n",
      "• After the underwriting stage, scores can be used to assess the risk associated with\n",
      "\n",
      "loans in the portfolio.\n",
      "\n",
      "Analytics  methods  have  been  used  for  decades  to  compute  these  probabilities.  For\n",
      "example,  the  FICO  score  has  been  used  since  1995  in  the  United  States.  Given  the\n",
      "direct  impact  they  have  on  the  institutions’  revenues  and  on  customers’  lives,  these\n",
      "predictive  models  have  always  been  under  great  scrutiny.  Consequently,  processes,\n",
      "\n",
      "129\n",
      "\n",
      "\f",
      "methods,  and  skills  have  been  formalized  into  a  highly  regulated  environment  to\n",
      "ensure the sustainable performance of models.\n",
      "\n",
      "Whether the models are based on expert-made rules, on classical statistical models,\n",
      "or on more recent machine learning algorithms, they all have to comply with similar\n",
      "regulations. Consumer credit risk management can therefore be seen as a precursor\n",
      "of  MLOps:  parallels  with  other  use  cases  as  well  as  best  practices  can  be  analyzed\n",
      "based on this use case.\n",
      "\n",
      "At the time a credit decision is made, information about the customer’s historical and\n",
      "current situation is usually available. How much credit does the customer hold? Has\n",
      "the customer ever not repaid a loan (in credit jargon, is the customer a delinquent)?\n",
      "In  some  countries,  organizations  called  credit  bureaus  collect  this  information  and\n",
      "make it available to creditors either directly or through the form of a score (like the\n",
      "aforementioned FICO score).\n",
      "\n",
      "The definition of the target to be predicted is more complex. A customer not repay‐\n",
      "ing as expected is a “bad” outcome in credit risk modeling. In theory, one should wait\n",
      "for the complete repayment to determine a “good” outcome and for the loss charge\n",
      "off to determine a “bad” outcome. However, it may take a long time to obtain these\n",
      "ultimate figures, and waiting for them would deter reactivity to changing conditions.\n",
      "As a result, trade-offs are usually made, based on various indicators, to declare “bad”\n",
      "outcomes before the losses are certain.\n",
      "\n",
      "Model Development\n",
      "Historically,  credit  risk  modeling  is  based  on  a  mix  of  rules  (“manual  feature  engi‐\n",
      "neering” in modern ML jargon) and logistic regression. Expert knowledge is vital to\n",
      "creating a good model. Building adapted customer segmentation as well as studying\n",
      "the  influence  of  each  variable  and  the  interactions  between  variables  requires  enor‐\n",
      "mous  time  and  effort.  Combined  with  advanced  techniques  like  two-stage  models\n",
      "with offset, advanced general linear models based on Tweedie distribution, or monot‐\n",
      "onicity  constraints  on  one  side  and  financial  risk  management  techniques  on  the\n",
      "other side, this makes the field a playground for actuaries.\n",
      "\n",
      "Gradient boosting algorithms like XGBoost have reduced the cost to build good mod‐\n",
      "els. However, their validation is made more complex by the black box effect: it’s hard\n",
      "to get the feeling that such models give sensible results whatever the inputs. Never‐\n",
      "theless, credit risk modelers have learned to use and validate these new types of mod‐\n",
      "els.  They  have  developed  new  validation  methodologies  based,  for  example,  on\n",
      "individual explanations (e.g., Shapley values) to build trust into their models, which is\n",
      "a critical component of MLOps, as we’ve explored throughout this book.\n",
      "\n",
      "130 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: MLOps in Practice: Consumer Credit Risk Management\n",
      "\n",
      "\f",
      "Model Bias Considerations\n",
      "The modeler also has to take into account selection biases, as the model will inevita‐\n",
      "bly be used to reject applicants. As a result, the population to which a loan is granted\n",
      "is not representative of the applicant population.\n",
      "\n",
      "By training a model version on the population selected by the previous model version\n",
      "without care, the data scientist would make a model unable to accurately predict on\n",
      "the rejected population because it is not represented in the training dataset, while it is\n",
      "exactly  what  is  expected  from  the  model.  This  effect  is  called  cherry-picking.  As  a\n",
      "result,  special  methods,  like  reweighting  based  on  the  applicant  population  or  cali‐\n",
      "brating the model based on external data, have to be used.\n",
      "\n",
      "Models that are used for risk assessment and not only to make decisions about grant‐\n",
      "ing  loans  have  to  produce  probabilities  and  not  only  yes/no  outcomes.  Usually,  the\n",
      "probability produced directly by prediction models is not accurate. While it is not an\n",
      "issue  if  data  scientists  apply  thresholding  to  obtain  a  binary  classification,  they  will\n",
      "usually  need  a  monotonous  transformation  called  a  calibration  to  recover  “true”\n",
      "probabilities as evaluated on historical data.\n",
      "\n",
      "The model validation for this use case typically consists of:\n",
      "\n",
      "• Testing  its  performance  on  out-of-sample  datasets,  chosen  after  (or,  in  some\n",
      "\n",
      "cases, before, as well) the training datasets.\n",
      "\n",
      "• Investigating the performance not only overall, but also per subpopulation. The\n",
      "subpopulations  would  typically  have  customer  segments  based  on  revenue,  and\n",
      "with  the  rise  of  Responsible  AI,  other  segmenting  variables  like  gender  or  any\n",
      "protected attribute according to local regulation. Risks of not doing so can result\n",
      "in serious damages, as Apple learned the hard way in 2019 when its credit card\n",
      "was said to be “sexist” against women applying for credit.\n",
      "\n",
      "Prepare for Production\n",
      "Given  the  significant  impact  of  credit  risk  models,  their  validation  process  involves\n",
      "significant work with regard to the modeling part of the life cycle, and it includes the\n",
      "full documentation of:\n",
      "\n",
      "• The data used\n",
      "\n",
      "• The model and the hypothesis made to build it\n",
      "\n",
      "• The validation methodology and the validation results\n",
      "\n",
      "• The monitoring methodology\n",
      "\n",
      "Model Bias Considerations \n",
      "\n",
      "| \n",
      "\n",
      "131\n",
      "\n",
      "\f",
      "The monitoring methodology in this scenario is twofold: data and performance drift.\n",
      "As the delay between the prediction and obtaining the ground truth is long (typically\n",
      "the duration of the loan plus a few months to take into account late payments), it is\n",
      "not enough to monitor the model performance: data drift also has to be monitored\n",
      "carefully.\n",
      "\n",
      "For  example,  should  an  economic  recession  occur  or  should  the  commercial  policy\n",
      "change, it is likely that the applicant population would change in such a way that the\n",
      "model’s performance could not be guaranteed without further validation. Data drift is \n",
      "usually performed by customer segment with generic statistical metrics that measure\n",
      "distances  between  probability  distributions  (like  Kolmogorov-Smirnov  or  Wasser‐\n",
      "stein distances) and also with metrics that are specific to financial services, like popu‐\n",
      "lation  stability  index  and  characteristic  stability  index.  Performance  drift  is  also\n",
      "regularly assessed on subpopulations with generic metrics (AUC) or specific metrics\n",
      "(Kolmogorov-Smirnov, Gini).\n",
      "\n",
      "The model documentation is usually reviewed by an MRM team in a very formal and\n",
      "standalone process. Such an independent review is a good practice to make sure that\n",
      "the right questions are asked of the model development team. In some critical cases,\n",
      "the validation team may rebuild the model from scratch given the documentation. In\n",
      "some  cases,  the  second  implementation  is  made  using  an  alternative  technology  to\n",
      "establish  confidence  in  documented  understanding  of  the  model  and  to  highlight\n",
      "unseen bugs deriving from the original toolset.\n",
      "\n",
      "Complex and time-consuming model validation processes have an implication on the\n",
      "entire MLOps life cycle. Quick-fixes and rapid model iteration are not possible with\n",
      "such lengthy QA and lead to a very slow and deliberate MLOps life cycle.\n",
      "\n",
      "Deploy to Production\n",
      "In a typical large financial services organization, the production environment is not\n",
      "only separate from the design environment, but also likely to be based on a different\n",
      "technical  stack.  The  technical  stack  for  critical  operations—like  transaction  valida‐\n",
      "tion, but also potentially loan validation—will always evolve slowly.\n",
      "\n",
      "Historically,  the  production  environments  have  mainly  supported  rules  and  linear\n",
      "models  like  logistic  regression.  Some  can  handle  more  complex  models  such  as\n",
      "PMML  or  JAR  file.  For  less  critical  use  cases,  Docker  deployment  or  deployment\n",
      "through integrated data science and machine learning platforms may be possible. As\n",
      "a result, the operationalization of the model may involve operations that range from\n",
      "clicking on a button to writing a formula based on a Microsoft Word document.\n",
      "\n",
      "Activity  logging  of  the  deployed  model  is  essential  for  monitoring  model  perfor‐\n",
      "mance in such a high-value use case. Depending on the frequency of the monitoring,\n",
      "the  feedback  loop  may  be  automated  or  not.  For  example,  automation  may  not  be\n",
      "\n",
      "132 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: MLOps in Practice: Consumer Credit Risk Management\n",
      "\n",
      "\f",
      "necessary if the task is performed only once or twice a year and the largest amount of\n",
      "time is spent asking questions of the data. On the other hand, automation might be\n",
      "essential if the assessment is done weekly, which may be the case for short-term loans\n",
      "with durations of a few months.\n",
      "\n",
      "Closing Thoughts\n",
      "Financial services have been developing schemes for prediction model validation and\n",
      "monitoring for decades. They have been able to continuously adapt to new modeling\n",
      "technologies like gradient boosting methods. Given their important impact, the pro‐\n",
      "cesses around the life cycle management of these models are well formalized and even\n",
      "incorporated into many regulations. As a result, they can be a source of best practices\n",
      "for MLOps in other domains, though adaptations are needed as the trade-off between\n",
      "robustness  on  one  side  and  cost  efficiency,  time  to  value,  and—importantly—team\n",
      "frustration on the other may be different in other businesses.\n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "133\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 10\n",
      "MLOps in Practice: Marketing\n",
      "Recommendation Engines\n",
      "\n",
      "Makoto Miyazaki\n",
      "\n",
      "Recommendation  engines  have  become  very  popular  in  the  last  20  years,  from  the\n",
      "first  Amazon  book  recommendations  to  today’s  generalized  use  in  digital  shops,\n",
      "advertisements, and music and video streaming. We have all become accustomed to\n",
      "them. However, throughout the years, the underlying technologies behind these rec‐\n",
      "ommendation engines have evolved.\n",
      "\n",
      "This  chapter  covers  a  use  case  that  illustrates  the  adaption  of  and  need  for  MLOps\n",
      "strategies  given  the  particularities  of  a  fast-paced  and  rapidly  changing  machine\n",
      "learning model life cycle.\n",
      "\n",
      "The Rise of Recommendation Engines\n",
      "Historically,  marketing  recommendations  were  human-built.  Based  on  qualitative\n",
      "and quantitative marketing studies, marketing moguls would set up rules that stati‐\n",
      "cally  defined  the  impression  (in  the  sense  of  advertising  views)  sent  to  a  customer\n",
      "with  given  characteristics.  This  technique  gave  rise  to  the  marketing  data  mining\n",
      "urban legend that a grocery chain discovered that men who bought diapers on Thurs‐\n",
      "days  and  Saturdays  were  more  likely  to  buy  beer  as  well  and  hence  placing  the  two\n",
      "next to each other will increase beer sales.\n",
      "\n",
      "Overall, recommendation engines created manually presented numerous bottlenecks\n",
      "that  resulted  in  a  significant  amount  of  wasted  money:  it  was  hard  to  build  rules\n",
      "based  on  many  different  customer  features  because  the  rule  creation  process  was\n",
      "manual, it was hard to set up experiments to test many different kinds of impressions,\n",
      "and it was hard to update the rules when the behavior of the customers changed.\n",
      "\n",
      "135\n",
      "\n",
      "\f",
      "The Role of Machine Learning\n",
      "The  rise  of  ML  has  brought  a  new  paradigm  to  recommendation  engines,  allowing\n",
      "for the elimination of rules based on human insight. A new class of algorithm called\n",
      "collaborative filtering dominates the field. This algorithm is able to analyze customer\n",
      "and purchase data with millions of customers and tens of thousands of products to\n",
      "perform  recommendations  without  any  prior  marketing  knowledge.  By  identifying\n",
      "efficiently what customers that look like the current customer bought, marketers can\n",
      "rely  on  automatic  strategies  that  outperform  traditional  ones  both  in  cost  and\n",
      "efficiency.\n",
      "\n",
      "Because the process of building strategies is automatic, it is possible to update them\n",
      "regularly and to compare them using A/B testing or shadow scoring schemes (includ‐\n",
      "ing  the  way  to  choose  the  impression  among  all  possibilities).  Note  that  these  algo‐\n",
      "rithms may be combined with more classical business rules for various reasons—e.g.,\n",
      "avoiding  the  filtering  bubble,  not  selling  a  product  in  a  given  geographical  area,  or\n",
      "preventing the use of a specific association that is statistically meaningful but unethi‐\n",
      "cal to use (like proposing alcohol to a recovering alcoholic), to name a few.\n",
      "\n",
      "Push or Pull?\n",
      "When implementing a recommendation engine, it is important to keep in mind that\n",
      "its  structure  will  depend  on  whether  the  recommendations  are  pushed  or  pulled. \n",
      "Push  channels  are  the  easiest  to  handle;  for  example,  they  can  consist  of  sending\n",
      "emails or making outbound calls. \n",
      "\n",
      "The recommendation engine can be run on a regular basis in batch mode (typically\n",
      "between  once  a  day  and  once  a  month),  and  it  is  easy  to  split  the  customer  dataset\n",
      "into several parts to perform analysis within a sound experimental design. The regu‐\n",
      "larity of the process allows for regular review and optimization of the strategy.\n",
      "\n",
      "Pull channels are often more effective because they provide information to customers\n",
      "when they need it—for example, when doing an online search or when calling a cus‐\n",
      "tomer service line. Specific information from the session can be used (e.g., what the\n",
      "user has searched for) to precisely tailor the recommendation. Music streaming plat‐\n",
      "forms, for instance, provide pull-channel recommendations for playlists.\n",
      "\n",
      "Recommendations  can  be  prebaked,  as  illustrated  in  the  in-depth  example  in  this\n",
      "chapter, or made in real time. In the latter case, a special architecture has to be set up\n",
      "to compute recommendations on the fly.\n",
      "\n",
      "Comparing strategies in a pull context is more challenging. First, the customers who\n",
      "call  in  on  a  given  channel  are  likely  to  differ  from  the  average  customer.  In  simple\n",
      "cases, it is possible to randomly choose the strategy to use for each recommendation,\n",
      "but it also happens that the strategy needs to be used consistently over a given period\n",
      "for  a  given  customer.  This  usually  results  in  an  unbalanced  proportion  of\n",
      "\n",
      "136 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: MLOps in Practice: Marketing Recommendation Engines\n",
      "\n",
      "\f",
      "recommendations made with each strategy, which makes the statistical treatment to\n",
      "decide which one is the best more complex. However, once a good framework is set,\n",
      "this allows a very quick improvement cycle, as many strategies can be tested in real\n",
      "time.\n",
      "\n",
      "Data Preparation\n",
      "The customer data that is usually accessible to a recommendation engine is composed\n",
      "of the following:\n",
      "\n",
      "• Structural information about the customer (e.g., age, gender, location)\n",
      "\n",
      "• Information about historical activities (e.g., past views, purchases, searches)\n",
      "\n",
      "• Current context (e.g., current search, viewed product)\n",
      "\n",
      "Whatever  the  technique  used,  all  customer  information  has  to  be  aggregated  into  a\n",
      "vector (a list of fixed size) of characteristics. For example, from the historical activi‐\n",
      "ties, the following characteristics could be extracted:\n",
      "\n",
      "• Amount of purchases during the last week or the last month\n",
      "\n",
      "• Number of views during past periods\n",
      "\n",
      "• Increase in spending or in views during the last month\n",
      "\n",
      "• Previously seen impressions and customer’s response\n",
      "\n",
      "In  addition  to  customer  data,  the  recommendation  context  can  also  be  taken  into\n",
      "account.  For  example,  days  to  summer  for  seasonal  products  like  above-ground\n",
      "swimming pools or days to monthly pay day, as some customers delay purchases for\n",
      "cash flow reasons.\n",
      "\n",
      "Once the customer and context data is formatted, it is important to define the set of\n",
      "possible recommendations, and there are many choices to make. The same product\n",
      "may  be  presented  with  different  offers,  which  may  be  communicated  in  different\n",
      "ways.\n",
      "\n",
      "It is of the utmost importance not to forget the “do not recommend anything” option.\n",
      "Indeed, most of us have the personal experience that not all recommendations have a\n",
      "positive  impact.  Sometimes  not  showing  anything  might  be  better  than  the  alterna‐\n",
      "tives. It’s also important to consider that some customers may not be entitled to see\n",
      "certain recommendations, for example depending on their geographical origin.\n",
      "\n",
      "Data Preparation \n",
      "\n",
      "| \n",
      "\n",
      "137\n",
      "\n",
      "\f",
      "Design and Manage Experiments\n",
      "To leverage the continuous improvement potential of recommendation engines, it is\n",
      "necessary  to  experiment  with  different  strategies  within  a  sound  framework.  When\n",
      "designing a prediction model for a recommendation engine, the data scientist might\n",
      "well  focus  on  a  simple  strategy,  such  as  predicting  the  probability  that  a  given  cus‐\n",
      "tomer clicks on a given recommendation.\n",
      "\n",
      "This may seem a reasonable compromise compared to the more precise approach of\n",
      "trying to gather information about whether the customer purchased the product and\n",
      "whether to attribute this purchase to a given recommendation. However, this is not\n",
      "adequate from a business perspective, as phenomena like cannibalization may occur\n",
      "(i.e., by showing a low-margin product to a customer, one might prevent them from\n",
      "buying  a  high-margin  product).  As  a  result,  even  if  the  predictions  were  good  and\n",
      "resulted in increased sales volume, the overall revenue might be reduced.\n",
      "\n",
      "On the other hand, bluntly promoting the organization’s interest and not the custom‐\n",
      "er’s could also have detrimental long-term consequences. The overarching KPI that is\n",
      "used  to  assess  if  a  given  strategy  yields  better  results  should  be  carefully  chosen,\n",
      "together with the time period over which it is evaluated. Choosing the revenue over a\n",
      "two-week period after the recommendation as the main KPI is common practice.\n",
      "\n",
      "To be as close as possible to an experimental setting, also called A/B testing, the con‐\n",
      "trol  group  and  the  experimental  groups  have  to  be  carefully  chosen.  Ideally,  the\n",
      "groups are defined before the start of the experiment by randomly splitting the cus‐\n",
      "tomer base. If possible, customers should not have been involved in another experi‐\n",
      "mentation recently so that their historical data is not polluted by its impact. However,\n",
      "this may not be possible in a pull setting in which many new customers are coming\n",
      "in. In this case, the assignment could be decided on the fly. The size of the groups as\n",
      "well as the duration of the experiments depend on the expected magnitude of the KPI\n",
      "difference  between  the  two  groups:  the  larger  the  expected  effect,  the  smaller  the\n",
      "group size and the shorter the duration.\n",
      "\n",
      "The experimentation could also be done in two steps: in the first one, two groups of\n",
      "equal but limited size could be selected. If the experimentation is positive, the deploy‐\n",
      "ment could start with 10% on the new strategy, a proportion that can be raised pro‐\n",
      "gressively if results are in line with expectations.\n",
      "\n",
      "Model Training and Deployment\n",
      "To better illustrate the MLOps process for this type of use case, the following sections\n",
      "focus  on  the  specific  example  of  a  hypothetical  company  deploying  an  automated\n",
      "pipeline to train and deploy recommendation engines. The company is a global soft‐\n",
      "ware company (let’s call it MarketCloud) headquartered in Silicon Valley.\n",
      "\n",
      "138 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: MLOps in Practice: Marketing Recommendation Engines\n",
      "\n",
      "\f",
      "One  of  MarketCloud’s  products  is  a  software-as-a-service  (SaaS)  platform  called\n",
      "SalesCore. SalesCore is a B2B product that allows its users (businesses) to drive sales\n",
      "to  customers  in  a  simple  manner  by  keeping  track  of  deals,  clearing  tedious\n",
      "administration tasks off their desks, and creating customized product offers for each\n",
      "customer (see Figure 10-1).\n",
      "\n",
      "From time to time, MarketCloud’s clients use the cloud-based SalesCore while on a\n",
      "call with their customers, adjusting their sales strategy by looking at past interactions\n",
      "with  the  customers  as  well  as  at  the  product  offers  and  discounts  suggested  by\n",
      "SalesCore.\n",
      "\n",
      "MarketCloud is a mid-sized company with an annual revenue of around $200 million\n",
      "and a few thousand employees. From salespeople at a brewing company to those in a\n",
      "telecommunication entity, MarketCloud’s clients represent a range of businesses.\n",
      "\n",
      "Figure 10-1. Mock-up of the SalesCore platform, the basis of the theoretical company on\n",
      "which this section’s example is based\n",
      "\n",
      "MarketCloud would like to automatically display product suggestions on SalesCore to\n",
      "the salespeople trying to sell products to the customers. Suggestions would be made\n",
      "based on customers’ information and their past interaction records with the salesper‐\n",
      "son;  suggestions  would  therefore  be  customized  for  each  customer.  In  other  words,\n",
      "SalesCore  is  based  on  a  recommendation  engine  used  in  a  pull  (inbound  calls)  or\n",
      "push (outbound calls) context. Salespeople would be able to incorporate in their sales\n",
      "strategy the suggested products while on a call with their customers.\n",
      "\n",
      "To implement this idea, MarketCloud needs to build a recommendation engine and\n",
      "embed  it  into  SalesCore’s  platform,  which,  from  a  model  training  and  deployment\n",
      "standpoint, presents several challenges. We’ll present these challenges in this section,\n",
      "\n",
      "Model Training and Deployment \n",
      "\n",
      "| \n",
      "\n",
      "139\n",
      "\n",
      "\f",
      "and in the next section we’ll show MLOps strategies that allow the company to handle\n",
      "each of them.\n",
      "\n",
      "Scalability and Customizability\n",
      "MarketCloud’s business model (selling software for client companies to help them sell\n",
      "their  own  products)  presents  an  interesting  situation.  Each  client  company  has  its\n",
      "own  dataset,  mainly  about  its  products  and  customers,  and  it  doesn’t  wish  to  share\n",
      "the data with other companies.\n",
      "\n",
      "If MarketCloud has around four thousand clients using SalesCore, that means instead\n",
      "of having a universal recommender system for all the clients, it would need to create\n",
      "four thousand different systems. MarketCloud needs to come up with a way to build\n",
      "four  thousand  recommendation  systems  as  efficiently  as  possible  since  there  is  no\n",
      "way it can handcraft that many systems one by one.\n",
      "\n",
      "Monitoring and Retraining Strategy\n",
      "Each  of  the  four  thousand  recommendation  engines  would  be  trained  on  the  cus‐\n",
      "tomer data of the corresponding client. Therefore, each of them would be a different\n",
      "model,  yielding  a  different  performance  result  and  making  it  nearly  impossible  for\n",
      "the company to manually keep an eye on all four thousand. For example, the recom‐\n",
      "mendation engine for client A in the beverage industry might consistently give good\n",
      "product  suggestions,  while  the  engine  for  client  B  in  the  telecommunication  sector\n",
      "might seldom provide good suggestions. MarketCloud needed to come up with a way\n",
      "to automate the monitoring and the subsequent model retraining strategy in case the\n",
      "performance degraded.\n",
      "\n",
      "Real-Time Scoring\n",
      "In  many  situations,  MarketCloud’s  clients  use  SalesCore  when  they  are  talking  to\n",
      "their customers on the phone. The sales negotiation evolves every single minute dur‐\n",
      "ing  the  call,  and  the  salesperson  needs  to  adjust  the  strategy  during  the  interaction\n",
      "with the customer, so the recommendation engine has to be responsive to real-time\n",
      "requests.\n",
      "\n",
      "For  example,  imagine  you  as  a  salesperson  are  on  a  call  with  your  customer  to  sell\n",
      "telecommunication  devices.  The  customer  tells  you  what  his  office  looks  like,  the\n",
      "existing infrastructure at the office such as an optic fiber, the type of WiFi network,\n",
      "and so forth. Upon entering this information in SalesCore, you want the platform to\n",
      "give  you  a  suggestion  for  the  products  that  your  customer  could  feasibly  purchase.\n",
      "This response from the platform needs to be in real time, not 10 minutes later, after\n",
      "the call, or on the following day.\n",
      "\n",
      "140 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: MLOps in Practice: Marketing Recommendation Engines\n",
      "\n",
      "\f",
      "Ability to Turn Recommendations On and Off\n",
      "Responsible AI principles acknowledge that retaining human involvement is impor‐\n",
      "tant. This can be done through a human-in-command design,1 by which it should be\n",
      "possible  not  to  use  the  AI.  In  addition,  adoption  is  likely  to  be  low  if  users  cannot\n",
      "override  AI  recommendations.  Some  clients  value  using  their  own  intuition  about\n",
      "which  products  to  recommend  to  their  customers.  For  this  reason,  MarketCloud\n",
      "wants to give its clients full control to turn the recommendation engine on and off so\n",
      "that the clients can use the recommendations when they want.\n",
      "\n",
      "Pipeline Structure and Deployment Strategy\n",
      "To efficiently build four thousand recommendation engines, MarketCloud decided to\n",
      "make one data pipeline as a prototype and duplicate it four thousand times. This pro‐\n",
      "totype pipeline consists of the necessary data preprocessing steps and a single recom‐\n",
      "mendation  engine,  built  on  an  example  dataset.  The  algorithms  used  in  the\n",
      "recommendation engines will be the same across all four thousand pipelines, but they\n",
      "will be trained with the specific data associated with each client (see Figure 10-2).\n",
      "\n",
      "Figure 10-2. Image of data pipeline structure for MarketCloud’s recommendation engine\n",
      "project\n",
      "\n",
      "In this way, MarketCloud can efficiently launch four thousand recommendation sys‐\n",
      "tems. The users will still retain some room for customization, because the engine is\n",
      "trained with their own data, and each algorithm will work with different parameters\n",
      "—i.e., it’s adopted to the customer and product information of each client.\n",
      "\n",
      "What makes it possible to scale up a single pipeline to four thousand pipelines is the\n",
      "universal schema of the dataset. If a dataset from client A has 100 columns whereas\n",
      "\n",
      "1 For an explanation of human-in-command design, see Karen Yeung, “Responsibility and AI” (Council of\n",
      "\n",
      "Europe study, DGI(2019)05), 64, footnote 229.\n",
      "\n",
      "Pipeline Structure and Deployment Strategy \n",
      "\n",
      "| \n",
      "\n",
      "141\n",
      "\n",
      "\f",
      "client  B  has  50,  or  if  the  column  “number  of  purchased  items”  from  client  A  is  an\n",
      "integer  whereas  the  same  column  from  client  B  is  a  string,  they  would  need  to  go\n",
      "through different preprocessing pipelines.\n",
      "\n",
      "Although each client has different customer and product data, at the point that this\n",
      "data is registered on SalesCore, it acquires the same number of columns and the same\n",
      "data types for each column. This makes things easier, as MarketCloud simply needs\n",
      "to copy a single pipeline four thousand times.\n",
      "\n",
      "Each recommendation system embedded in the four thousand pipelines will have dif‐\n",
      "ferent API endpoints. On the surface, it looks like when a user clicks the “show prod‐\n",
      "uct recommendations” button, SalesCore displays a list of suggested products. But in\n",
      "the background, what is happening is that by clicking the button, the user is hitting\n",
      "the  specific  API  endpoint  associated  with  the  ranked  product  lists  for  the  specific\n",
      "customer.\n",
      "\n",
      "Monitoring and Feedback\n",
      "Maintaining  four  thousand  recommendation  systems  is  not  an  easy  task,  and  while\n",
      "there  have  already  been  many  MLOps  considerations  until  this  point,  this  is  maybe\n",
      "the most complex part. Each system’s performance needs to be monitored and upda‐\n",
      "ted as needed. To implement this monitoring strategy at a large scale, MarketCloud\n",
      "can automate the scenario for retraining and updating the models.\n",
      "\n",
      "Retraining Models\n",
      "Clients obtain  new customers, some of the customers churn, every once in a while\n",
      "new  products  are  added  to  or  dropped  from  their  catalogs;  the  bottom  line  is  that\n",
      "customer  and  product  data  are  constantly  changing,  and  recommendation  systems\n",
      "have  to  reflect  the  latest  data.  It’s  the  only  way  they  can  maintain  the  quality  of  the\n",
      "recommendation, and, more importantly, avoid a situation such as recommending a\n",
      "WiFi router that is outdated and no longer supported.\n",
      "\n",
      "To reflect the latest data, the team could program a scenario to automatically update\n",
      "the database with the newest customer and product data, retraining the model with\n",
      "the  latest  datasets  every  day  at  midnight.  This  automation  scenario  could  then  be\n",
      "implemented in all four thousand data pipelines.\n",
      "\n",
      "The  retraining  frequency  can  differ  depending  on  the  use  case.  Thanks  to  the  high\n",
      "degree of automation, retraining every night in this case is possible. In other contexts,\n",
      "retraining  could  be  triggered  by  various  signals  (e.g.,  signification  volume  of  new\n",
      "information or drift in customer behavior, be it aperiodic or seasonal).\n",
      "\n",
      "In addition, the delay between the recommendation and the point in time at which its\n",
      "effect  is  evaluated  has  to  be  taken  into  account.  If  the  impact  is  only  known  with  a\n",
      "\n",
      "142 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: MLOps in Practice: Marketing Recommendation Engines\n",
      "\n",
      "\f",
      "delay of several months, it is unlikely that retraining every day is adequate. Indeed, if\n",
      "the behavior changes so fast that retraining it every day is needed, it is likely that the\n",
      "model  is  completely  outdated  when  it  is  used  to  make  recommendations  several\n",
      "months after the most recent ones in the training data.\n",
      "\n",
      "Updating Models\n",
      "Updating models is also one of the key features of automation strategies at scale. In\n",
      "this case, for each of the four thousand pipelines, retrained models must be compared\n",
      "to  the  existing  models.  Their  performances  can  be  compared  using  metrics  such  as\n",
      "RMSE  (root-mean-square  error),  and  only  when  the  performance  of  the  retrained\n",
      "model beats the prior one does the retrained model get deployed to SalesCore.\n",
      "\n",
      "Runs Overnight, Sleeps During Daytime\n",
      "Although  the  model  is  retrained  every  day,  users  do  not  interact  directly  with  the\n",
      "model. Using the updated model, the platform actually finishes calculating the ranked\n",
      "list of products for all the customers during the night. On the following day, when a\n",
      "user hits the “show product recommendations” button, the platform simply looks at\n",
      "the customer ID and returns the ranked list of products for the specific customer.\n",
      "\n",
      "To the user, it looks as if the recommendation engine is running in real time. In real‐\n",
      "ity, however, everything is already prepared overnight, and the engine is sleeping dur‐\n",
      "ing daytime. This makes it possible to get the recommendation instantly without any\n",
      "downtime.\n",
      "\n",
      "Option to Manually Control Models\n",
      "Although the monitoring, retraining, and updating of the models is fully automated,\n",
      "MarketCloud still leaves room for its clients to turn the models on and off. More pre‐\n",
      "cisely, MarketCloud allows the users to choose from three options to interact with the\n",
      "models:\n",
      "\n",
      "• Turn on to get the recommendation based on the most updated dataset\n",
      "\n",
      "• Freeze to stop retraining with the new data, but keep using the same model\n",
      "\n",
      "• Turn off to completely stop using the recommendation functionality of SalesCore\n",
      "\n",
      "Machine learning algorithms attempt to convert practical knowledge into meaningful\n",
      "algorithms  to  automate  processing  tasks.  However,  it  is  still  good  practice  to  leave\n",
      "room  for  users  to  rely  on  their  domain  knowledge,  as  they  are  presumed  to  be  far\n",
      "more  capable  of  identifying,  articulating,  and  demonstrating  day-to-day  process\n",
      "problems in business.\n",
      "\n",
      "Monitoring and Feedback \n",
      "\n",
      "| \n",
      "\n",
      "143\n",
      "\n",
      "\f",
      "The second option is important because it allows users to stay in the current quality\n",
      "of  the  recommendation  without  having  the  recommendation  engines  updated  with\n",
      "the newer data. Whether the current model is replaced with a retrained one depends\n",
      "on the mathematical evaluation based on metrics such as the RMSE. However, if users\n",
      "feel  that  the  product  recommendations  on  SalesCore  are  already  working  well  for\n",
      "pushing sales, they have the choice not to risk changing the recommendation quality.\n",
      "\n",
      "Option to Automatically Control Models\n",
      "For those that don’t want to manually handle the models, the platform could also pro‐\n",
      "pose A/B testing so that the impact of new versions is tested before fully switching to\n",
      "them. Multi-armed bandit algorithms (an algorithm that allows for maximization of\n",
      "the revenue of a user facing multiple slot machines, each with a different probability\n",
      "to win and a different proportion of the money given back on average) are used for\n",
      "this purpose.\n",
      "\n",
      "Let’s assume that several model versions are available. The goal is to use the most effi‐\n",
      "cient one, but to do that, the algorithm obviously has to first learn which is the most\n",
      "efficient.  Therefore,  it  balances  these  two  objectives:  sometimes,  it  tries  algorithms\n",
      "that  may  not  be  the  most  efficient  to  learn  if  they  are  efficient  (exploration),  and\n",
      "sometimes it uses the version that is likely to be the most efficient to maximize the\n",
      "revenue  (exploitation).  In  addition,  it  forgets  past  information,  as  the  algorithm\n",
      "knows the most efficient today may not be the most efficient tomorrow.\n",
      "\n",
      "The  most  advanced  option  consists  in  training  different  models  for  different  KPIs\n",
      "(click, buy, expected revenue, etc.). A method inspired from ensemble models would\n",
      "then allow for the solving of conflicts between models.\n",
      "\n",
      "Monitoring Performance\n",
      "When  a  salesperson  suggests  a  customer  buy  the  products  recommended  by  Sales‐\n",
      "Core,  the  interaction  of  the  customer  with  the  recommended  products  as  well  as\n",
      "whether the customer bought them or not is recorded. This record can then be used\n",
      "to  keep  track  of  the  performance  of  the  recommender  system,  overwriting  the  cus‐\n",
      "tomer and product dataset with this record to feed the most updated information to\n",
      "the model when it is retrained.\n",
      "\n",
      "Thanks  to  this  ground  truth  recording  process,  dashboards  showing  model  perfor‐\n",
      "mance  can  be  presented  to  the  user,  including  performance  comparison  from  A/B\n",
      "testing. Because the ground truth is obtained quickly, data drift monitoring is secon‐\n",
      "dary. A version of the model is trained every night, but, thanks to the freeze mecha‐\n",
      "nism, the user can choose the active version based on the quantitative information. It\n",
      "is customary to keep the human in the loop on these high-impact decisions where the\n",
      "performance metrics have a hard time capturing the full context around the decision.\n",
      "\n",
      "144 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: MLOps in Practice: Marketing Recommendation Engines\n",
      "\n",
      "\f",
      "In the case of A/B testing, it is important that only one experiment be done at a time\n",
      "on a group of customers; the impact of combined strategies cannot be simply added.\n",
      "With such considerations in mind, it is possible to build a sound baseline to perform\n",
      "a  counterfactual  analysis  and  derive  the  increased  revenue  and/or  the  decreased\n",
      "churn linked to a new strategy.\n",
      "\n",
      "Apart  from  this,  MarketCloud  can  also  monitor  the  algorithm  performance  at  a\n",
      "macro level, by checking how many clients froze or turned off the recommender sys‐\n",
      "tems.  If  many  clients  turned  off  the  recommender  systems,  that’s  a  strong  indicator\n",
      "that they are not satisfied with the recommendation quality.\n",
      "\n",
      "Closing Thoughts\n",
      "This  use  case  is  peculiar  in  the  sense  that  MarketCloud  built  a  sales  platform  that\n",
      "many other companies use to sell products, where the ownership of the data belongs\n",
      "to each company, and the data cannot be shared across companies. This brings a chal‐\n",
      "lenging situation where MarketCloud must create different recommender systems for\n",
      "each of the users instead of pooling all the data to create a universal recommendation\n",
      "engine.\n",
      "\n",
      "MarketCloud can overcome this obstacle by creating a single pipeline into which data\n",
      "from many different companies can be fed. By having the data go through an automa‐\n",
      "ted  recommendation  engine  training  scenario,  MarketCloud  created  many  recom‐\n",
      "mendation  engines  trained  on  different  datasets.  Good  MLOps  processes  are  what\n",
      "allow the company to do this at scale.\n",
      "\n",
      "It’s worth noting that though this use case is fictionalized, it is based on reality. The\n",
      "real-life team that tackled a similar project took around three months to finish. The\n",
      "team used a data science and machine learning platform to orchestrate the duplica‐\n",
      "tion of a single pipeline to four thousand copies and to automate the processes to feed\n",
      "corresponding  datasets  to  each  pipeline  and  train  the  models.  Of  necessity,  they\n",
      "accepted trade-offs between the recommendation quality and scalability to efficiently\n",
      "launch  the  product.  If  the  team  had  carefully  crafted  a  custom  recommendation\n",
      "engine  for  each  of  the  four  thousand  pipelines  by,  for  example,  choosing  the  best\n",
      "algorithm for each client, the recommendation engines would have been of a higher\n",
      "quality, but they would have never been able to complete the project with such a small\n",
      "team in such a short period of time.\n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "145\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 11\n",
      "MLOps in Practice: Consumption Forecast\n",
      "\n",
      "Nicolas Omont\n",
      "\n",
      "Predictions  at  various  times  and  geographical  scales  play  an  important  role  in  the\n",
      "operation of a power grid. They allow for simulation of possible future states of the\n",
      "system and for making sure that it can safely operate. This chapter will walk through\n",
      "a machine learning model life cycle and MLOps use case for consumption forecast‐\n",
      "ing,  including  business  considerations,  data  collection,  and  implementation  deci‐\n",
      "sions.  Though  this  particular  chapter  is  focused  on  power  grids,  the  considerations\n",
      "and particularities of the use case can be generalized to other industrial cases that use\n",
      "consumption forecasting.\n",
      "\n",
      "Power Systems\n",
      "Bulk  power  systems  are  the  backbone  of  power  grids.  Also  called  transmission  net‐\n",
      "works, they form the core of the system that keeps the lights on. These systems are\n",
      "mainly  composed  of  lines  and  transformers,  which  are  indirectly  connected  with\n",
      "most  producers  and  consumers  through  distribution  networks  that  take  care  of  the\n",
      "last few kilometers of transmission. As illustrated in Figure 11-1, only the largest pro‐\n",
      "ducers and consumers are directly connected to the bulk system.\n",
      "\n",
      "147\n",
      "\n",
      "\f",
      "Figure 11-1. A sample bulk power system, to which only the largest producers and con‐\n",
      "sumers are directly connected\n",
      "\n",
      "The longer the transmission distance and the larger the energy volume to be trans‐\n",
      "mitted, the higher the voltage used: on the lower end, a few tens of kilovolts for a few\n",
      "tens of megawatts over a few tens of kilometers; on the upper end, one million volts\n",
      "for a few thousand megawatts over a few thousand kilometers. (A line with a capacity\n",
      "of one megawatt can be used to provide power to around one thousand inhabitants in\n",
      "Europe.) The operation of transmission systems has always required a lot of commu‐\n",
      "nications and computations because of its properties:\n",
      "\n",
      "No energy storage\n",
      "\n",
      "The  network  stores  a  meaningless  amount  of  energy—less  than  one  second  of\n",
      "consumption in the grid and up to 30 seconds in the alternators and motors. By\n",
      "way  of  contrast,  a  gas  network  stores  several  hours  of  consumption  in  its  pipe‐\n",
      "line. Therefore, actions have to be taken very quickly to balance production and\n",
      "consumption and avoid blackouts.\n",
      "\n",
      "Weak flow control\n",
      "\n",
      "On  telecommunication  networks,  congestions  are  handled  by  dropping  packets\n",
      "or by not establishing a call. There is no equivalent mechanism in power grids,\n",
      "which means the power flow on a grid element can be higher than its operating\n",
      "limit.  Actions  have  to  be  taken  after  a  few  seconds  to  a  few  hours  of  overload\n",
      "depending on the technology and the severity. Flow control technologies do exist,\n",
      "\n",
      "148 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: MLOps in Practice: Consumption Forecast\n",
      "\n",
      "\f",
      "but  there  is  a  trade-off  between  flow  control  and  instantaneous  balance:  the\n",
      "power has to find a path from generation to consumption.\n",
      "\n",
      "Because of these two properties, the grid operator always has to anticipate the contin‐\n",
      "gencies: if this grid element fails, will the overload on the remaining elements remain\n",
      "acceptable? The anticipation is done on several timescales, from the next five minutes\n",
      "to the next five decades. The actions to be taken depend on the horizon. For example:\n",
      "\n",
      "• Below  five  minutes:  no  human  action  is  possible.  Automatic  actions  should\n",
      "\n",
      "already be well defined.\n",
      "\n",
      "• Between five minutes and a few hours ahead: production schedule and grid top‐\n",
      "\n",
      "ology adaptation (opening of breakers and other flow control technologies).\n",
      "\n",
      "• A few days ahead: maintenance schedule adaptations.\n",
      "\n",
      "• A few seasons ahead: maintenance schedule adaptations, contracts with produc‐\n",
      "ers  or  consumers  to  guarantee  power  capacity  or  limit  power  generation  or\n",
      "consumption.\n",
      "\n",
      "• From  5  to  50  years  ahead:  investment  in  grid  elements.  Lines  and  transformers\n",
      "have standard life expectancies of several decades; practically, it is expected that\n",
      "some grid elements will last over one hundred years.\n",
      "\n",
      "Another concern is anticipating at different geographical scales. While some contin‐\n",
      "gencies  only  have  effects  on  a  small  part  of  the  grid,  some  may  have  a  continental\n",
      "impact  and  may  require  coordinated  actions  among  several  countries  to  mitigate\n",
      "their effects. As a result, operating the grid requires:\n",
      "\n",
      "1. Collecting data over a wide geographical area with strong time constraints.\n",
      "\n",
      "2. Processing data to anticipate and act accordingly.\n",
      "\n",
      "Data Collection\n",
      "Collecting past data is the first step to making forecasts. There are two largely inde‐\n",
      "pendent sources of data: the SCADA (supervisory control and data acquisition) sys‐\n",
      "tem and the metering system. Depending on the prediction use case, one or the other\n",
      "may be used.\n",
      "\n",
      "The  SCADA  system  collects  data  in  real  time  to  provide  an  up-to-date  view  of  the\n",
      "system to the operator. It also allows commands to be sent to network equipment—\n",
      "for example to open and close a breaker. The most impressive representation of the\n",
      "system is the synoptic screen found in most control rooms as shown in Figure 11-2.\n",
      "\n",
      "Data Collection \n",
      "\n",
      "| \n",
      "\n",
      "149\n",
      "\n",
      "\f",
      "Figure 11-2. The SCADA system typically refreshes thousands of measurements about\n",
      "flows, consumption, and generation on the grid every 10 seconds or less\n",
      "\n",
      "Some  measurements  are  intentionally  redundant,  such  as  measuring  power  loss.  If\n",
      "the power flow is measured at each end of a line, then the difference between them is\n",
      "equal  to  the  losses  on  the  line.  These  losses  can  be  physically  estimated  so  that  it  is\n",
      "possible to handle the case when one measure is missing, to detect anomalies, or to\n",
      "improve the precision of the estimates.\n",
      "\n",
      "The process that uses this redundancy to produce a state of the network is called the\n",
      "state estimation, and it is run every few minutes. When an operating limit is not satis‐\n",
      "fied, the SCADA system raises an alarm. However, the SCADA cannot raise an alarm\n",
      "when an operating limit would not be satisfied if one of the grid elements went out of\n",
      "order.\n",
      "\n",
      "Simulations  of  network  element  loss  (N-1  simulation)  on  the  consistent  state  pro‐\n",
      "duced by the state estimation are run on a regular basis, and the value of SCADA data\n",
      "fades  quickly;  therefore,  when  it  is  historized,  it  is  not  consolidated;  missing  values\n",
      "are usually not input, and anomalies are usually not corrected. State estimations are\n",
      "used by a variety of processes so that they are usually historized over a few months to\n",
      "a few years.\n",
      "\n",
      "The metering system that is used for invoicing does not need to be as reactive as the\n",
      "SCADA system, but should be precise. It focuses on generation and consumption, not\n",
      "\n",
      "150 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: MLOps in Practice: Consumption Forecast\n",
      "\n",
      "\f",
      "flow. Rather than monitoring instantaneous power, it records the withdrawn or injec‐\n",
      "ted energy over a period of time that ranges between a few minutes and one hour.\n",
      "\n",
      "The  information  it  gathers  was  previously  made  available  after  a  delay  of  a  day  or\n",
      "more. Newer systems make it available within a few minutes. However, consolidation\n",
      "and validation are usually needed when there are missing measurements or anoma‐\n",
      "lies so that the final data is still usually available within a few working days. This data\n",
      "is well historized.\n",
      "\n",
      "Problem Definition: Machine Learning,\n",
      "or Not Machine Learning?\n",
      "Not  all  use  cases  are  appropriate  for  machine  learning.  Some  can  be  solved  more\n",
      "easily and cheaply in other ways. The techniques used to make forecasts for the type\n",
      "of  use  case  presented  here  are  different  in  these  three  situations  as  shown  in\n",
      "Table 11-1.\n",
      "\n",
      "Table 11-1. Forecasting techniques by use case\n",
      "\n",
      "Use case\n",
      "The forecast uncertainty\n",
      "comes from a part of the\n",
      "system that the operator\n",
      "cannot change.\n",
      "\n",
      "The forecast uncertainty\n",
      "comes from a part of the\n",
      "system that the operator\n",
      "can somehow influence.\n",
      "\n",
      "The forecast uncertainty\n",
      "comes from a part of the\n",
      "system that some other\n",
      "actors can control and\n",
      "anticipate.\n",
      "\n",
      "Forecasting technique\n",
      "Changing the weather is, practically speaking, impossible. As a result, wind and photovoltaic\n",
      "(PV) generation, as well as heating and air conditioning, can safely be considered exogenous.\n",
      "This makes them good candidates for direct machine learning forecasting. These forecasts can\n",
      "leverage meteorological forecasts or climatic scenarios, depending on the horizon.\n",
      "Meteorological forecasts are available only a few days ahead, though some models now predict\n",
      "trends over a few months.\n",
      "For example, strictly speaking, the consumption should not be forecasted, but rather the\n",
      "demand. The difference between consumption and demand is that the consumption is somehow\n",
      "at the hand of the operator that can choose not to serve the demand by switching off the\n",
      "consumers. For the same reason, the photovoltaic and wind production potential is forecasted,\n",
      "not the actual production.\n",
      "For example, for dispatchable power units where the operator can switch them on or off, it is\n",
      "better to ask for the schedules from the operator. If this is not possible, it may be better to\n",
      "reproduce the way the schedules are made—for instance, the operator may start the plant if\n",
      "the power price is higher than the plant fuel cost. In such cases, the forecasts may rely on\n",
      "techniques like agent-based modeling.\n",
      "Large factories are likely to have consumption schedules based on their operational production\n",
      "schedules. Distribution grid topology is also likely to be scheduled ahead of time, as\n",
      "maintenance operations require advanced planning. In all these cases, it is often better to ask for\n",
      "the schedules than to use machine learning to forecast them.\n",
      "\n",
      "Spatial and Temporal Resolution\n",
      "Due  to  the  law  of  large  numbers,  the  forecast  uncertainty  decreases  when  the  con‐\n",
      "sumption is spatially or temporally aggregated. While it is hard to forecast the hourly\n",
      "\n",
      "Problem Definition: Machine Learning, or Not Machine Learning? \n",
      "\n",
      "| \n",
      "\n",
      "151\n",
      "\n",
      "\f",
      "consumption of an individual household because people are not machines, it is sur‐\n",
      "prisingly  easy  for  populations  of  a  few  million,  and  is  relatively  easy  to  forecast  the\n",
      "monthly consumption of such a population as well.\n",
      "\n",
      "As a result, a forecast system is often hierarchical, with several levels of forecasts that\n",
      "are  linked  together  by  constraints.  That  is,  regional  forecasts  should  sum  up  to  the\n",
      "country-wide forecasts, and hourly forecasts should sum up to the daily forecast.\n",
      "\n",
      "Let’s take a striking example to illustrate this. Electric traction trains have a worrying\n",
      "consumption pattern for grid operators because they move, with a typical train line\n",
      "being fed by a different substation every 10 to 50 kilometers. As a result, the operator\n",
      "sees consumption of a few megawatts switching from substation to substation every\n",
      "10 minutes or so. It creates several issues:\n",
      "\n",
      "• The forecast is relatively easy at the line level because the train is always consum‐\n",
      "ing somewhere and because trains usually circulate at fixed hours. As a result, a\n",
      "machine learning approach is likely to work.\n",
      "\n",
      "• The forecast of the energy withdrawn over a long period at a given substation is\n",
      "also  relatively  easy,  because  the  train  will  go  through  the  corresponding  part  of\n",
      "the line.\n",
      "\n",
      "• But because the operator wants to know if the train will create an overload when\n",
      "\n",
      "circulating, a consistent set of forecasts is needed:\n",
      "\n",
      "— The train should withdraw power at one location at a time only.\n",
      "\n",
      "— Each substation should see a consumption spike at some point in time so that\n",
      "\n",
      "a fine-grained time resolution is needed.\n",
      "\n",
      "As a result, the solution depends on the goal of the prediction:\n",
      "\n",
      "• On a day-to-day basis, an average solution that splits the train consumption over\n",
      "all substations is not acceptable, as potential overloads may be missed. A worst-\n",
      "case solution that assigns the train consumption to all substations may be more\n",
      "acceptable, though it will anticipate spurious overloads as the overall consump‐\n",
      "tion will be too large.\n",
      "\n",
      "• However, to schedule the maintenance of one of the lines that feeds the region,\n",
      "the exact location of the consumption is likely to have no impact as long as it is\n",
      "not counted several times.\n",
      "\n",
      "When  designing  the  forecast  system,  trade-offs  will  have  to  be  made,  as  the  perfect\n",
      "system is unlikely to exist. If the system has a lot of margin, few or no overloads are\n",
      "expected so that the forecasting system can be coarse. However, if the grid is operated\n",
      "close to its limits, the system has to be carefully crafted.\n",
      "\n",
      "152 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: MLOps in Practice: Consumption Forecast\n",
      "\n",
      "\f",
      "Implementation\n",
      "Once data is collected, either by the SCADA system or by the metering system, it has\n",
      "to be historized. In addition to storing the raw data, some processing is required:\n",
      "\n",
      "• Temporal aggregations, for example over a five-minute period: Either the average\n",
      "value or a high quantile value is kept. The average is representative of the energy\n",
      "consumed over the period, and the high quantile is useful to assess if constraints\n",
      "occurred.\n",
      "\n",
      "• Disaggregations: When only the withdrawal is measured, the production and the\n",
      "consumption  have  to  be  separately  estimated.  Usually,  consumption  is  what\n",
      "remains  after  removing  the  best  possible  estimation  of  distributed  generation\n",
      "(wind, PV, etc.). Machine learning can be useful to perform these estimations.\n",
      "\n",
      "• Spatial aggregations: As the system is balanced, it is possible to compute the con‐\n",
      "sumption of a region by computing the difference between the local production\n",
      "and the exchanges with the neighboring regions. This was historically very useful\n",
      "because the production was easy to monitor because there were only a few very\n",
      "large generation units and a few lines with neighboring countries. Nowadays, it\n",
      "tends to be more complex as distributed generation is more widespread.\n",
      "\n",
      "• Missing value imputation: A measurement may be missing. In the SCADA sys‐\n",
      "tem, rules exist to replace a missing value with an older or a typical value in real\n",
      "time. In the metering system, the imputation is a heavy impact process as it will\n",
      "be reflected directly on the customer’s invoice.\n",
      "\n",
      "Data is then stored in different databases. Data used in short-term critical processes is\n",
      "stored in high-availability systems in which redundancy allows rapid recovery from\n",
      "the loss of a data center. Data used in longer-term processes (invoicing, reports, ML\n",
      "model training) is stored in ordinary IT databases. Overall, the number of monitored\n",
      "grid elements will range between 1,000 and 100,000. This means that they generate a\n",
      "reasonable volume of data by today’s standards. Scalability is not such an issue either,\n",
      "as bulk power grids do not grow anymore in developed countries.\n",
      "\n",
      "Modeling\n",
      "Once the data preparation has been finished, the data scientist typically has access to\n",
      "a  few  hundred  time  series  of  production  and  consumption  at  various  withdrawal\n",
      "points of the grid. They have to develop methods to predict some of them at various\n",
      "horizons. Their focus is usually on wind, PV, and sometimes run-of-the river hydro‐\n",
      "electricity production potential and on demand. While wind and PV mainly depend\n",
      "on meteorological factors, the demand is mainly driven by economic activity, but part\n",
      "of it is also dependent on meteorology (for example heating and cooling).\n",
      "\n",
      "Implementation \n",
      "\n",
      "| \n",
      "\n",
      "153\n",
      "\n",
      "\f",
      "Depending on the horizon, the modeling might look very different:\n",
      "\n",
      "• Short-term: Up to a few days ahead, the last known values are very important to\n",
      "make predictions. In addition, for the same reasons, meteorological forecasts are\n",
      "available. Therefore, methods will leverage this information. In this case, deter‐\n",
      "ministic forecasts make sense.\n",
      "\n",
      "• Mid-term:  Between  a  few  days  and  a  few  years,  the  meteorology  is  not  known,\n",
      "but  the  climate  is.  Statistical  extrapolation  of  past  year  tendencies  make  sense,\n",
      "except if an economic crisis occurs. As a result, it is possible to draw scenarios to\n",
      "obtain statistical indicators (mean, confidence intervals, quantiles, etc.) about the\n",
      "future consumptions.\n",
      "\n",
      "• Long-term: Investment decisions require forecasts over several decades. On this\n",
      "horizon, statistical extrapolations of the current trend are not enough, neither on\n",
      "the  socio-economic  side  nor  on  the  climatic  side  given  global  warming.  As  a\n",
      "result,  statistical  approaches  have  to  be  completed  with  bottom-up  usage-based\n",
      "approaches and expert-made diversified scenarios about the future.\n",
      "\n",
      "ML  and  MLOps  mainly  concern  short-term  and  mid-term  forecasts.  Of  the  two,  in\n",
      "this case, mid-term models are easier to start with: given a few years of data, the goal\n",
      "is to predict consumption based on:\n",
      "\n",
      "• The calendar, with a superposition of daily, weekly, and annual cycles. Bank holi‐\n",
      "days and school vacations also have a big impact, in addition to daylight saving\n",
      "time.\n",
      "\n",
      "• The  meteorological  variables  (temperature,  wind,  sun).  As  buildings  have  very\n",
      "large thermal inertia, at least two days and up to three weeks of past temperatures\n",
      "may be needed.\n",
      "\n",
      "While any kind of ML algorithm can be used, the smoothness of the predicted curve\n",
      "is important because the predictions are not used individually, but as daily, weekly, or\n",
      "annual  scenarios.  Many  algorithms  do  not  consider  smoothness  in  their  metrics\n",
      "because they rely on the hypothesis that the data is independent and identically dis‐\n",
      "tributed, which in our case is incorrect, since the consumption of a given day is usu‐\n",
      "ally correlated with the one of the previous day and the one of the previous week.\n",
      "\n",
      "Generalized additive models (GAM) are often a good starting point: they are based\n",
      "on  splines,  so  that  the  smoothness  is  guaranteed.  In  fact,  consumption  forecasting\n",
      "was one of the use cases for which they were developed. Combined with climatic sce‐\n",
      "narios, the ML model is then able to yield yearly consumption scenarios.\n",
      "\n",
      "Short-term forecasts are more complex. The simplest way to proceed is to remove the\n",
      "mid-term forecast from the recent historical data and use standard time series techni‐\n",
      "ques,  such  as  ARIMA  (autoregressive  integrated  moving  average)  or  exponential\n",
      "\n",
      "154 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: MLOps in Practice: Consumption Forecast\n",
      "\n",
      "\f",
      "smoothing, on the residuals. This allows the generation of forecasts over several days.\n",
      "An integrated short-term model trained on several years of data has potential advan‐\n",
      "tages over this simple approach.\n",
      "\n",
      "For example, the mid-term model is trained on realized meteorological data and not\n",
      "on meteorological forecasts. As a result, it gives too much importance to meteorolog‐\n",
      "ical forecasts even though they may be wrong. A short-term model trained on mete‐\n",
      "orological  forecasts  would  address  this  issue.  However,  although  new  algorithms,\n",
      "such as long short-term memory (LSTM) neural networks, are promising, it is hard\n",
      "to  find  a  method  that  allows  for  forecasting  at  any  time  of  the  day  for  several  time\n",
      "horizons at once in a consistent way.\n",
      "\n",
      "When  the  resolution  is  such  that  the  stochasticity  is  too  large  to  make  meaningful\n",
      "predictions, it is better to aggregate time series spatially or temporally and then use\n",
      "non-ML heuristics to split the aggregated forecasts:\n",
      "\n",
      "• A sharing key based on past observations in the case of spatial aggregation\n",
      "\n",
      "• An average profile based on past observations in the case of temporal aggregation\n",
      "\n",
      "Because the grid is under constant evolution, it is likely that new injections and with‐\n",
      "drawals  appear  for  which  no  historical  data  is  available  and  that  ruptures  occur  in\n",
      "consumption patterns, so that past data is not relevant anymore. The forecast method\n",
      "has to take into account these edge cases. Ruptures could be spotted using anomaly\n",
      "detection  methods.  As  soon  as  a  rupture  is  identified,  a  simplified  model  could  be\n",
      "used for as long as necessary until enough historical data is available.\n",
      "\n",
      "Once again, neural networks could become an appealing alternative with the promise\n",
      "that only one model could be trained for all the consumptions instead of one model\n",
      "per consumption with standard methods. Indeed, with only one model, the forecast\n",
      "of a consumption with shallow historical data would be possible provided that its pat‐\n",
      "tern looks similar to an existing pattern.\n",
      "\n",
      "Deployment\n",
      "Nowadays, the models are likely to be prototyped by a data scientist in R, Python, or\n",
      "MATLAB scripts. The prototype is able to prepare the data, train the model on one\n",
      "dataset, and score it on another. The operationalization could follow several paths:\n",
      "\n",
      "• The prototype is fully rewritten. This is costly and not flexible but may be neces‐\n",
      "\n",
      "sary if embedding in an operational technology (OT) system is needed.\n",
      "\n",
      "• Only the data preparation and the scoring are rewritten, which allows for train‐\n",
      "ing on a different schedule. It makes sense if the training occurs once a year or so\n",
      "because it is good practice to regularly perform a model review to ensure that it\n",
      "works well and that the skills to maintain it are in place.\n",
      "\n",
      "Deployment \n",
      "\n",
      "| \n",
      "\n",
      "155\n",
      "\n",
      "\f",
      "• Data  science  and  machine  learning  platforms  can  be  used  to  operationalize  the\n",
      "prototype.  These  platforms  are  flexible  and  allow  the  transfer  of  prototypes  to\n",
      "production environments in which security and scalability are guaranteed. Most\n",
      "consumption  forecast  models  will  be  run  periodically  in  batch  mode.  For  more\n",
      "specific use cases, these platforms are able to export trained models as JAR files,\n",
      "SQL,  PMML,  PFA,  and  ONNX  so  that  they  can  be  flexibly  integrated  into  any\n",
      "kind of application.\n",
      "\n",
      "Monitoring\n",
      "This section mainly discusses short-term forecasts. Indeed, mid-term and long-term\n",
      "forecasts are systematically impacted by drift, as the past never looks like the future,\n",
      "so they are almost systematically trained again before being used to make predictions.\n",
      "For  short-term  forecasts,  besides  IT  monitoring  to  raise  alarms  if  forecasts  are  not\n",
      "produced on time and warnings for events that may result in missing deadlines, the\n",
      "models themselves should be monitored.\n",
      "\n",
      "The  first  kind  of  monitoring  is  drift  monitoring.  For  electricity  consumption,  it  is\n",
      "critical that drift monitoring is deployed together with the model. Anomaly detection\n",
      "and rupture detection allow teams to make sure that the trained model can be used. If\n",
      "not, fallback models based on shallow historical data or normative disaggregation of\n",
      "multiple  consumption  forecasts  should  be  used.  This  first  layer  will  detect  drastic\n",
      "drifts online.\n",
      "\n",
      "Though the data scientist will try to design models that are adaptive to the consump‐\n",
      "tion level (like ARIMA), it can be useful to detect that some consumption levels are\n",
      "higher or lower than in the training period. This may have happened slowly, so that it\n",
      "was  not  detected  online.  The  offline  analysis  of  the  forecasts,  for  example  once  a\n",
      "month if the forecasts are computed every day for the next day, offers the possibility\n",
      "to detect these slow drifts. In these cases, if no additional ground truth is available, it\n",
      "would make sense to shift to a fallback model for these consumptions.\n",
      "\n",
      "Lastly, after the operations, it is possible to assess the performance of the prediction\n",
      "through  various  metrics  like  mean  absolute  percentage  error  (MAPE).  If  a  perfor‐\n",
      "mance  drop  is  detected  during  a  significant  amount  of  time  (for  example,  one\n",
      "month),  retraining  the  corresponding  models  is  an  option  as  new  data  is  available,\n",
      "and the retrained models may increase performance.\n",
      "\n",
      "This requires a tight integration of the design and the production environment with\n",
      "CI/CD processes (as discussed at length in Chapter 6). If it is possible to handle man‐\n",
      "ually the deployment of new models once a year, it is usually too costly to do so once\n",
      "a  month.  With  an  advanced  data  science  and  machine  learning  platform,  it  is  also\n",
      "possible to perform shadow scoring with the new model for a few days before using it\n",
      "for the forecasts.\n",
      "\n",
      "156 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: MLOps in Practice: Consumption Forecast\n",
      "\n",
      "\f",
      "Closing Thoughts\n",
      "In this chapter, we have seen how to make the data speak to assist the operation of a\n",
      "transmission power grid. Various ML and non-ML techniques can be used to provide\n",
      "forecasts for up to thousands of consumptions on timescales ranging from minutes to\n",
      "decades.\n",
      "\n",
      "Thanks  to  MLOps,  design,  deployment,  and  monitoring  processes  have  been  stan‐\n",
      "dardized across several industries, and data science and machine learning platforms\n",
      "have been developed to support this process. Designers of consumption forecast sys‐\n",
      "tems can leverage these standard processes and platforms to improve the efficiency of\n",
      "these systems from the cost, quality, or time to value perspective.\n",
      "\n",
      "Taking  a  larger  step  back,  it’s  clear  that  different  industries  have  a  wide  range  of\n",
      "machine learning use cases, all of which have their own intricacies when it comes to\n",
      "defining  the  problem,  building  models,  pushing  to  production—everything  we’ve\n",
      "covered in this book. But no matter what the industry or use case, MLOps processes\n",
      "are consistently the thread that allows data teams (and more widely, entire organiza‐\n",
      "tions) to scale their machine learning efforts.\n",
      "\n",
      "Closing Thoughts \n",
      "\n",
      "| \n",
      "\n",
      "157\n",
      "\n",
      "\f",
      "\f",
      "Index\n",
      "\n",
      "A\n",
      "A/B testing\n",
      "\n",
      "canary releases, 79\n",
      "considerations in MLOps context, 102\n",
      "of new and existing model versions, 33\n",
      "performance monitoring for marketing rec‐\n",
      "\n",
      "ommendation engine, 144\n",
      "\n",
      "for recommendation engines using collabo‐\n",
      "\n",
      "rative filtering, 136\n",
      "\n",
      "using in online evaluation of models, 99,\n",
      "\n",
      "101\n",
      "\n",
      "accountability\n",
      "\n",
      "GxP guidelines focus on, 109\n",
      "in Responsible AI, 10, 112\n",
      "accuracy, precision, and recall, 132\n",
      "\n",
      "metrics collected in preproduction model\n",
      "\n",
      "testing, 65\n",
      "adversarial attacks, 68\n",
      "AI, 112\n",
      "\n",
      "(see also Responsible AI)\n",
      "new wave of AI-specific regulations,\n",
      "\n",
      "111-112\n",
      "\n",
      "Responsible AI, MLOps for, 9\n",
      "\n",
      "AIOps versus MLOps, 3\n",
      "algorithms (machine learning), 23, 44\n",
      "\n",
      "computing power considerations, 45\n",
      "MLOps considerations by algorithm type,\n",
      "\n",
      "45\n",
      "\n",
      "online learning, 88\n",
      "requirement for tabular input data, 47\n",
      "smoothness of predicted curve, 154\n",
      "\n",
      "anonymizing or pseudo-anonymizing data, 36\n",
      "Apache Spark, 78\n",
      "APIs\n",
      "\n",
      "marketing recommendation engine API\n",
      "\n",
      "endpoints, 142\n",
      "\n",
      "REST API for model-as-a-service or live-\n",
      "\n",
      "scoring model, 28\n",
      "\n",
      "ARIMA (autoregressive integrated moving\n",
      "\n",
      "average), 154, 156\n",
      "artifacts (ML), 75-76\n",
      "assumptions (model), 57\n",
      "auditability, 112, 116\n",
      "\n",
      "aiding with QA for machine learning, 67\n",
      "and reproducibility, 67\n",
      "\n",
      "automation\n",
      "\n",
      "automated feature selection, 48\n",
      "automated model deployment, 29\n",
      "automated model documentation, 27\n",
      "automated model packaging and delivery,\n",
      "\n",
      "14, 18\n",
      "\n",
      "automated reporting tools on all models, 14\n",
      "automatically controlling models, marketing\n",
      "\n",
      "recommendation system, 144\n",
      "\n",
      "in experimentation during model develop‐\n",
      "\n",
      "ment, 50\n",
      "\n",
      "feedback loop, 132\n",
      "of tests in testing pipeline, 76\n",
      "of versioning and reproducibility tasks, 58\n",
      "\n",
      "B\n",
      "batch scoring, 77\n",
      "\n",
      "analytics use cases, understanding and classify‐\n",
      "\n",
      "volume of data becoming too large, distrib‐\n",
      "\n",
      "ing, 118\n",
      "\n",
      "anomaly detection, 71\n",
      "\n",
      "uting computation, 82\n",
      "\n",
      "Bayesian tests, 34\n",
      "\n",
      "159\n",
      "\n",
      "\f",
      "biases\n",
      "\n",
      "analyzing models for fairness, 66\n",
      "inappropriate biases in models, 36\n",
      "introduced by ground truth monitoring, 90\n",
      "in ML black box judgments, 106\n",
      "model bias considerations in consumer\n",
      "\n",
      "credit risk management, 131\n",
      "\n",
      "reputational risk due to, 64\n",
      "Responsible AI position on, 114\n",
      "sample selection bias, 93\n",
      "tuning bias/variance trade-off, 50\n",
      "\n",
      "problem definition and data acquisition,\n",
      "\n",
      "130\n",
      "\n",
      "deploying model to production, 132\n",
      "model development, 130\n",
      "\n",
      "bias considerations, 131\n",
      "\n",
      "preparing model for production, 131\n",
      "consumption forecast for power grid, 147-157\n",
      "\n",
      "data collection, 149-151\n",
      "deployment of models, 155\n",
      "implementation, 153-155\n",
      "\n",
      "modeling, 153\n",
      "\n",
      "black box models, 54\n",
      "blue-green deployment, 28, 78\n",
      "bounds for retraining frequency, 87\n",
      "business concerns in monitoring ML models,\n",
      "\n",
      "31\n",
      "\n",
      "business decision modeling, 15\n",
      "business metrics for model performance moni‐\n",
      "\n",
      "toring, 89, 94\n",
      "\n",
      "monitoring, 156\n",
      "power systems, 147-149\n",
      "problem definition, using machine learning\n",
      "\n",
      "or not, 151-152\n",
      "spatial and temporal resolution, 151\n",
      "\n",
      "containerization, 79-81\n",
      "\n",
      "solving problems of dependencies in ML\n",
      "\n",
      "model deployments, 28\n",
      "\n",
      "business objectives, establishing for ML model,\n",
      "\n",
      "continuous integration and continuous delivery\n",
      "\n",
      "24\n",
      "\n",
      "(see CI/CD pipelines)\n",
      "\n",
      "business use case, consumer credit risk man‐\n",
      "\n",
      "costs\n",
      "\n",
      "agement application, 129\n",
      "\n",
      "C\n",
      "canary releases, 78\n",
      "CCPA (California Consumer Privacy Act), 35,\n",
      "\n",
      "111\n",
      "\n",
      "champion/challenger, 66, 100\n",
      "Chi-squared test, 94\n",
      "CI/CD pipelines, 73-74\n",
      "\n",
      "continuous delivery for end-to-end machine\n",
      "\n",
      "learning process, 95\n",
      "\n",
      "DevOps role in managing, 20\n",
      "for different models, 82\n",
      "Jenkins build system, 76\n",
      "ML artifacts in, 75\n",
      "robust, for model deployments, 29\n",
      "\n",
      "collaborative filtering, 136\n",
      "compression techniques, use in model defini‐\n",
      "\n",
      "tion optimization, 61\n",
      "\n",
      "computational metrics from model testing, 65\n",
      "computing power\n",
      "\n",
      "ML model development and, 45\n",
      "requred for inference on ML models, 62\n",
      "\n",
      "concept drift, 92\n",
      "conformal prediction, 72\n",
      "consumer credit risk management, 129-133\n",
      "\n",
      "business use case, 129\n",
      "\n",
      "for algorithms that train themselves, 88\n",
      "for retraining models versus performance\n",
      "\n",
      "improvement, 86\n",
      "cross-trained models, 88\n",
      "curse of dimensionality, 71\n",
      "customizability, marketing recommendation\n",
      "\n",
      "engine model, 140\n",
      "\n",
      "D\n",
      "dark launch, 100\n",
      "\n",
      "(see also champion/challenger)\n",
      "\n",
      "data\n",
      "\n",
      "collection for consumption forecast for\n",
      "\n",
      "power grid, 149-151\n",
      "\n",
      "considerations in Responsible AI, 113\n",
      "customer data, preparation for marketing\n",
      "\n",
      "recommendation engine, 137\n",
      "\n",
      "processing for consumption forecast system,\n",
      "\n",
      "153\n",
      "\n",
      "reproducibility, 57\n",
      "\n",
      "data access before validation and launch to pro‐\n",
      "\n",
      "duction, 62\n",
      "data architects, 21\n",
      "data cleansing, 25\n",
      "data drift, 91\n",
      "\n",
      "for consumer credit risk management\n",
      "\n",
      "model, 132\n",
      "\n",
      "160 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "\f",
      "example causes of, 93\n",
      "\n",
      "data engineers, 19\n",
      "data exploration, 46\n",
      "data governance, 36\n",
      "\n",
      "questions for ML model data sources, 25\n",
      "data pipeline structure for recommendation\n",
      "\n",
      "engine project, 141\n",
      "\n",
      "data privacy, 35, 108\n",
      "\n",
      "strategies for, 77-79\n",
      "deployment, defined, 77\n",
      "marketing recommendation engine model,\n",
      "\n",
      "138-141\n",
      "\n",
      "model deployment types and contents, 28\n",
      "model deploymnt requirements, 29\n",
      "\n",
      "development environments, adaptation to pro‐\n",
      "\n",
      "duction environments, 60-62\n",
      "\n",
      "GDPR and CCPA regulations on, 35, 110\n",
      "\n",
      "DevOps, 20\n",
      "\n",
      "data scientists, 17-19\n",
      "\n",
      "collaboration with SMEs in ML model life\n",
      "\n",
      "cycle, 16\n",
      "\n",
      "concerns in ML model monitoring, 30\n",
      "\n",
      "ground truth, 30\n",
      "input drift, 31\n",
      "\n",
      "role in and needs from MLOps, 18\n",
      "role in machine learning model life cycle, 17\n",
      "\n",
      "data sources for machine learning models, 24\n",
      "DataOps, 7\n",
      "decision modeling (business), 16\n",
      "decision-making processes, statistically driven,\n",
      "\n",
      "105\n",
      "\n",
      "deep learning, 45, 48, 54\n",
      "degradation of model performance\n",
      "\n",
      "concerns in ML model monitoring, 30\n",
      "MLOps and, 6\n",
      "monitoring of ML models, 86\n",
      "role in and needs from MLOps, 21\n",
      "role in machine learning model life cycle, 20\n",
      "\n",
      "DI (Data Integrity), 109\n",
      "dimensionality, curse of, 71\n",
      "dimensioning constraints on model develop‐\n",
      "\n",
      "ment, 54\n",
      "\n",
      "disaggregations of data, 153, 156\n",
      "distances between probability distributions, 132\n",
      "distillation (model), 61\n",
      "distributed computation, 82\n",
      "distribution of data, 92\n",
      "\n",
      "divergence between training and testing\n",
      "\n",
      "common approaches to discovering, 30\n",
      "understanding, 89-92\n",
      "\n",
      "phases, 92\n",
      "\n",
      "Docker, 80\n",
      "\n",
      "delivery, 77\n",
      "\n",
      "(see also CI/CD pipelines)\n",
      "continuous delivery versus deployment, 77\n",
      "\n",
      "dependencies\n",
      "\n",
      "deployment of models through, 132\n",
      "using Kubernetes with, 80\n",
      "\n",
      "documentation of model development, 26\n",
      "domain knowledge, importance in data explo‐\n",
      "\n",
      "partial dependency plots, 27\n",
      "on production environment, reducing, 28\n",
      "\n",
      "ration for models, 46\n",
      "\n",
      "domains\n",
      "\n",
      "deployment strategies, 77-79\n",
      "\n",
      "categories of model deployment, 77\n",
      "concepts and terminology, 77\n",
      "considerations in sending models to pro‐\n",
      "\n",
      "duction, 78\n",
      "\n",
      "maintenance of models in production, 79\n",
      "\n",
      "deployments\n",
      "\n",
      "broader model deployment, greater risks\n",
      "\n",
      "from, 69\n",
      "\n",
      "consumption forecast models, 155\n",
      "deploying to production, 73-84\n",
      "building ML artifacts, 75-76\n",
      "CI/CD pipelines, 73\n",
      "consumer credit risk management\n",
      "\n",
      "model, 132\n",
      "\n",
      "containerization, 79-81\n",
      "scaling deployments, 81-83\n",
      "\n",
      "domain classsifier, 94\n",
      "model retraining frequency and, 86\n",
      "\n",
      "drift, 91\n",
      "\n",
      "(see also input drift)\n",
      "detection in practice, 92-95\n",
      "\n",
      "example causes of data drift, 93\n",
      "input drift detection techniques, 93\n",
      "measuring for consumer credit risk assess‐\n",
      "\n",
      "ment model, 132\n",
      "\n",
      "monitoring, 156\n",
      "monitoring and mitigation measures, 103\n",
      "\n",
      "E\n",
      "EDA (exploratory data analysis), 24, 25, 46\n",
      "effienciency, data scientists' need for from\n",
      "\n",
      "MLOps, 19\n",
      "elastic systems, 81\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "161\n",
      "\n",
      "\f",
      "embedded models, 28\n",
      "embeddings, 48\n",
      "engaging and educating groups responsible for\n",
      "\n",
      "governance, 123\n",
      "\n",
      "environmental, social, and governance (ESG)\n",
      "\n",
      "performance indicators, 36\n",
      "\n",
      "environments\n",
      "\n",
      "changing rapidly, multiplying model risks,\n",
      "\n",
      "70\n",
      "\n",
      "federated learning approach to model retrain‐\n",
      "\n",
      "ing, 34\n",
      "\n",
      "feedback loop, 95-103\n",
      "\n",
      "automating or not, 132\n",
      "infrastructure, main components of, 96\n",
      "logging, 96\n",
      "model evaluation, 97-99\n",
      "logical model, 97\n",
      "model evaluation store, 98\n",
      "\n",
      "information needed by data scientists, 53\n",
      "providing exact description for model ver‐\n",
      "\n",
      "sion management, 79\n",
      "\n",
      "reproducibility and, 58\n",
      "\n",
      "ethical position, establishing in MLOps, 118\n",
      "EU\n",
      "\n",
      "online evaluation of models in production,\n",
      "\n",
      "99-102\n",
      "A/B testing, 101\n",
      "champion/challenger, 100\n",
      "financial crisis of 2007-2008, 109\n",
      "financial model risk management regulation,\n",
      "\n",
      "General Data Protection Regulation\n",
      "\n",
      "109\n",
      "\n",
      "(GDPR), 35, 110\n",
      "\n",
      "key requirements for trustworthy AI appli‐\n",
      "\n",
      "cations, 111\n",
      "\n",
      "evaluation datasets, 44\n",
      "experimentation in model development, 49-51\n",
      "\n",
      "impacts on MLOps strategy, 50\n",
      "marketing recommendation engines, 138\n",
      "\n",
      "explainability, 27\n",
      "\n",
      "for decisions made by ML models affecting\n",
      "\n",
      "humans, 54\n",
      "\n",
      "in Responsible AI, 113\n",
      "\n",
      "exploratory data analysis (see EDA)\n",
      "\n",
      "F\n",
      "Facebook-Cambridge Analytica affair, 106\n",
      "fairness\n",
      "\n",
      "reassuring public that ML is fair, 106\n",
      "requirements having dimensioning con‐\n",
      "straints on model development, 54\n",
      "subpopulation analysis and model fairness,\n",
      "\n",
      "66\n",
      "feature drift, 92\n",
      "feature stores, 49\n",
      "features, 24\n",
      "\n",
      "controlling feature-value intervals, 71\n",
      "drift attributed to, use in mitigating impact\n",
      "\n",
      "of drift, 95\n",
      "\n",
      "engineering and selection, 25, 47-49\n",
      "\n",
      "feature engineering techniques, 47\n",
      "impacts of feature selection on MLOps\n",
      "\n",
      "strategy, 48\n",
      "\n",
      "financial risk management techniques, 130\n",
      "Flask framework, 29\n",
      "forecasting, 147\n",
      "\n",
      "(see also consumption forecast for power\n",
      "\n",
      "grid)\n",
      "\n",
      "forecasting techniques by use case, 151\n",
      "spatial and temporal resolution of consump‐\n",
      "\n",
      "tion, 151\n",
      "\n",
      "G\n",
      "GAM (generalized additive models), 154\n",
      "GDPR (General Data Protection Regulation),\n",
      "\n",
      "35, 110\n",
      "\n",
      "generalization capacity (models), 42\n",
      "Git, 75\n",
      "Google, smartphone keyboard software,\n",
      "\n",
      "GBoard, 34\n",
      "\n",
      "governance, 34-38, 105-125\n",
      "\n",
      "AI-specific regulations, new wave of,\n",
      "\n",
      "111-112\n",
      "status of AI governance initiatives\n",
      "\n",
      "worldwide, 111\n",
      "\n",
      "application to MLOps, 36\n",
      "data governance, 36\n",
      "process governance, 37\n",
      "\n",
      "critical role in machine learning security, 69\n",
      "current regulations driving MLOps gover‐\n",
      "\n",
      "nance, 108-111\n",
      "financial model risk management, 109\n",
      "GDPR and CCPA data privacy regula‐\n",
      "\n",
      "tions, 110\n",
      "\n",
      "statistical test on data from source and tar‐\n",
      "\n",
      "pharmaceutical regulation in US, GxP,\n",
      "\n",
      "get distribution, 94\n",
      "\n",
      "109\n",
      "\n",
      "162 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "\f",
      "key elements of Responsible AI, 112-116\n",
      "\n",
      "bias, 114\n",
      "data, 113\n",
      "governance techniques, 116\n",
      "inclusiveness, 115\n",
      "model management at scale, 116\n",
      "\n",
      "matching with risk level, 107\n",
      "template for MLOps, 117-125\n",
      "\n",
      "determining governance policies, 120\n",
      "engaging end educating, 123\n",
      "establishing an ethical position, 118\n",
      "establishing responsibilities, 119\n",
      "integrating governance policies into\n",
      "\n",
      "detection of, 91\n",
      "detection techniques, 93\n",
      "domain classifier, 94\n",
      "interpretation of results, 94\n",
      "univariate statistical tests, 93\n",
      "model retraining motivated by, 33\n",
      "\n",
      "integration, 77\n",
      "\n",
      "(see also CI/CD pipelines)\n",
      "\n",
      "intentionality, Responsible AI, 9, 112\n",
      "interactions between models, risks generated\n",
      "\n",
      "by, 70\n",
      "\n",
      "interpretability (ML), bias detection through,\n",
      "\n",
      "114\n",
      "\n",
      "MLOps process, 121\n",
      "\n",
      "iterations of ML models, 32\n",
      "\n",
      "monitoring and refining governance,\n",
      "\n",
      "124\n",
      "\n",
      "feedback loop, 33\n",
      "iterating on model deployed to millions of\n",
      "\n",
      "selecting tools for centralized gover‐\n",
      "\n",
      "devices, 34\n",
      "\n",
      "nance management, 122\n",
      "\n",
      "understanding/classifying analytics use\n",
      "\n",
      "cases, 118\n",
      "\n",
      "who decides on organization needs, 105\n",
      "\n",
      "gradient boosting algorithms, 130\n",
      "ground truth, 30\n",
      "\n",
      "evaluation of, in monitoring of performance\n",
      "\n",
      "degradation, 89\n",
      "\n",
      "GxP pharmaceutical regulations in US, 109\n",
      "\n",
      "H\n",
      "health checks, 79\n",
      "health diagnosis or prediction example, 43\n",
      "high bias model (underfitting), 50\n",
      "high variance model (overfitting), 50\n",
      "historizing data, 153\n",
      "house price example, model prediction and\n",
      "\n",
      "generalization, 43\n",
      "\n",
      "human-centered approach, 113\n",
      "human-in-command, 141\n",
      "human-in-the-loop (HITL), 115\n",
      "hyperparameters of ML algorithms, 44\n",
      "\n",
      "I\n",
      "impact coding, 47\n",
      "implementation, reproducibility of models and,\n",
      "\n",
      "57\n",
      "\n",
      "inclusiveness, 115\n",
      "individual conditional expectation (ICE) com‐\n",
      "\n",
      "putations, 54\n",
      "\n",
      "inference, computing power required for, 62\n",
      "input drift, 31\n",
      "\n",
      "J\n",
      "Java-based environment, model developed in\n",
      "\n",
      "Python, 61\n",
      "\n",
      "Jenkins build system, 76\n",
      "\n",
      "K\n",
      "Kolmogorov-Smirnov test, 94, 132\n",
      "KPIs (key performance indicators), 24, 107\n",
      "\n",
      "in business scoping, 122\n",
      "for governance, 124\n",
      "measurement of, 25\n",
      "monitoring for deployed ML models, 31\n",
      "subject matter experts contributing, 15\n",
      "tracking model performance against, 123\n",
      "training different models for different KPIs,\n",
      "\n",
      "144\n",
      "Kubernetes, 28, 80\n",
      "\n",
      "autoscaling properties, 81\n",
      "using with Spark, 82\n",
      "\n",
      "L\n",
      "labeled sample subset, problems in ground\n",
      "\n",
      "truth monitoring, 90\n",
      "\n",
      "latency\n",
      "\n",
      "metrics on, collection in preproduction\n",
      "\n",
      "model testing, 65\n",
      "\n",
      "model conversions and, 61\n",
      "\n",
      "linear algorithms, 45\n",
      "linear models, advanced, 130\n",
      "Linux cgroups, 29\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "163\n",
      "\n",
      "\f",
      "live-scoring model, 28\n",
      "logging\n",
      "\n",
      "activity logging of deployed model, 132\n",
      "component in ML feedback loop, 96\n",
      "using to generate centralized datasets for use\n",
      "\n",
      "by model designer, 83\n",
      "\n",
      "logical model, 98\n",
      "\n",
      "MLOps\n",
      "\n",
      "considerations with A/B testing, 102\n",
      "defining, and its challenges, 4-7\n",
      "governance template for, 117-125\n",
      "mitigating risks of ML models, 7-10\n",
      "versus ModelOps versus AIOps, 3\n",
      "real-world examples\n",
      "\n",
      "comparing performance between different\n",
      "\n",
      "consumer credit risk management,\n",
      "\n",
      "versions, 99\n",
      "\n",
      "versioning evolution of, 99\n",
      "\n",
      "LTSM (long short-term memory) neural net‐\n",
      "\n",
      "works, 155\n",
      "\n",
      "M\n",
      "machine learning, 41\n",
      "(see also models)\n",
      "continuous delivery for end-to-end process,\n",
      "\n",
      "95\n",
      "\n",
      "deciding whether to use or not, 151-152\n",
      "primer, 23\n",
      "quality assurance for, 64-66\n",
      "role in recommendation engines, 136\n",
      "security issues, 67-69\n",
      "\n",
      "machine learning architects, 21\n",
      "\n",
      "role in and needs from MLOps, 22\n",
      "role in machine learning model life cycle, 22\n",
      "\n",
      "machine learning metrics, monitoring, 79\n",
      "machine learning models (see models)\n",
      "maintenance of models in production, 79\n",
      "manual control of models, 143\n",
      "MAPE (mean absolute percentage error), 156\n",
      "marketing data mining urban legend, 135\n",
      "marketing recommendation engines, 135-145\n",
      "\n",
      "data preparation for, 137\n",
      "designing and managing experiments on,\n",
      "\n",
      "138\n",
      "\n",
      "model training and deployment, 138-141\n",
      "\n",
      "challenges in, 139\n",
      "\n",
      "monitoring and feedback, 142-145\n",
      "pipeline structure and deployment strategy,\n",
      "\n",
      "141\n",
      "\n",
      "rise of recommendation engines, 135-137\n",
      "push or pull recommendations, 136\n",
      "role of machine learning, 136\n",
      "\n",
      "MATLAB scripts, 155\n",
      "minority populations, representation in ML\n",
      "\n",
      "model data sources, 25\n",
      "misbehavior of models, 71\n",
      "missing value imputation, 153\n",
      "\n",
      "164 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "129-133\n",
      "\n",
      "consumption forecast, 147-157\n",
      "marketing recommendation engines,\n",
      "\n",
      "135-145\n",
      "\n",
      "for scale, 10\n",
      "\n",
      "model evaluation stores, 97, 98\n",
      "model risk manager/auditor, 21\n",
      "\n",
      "role in and needs from MLOps, 21\n",
      "role in machine learning model life cycle, 21\n",
      "\n",
      "model-as-a-service, or live-scoring model, 28\n",
      "models\n",
      "\n",
      "developing, 24-27, 41-58\n",
      "\n",
      "consumer credit risk management\n",
      "\n",
      "model, 130\n",
      "data exploration, 46\n",
      "data sources and exploratory data analy‐\n",
      "\n",
      "sis, 24\n",
      "\n",
      "establishing business objectives, 24\n",
      "evaluating and comparing models, 51-56\n",
      "experimentation, 49-51\n",
      "feature engineering and selection, 25,\n",
      "\n",
      "47-49\n",
      "in practice, 43\n",
      "in theory, 42\n",
      "MLOps considerations by algorithm\n",
      "\n",
      "type, 45\n",
      "\n",
      "required components, 44\n",
      "Responsible AI, 26\n",
      "training and evaluation, 26\n",
      "version management and reproducibil‐\n",
      "\n",
      "ity, 56-58\n",
      "\n",
      "iterations and life cycle, 32-34\n",
      "life cycle, 4\n",
      "mitigating risks with MLOps, 7-10\n",
      "monitoring after deployment to production,\n",
      "\n",
      "29-32\n",
      "\n",
      "productionalization and deploymeent,\n",
      "\n",
      "27-29\n",
      "\n",
      "monitoring and refining governance, 124\n",
      "monitoring machine learning models, 29-32,\n",
      "\n",
      "85-103\n",
      "\n",
      "\f",
      "business concerns, 31\n",
      "consumer credit risk management model,\n",
      "\n",
      "132, 132\n",
      "\n",
      "consumption forecast for power grid model,\n",
      "\n",
      "156\n",
      "\n",
      "data scientist concerns, 18, 30\n",
      "deciding how often to retrain models, 86-89\n",
      "degradation of performance, understanding,\n",
      "\n",
      "89-92\n",
      "ground truth evaluation, 89\n",
      "input drift detection, 91\n",
      "\n",
      "DevOps concerns, 30\n",
      "drift detection in practice, 92-95\n",
      "\n",
      "input drift detection techniques, 93\n",
      "\n",
      "feedback loop, 95-103\n",
      "\n",
      "logging, 96\n",
      "model evaluation, 97-99\n",
      "online evaluation of models in produc‐\n",
      "\n",
      "tion, 99-102\n",
      "\n",
      "marketing recommendation engine, 140,\n",
      "\n",
      "142-145\n",
      "model runs overnight, sleeps in daytime,\n",
      "\n",
      "143\n",
      "\n",
      "monitoring performance, 144\n",
      "option to automatically control models,\n",
      "\n",
      "144\n",
      "\n",
      "option to manually control models, 143\n",
      "retraining models, 142\n",
      "updating models, 143\n",
      "\n",
      "model risk manager/auditor, role in, 21\n",
      "in production, 79\n",
      "\n",
      "monotonicity constraints, 130\n",
      "MRM (model risk management), 21\n",
      "\n",
      "(see also model risk manager/auditor)\n",
      "principles for good MRM, defined by UK\n",
      "\n",
      "PRA, 109\n",
      "\n",
      "regulation for fianancial models, 109\n",
      "\n",
      "multi-armed bandit testing, 34\n",
      "\n",
      "N\n",
      "neural networks, 45\n",
      "\n",
      "adversarial attacks on, 68\n",
      "long short-term memory (LSTM), 155\n",
      "NLP (natural language processing) pretrained\n",
      "\n",
      "models, compression used with, 61\n",
      "\n",
      "null hypothesis to p-values, 90\n",
      "\n",
      "O\n",
      "one-hot encoding, 47\n",
      "\n",
      "online evaluation of models in production,\n",
      "\n",
      "99-102\n",
      "A/B testing, 101\n",
      "champion/challenger, 100\n",
      "\n",
      "online machine learning, 87\n",
      "operationalization and MLOps, 18\n",
      "orchestration of containers, 28\n",
      "\n",
      "P\n",
      "p-values, 89\n",
      "\n",
      "advantages and drawbacks of, 94\n",
      "\n",
      "parallelization of batch scoring, 78\n",
      "partitioning (or sharding), 78\n",
      "\n",
      "distributing batch processing by partition‐\n",
      "\n",
      "ing data, 82\n",
      "\n",
      "people in MLOps, roles and responsibilities,\n",
      "\n",
      "13-22\n",
      "data engineers, 19\n",
      "data scientists, 17-19\n",
      "DevOps, 20\n",
      "machine learning architects, 21\n",
      "model risk manager/auditor, 21\n",
      "overview, 13\n",
      "software engineers, 20\n",
      "subject matter experts (SMEs), 15-17\n",
      "\n",
      "performance\n",
      "\n",
      "considerations when converting from devel‐\n",
      "opment to production environments, 61\n",
      "\n",
      "degradation of model performance, moni‐\n",
      "\n",
      "toring, 89-92\n",
      "\n",
      "drift, assessing for subpopulations in credit\n",
      "\n",
      "risk management model, 132\n",
      "\n",
      "model retraining and, 86\n",
      "monitoring for marketing recommendation\n",
      "\n",
      "engines, 144\n",
      "\n",
      "monitoring ML models for, 86\n",
      "testing on out-of-sample datasets for con‐\n",
      "sumer credit risk management model,\n",
      "131\n",
      "\n",
      "performance metrics, 44\n",
      "personally identifiable information (PII), 25\n",
      "making sure no PII used to train a model,\n",
      "\n",
      "36\n",
      "\n",
      "pharmaceutical regulations in US, GxP, 109\n",
      "poisoning attacks, 68\n",
      "portable formats for models, 28, 156\n",
      "power systems, 147-149\n",
      "predictions\n",
      "\n",
      "conformal, 72\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "165\n",
      "\n",
      "\f",
      "decoupled ground truth and prediction, 90\n",
      "examples of model prediction and generali‐\n",
      "\n",
      "zation, 43\n",
      "\n",
      "adaptation from development to produc‐\n",
      "\n",
      "tion environments, 60-62\n",
      "\n",
      "data access before validation and launch\n",
      "\n",
      "ground truth for model predictions, 31\n",
      "input drift and, 31\n",
      "\n",
      "to poduction, 62\n",
      "final thoughts on, 62\n",
      "\n",
      "probabilities\n",
      "\n",
      "distances between probability distributions,\n",
      "\n",
      "measuring, 132\n",
      "\n",
      "produced by models used for risk assess‐\n",
      "\n",
      "ment, 131\n",
      "\n",
      "problem definition and data acquisition, com‐\n",
      "sumer credit risk management MLOps\n",
      "application, 130\n",
      "\n",
      "problem definition, using machine learning or\n",
      "\n",
      "not, 151-152\n",
      "\n",
      "process governance, 37\n",
      "\n",
      "effective implementation, difficulties of, 37\n",
      "\n",
      "production, deploying to, 73-84\n",
      "building ML artifacts, 75-76\n",
      "\n",
      "using testing pipeline on model, 75\n",
      "\n",
      "CI/CD pipelines, 73\n",
      "consumer credit risk management model,\n",
      "\n",
      "132\n",
      "\n",
      "consumption forecast prototype models,\n",
      "\n",
      "155\n",
      "\n",
      "containerization, 79-81\n",
      "deployment strategies, 77-79\n",
      "\n",
      "categories of model deployment, 77\n",
      "considerations in sending models to pro‐\n",
      "\n",
      "duction, 78\n",
      "\n",
      "maintenance of models in production,\n",
      "\n",
      "79\n",
      "\n",
      "scaling deployments, 81-83\n",
      "\n",
      "requirements and challenges, 83\n",
      "\n",
      "production, preparing for, 59-72\n",
      "\n",
      "consumer credit risk management model,\n",
      "\n",
      "131\n",
      "\n",
      "key ideas, summary of, 72\n",
      "machine learning security, 67-69\n",
      "model risk evaluation, 63-64\n",
      "model risk mitigation, 69-72\n",
      "\n",
      "changing environments, 70\n",
      "interactions between models, 70\n",
      "model misbehavior, 71\n",
      "\n",
      "quality assurance for machine learning,\n",
      "\n",
      "64-66\n",
      "\n",
      "reproducibility and auditability for models,\n",
      "\n",
      "66\n",
      "\n",
      "runtime environments, 60-63\n",
      "\n",
      "166 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "productionalization and deploymnt of models,\n",
      "\n",
      "27-29\n",
      "model deployment requirements, 29\n",
      "model deployment types and contents, 28\n",
      "\n",
      "progressive or canary rollouts, 69\n",
      "project criticality and operationalization\n",
      "approaches to risk assessment, 107\n",
      "\n",
      "provenance of data, 113\n",
      "pruning models, 61\n",
      "public opinion, influence on ML governance,\n",
      "\n",
      "106\n",
      "\n",
      "push or pull recommendations, 136\n",
      "Python, 9, 61, 155\n",
      "\n",
      "Q\n",
      "QA (quality assurance) for machine learning,\n",
      "\n",
      "64-66\n",
      "key testing considerations, 65\n",
      "providing clear view of model performance\n",
      "\n",
      "and facilitating auditability, 67\n",
      "\n",
      "quantization, 61\n",
      "\n",
      "R\n",
      "R language, 155\n",
      "RACI (responsible, accountable, consulted,\n",
      "\n",
      "informed), 119\n",
      "\n",
      "randomness, 57\n",
      "\n",
      "random sampling, 91\n",
      "\n",
      "real-time scoring, 77\n",
      "\n",
      "logging streaming data, 83\n",
      "marketing recommendation engine model,\n",
      "\n",
      "140\n",
      "\n",
      "recommendation engines, 135\n",
      "\n",
      "(see also marketing recommendation\n",
      "\n",
      "engines)\n",
      "rise of, 135-137\n",
      "\n",
      "deciding on push or pull recommenda‐\n",
      "\n",
      "tions, 136\n",
      "\n",
      "turning recommendations on/off, 141\n",
      "\n",
      "red-black deployment, 78\n",
      "regulations, 118\n",
      "\n",
      "AI-specific, new wave of, 111-112\n",
      "current, driving MLOps governance,\n",
      "\n",
      "108-111\n",
      "\n",
      "\f",
      "financial model risk management, 109\n",
      "GDPR and CCPA data privacy regula‐\n",
      "\n",
      "tions, 110\n",
      "\n",
      "(see also biases)\n",
      "\n",
      "in consumer credit risk management model\n",
      "\n",
      "bias, 131\n",
      "\n",
      "pharmaceutical regulation in US, GxP,\n",
      "\n",
      "feature that causes drift, 95\n",
      "\n",
      "109\n",
      "\n",
      "risks\n",
      "\n",
      "governance and regulatory checks in model\n",
      "\n",
      "deployments, 29\n",
      "\n",
      "assessing, considerations in MLOps, 107\n",
      "credit risk modeling, 130\n",
      "\n",
      "government regulations on ML to mitigate\n",
      "\n",
      "(see also consumer credit risk manage‐\n",
      "\n",
      "negative impact of its use, 35\n",
      "\n",
      "ment)\n",
      "\n",
      "government regulations on use of personal\n",
      "\n",
      "financial model risk management regula‐\n",
      "\n",
      "data by businesses, 35\n",
      "process governance and, 38\n",
      "\n",
      "releases, 77\n",
      "reproducibility\n",
      "\n",
      "auditability and, 67\n",
      "of experiments with ML models, 26\n",
      "importance as model property, 56\n",
      "in MLOps versus academia, 66\n",
      "\n",
      "resource demands of models\n",
      "\n",
      "capped, 29\n",
      "computing power, 45\n",
      "scalability of compute resources for\n",
      "\n",
      "deployed models, 30\n",
      "\n",
      "resource monitoring, 79\n",
      "resources\n",
      "\n",
      "monitoring for ML models, 85\n",
      "optimizing usage with elastic systems, 81\n",
      "responsibilities, establishing in MLOps, 119\n",
      "Responsible AI, 112-116\n",
      "\n",
      "businesses engaging with, 36\n",
      "impacts on modeling, 53\n",
      "key elements of, 113-116\n",
      "\n",
      "bias, 114\n",
      "data, 113\n",
      "governance, 116\n",
      "inclusiveness, 115\n",
      "model management at scale, 116\n",
      "\n",
      "in marketing recommendation engine, 141\n",
      "in ML model development, 26\n",
      "MLOps for, 9\n",
      "model explainability, 27\n",
      "\n",
      "results of in-depth analysis of models, compar‐\n",
      "\n",
      "ing, 57\n",
      "\n",
      "retraining models\n",
      "\n",
      "deciding how often to retrain, 86-89\n",
      "marketing recommendation engine, 140,\n",
      "\n",
      "142\n",
      "reweighting, 93\n",
      "\n",
      "for biased samples, 91, 93\n",
      "\n",
      "tion, 109\n",
      "\n",
      "matching governance with risk level, 107\n",
      "model risk evaluation, 63-64\n",
      "\n",
      "origins of ML model risk, 64\n",
      "purpose of model validation, 63\n",
      "\n",
      "model risk manager/auditor, 21\n",
      "model risk mitigation, 69-72\n",
      "\n",
      "changing environments, 70\n",
      "interactions between models, 70\n",
      "model misbehavior, 71\n",
      "\n",
      "need for proactively addressing in ML, 106\n",
      "risk assessment, 8\n",
      "risk mitigation, MLOps for, 9\n",
      "RMSE (root-mean-square error), 143\n",
      "rollbacks to previous model versions, 79\n",
      "runtime environments, 60-63\n",
      "\n",
      "adaptation from development to production\n",
      "\n",
      "environments, 60-62\n",
      "performane considerations, 61\n",
      "tooling considerations, 61\n",
      "\n",
      "data access before validation and launch to\n",
      "\n",
      "production, 62\n",
      "final thoughts on, 62\n",
      "\n",
      "S\n",
      "sampling\n",
      "\n",
      "random, 91\n",
      "selection bias, 93\n",
      "\n",
      "SCADA (supervisory control and data acquisi‐\n",
      "\n",
      "tion) system, 149\n",
      "\n",
      "scale\n",
      "\n",
      "MLOps for, 10\n",
      "model management at scale (Responsible\n",
      "\n",
      "AI), 116\n",
      "\n",
      "scalability of compute resources for\n",
      "\n",
      "deployed models, 30\n",
      "\n",
      "scalability of marketing recommendation\n",
      "\n",
      "engine model, 140\n",
      "scaling deployments, 81-83\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "167\n",
      "\n",
      "\f",
      "requirements and challenges, 83\n",
      "scalable and elastic systems, 81\n",
      "scaling the number of models, 82\n",
      "\n",
      "scaling inference on models, 62\n",
      "\n",
      "scikit-learn, 9\n",
      "\n",
      "temporal aggegrations of data, 153\n",
      "testing\n",
      "\n",
      "key testing considerations, 65\n",
      "for models using self-training algorithm, 88\n",
      "testing pipeline for ML artifacts, 75\n",
      "\n",
      "models developed with, adpatation to pro‐\n",
      "\n",
      "tools\n",
      "\n",
      "ducton environments, 61\n",
      "\n",
      "security in machine learning, 67-69\n",
      "\n",
      "adversarial attacks, 68\n",
      "other vulnerabilities, 68\n",
      "\n",
      "selection bias, 93\n",
      "settings, reproducibility and, 57\n",
      "shadow testing, 99\n",
      "\n",
      "(see also champion/challenger)\n",
      "of new model version deployed alongside\n",
      "\n",
      "existing model, 33\n",
      "\n",
      "Shapley values, 27, 53, 130\n",
      "sharding, 78\n",
      "SMEs (see subject matter experts)\n",
      "software engineers, 20\n",
      "\n",
      "role in and needs from MLOps, 20\n",
      "role in machine learning model life cycle, 20\n",
      "\n",
      "Spark, 82\n",
      "spatial aggregations of data, 153\n",
      "spatial and temporal resolution, forecast uncer‐\n",
      "\n",
      "tainty and, 151\n",
      "\n",
      "statistics\n",
      "\n",
      "from null hypothesis to p-values, 90\n",
      "statistical metrics for model performance\n",
      "\n",
      "monitoring, 89\n",
      "\n",
      "statistical results on model testing, 65\n",
      "statistically driven decision-making pro‐\n",
      "\n",
      "cesses in ML, 105\n",
      "\n",
      "univariate statistical tests, 93\n",
      "\n",
      "stochasticity, 155\n",
      "subject matter experts (SMEs), 15-17\n",
      "role in and needs from MLOps, 16\n",
      "role in machine learning model life cycle, 15\n",
      "\n",
      "subpopulations\n",
      "\n",
      "analyses of, 27\n",
      "analysis and model fairness, 66\n",
      "investing model performance for in con‐\n",
      "sumer credit risk management, 131\n",
      "statistical tests on results in preproduction\n",
      "\n",
      "model testing, 66\n",
      "supervised learning, 44\n",
      "\n",
      "T\n",
      "technical debt in machine learning, 49\n",
      "\n",
      "168 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "considerations in adaptation from develop‐\n",
      "ment to production environments, 61\n",
      "selecting for centralized governance man‐\n",
      "\n",
      "agement, 122\n",
      "\n",
      "traceability in pharmaceutical industry, 109\n",
      "training data, 23, 44\n",
      "\n",
      "adversarial attacks on, 68\n",
      "data governance concerns, 36\n",
      "keeping up to date for deployed models, 30\n",
      "non-stationary environment, 93\n",
      "quality of, determining model performance,\n",
      "\n",
      "86\n",
      "\n",
      "training models, 26\n",
      "\n",
      "automation in, 50\n",
      "deciding how often to retrain models, 86-89\n",
      "marketing recommendation engine model,\n",
      "\n",
      "138-141\n",
      "\n",
      "retraining existing model with latest train‐\n",
      "\n",
      "ing data, 32\n",
      "\n",
      "transfer learning, 48\n",
      "transparency\n",
      "\n",
      "data scientists' need for from MLOps, 18\n",
      "subject matter experts, role in MLOps, 17\n",
      "transparent strategies for machine learning,\n",
      "\n",
      "11\n",
      "\n",
      "tree-based algorithms, 45\n",
      "trust\n",
      "\n",
      "EU's requirements for trustworthy AI appli‐\n",
      "\n",
      "cations, 111\n",
      "\n",
      "importance to consumers and businesses, 36\n",
      "\n",
      "Tweedie distribution, 130\n",
      "Twitter chatbot by Microsoft, 68\n",
      "two-stage models with offset, 130\n",
      "\n",
      "U\n",
      "U.S. Food and Drug Administration (FDA), 109\n",
      "UCI Wine Quality dataset, 91\n",
      "UK Prudential Regulation Authority’s (PRA)\n",
      "\n",
      "regulation, 109\n",
      "\n",
      "underfitting, 50\n",
      "univariate statistical tests, 93\n",
      "updating models, 143\n",
      "\n",
      "\f",
      "V\n",
      "validation\n",
      "\n",
      "data access before model validation, 62\n",
      "model for consumer credit risk manage‐\n",
      "\n",
      "ment, 131\n",
      "\n",
      "purpose of model validation, 63\n",
      "testing on recent production data from\n",
      "\n",
      "models, 76\n",
      "variance in models, 50\n",
      "version control systems, central, 75\n",
      "version management, 79\n",
      "\n",
      "version management and reproducibility, 26,\n",
      "\n",
      "56-58\n",
      "\n",
      "VMs (virtual machines), containers versus, 80\n",
      "\n",
      "W\n",
      "Wasserstein distance, 132\n",
      "what-if analysis, 27\n",
      "\n",
      "X\n",
      "XGBoost algorithm, 130\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "169\n",
      "\n",
      "\f",
      "About the Authors\n",
      "\n",
      "Mark Treveil has designed products in fields as diverse as telecommunications, bank‐\n",
      "ing, and online trading. His own startup led a revolution in governance in UK local\n",
      "government,  where  it  still  dominates.  He  is  now  part  of  the  Dataiku  Product  Team\n",
      "based in Paris.\n",
      "\n",
      "Nicolas Omont is VP of operations at Artelys, where he is developing mathematical\n",
      "optimization solutions for energy and transport. He previously held the role of Data‐\n",
      "iku  product  manager  for  ML  and  advanced  analytics.  He  holds  a  PhD  in  computer\n",
      "science,  and  he’s  been  working  in  operations  research  and  statistics  for  the  past  15\n",
      "years, mainly in the telecommunications and energy utility sectors.\n",
      "\n",
      "Clément  Stenac  is  a  passionate  software  engineer,  CTO,  and  cofounder  at  Dataiku.\n",
      "He oversees the design and development of the Dataiku DSS Enterprise AI Platform.\n",
      "Clément was previously head of product development at Exalead, leading the design\n",
      "and implementation of web-scale search engine software. He also has extensive expe‐\n",
      "rience with open source software, as a former developer of the VideoLAN (VLC) and\n",
      "Debian projects.\n",
      "\n",
      "Kenji Lefèvre is VP of product at Dataiku. He oversees the product road map and the\n",
      "user experience of the Dataiku DSS Enterprise AI Platform. He holds a PhD in pure\n",
      "mathematics  from  University  of  Paris  VII,  and  he  directed  documentary  movies\n",
      "before switching to data science and product management.\n",
      "\n",
      "Du Phan is a machine learning engineer at Dataiku, where he works in democratiz‐\n",
      "ing data science. In the past few years, he has been dealing with a variety of data prob‐\n",
      "lems,  from  geospatial  analysis  to  deep  learning.  His  work  now  focuses  on  different\n",
      "facets and challenges of MLOps.\n",
      "\n",
      "Joachim Zentici is an engineering director at Dataiku. Joachim graduated in applied\n",
      "mathematics  from  Ecole  Centrale  Paris.  Prior  to  joining  Dataiku  in  2014,  he  was  a\n",
      "research  engineer  in  computer  vision  at  Siemens  Molecular  Imaging  and  Inria.  He\n",
      "has also been a teacher and a lecturer. At Dataiku, Joachim has made multiple contri‐\n",
      "butions including managing the engineers in charge of the core infrastructure, build‐\n",
      "ing the team for the plug-ins and ecosystem efforts, and leading the global technology\n",
      "training program for customer-facing engineers.\n",
      "\n",
      "Adrien  Lavoillotte  is  an  engineering  director  at  Dataiku  where  he  leads  the  team\n",
      "responsible for machine learning and statistics features in the software. He studied at\n",
      "ECE Paris, a graduate school of engineering, and worked for several startups before\n",
      "joining Dataiku in 2015.\n",
      "\n",
      "\f",
      "Makoto Miyazaki is a data scientist at Dataiku and responsible for delivering hands-\n",
      "on consulting services using Dataiku DSS for European and Japanese clients. Makoto\n",
      "holds a bachelor’s degree in economics and a master’s degree in data science, and he\n",
      "was  also  a  former  financial  journalist  with  a  wide  range  of  beats,  including  nuclear\n",
      "energy and economic recoveries from the tsunami.\n",
      "\n",
      "Lynn Heidmann received her bachelor’s degree in journalism/mass communications\n",
      "and anthropology from the University of Wisconsin-Madison in 2008 and decided to\n",
      "bring  her  passion  for  research  and  writing  into  the  world  of  tech.  She  spent  seven\n",
      "years in the San Francisco Bay Area writing and running operations with Google and\n",
      "subsequently Niantic before moving to Paris to head content initiatives at Dataiku. In\n",
      "her  current  role,  Lynn  follows  and  writes  about  technological  trends  and  develop‐\n",
      "ments in the world of data and AI.\n",
      "\n",
      "Colophon\n",
      "\n",
      "The animal on the cover of Introducing MLOps is an African moth called Bunaeopsis\n",
      "oubie, also known as Zaddach’s Emperor, that can be found across central and eastern\n",
      "Africa,  from  Angola  to  Eritrea.  It  is  a  member  of  the  Saturniidae  family,  which\n",
      "includes one thousand species of the world’s largest moths.\n",
      "\n",
      "This African moth has one of the largest wingspans, stretching up to 10 inches, mak‐\n",
      "ing it bigger than some birds. Its wings have distinctive markings: one reddish brown\n",
      "circle  on  each  of  the  four  wings,  dark  brown  stripes  underneath,  and  white  strokes\n",
      "bordering  the  thorax  and  along  the  outer  edges  of  each  wing.  Moth  antennae  are\n",
      "thick  and  feathered.  Their  entire  bodies  repel  water  with  a  wax  coating  that  covers\n",
      "their hairs and the scales on their wings.\n",
      "\n",
      "Moths  tend  to  be  attracted  to  white,  fragrant  flowers,  which  they  sniff  out  easily  at\n",
      "night  and  pollinate  well  with  their  fuzzy,  sticky  bodies.  Many  animals  and  birds\n",
      "depend on moths in their diets, including owls and bats. Moth caterpillars are prey to\n",
      "lizards, birds, and many small mammals.\n",
      "\n",
      "Many of the animals on O’Reilly’s covers are endangered; all of them are important to\n",
      "the world.\n",
      "\n",
      "The cover illustration is by Karen Montgomery, based on a black and white engraving\n",
      "from  Encyclopedie  D’Histoire  Naturelle.  The  cover  fonts  are  Gilroy  Semibold  and\n",
      "Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\n",
      "Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "text=extract_text('D:\\Learnbay\\Introduction to ML ops.pdf')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3a432",
   "metadata": {},
   "source": [
    "Extract image from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbcc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\achyu\\anaconda3\\lib\\site-packages (1.22.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\achyu\\anaconda3\\lib\\site-packages (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed532292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyMuPDF\n",
    "import fitz \n",
    "    \n",
    "#pillow\n",
    "import PIL.Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a6185e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=fitz.open(\"D:\\Learnbay\\Introduction to ML ops.pdf\")\n",
    "counter=1\n",
    "\n",
    "for i in  range(len(pdf)):\n",
    "    page=pdf[i]\n",
    "    images=page.get_images()\n",
    "    for image in images:\n",
    "        base_img=pdf.extract_image(image[0])\n",
    "        image_data=base_img[\"image\"]\n",
    "        img=PIL.Image.open(io.BytesIO(image_data))\n",
    "        extension=base_img['ext']\n",
    "        img.save(open(f\"imaage{counter}.{extension}\",\"wb\"))\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ac6b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gTTS in c:\\users\\achyu\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from gTTS) (8.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from gTTS) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTS\n",
    "#gTTS - google text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "000e5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325ffc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your paragraph here:\n",
      ">Times Now is an English news channel in India owned and operated by The Times Group. The channel launched on 23 January 2006 in partnership with Reuters. It is a pay television throughout India. Until 2016, it was India's most popular and the most viewed\n"
     ]
    }
   ],
   "source": [
    "paragraph=input(\"Enter your paragraph here:\\n>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22ff2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_audio=gTTS(text=paragraph,lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb9afc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_audio.save('english.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9049ef",
   "metadata": {},
   "source": [
    "# Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7761443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechrecognition in c:\\users\\achyu\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from speechrecognition) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->speechrecognition) (3.3)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\achyu\\anaconda3\\lib\\site-packages (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install speechrecognition\n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99685b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78504bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1491922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please speak to record your voice....\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"please speak to record your voice....\")\n",
    "    my_text=abc.listen(source)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.recognize_google(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1946e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
