{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7cce97",
   "metadata": {},
   "source": [
    "# ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad312913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9be712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'the')\n",
      "('ngramize', 'the', 'given')\n",
      "('the', 'given', 'sentences')\n"
     ]
    }
   ],
   "source": [
    "user_input = 'I want to ngramize the given sentences'\n",
    "n = 3\n",
    "\n",
    "sixgrams = ngrams(user_input.split(),n)\n",
    "\n",
    "for ngrams in sixgrams:\n",
    "    print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defcc51",
   "metadata": {},
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3dd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6dbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I want to ngramize the given sentences\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = TextBlob('I want to ngramize the given sentences')\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2093fd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I']),\n",
       " WordList(['want']),\n",
       " WordList(['to']),\n",
       " WordList(['ngramize']),\n",
       " WordList(['the']),\n",
       " WordList(['given']),\n",
       " WordList(['sentences'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9122101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want']),\n",
       " WordList(['want', 'to']),\n",
       " WordList(['to', 'ngramize']),\n",
       " WordList(['ngramize', 'the']),\n",
       " WordList(['the', 'given']),\n",
       " WordList(['given', 'sentences'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5211042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want', 'to']),\n",
       " WordList(['want', 'to', 'ngramize']),\n",
       " WordList(['to', 'ngramize', 'the']),\n",
       " WordList(['ngramize', 'the', 'given']),\n",
       " WordList(['the', 'given', 'sentences'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2daac66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want', 'to', 'ngramize']),\n",
       " WordList(['want', 'to', 'ngramize', 'the']),\n",
       " WordList(['to', 'ngramize', 'the', 'given']),\n",
       " WordList(['ngramize', 'the', 'given', 'sentences'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0e363",
   "metadata": {},
   "source": [
    "# Translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778584db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinse_lang = TextBlob(\"现年93岁的王治全曾是朝鲜战俘，后来来到台湾并最终在金门作战，他在13岁时就正式参军。他的胸前纹有中华民国国旗。“我参军的时候是被押着走的。如果他们抓不到成年人，就会抓小孩。”他说。“直到今天，我都不想在电视上看到战争。战争是残酷无情的。”2024年台湾大选即将在明年1月13日举行，“中国议题”不仅影响着选民手中的选票，同时也引起了世代之间的分歧。近年来中国对台湾的主权宣示变得更具威胁性，不断增加对台军事和政治压力。随着两岸关系紧张升温，台湾选举已演变成对抗北京、展示台湾民主不屈服于威胁的选举，围绕“亲中”、“台湾主权”等议题展开持续讨论。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e75f49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"现年93岁的王治全曾是朝鲜战俘，后来来到台湾并最终在金门作战，他在13岁时就正式参军。他的胸前纹有中华民国国旗。“我参军的时候是被押着走的。如果他们抓不到成年人，就会抓小孩。”他说。“直到今天，我都不想在电视上看到战争。战争是残酷无情的。”2024年台湾大选即将在明年1月13日举行，“中国议题”不仅影响着选民手中的选票，同时也引起了世代之间的分歧。近年来中国对台湾的主权宣示变得更具威胁性，不断增加对台军事和政治压力。随着两岸关系紧张升温，台湾选举已演变成对抗北京、展示台湾民主不屈服于威胁的选举，围绕“亲中”、“台湾主权”等议题展开持续讨论。\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinse_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64020845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Wang Zhiquan, 93, was a North Korean prisoner of war. He later came to Taiwan and eventually fought in Kinmen. He officially joined the army when he was 13 years old. His chest lines are the National flag of the Republic of China. \"I was taken away when I joined the army. If they couldn't catch adults, they would catch the children.\" He said. \"Until today, I don't want to see war on TV. War is cruel and ruthless.\" The 2024 Taiwan election will be held on January 13 next year. Differential between generations. In recent years, China ’s sovereign declaration on Taiwan has become more threatened and has continuously increased military and political pressure on Taiwan. With the tension of cross -strait relations heating up, the Taiwan election has evolved into an election that fights Beijing and shows that Taiwan's democracy does not succumb to threats, and continuous discussion is carried out around issues such as \"pro -China\" and \"Taiwan sovereignty\".\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinse_lang.translate(from_lang='zh-CN', to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2eec5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"93 वर्षीय वांग ज़िकिकन युद्ध के उत्तर कोरियाई कैदी थे। वह बाद में ताइवान आए और अंततः किनमेन में लड़े। वह आधिकारिक तौर पर सेना में शामिल हो गए जब वह 13 साल के थे। उनकी छाती की रेखाएं चीन गणराज्य का राष्ट्रीय ध्वज हैं। \"जब मैं सेना में शामिल हुआ तो मुझे ले जाया गया। अगर वे वयस्कों को नहीं पकड़ सकते, तो वे बच्चों को पकड़ लेंगे।\" \"आज तक, मैं टीवी पर युद्ध नहीं देखना चाहता। युद्ध क्रूर और निर्दयी है।\" 2024 ताइवान चुनाव अगले साल 13 जनवरी को आयोजित किया जाएगा। पीढ़ियों के बीच अंतर। हाल के वर्षों में, ताइवान पर चीन की संप्रभु घोषणा अधिक खतरा हो गई है और ताइवान पर लगातार सैन्य और राजनीतिक दबाव में वृद्धि हुई है। क्रॉस -स्ट्रेट संबंधों के तनाव को गर्म करने के साथ, ताइवान चुनाव एक चुनाव में विकसित हुआ है जो बीजिंग से लड़ता है और दिखाता है कि ताइवान का लोकतंत्र खतरों के आगे नहीं झुकता है, और \"प्रो -चिना\" और \"प्रो -चिना\" और जैसे मुद्दों पर निरंतर चर्चा की जाती है, \" ताइवान संप्रभुता \"।\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinse_lang.translate(from_lang='zh-CN', to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "707c8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi = TextBlob(\"मध्य प्रदेश में मोहन यादव के बतौर मुख्यमंत्री शपथ लेने के साथ ही शिवराज सिंह चौहान के लगभग 20 साल के शासन का अंत हो गया है और भाजपा ने एक नये युग की शुरुआत कर दी है। मध्य प्रदेश, छत्तीसगढ़ और राजस्थान में भाजपा ने नया नेतृत्व चुना है। हालांकि राज्य में चौहान की ठीकठाक लोकप्रियता कायम है, मगर अब विभिन्न हित समूहों और नेताओं का नये सिरे से संयोजन संभावित है। भाजपा की बड़ी जीत का श्रेय मुख्यत: चौहान को दिया जा रहा है, जिन्होंने मतदान से पहले 165 रैलियां कीं। लेकिन उनकी यही मजबूती नये मुख्यमंत्री को अपना एक अलग रास्ता बनाने के लिए मजबूर कर सकती है। ओबीसी नेता मोहन यादव को मुख्यमंत्री और अनुसूचित जाति के नेता जगदीश देवड़ा व ब्राह्मण नेता राजेंद्र शुक्ला को उप-मुख्यमंत्री बनाकर भाजपा का लक्ष्य राज्य में अपने इंद्रधनुषी जातीय गठबंधन को मजबूत करना है, जिसमें मुसलमान साफ तौर पर बहिष्कृत हैं। इस बीच, आदिवासियों में अपनी पैठ बढ़ाने की कोशिश जारी रखते हुए, नयी सरकार ने अपने एक शुरुआती निर्णय में तेंदू पत्ता संग्रह का मेहनताना 3000 रुपये से बढ़ाकर 4000 रुपये प्रति बोरी कर दिया। यह वादा उसके घोषणापत्र में शामिल था। पार्टी को यह भी उम्मीद है कि इस मजबूत इंद्रधनुषी गठबंधन का असर हिंदी पट्टी के पड़ोसी राज्यों तक भी पहुंचेगा। मध्य प्रदेश में, अकेले ओबीसी ही आबादी के 50 फीसदी से ज्यादा है, जबकि एससी 17 फीसदी हैं। लंबे समय से भाजपा का मजबूत गढ़ रहे, राज्य के विंध्य क्षेत्र में ब्राह्मणों को रसूखदार समुदाय माना जाता है। ओबीसी जातियों की ओर भाजपा की पहलकदमियों के प्रति यादव लोग दूसरों के मुकाबले अपेक्षाकृत कम उत्साही रहे हैं। लिहाजा, मोहन यादव की नियुक्ति भाजपा की सोशल इंजीनियरिंग में एक बड़ी छलांग है।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f233eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"मध्य प्रदेश में मोहन यादव के बतौर मुख्यमंत्री शपथ लेने के साथ ही शिवराज सिंह चौहान के लगभग 20 साल के शासन का अंत हो गया है और भाजपा ने एक नये युग की शुरुआत कर दी है। मध्य प्रदेश, छत्तीसगढ़ और राजस्थान में भाजपा ने नया नेतृत्व चुना है। हालांकि राज्य में चौहान की ठीकठाक लोकप्रियता कायम है, मगर अब विभिन्न हित समूहों और नेताओं का नये सिरे से संयोजन संभावित है। भाजपा की बड़ी जीत का श्रेय मुख्यत: चौहान को दिया जा रहा है, जिन्होंने मतदान से पहले 165 रैलियां कीं। लेकिन उनकी यही मजबूती नये मुख्यमंत्री को अपना एक अलग रास्ता बनाने के लिए मजबूर कर सकती है। ओबीसी नेता मोहन यादव को मुख्यमंत्री और अनुसूचित जाति के नेता जगदीश देवड़ा व ब्राह्मण नेता राजेंद्र शुक्ला को उप-मुख्यमंत्री बनाकर भाजपा का लक्ष्य राज्य में अपने इंद्रधनुषी जातीय गठबंधन को मजबूत करना है, जिसमें मुसलमान साफ तौर पर बहिष्कृत हैं। इस बीच, आदिवासियों में अपनी पैठ बढ़ाने की कोशिश जारी रखते हुए, नयी सरकार ने अपने एक शुरुआती निर्णय में तेंदू पत्ता संग्रह का मेहनताना 3000 रुपये से बढ़ाकर 4000 रुपये प्रति बोरी कर दिया। यह वादा उसके घोषणापत्र में शामिल था। पार्टी को यह भी उम्मीद है कि इस मजबूत इंद्रधनुषी गठबंधन का असर हिंदी पट्टी के पड़ोसी राज्यों तक भी पहुंचेगा। मध्य प्रदेश में, अकेले ओबीसी ही आबादी के 50 फीसदी से ज्यादा है, जबकि एससी 17 फीसदी हैं। लंबे समय से भाजपा का मजबूत गढ़ रहे, राज्य के विंध्य क्षेत्र में ब्राह्मणों को रसूखदार समुदाय माना जाता है। ओबीसी जातियों की ओर भाजपा की पहलकदमियों के प्रति यादव लोग दूसरों के मुकाबले अपेक्षाकृत कम उत्साही रहे हैं। लिहाजा, मोहन यादव की नियुक्ति भाजपा की सोशल इंजीनियरिंग में एक बड़ी छलांग है।\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91db1d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"মধ্য প্রদেশে মুখ্যমন্ত্রী হিসাবে মোহন যাদবের শপথ নিয়ে শিবরাজ সিং চৌহান প্রায় ২০ বছরের শাসনের অবসান ঘটিয়েছেন এবং বিজেপি একটি নতুন যুগ শুরু করেছে। মধ্য প্রদেশ, ছত্তিশগড় ও রাজস্থানে বিজেপি নতুন নেতৃত্ব বেছে নিয়েছে। যদিও চৌহানের যথাযথ জনপ্রিয়তা রাজ্যে বজায় রয়েছে, এখন বিভিন্ন আগ্রহী গোষ্ঠী এবং নেতাদের একটি নতুন সংমিশ্রণ সম্ভবত। বিজেপির প্রধান বিজয়ের কৃতিত্ব মূলত চৌহানকে দেওয়া হচ্ছে, যিনি ভোট দেওয়ার আগে ১5৫ টি সমাবেশ করেছিলেন। তবে এই শক্তি নতুন মুখ্যমন্ত্রীকে অন্যরকমভাবে তৈরি করতে বাধ্য করতে পারে। ওবিসির নেতা মোহন যাদবকে মুখ্যমন্ত্রী হিসাবে এবং তফসিলি বর্ণের নেতা জগদীশ দেওরা এবং ব্রাহ্মণ নেতা রাজেন্দ্র শুক্লা উপ -মুখ্যমন্ত্রী হিসাবে, বিজেপি লক্ষ্যটি রাজ্যে তাদের রংধনু জাতিগত জোটকে শক্তিশালী করা, যেখানে মুসলিমরা স্পষ্টতই অপসারণ করা হয়েছে। এদিকে, আদিবাসীদের মধ্যে অনুপ্রবেশ বাড়ানোর প্রচেষ্টা অব্যাহত রাখার সময়, নতুন সরকার তার প্রাথমিক সিদ্ধান্তগুলির একটিতে টেন্ডু পাতার সংগ্রহের পারিশ্রমিক 3000 থেকে 4000 রুপি থেকে 4000 রুপি বাড়িয়েছে। এই প্রতিশ্রুতি তার ইশতেহারে জড়িত ছিল। দলটিও আশাবাদী যে এই শক্তিশালী রেইনবো জোটের প্রভাব হিন্দি বেল্টের পার্শ্ববর্তী রাজ্যেও পৌঁছে যাবে। মধ্য প্রদেশে, একমাত্র ওবিসি জনসংখ্যার ৫০ শতাংশেরও বেশি, এবং এসসি ১ 17 শতাংশ। দীর্ঘকাল ধরে, বিজেপির শক্তিশালী দুর্গ, ব্রাহ্মণরা রাজ্যের বিন্ধ্য অঞ্চলে প্রভাবশালী সম্প্রদায় হিসাবে বিবেচিত হয়। যাদব জনগণ ওবিসি বর্ণের প্রতি বিজেপির উদ্যোগের প্রতি অন্যদের তুলনায় তুলনামূলকভাবে কম উত্সাহী ছিল। সুতরাং, মোহন যাদব নিয়োগ বিজেপির সামাজিক প্রকৌশল ক্ষেত্রে একটি বড় লাফ।\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi.translate(from_lang='hi',to='bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba44e9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"モハン・ヤダブがマディヤ・プラデシュ州の首相として宣誓されたため、シヴラジ・シン・チャウハンは20年近くの支配を終えており、BJPは新しい時代を開始しました。マディヤ・プラデシュ州、チャッティースガル、ラジャスタン州では、BJPが新しいリーダーシップを選択しました。チャウハンの適切な人気は州で維持されていますが、今ではさまざまな利益団体とリーダーの新鮮な組み合わせが可能性があります。 BJPの主要な勝利の功績は、主に投票前に165の集会を開催したチャウハンに与えられています。しかし、この強さは、新しい首相に別の方法を作るように強制することができます。 OBCのリーダーであるモハン・ヤダブを首相として、カーストのリーダーであるジャグディッシュ・デオラとバラモンのリーダーであるラジェンドラ・シュクラを副首相として予定することにより、BJPの目的は、イスラム教徒が明らかに追放されている州での虹の民族同盟を強化することです。一方、部族間の浸透を増やす努力を継続している間、新政府は、最初の決定の1つで、袋あたり3000ルピーから4000ルピーに葉の葉のコレクションの報酬を増やしました。この約束は彼のマニフェストに関与していました。党はまた、この強力なレインボーアライアンスの影響がヒンディー語のベルトの近隣州にも到達することを望んでいます。マディヤプラデシュ州では、OBCだけが人口の50％以上、SCは17％です。長い間、BJPの強力な拠点であるブラフミンは、州のヴィンディヤ地域の影響力のあるコミュニティと見なされています。ヤダブの人々は、OBCカーストに対するBJPのイニシアチブに向けて他の人々よりも比較的熱心ではありません。したがって、Mohan Yadavの任命は、BJPのソーシャルエンジニアリングの大きなジャンプです。\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi.translate(from_lang='hi',to='ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8bc25fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"మధ్యప్రదేశ్‌లో ముఖ్యమంత్రిగా మోహన్ యాదవ్ ప్రమాణం చేయడంతో, శివరాజ్ సింగ్ చౌహాన్ దాదాపు 20 సంవత్సరాల పాలన కోసం ముగించారు మరియు బిజెపి కొత్త యుగాన్ని ప్రారంభించింది. మధ్యప్రదేశ్, ఛత్తీస్‌గ h ్, రాజస్థాన్‌లో బిజెపి కొత్త నాయకత్వాన్ని ఎంచుకుంది. చౌహాన్ యొక్క సరైన ప్రజాదరణ రాష్ట్రంలో నిర్వహించబడుతున్నప్పటికీ, ఇప్పుడు వివిధ ఆసక్తి సమూహాలు మరియు నాయకుల తాజా కలయిక అవకాశం ఉంది. బిజెపి యొక్క ప్రధాన విజయానికి క్రెడిట్ ప్రధానంగా ఓటు వేయడానికి ముందు 165 ర్యాలీలు నిర్వహించిన చౌహన్‌కు ఇవ్వబడింది. కానీ ఈ బలం కొత్త ముఖ్యమంత్రిని వేరే విధంగా చేయమని బలవంతం చేస్తుంది. OBC నాయకుడు మోహన్ యాదవ్‌ను ముఖ్యమంత్రిగా మరియు షెడ్యూల్ చేసిన కుల నాయకుడు జగదీష్ డియోరా మరియు బ్రాహ్మణ నాయకుడు రాజేంద్ర షుక్లాను ఉప ముఖ్యమంత్రిగా మార్చడం ద్వారా, బిజెపి లక్ష్యం రాష్ట్రంలో తమ ఇంద్రధనస్సు జాతి కూటమిని బలోపేతం చేయడం, ఇందులో ముస్లింలు స్పష్టంగా బహిష్కరించబడ్డారు. ఇంతలో, గిరిజనులలో తన ప్రవేశాన్ని పెంచడానికి నిరంతర ప్రయత్నాలు చేస్తున్నప్పుడు, కొత్త ప్రభుత్వం టెండూ ఆకు సేకరణ యొక్క వేతనం దాని ప్రారంభ నిర్ణయాలలో ఒకదానిలో రూ .3000 నుండి రూ .4000 కు రూ .4000 కు పెరిగింది. ఈ వాగ్దానం అతని మ్యానిఫెస్టోలో పాల్గొంది. ఈ బలమైన ఇంద్రధనస్సు కూటమి యొక్క ప్రభావం పొరుగున ఉన్న హిందీ బెల్ట్‌కు కూడా చేరుకుంటుందని పార్టీ భావిస్తోంది. మధ్యప్రదేశ్‌లో, ఓబిసి మాత్రమే జనాభాలో 50 శాతానికి పైగా ఉండగా, ఎస్సీ 17 శాతం. చాలా కాలంగా, బిజెపి యొక్క బలమైన బలమైన కోట, బ్రాహ్మణులను రాష్ట్రంలోని వింధ్య ప్రాంతంలో ప్రభావవంతమైన వర్గాలుగా భావిస్తారు. ఓబిసి కులాల పట్ల బిజెపి చేసిన కార్యక్రమాల పట్ల యాదవ్ ప్రజలు ఇతరులకన్నా తక్కువ ఉత్సాహంగా ఉన్నారు. అందువల్ల, మోహన్ యాదవ్ నియామకం బిజెపి యొక్క సోషల్ ఇంజనీరింగ్‌లో పెద్ద జంప్.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi.translate(from_lang='hi',to='te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b42829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"மத்திய பிரதேசத்தின் முதல்வராக மோகன் யாதவ் சத்தியப்பிரமாணத்துடன், சிவ்ராஜ் சிங் சவுகான் கிட்டத்தட்ட 20 ஆண்டுகால ஆட்சிக்காக முடிவுக்கு வந்துள்ளார், பாஜக ஒரு புதிய சகாப்தத்தைத் தொடங்கியுள்ளது. மத்திய பிரதேசம், சத்தீஸ்கர் மற்றும் ராஜஸ்தான் ஆகிய நாடுகளில் பாஜக புதிய தலைமையைத் தேர்ந்தெடுத்துள்ளனர். சவுகானின் சரியான புகழ் மாநிலத்தில் பராமரிக்கப்பட்டாலும், இப்போது பல்வேறு வட்டி குழுக்கள் மற்றும் தலைவர்களின் புதிய கலவையாகும். பாஜகவின் முக்கிய வெற்றியின் கடன் முக்கியமாக வாக்களிப்பதற்கு முன்பு 165 பேரணிகளை நடத்திய சவுகானுக்கு வழங்கப்படுகிறது. ஆனால் இந்த வலிமை புதிய முதலமைச்சரை வேறு வழியை உருவாக்க கட்டாயப்படுத்தும். ஓபிசி தலைவர் மோகன் யாதவ் முதலமைச்சராகவும், திட்டமிடப்பட்ட சாதித் தலைவர் ஜகதீஷ் தியோராவாகவும், பிராமண தலைவர் ராஜேந்திர சுக்லாவும் துணை முதல்வராக மாற்றுவதன் மூலம், பாஜக நோக்கம் மாநிலத்தில் தங்கள் வானவில் இன கூட்டணியை வலுப்படுத்துவதாகும், அதில் முஸ்லிம்கள் தெளிவாக ஒதுக்கித் தள்ளப்படுகிறார்கள். இதற்கிடையில், பழங்குடியினரிடையே அதன் ஊடுருவலை அதிகரிப்பதற்கான தொடர்ச்சியான முயற்சிகள், புதிய அரசாங்கம் அதன் ஆரம்ப முடிவுகளில் ஒன்றில் டெண்டு இலை சேகரிப்பின் ஊதியத்தை ஒரு சாக்குக்கு ரூ .3000 இலிருந்து ரூ .4000 ஆக உயர்த்தியது. இந்த வாக்குறுதி அவரது அறிக்கையில் ஈடுபட்டது. இந்த வலுவான ரெயின்போ கூட்டணியின் தாக்கம் இந்தி பெல்ட்டின் அண்டை மாநிலங்களையும் எட்டும் என்றும் கட்சி நம்புகிறது. மத்திய பிரதேசத்தில், ஓபிசி மட்டும் மக்கள்தொகையில் 50 சதவீதத்திற்கும் அதிகமாக உள்ளது, எஸ்சி 17 சதவீதம். நீண்ட காலமாக, பாஜகவின் வலுவான கோட்டையான பிராமணர்கள் மாநிலத்தின் விந்த்ய பிராந்தியத்தில் செல்வாக்கு மிக்க சமூகங்களாகக் கருதப்படுகிறார்கள். ஓபிசி சாதிகளை நோக்கிய பாஜகவின் முன்முயற்சிகளை நோக்கி யாதவ் மக்கள் மற்றவர்களை விட ஒப்பீட்டளவில் குறைந்த ஆர்வத்துடன் உள்ளனர். எனவே, மோகன் யாதவ் நியமனம் பாஜகவின் சமூக பொறியியலில் ஒரு பெரிய தாவலாகும்.\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi.translate(from_lang='hi',to='ta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7aaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "819d6fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I haave too say thatt yoou haav a goood knodlege about Engliish\n",
      "**********\n",
      "I have too say that you had a good knowledge about English\n"
     ]
    }
   ],
   "source": [
    "sentence = TextBlob(\"I haave too say thatt yoou haav a goood knodlege about Engliish\")\n",
    "print(sentence)\n",
    "print(\"**\"*5)\n",
    "print(sentence.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c93ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf0d672",
   "metadata": {},
   "source": [
    "# PDF - Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309166d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9328322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a327dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = extract_text(\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\E-Book\\\\ML.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da6cf621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python \n",
      "Machine \n",
      "Learning \n",
      "Cookbook\n",
      "\n",
      "PRACTICAL SOLUTIONS FROM PREPROCESSING TO DEEP LEARNING\n",
      "\n",
      "Chris Albon\n",
      "\n",
      "\f",
      "Machine Learning with\n",
      "Python Cookbook\n",
      "Practical Solutions from Preprocessing\n",
      "to Deep Learning\n",
      "\n",
      "Chris Albon\n",
      "\n",
      "\f",
      "Machine Learning with Python Cookbook\n",
      "by Chris Albon\n",
      "\n",
      "Copyright © 2018 Chris Albon. All rights reserved.\n",
      "\n",
      "Printed in the United States of America.\n",
      "\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐\n",
      "tutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "\n",
      "Editors: Rachel Roumeliotis and Jeff Bleiel\n",
      "Production Editor: Melanie Yarbrough\n",
      "Copyeditor: Kim Cofer\n",
      "Proofreader: Rachel Monaghan\n",
      "\n",
      "Indexer: Wendy Catalano\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Rebecca Demarest\n",
      "\n",
      "April 2018:\n",
      "\n",
      " First Edition\n",
      "\n",
      "Revision History for the First Edition\n",
      "2018-03-09:  First Release\n",
      "\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781491989388 for release details.\n",
      "\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Machine Learning with Python Cook‐\n",
      "book, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n",
      "\n",
      "While  the  publisher  and  the  author  have  used  good  faith  efforts  to  ensure  that  the  information  and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\n",
      "licenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\n",
      "thereof complies with such licenses and/or rights.\n",
      "\n",
      "978-1-491-98938-8\n",
      "\n",
      "[LSI]\n",
      "\n",
      "\f",
      "Table of Contents\n",
      "\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xi\n",
      "\n",
      "1. Vectors, Matrices, and Arrays. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "1.0 Introduction                                                                                                                  1\n",
      "1.1 Creating a Vector                                                                                                          1\n",
      "1.2 Creating a Matrix                                                                                                          2\n",
      "1.3 Creating a Sparse Matrix                                                                                             3\n",
      "1.4 Selecting Elements                                                                                                        4\n",
      "1.5 Describing a Matrix                                                                                                      6\n",
      "1.6 Applying Operations to Elements                                                                              6\n",
      "1.7 Finding the Maximum and Minimum Values                                                         7\n",
      "1.8 Calculating the Average, Variance, and Standard Deviation                                 8\n",
      "1.9 Reshaping Arrays                                                                                                          9\n",
      "1.10 Transposing a Vector or Matrix                                                                             10\n",
      "1.11 Flattening a Matrix                                                                                                   11\n",
      "1.12 Finding the Rank of a Matrix                                                                                 12\n",
      "1.13 Calculating the Determinant                                                                                  12\n",
      "1.14 Getting the Diagonal of a Matrix                                                                           13\n",
      "1.15 Calculating the Trace of a Matrix                                                                           14\n",
      "1.16 Finding Eigenvalues and Eigenvectors                                                                  15\n",
      "1.17 Calculating Dot Products                                                                                        16\n",
      "1.18 Adding and Subtracting Matrices                                                                          17\n",
      "1.19 Multiplying Matrices                                                                                               18\n",
      "1.20 Inverting a Matrix                                                                                                    19\n",
      "1.21 Generating Random Values                                                                                    20\n",
      "\n",
      "2. Loading Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   23\n",
      "2.0 Introduction                                                                                                                23\n",
      "\n",
      "iii\n",
      "\n",
      "\f",
      "2.1 Loading a Sample Dataset                                                                                         23\n",
      "2.2 Creating a Simulated Dataset                                                                                    24\n",
      "2.3 Loading a CSV File                                                                                                     27\n",
      "2.4 Loading an Excel File                                                                                                 28\n",
      "2.5 Loading a JSON File                                                                                                   29\n",
      "2.6 Querying a SQL Database                                                                                         30\n",
      "\n",
      "3. Data Wrangling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   33\n",
      "3.0 Introduction                                                                                                                33\n",
      "3.1 Creating a Data Frame                                                                                               34\n",
      "3.2 Describing the Data                                                                                                   35\n",
      "3.3 Navigating DataFrames                                                                                             37\n",
      "3.4 Selecting Rows Based on Conditionals                                                                   38\n",
      "3.5 Replacing Values                                                                                                         39\n",
      "3.6 Renaming Columns                                                                                                   41\n",
      "3.7 Finding the Minimum, Maximum, Sum, Average, and Count                           42\n",
      "3.8 Finding Unique Values                                                                                              43\n",
      "3.9 Handling Missing Values                                                                                          44\n",
      "3.10 Deleting a Column                                                                                                   46\n",
      "3.11 Deleting a Row                                                                                                         47\n",
      "3.12 Dropping Duplicate Rows                                                                                      48\n",
      "3.13 Grouping Rows by Values                                                                                       50\n",
      "3.14 Grouping Rows by Time                                                                                         51\n",
      "3.15 Looping Over a Column                                                                                         53\n",
      "3.16 Applying a Function Over All Elements in a Column                                        54\n",
      "3.17 Applying a Function to Groups                                                                             55\n",
      "3.18 Concatenating DataFrames                                                                                    55\n",
      "3.19 Merging DataFrames                                                                                               57\n",
      "\n",
      "4. Handling Numerical Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   61\n",
      "4.0 Introduction                                                                                                                61\n",
      "4.1 Rescaling a Feature                                                                                                     61\n",
      "4.2 Standardizing a Feature                                                                                             63\n",
      "4.3 Normalizing Observations                                                                                        64\n",
      "4.4 Generating Polynomial and Interaction Features                                                 66\n",
      "4.5 Transforming Features                                                                                              68\n",
      "4.6 Detecting Outliers                                                                                                      69\n",
      "4.7 Handling Outliers                                                                                                      71\n",
      "4.8 Discretizating Features                                                                                              73\n",
      "4.9 Grouping Observations Using Clustering                                                              74\n",
      "4.10 Deleting Observations with Missing Values                                                        76\n",
      "4.11 Imputing Missing Values                                                                                        78\n",
      "\n",
      "iv \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\f",
      "5. Handling Categorical Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   81\n",
      "5.0 Introduction                                                                                                                81\n",
      "5.1 Encoding Nominal Categorical Features                                                                82\n",
      "5.2 Encoding Ordinal Categorical Features                                                                  84\n",
      "5.3 Encoding Dictionaries of Features                                                                           86\n",
      "5.4 Imputing Missing Class Values                                                                                88\n",
      "5.5 Handling Imbalanced Classes                                                                                  90\n",
      "\n",
      "6. Handling Text. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\n",
      "6.0 Introduction                                                                                                                95\n",
      "6.1 Cleaning Text                                                                                                              95\n",
      "6.2 Parsing and Cleaning HTML                                                                                   97\n",
      "6.3 Removing Punctuation                                                                                              98\n",
      "6.4 Tokenizing Text                                                                                                          98\n",
      "6.5 Removing Stop Words                                                                                               99\n",
      "6.6 Stemming Words                                                                                                      100\n",
      "6.7 Tagging Parts of Speech                                                                                           101\n",
      "6.8 Encoding Text as a Bag of Words                                                                           104\n",
      "6.9 Weighting Word Importance                                                                                  106\n",
      "\n",
      "7. Handling Dates and Times. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   109\n",
      "7.0 Introduction                                                                                                              109\n",
      "7.1 Converting Strings to Dates                                                                                    109\n",
      "7.2 Handling Time Zones                                                                                              111\n",
      "7.3 Selecting Dates and Times                                                                                      112\n",
      "7.4 Breaking Up Date Data into Multiple Features                                                    113\n",
      "7.5 Calculating the Difference Between Dates                                                           114\n",
      "7.6 Encoding Days of the Week                                                                                    115\n",
      "7.7 Creating a Lagged Feature                                                                                      116\n",
      "7.8 Using Rolling Time Windows                                                                                117\n",
      "7.9 Handling Missing Data in Time Series                                                                 118\n",
      "\n",
      "8. Handling Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   121\n",
      "8.0 Introduction                                                                                                              121\n",
      "8.1 Loading Images                                                                                                        122\n",
      "8.2 Saving Images                                                                                                           124\n",
      "8.3 Resizing Images                                                                                                        125\n",
      "8.4 Cropping Images                                                                                                      126\n",
      "8.5 Blurring Images                                                                                                        128\n",
      "8.6 Sharpening Images                                                                                                   131\n",
      "8.7 Enhancing Contrast                                                                                                 133\n",
      "8.8 Isolating Colors                                                                                                        135\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "v\n",
      "\n",
      "\f",
      "8.9 Binarizing Images                                                                                                    137\n",
      "8.10 Removing Backgrounds                                                                                        140\n",
      "8.11 Detecting Edges                                                                                                      144\n",
      "8.12 Detecting Corners                                                                                                  146\n",
      "8.13 Creating Features for Machine Learning                                                            150\n",
      "8.14 Encoding Mean Color as a Feature                                                                      152\n",
      "8.15 Encoding Color Histograms as Features                                                            153\n",
      "\n",
      "9. Dimensionality Reduction Using Feature Extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . .   157\n",
      "9.0 Introduction                                                                                                              157\n",
      "9.1 Reducing Features Using Principal Components                                                158\n",
      "9.2 Reducing Features When Data Is Linearly Inseparable                                      160\n",
      "9.3 Reducing Features by Maximizing Class Separability                                        162\n",
      "9.4 Reducing Features Using Matrix Factorization                                                   165\n",
      "9.5 Reducing Features on Sparse Data                                                                        166\n",
      "\n",
      "10. Dimensionality Reduction Using Feature Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . .   169\n",
      "10.0 Introduction                                                                                                            169\n",
      "10.1 Thresholding Numerical Feature Variance                                                        170\n",
      "10.2 Thresholding Binary Feature Variance                                                               171\n",
      "10.3 Handling Highly Correlated Features                                                                 172\n",
      "10.4 Removing Irrelevant Features for Classification                                               174\n",
      "10.5 Recursively Eliminating Features                                                                        176\n",
      "\n",
      "11. Model Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      " 179\n",
      "11.0 Introduction                                                                                                            179\n",
      "11.1 Cross-Validating Models                                                                                       179\n",
      "11.2 Creating a Baseline Regression Model                                                                183\n",
      "11.3 Creating a Baseline Classification Model                                                           184\n",
      "11.4 Evaluating Binary Classifier Predictions                                                            186\n",
      "11.5 Evaluating Binary Classifier Thresholds                                                             189\n",
      "11.6 Evaluating Multiclass Classifier Predictions                                                      192\n",
      "11.7 Visualizing a Classifier’s Performance                                                                194\n",
      "11.8 Evaluating Regression Models                                                                             196\n",
      "11.9 Evaluating Clustering Models                                                                              198\n",
      "11.10 Creating a Custom Evaluation Metric                                                              199\n",
      "11.11 Visualizing the Effect of Training Set Size                                                        201\n",
      "11.12 Creating a Text Report of Evaluation Metrics                                                  203\n",
      "11.13 Visualizing the Effect of Hyperparameter Values                                           205\n",
      "\n",
      "12. Model Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   209\n",
      "12.0 Introduction                                                                                                            209\n",
      "\n",
      "vi \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\f",
      "12.1 Selecting Best Models Using Exhaustive Search                                                210\n",
      "12.2 Selecting Best Models Using Randomized Search                                            212\n",
      "12.3 Selecting Best Models from Multiple Learning Algorithms                            214\n",
      "12.4 Selecting Best Models When Preprocessing                                                      215\n",
      "12.5 Speeding Up Model Selection with Parallelization                                           217\n",
      "12.6 Speeding Up Model Selection Using Algorithm-Specific Methods                219\n",
      "12.7 Evaluating Performance After Model Selection                                                220\n",
      "\n",
      "13. Linear Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   223\n",
      "13.0 Introduction                                                                                                            223\n",
      "13.1 Fitting a Line                                                                                                           223\n",
      "13.2 Handling Interactive Effects                                                                                 225\n",
      "13.3 Fitting a Nonlinear Relationship                                                                         227\n",
      "13.4 Reducing Variance with Regularization                                                             229\n",
      "13.5 Reducing Features with Lasso Regression                                                          231\n",
      "\n",
      "14. Trees and Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  233\n",
      "14.0 Introduction                                                                                                            233\n",
      "14.1 Training a Decision Tree Classifier                                                                      233\n",
      "14.2 Training a Decision Tree Regressor                                                                     235\n",
      "14.3 Visualizing a Decision Tree Model                                                                      236\n",
      "14.4 Training a Random Forest Classifier                                                                   238\n",
      "14.5 Training a Random Forest Regressor                                                                  240\n",
      "14.6 Identifying Important Features in Random Forests                                         241\n",
      "14.7 Selecting Important Features in Random Forests                                             243\n",
      "14.8 Handling Imbalanced Classes                                                                              245\n",
      "14.9 Controlling Tree Size                                                                                             246\n",
      "14.10 Improving Performance Through Boosting                                                    247\n",
      "14.11 Evaluating Random Forests with Out-of-Bag Errors                                     249\n",
      "\n",
      "15. K-Nearest Neighbors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\n",
      "15.0 Introduction                                                                                                            251\n",
      "15.1 Finding an Observation’s Nearest Neighbors                                                     251\n",
      "15.2 Creating a K-Nearest Neighbor Classifier                                                          254\n",
      "15.3 Identifying the Best Neighborhood Size                                                             256\n",
      "15.4 Creating a Radius-Based Nearest Neighbor Classifier                                     257\n",
      "\n",
      "16. Logistic Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   259\n",
      "16.0 Introduction                                                                                                            259\n",
      "16.1 Training a Binary Classifier                                                                                  259\n",
      "16.2 Training a Multiclass Classifier                                                                            261\n",
      "16.3 Reducing Variance Through Regularization                                                      262\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "vii\n",
      "\n",
      "\f",
      "16.4 Training a Classifier on Very Large Data                                                            263\n",
      "16.5 Handling Imbalanced Classes                                                                              264\n",
      "\n",
      "17. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   267\n",
      "17.0 Introduction                                                                                                            267\n",
      "17.1 Training a Linear Classifier                                                                                  267\n",
      "17.2 Handling Linearly Inseparable Classes Using Kernels                                     270\n",
      "17.3 Creating Predicted Probabilities                                                                          274\n",
      "17.4 Identifying Support Vectors                                                                                 276\n",
      "17.5 Handling Imbalanced Classes                                                                              277\n",
      "\n",
      "18. Naive Bayes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   279\n",
      "18.0 Introduction                                                                                                            279\n",
      "18.1 Training a Classifier for Continuous Features                                                   280\n",
      "18.2 Training a Classifier for Discrete and Count Features                                     282\n",
      "18.3 Training a Naive Bayes Classifier for Binary Features                                      283\n",
      "18.4 Calibrating Predicted Probabilities                                                                     284\n",
      "\n",
      "19. Clustering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   287\n",
      "19.0 Introduction                                                                                                            287\n",
      "19.1 Clustering Using K-Means                                                                                   287\n",
      "19.2 Speeding Up K-Means Clustering                                                                       290\n",
      "19.3 Clustering Using Meanshift                                                                                  291\n",
      "19.4 Clustering Using DBSCAN                                                                                  292\n",
      "19.5 Clustering Using Hierarchical Merging                                                              294\n",
      "\n",
      "20. Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   297\n",
      "20.0 Introduction                                                                                                            297\n",
      "20.1 Preprocessing Data for Neural Networks                                                           298\n",
      "20.2 Designing a Neural Network                                                                                300\n",
      "20.3 Training a Binary Classifier                                                                                  303\n",
      "20.4 Training a Multiclass Classifier                                                                            305\n",
      "20.5 Training a Regressor                                                                                              307\n",
      "20.6 Making Predictions                                                                                                309\n",
      "20.7 Visualize Training History                                                                                    310\n",
      "20.8 Reducing Overfitting with Weight Regularization                                            313\n",
      "20.9 Reducing Overfitting with Early Stopping                                                         315\n",
      "20.10 Reducing Overfitting with Dropout                                                                 317\n",
      "20.11 Saving Model Training Progress                                                                        319\n",
      "20.12 k-Fold Cross-Validating Neural Networks                                                       321\n",
      "20.13 Tuning Neural Networks                                                                                    322\n",
      "20.14 Visualizing Neural Networks                                                                             325\n",
      "\n",
      "viii \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\f",
      "20.15 Classifying Images                                                                                               327\n",
      "20.16 Improving Performance with Image Augmentation                                      331\n",
      "20.17 Classifying Text                                                                                                    333\n",
      "\n",
      "21. Saving and Loading Trained Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  337\n",
      "21.0 Introduction                                                                                                            337\n",
      "21.1 Saving and Loading a scikit-learn Model                                                           337\n",
      "21.2 Saving and Loading a Keras Model                                                                     339\n",
      "\n",
      "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   341\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "ix\n",
      "\n",
      "\f",
      "\f",
      "Preface\n",
      "\n",
      "Over the last few years machine learning has become embedded in a wide variety of\n",
      "day-to-day  business,  nonprofit,  and  government  operations.  As  the  popularity  of\n",
      "machine learning increased, a cottage industry of high-quality literature that taught\n",
      "applied machine learning to practitioners developed. This literature has been highly\n",
      "successful  in  training  an  entire  generation  of  data  scientists  and  machine  learning\n",
      "engineers.  This  literature  also  approached  the  topic  of  machine  learning  from  the\n",
      "perspective  of  providing  a  learning  resource  to  teach  an  individual  what  machine\n",
      "learning is and how it works. However, while fruitful, this approach left out a differ‐\n",
      "ent perspective on the topic: the nuts and bolts of doing machine learning day to day.\n",
      "That  is  the  motivation  of  this  book—not  as  a  tome  of  machine  learning  knowledge\n",
      "for the student but as a wrench for the professional, to sit with dog-eared pages on\n",
      "desks ready to solve the practical day-to-day problems of a machine learning practi‐\n",
      "tioner.\n",
      "\n",
      "More  specifically,  the  book  takes  a  task-based  approach  to  machine  learning,  with\n",
      "almost  200  self-contained  solutions  (you  can  copy  and  paste  the  code  and  it’ll  run)\n",
      "for the most common tasks a data scientist or machine learning engineer building a\n",
      "model will run into.\n",
      "\n",
      "The ultimate goal is for the book to be a reference for people building real machine\n",
      "learning systems. For example, imagine a reader has a JSON file containing 1,000 cat‐\n",
      "egorical and numerical features with missing data and categorical target vectors with\n",
      "imbalanced classes, and wants an interpretable model. The motivation for this book is\n",
      "to provide recipes to help the reader learn processes such as:\n",
      "\n",
      "• 2.5 Loading a JSON file\n",
      "\n",
      "• 4.2 Standardizing a Feature\n",
      "\n",
      "• 5.3 Encoding Dictionaries of Features\n",
      "\n",
      "• 5.4 Imputing Missing Class Values\n",
      "\n",
      "xi\n",
      "\n",
      "\f",
      "• 9.1 Reducing Features Using Principal Components\n",
      "\n",
      "• 12.2 Selecting Best Models Using Randomized Search\n",
      "\n",
      "• 14.4 Training a Random Forest Classifier\n",
      "\n",
      "• 14.7 Selecting Random Features in Random Forests\n",
      "\n",
      "The goal is for the reader to be able to:\n",
      "\n",
      "1. Copy/paste the code and gain confidence that it actually works with the included\n",
      "\n",
      "toy dataset.\n",
      "\n",
      "2. Read the discussion to gain an understanding of the theory behind the technique\n",
      "\n",
      "the code is executing and learn which parameters are important to consider.\n",
      "\n",
      "3. Insert/combine/adapt the code from the recipes to construct the actual applica‐\n",
      "\n",
      "tion.\n",
      "\n",
      "Who This Book Is For\n",
      "This book is not an introduction to machine learning. If you are not comfortable with\n",
      "the  basic  concepts  of  machine  learning  or  have  never  spent  time  learning  machine\n",
      "learning, do not buy this book. Instead, this book is for the machine learning practi‐\n",
      "tioner  who,  while  comfortable  with  the  theory  and  concepts  of  machine  learning,\n",
      "would benefit from a quick reference containing code to solve challenges he runs into\n",
      "working on machine learning on an everyday basis.\n",
      "\n",
      "This book assumes the reader is comfortable with the Python programming language\n",
      "and package management.\n",
      "\n",
      "Who This Book Is Not For\n",
      "As stated previously, this book is not an introduction to machine learning. This book\n",
      "should  not  be  your  first.  If  you  are  unfamiliar  with  concepts  like  cross-validation,\n",
      "random  forest,  and  gradient  descent,  you  will  likely  not  benefit  from  this  book  as\n",
      "much as one of the many high-quality texts specifically designed to introduce you to\n",
      "the  topic.  I  recommend  reading  one  of  those  books  and  then  coming  back  to  this\n",
      "book to learn working, practical solutions for machine learning.\n",
      "\n",
      "Terminology Used in This Book\n",
      "Machine learning draws upon techniques from a wide range of fields, including com‐\n",
      "puter science, statistics, and mathematics. For this reason, there is significant varia‐\n",
      "tion in the terminology used in the discussions of machine learning:\n",
      "\n",
      "xii \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\f",
      "Observation\n",
      "\n",
      "A  single  unit  in  our  level  of  observation—for  example,  a  person,  a  sale,  or  a\n",
      "record.\n",
      "\n",
      "Learning algorithms\n",
      "\n",
      "An algorithm used to learn the best parameters of a model—for example, linear\n",
      "regression, naive Bayes, or decision trees.\n",
      "\n",
      "Models\n",
      "\n",
      "An output of a learning algorithm’s training. Learning algorithms train models,\n",
      "which we then use to make predictions.\n",
      "\n",
      "Parameters\n",
      "\n",
      "The weights or coefficients of a model learned through training.\n",
      "\n",
      "Hyperparameters\n",
      "\n",
      "The settings of a learning algorithm that need to be set before training.\n",
      "\n",
      "Performance\n",
      "\n",
      "A metric used to evaluate a model.\n",
      "\n",
      "Loss\n",
      "\n",
      "A metric to maximize or minimize through training.\n",
      "\n",
      "Train\n",
      "\n",
      "Applying a learning algorithm to data using numerical approaches like gradient\n",
      "descent.\n",
      "\n",
      "Fit\n",
      "\n",
      "Applying a learning algorithm to data using analytical approaches.\n",
      "\n",
      "Data\n",
      "\n",
      "A collection of observations.\n",
      "\n",
      "Acknowledgments\n",
      "This book would not have been possible without the kind help of a number of friends\n",
      "and strangers. Listing everyone who lent a hand to this project would be impossible,\n",
      "but  I  wanted  to  at  least  mention:  Angela  Bassa,  Teresa  Borcuch,  Justin  Bozonier,\n",
      "Andre deBruin, Numa Dhamani, Dan Friedman, Joel Grus, Sarah Guido, Bill Kam‐\n",
      "bouroglou, Mat Kelcey, Lizzie Kumar, Hilary Parker, Niti Paudyal, Sebastian Raschka,\n",
      "and Shreya Shankar.\n",
      "\n",
      "I owe them all a beer or five.\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xiii\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 1\n",
      "Vectors, Matrices, and Arrays\n",
      "\n",
      "1.0 Introduction\n",
      "NumPy  is  the  foundation  of  the  Python  machine  learning  stack.  NumPy  allows  for\n",
      "efficient  operations  on  the  data  structures  often  used  in  machine  learning:  vectors,\n",
      "matrices, and tensors. While NumPy is not the focus of this book, it will show up fre‐\n",
      "quently  throughout  the  following  chapters.  This  chapter  covers  the  most  common\n",
      "NumPy  operations  we  are  likely  to  run  into  while  working  on  machine  learning\n",
      "workflows.\n",
      "\n",
      "1.1 Creating a Vector\n",
      "\n",
      "Problem\n",
      "You need to create a vector.\n",
      "\n",
      "Solution\n",
      "Use NumPy to create a one-dimensional array:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create a vector as a row\n",
      "vector_row = np.array([1, 2, 3])\n",
      "\n",
      "# Create a vector as a column\n",
      "vector_column = np.array([[1],\n",
      "                          [2],\n",
      "                          [3]])\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Discussion\n",
      "NumPy’s  main  data  structure  is  the  multidimensional  array.  To  create  a  vector,  we\n",
      "simply  create  a  one-dimensional  array.  Just  like  vectors,  these  arrays  can  be  repre‐\n",
      "sented horizontally (i.e., rows) or vertically (i.e., columns).\n",
      "\n",
      "See Also\n",
      "\n",
      "• Vectors, Math Is Fun\n",
      "\n",
      "• Euclidean vector, Wikipedia\n",
      "\n",
      "1.2 Creating a Matrix\n",
      "\n",
      "Problem\n",
      "You need to create a matrix.\n",
      "\n",
      "Solution\n",
      "Use NumPy to create a two-dimensional array:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create a matrix\n",
      "matrix = np.array([[1, 2],\n",
      "                   [1, 2],\n",
      "                   [1, 2]])\n",
      "\n",
      "Discussion\n",
      "To create a matrix we can use a NumPy two-dimensional array. In our solution, the\n",
      "matrix contains three rows and two columns (a column of 1s and a column of 2s).\n",
      "\n",
      "NumPy actually has a dedicated matrix data structure:\n",
      "\n",
      "matrix_object = np.mat([[1, 2],\n",
      "                        [1, 2],\n",
      "                        [1, 2]])\n",
      "\n",
      "matrix([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]])\n",
      "\n",
      "However, the matrix data structure is not recommended for two reasons. First, arrays\n",
      "are  the  de  facto  standard  data  structure  of  NumPy.  Second,  the  vast  majority  of\n",
      "NumPy operations return arrays, not matrix objects.\n",
      "\n",
      "2 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Matrix, Wikipedia\n",
      "\n",
      "• Matrix, Wolfram MathWorld\n",
      "\n",
      "1.3 Creating a Sparse Matrix\n",
      "\n",
      "Problem\n",
      "Given data with very few nonzero values, you want to efficiently represent it.\n",
      "\n",
      "Solution\n",
      "Create a sparse matrix:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "\n",
      "# Create a matrix\n",
      "matrix = np.array([[0, 0],\n",
      "                   [0, 1],\n",
      "                   [3, 0]])\n",
      "\n",
      "# Create compressed sparse row (CSR) matrix\n",
      "matrix_sparse = sparse.csr_matrix(matrix)\n",
      "\n",
      "Discussion\n",
      "A frequent situation in machine learning is having a huge amount of data; however,\n",
      "most of the elements in the data are zeros. For example, imagine a matrix where the\n",
      "columns are every movie on Netflix, the rows are every Netflix user, and the values\n",
      "are  how  many  times  a  user  has  watched  that  particular  movie.  This  matrix  would\n",
      "have tens of thousands of columns and millions of rows! However, since most users\n",
      "do not watch most movies, the vast majority of elements would be zero.\n",
      "\n",
      "Sparse matrices only store nonzero elements and assume all other values will be zero,\n",
      "leading  to  significant  computational  savings.  In  our  solution,  we  created  a  NumPy\n",
      "array with two nonzero values, then converted it into a sparse matrix. If we view the\n",
      "sparse matrix we can see that only the nonzero values are stored:\n",
      "\n",
      "# View sparse matrix\n",
      "print(matrix_sparse)\n",
      "\n",
      "  (1, 1)    1\n",
      "  (2, 0)    3\n",
      "\n",
      "1.3 Creating a Sparse Matrix \n",
      "\n",
      "| \n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "There  are  a  number  of  types  of  sparse  matrices.  However,  in  compressed  sparse  row\n",
      "(CSR) matrices, (1, 1) and (2, 0) represent the (zero-indexed) indices of the non-\n",
      "zero values 1 and 3, respectively. For example, the element 1 is in the second row and\n",
      "second  column.  We  can  see  the  advantage  of  sparse  matrices  if  we  create  a  much\n",
      "larger  matrix  with  many  more  zero  elements  and  then  compare  this  larger  matrix\n",
      "with our original sparse matrix:\n",
      "\n",
      "# Create larger matrix\n",
      "matrix_large = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                         [3, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "# Create compressed sparse row (CSR) matrix\n",
      "matrix_large_sparse = sparse.csr_matrix(matrix_large)\n",
      "\n",
      "# View original sparse matrix\n",
      "print(matrix_sparse)\n",
      "\n",
      "  (1, 1)    1\n",
      "  (2, 0)    3\n",
      "\n",
      "# View larger sparse matrix\n",
      "print(matrix_large_sparse)\n",
      "\n",
      "  (1, 1)    1\n",
      "  (2, 0)    3\n",
      "\n",
      "As we can see, despite the fact that we added many more zero elements in the larger\n",
      "matrix,  its  sparse  representation  is  exactly  the  same  as  our  original  sparse  matrix.\n",
      "That is, the addition of zero elements did not change the size of the sparse matrix.\n",
      "\n",
      "As mentioned, there are many different types of sparse matrices, such as compressed\n",
      "sparse column, list of lists, and dictionary of keys. While an explanation of the differ‐\n",
      "ent types and their implications is outside the scope of this book, it is worth noting\n",
      "that  while  there  is  no  “best”  sparse  matrix  type,  there  are  meaningful  differences\n",
      "between them and we should be conscious about why we are choosing one type over\n",
      "another.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Sparse matrices, SciPy documentation\n",
      "\n",
      "• 101 Ways to Store a Sparse Matrix\n",
      "\n",
      "1.4 Selecting Elements\n",
      "\n",
      "Problem\n",
      "You need to select one or more elements in a vector or matrix.\n",
      "\n",
      "4 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "Solution\n",
      "NumPy’s arrays make that easy:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create row vector\n",
      "vector = np.array([1, 2, 3, 4, 5, 6])\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "# Select third element of vector\n",
      "vector[2]\n",
      "\n",
      "3\n",
      "\n",
      "# Select second row, second column\n",
      "matrix[1,1]\n",
      "\n",
      "5\n",
      "\n",
      "Discussion\n",
      "Like most things in Python, NumPy arrays are zero-indexed, meaning that the index\n",
      "of the first element is 0, not 1. With that caveat, NumPy offers a wide variety of meth‐\n",
      "ods for selecting (i.e., indexing and slicing) elements or groups of elements in arrays:\n",
      "\n",
      "# Select all elements of a vector\n",
      "vector[:]\n",
      "\n",
      "array([1, 2, 3, 4, 5, 6])\n",
      "\n",
      "# Select everything up to and including the third element\n",
      "vector[:3]\n",
      "\n",
      "array([1, 2, 3])\n",
      "\n",
      "# Select everything after the third element\n",
      "vector[3:]\n",
      "\n",
      "array([4, 5, 6])\n",
      "\n",
      "# Select the last element\n",
      "vector[-1]\n",
      "\n",
      "6\n",
      "\n",
      "# Select the first two rows and all columns of a matrix\n",
      "matrix[:2,:]\n",
      "\n",
      "array([[1, 2, 3],\n",
      "       [4, 5, 6]])\n",
      "\n",
      "1.4 Selecting Elements \n",
      "\n",
      "| \n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "# Select all rows and the second column\n",
      "matrix[:,1:2]\n",
      "\n",
      "array([[2],\n",
      "       [5],\n",
      "       [8]])\n",
      "\n",
      "1.5 Describing a Matrix\n",
      "\n",
      "Problem\n",
      "You want to describe the shape, size, and dimensions of the matrix.\n",
      "\n",
      "Solution\n",
      "Use shape, size, and ndim:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3, 4],\n",
      "                   [5, 6, 7, 8],\n",
      "                   [9, 10, 11, 12]])\n",
      "\n",
      "# View number of rows and columns\n",
      "matrix.shape\n",
      "\n",
      "(3, 4)\n",
      "\n",
      "# View number of elements (rows * columns)\n",
      "matrix.size\n",
      "\n",
      "12\n",
      "\n",
      "# View number of dimensions\n",
      "matrix.ndim\n",
      "\n",
      "2\n",
      "\n",
      "Discussion\n",
      "This might seem basic (and it is); however, time and again it will be valuable to check\n",
      "the shape and size of an array both for further calculations and simply as a gut check\n",
      "after some operation.\n",
      "\n",
      "1.6 Applying Operations to Elements\n",
      "\n",
      "Problem\n",
      "You want to apply some function to multiple elements in an array.\n",
      "\n",
      "6 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "Solution\n",
      "Use NumPy’s vectorize:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "# Create function that adds 100 to something\n",
      "add_100 = lambda i: i + 100\n",
      "\n",
      "# Create vectorized function\n",
      "vectorized_add_100 = np.vectorize(add_100)\n",
      "\n",
      "# Apply function to all elements in matrix\n",
      "vectorized_add_100(matrix)\n",
      "\n",
      "array([[101, 102, 103],\n",
      "       [104, 105, 106],\n",
      "       [107, 108, 109]])\n",
      "\n",
      "Discussion\n",
      "NumPy’s vectorize class converts a function into a function that can apply to all ele‐\n",
      "ments in an array or slice of an array. It’s worth noting that vectorize is essentially a\n",
      "for loop over the elements and does not increase performance. Furthermore, NumPy\n",
      "arrays allow us to perform operations between arrays even if their dimensions are not\n",
      "the same (a process called broadcasting). For example, we can create a much simpler\n",
      "version of our solution using broadcasting:\n",
      "\n",
      "# Add 100 to all elements\n",
      "matrix + 100\n",
      "\n",
      "array([[101, 102, 103],\n",
      "       [104, 105, 106],\n",
      "       [107, 108, 109]])\n",
      "\n",
      "1.7 Finding the Maximum and Minimum Values\n",
      "\n",
      "Problem\n",
      "You need to find the maximum or minimum value in an array.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s max and min:\n",
      "\n",
      "1.7 Finding the Maximum and Minimum Values \n",
      "\n",
      "| \n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "# Return maximum element\n",
      "np.max(matrix)\n",
      "\n",
      "9\n",
      "\n",
      "# Return minimum element\n",
      "np.min(matrix)\n",
      "\n",
      "1\n",
      "\n",
      "Discussion\n",
      "Often we want to know the maximum and minimum value in an array or subset of an\n",
      "array.  This  can  be  accomplished  with  the  max  and  min  methods.  Using  the  axis \n",
      "parameter we can also apply the operation along a certain axis:\n",
      "\n",
      "# Find maximum element in each column\n",
      "np.max(matrix, axis=0)\n",
      "\n",
      "array([7, 8, 9])\n",
      "\n",
      "# Find maximum element in each row\n",
      "np.max(matrix, axis=1)\n",
      "\n",
      "array([3, 6, 9])\n",
      "\n",
      "1.8 Calculating the Average, Variance, and Standard\n",
      "Deviation\n",
      "\n",
      "Problem\n",
      "You want to calculate some descriptive statistics about an array.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s mean, var, and std:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "8 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "# Return mean\n",
      "np.mean(matrix)\n",
      "\n",
      "5.0\n",
      "\n",
      "# Return variance\n",
      "np.var(matrix)\n",
      "\n",
      "6.666666666666667\n",
      "\n",
      "# Return standard deviation\n",
      "np.std(matrix)\n",
      "\n",
      "2.5819888974716112\n",
      "\n",
      "Discussion\n",
      "Just  like  with  max  and  min,  we  can  easily  get  descriptive  statistics  about  the  whole\n",
      "matrix or do calculations along a single axis:\n",
      "\n",
      "# Find the mean value in each column\n",
      "np.mean(matrix, axis=0)\n",
      "\n",
      "array([ 4.,  5.,  6.])\n",
      "\n",
      "1.9 Reshaping Arrays\n",
      "\n",
      "Problem\n",
      "You  want  to  change  the  shape  (number  of  rows  and  columns)  of  an  array  without\n",
      "changing the element values.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s reshape:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create 4x3 matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9],\n",
      "                   [10, 11, 12]])\n",
      "\n",
      "# Reshape matrix into 2x6 matrix\n",
      "matrix.reshape(2, 6)\n",
      "\n",
      "array([[ 1,  2,  3,  4,  5,  6],\n",
      "       [ 7,  8,  9, 10, 11, 12]])\n",
      "\n",
      "1.9 Reshaping Arrays \n",
      "\n",
      "| \n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Discussion\n",
      "reshape allows us to restructure an array so that we maintain the same data but it is\n",
      "organized as a different number of rows and columns. The only requirement is that\n",
      "the shape of the original and new matrix contain the same number of elements (i.e.,\n",
      "the same size). We can see the size of a matrix using size:\n",
      "\n",
      "matrix.size\n",
      "\n",
      "12\n",
      "\n",
      "One useful argument in reshape is -1, which effectively means “as many as needed,”\n",
      "so reshape(-1, 1) means one row and as many columns as needed:\n",
      "\n",
      "matrix.reshape(1, -1)\n",
      "\n",
      "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n",
      "\n",
      "Finally, if we provide one integer, reshape will return a 1D array of that length:\n",
      "\n",
      "matrix.reshape(12)\n",
      "\n",
      "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "\n",
      "1.10 Transposing a Vector or Matrix\n",
      "\n",
      "Problem\n",
      "You need to transpose a vector or matrix.\n",
      "\n",
      "Solution\n",
      "Use the T method:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "# Transpose matrix\n",
      "matrix.T\n",
      "\n",
      "array([[1, 4, 7],\n",
      "       [2, 5, 8],\n",
      "       [3, 6, 9]])\n",
      "\n",
      "10 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "Discussion\n",
      "Transposing  is  a  common  operation  in  linear  algebra  where  the  column  and  row\n",
      "indices of each element are swapped. One nuanced point that is typically overlooked\n",
      "outside  of  a  linear  algebra  class  is  that,  technically,  a  vector  cannot  be  transposed\n",
      "because it is just a collection of values:\n",
      "\n",
      "# Transpose vector\n",
      "np.array([1, 2, 3, 4, 5, 6]).T\n",
      "\n",
      "array([1, 2, 3, 4, 5, 6])\n",
      "\n",
      "However, it is common to refer to transposing a vector as converting a row vector to\n",
      "a column vector (notice the second pair of brackets) or vice versa:\n",
      "\n",
      "# Tranpose row vector\n",
      "np.array([[1, 2, 3, 4, 5, 6]]).T\n",
      "\n",
      "array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5],\n",
      "       [6]])\n",
      "\n",
      "1.11 Flattening a Matrix\n",
      "\n",
      "Problem\n",
      "You need to transform a matrix into a one-dimensional array.\n",
      "\n",
      "Solution\n",
      "Use flatten:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [4, 5, 6],\n",
      "                   [7, 8, 9]])\n",
      "\n",
      "# Flatten matrix\n",
      "matrix.flatten()\n",
      "\n",
      "array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "1.11 Flattening a Matrix \n",
      "\n",
      "| \n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Discussion\n",
      "flatten  is  a  simple  method  to  transform  a  matrix  into  a  one-dimensional  array.\n",
      "Alternatively, we can use reshape to create a row vector:\n",
      "\n",
      "matrix.reshape(1, -1)\n",
      "\n",
      "array([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "\n",
      "1.12 Finding the Rank of a Matrix\n",
      "\n",
      "Problem\n",
      "You need to know the rank of a matrix.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s linear algebra method matrix_rank:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 1, 1],\n",
      "                   [1, 1, 10],\n",
      "                   [1, 1, 15]])\n",
      "\n",
      "# Return matrix rank\n",
      "np.linalg.matrix_rank(matrix)\n",
      "\n",
      "2\n",
      "\n",
      "Discussion\n",
      "The rank of a matrix is the dimensions of the vector space spanned by its columns or\n",
      "rows. Finding the rank of a matrix is easy in NumPy thanks to matrix_rank.\n",
      "\n",
      "See Also\n",
      "\n",
      "• The Rank of a Matrix, CliffsNotes\n",
      "\n",
      "1.13 Calculating the Determinant\n",
      "\n",
      "Problem\n",
      "You need to know the determinant of a matrix.\n",
      "\n",
      "12 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "Solution\n",
      "Use NumPy’s linear algebra method det:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [2, 4, 6],\n",
      "                   [3, 8, 9]])\n",
      "\n",
      "# Return determinant of matrix\n",
      "np.linalg.det(matrix)\n",
      "\n",
      "0.0\n",
      "\n",
      "Discussion\n",
      "It  can  sometimes  be  useful  to  calculate  the  determinant  of  a  matrix.  NumPy  makes\n",
      "this easy with det.\n",
      "\n",
      "See Also\n",
      "\n",
      "• The determinant | Essence of linear algebra, chapter 5, 3Blue1Brown\n",
      "\n",
      "• Determinant, Wolfram MathWorld\n",
      "\n",
      "1.14 Getting the Diagonal of a Matrix\n",
      "\n",
      "Problem\n",
      "You need to get the diagonal elements of a matrix.\n",
      "\n",
      "Solution\n",
      "Use diagonal:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [2, 4, 6],\n",
      "                   [3, 8, 9]])\n",
      "\n",
      "# Return diagonal elements\n",
      "matrix.diagonal()\n",
      "\n",
      "1.14 Getting the Diagonal of a Matrix \n",
      "\n",
      "| \n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "array([1, 4, 9])\n",
      "\n",
      "Discussion\n",
      "NumPy makes getting the diagonal elements of a matrix easy with diagonal. It is also\n",
      "possible to get a diagonal off from the main diagonal by using the offset parameter:\n",
      "\n",
      "# Return diagonal one above the main diagonal\n",
      "matrix.diagonal(offset=1)\n",
      "\n",
      "array([2, 6])\n",
      "\n",
      "# Return diagonal one below the main diagonal\n",
      "matrix.diagonal(offset=-1)\n",
      "\n",
      "array([2, 8])\n",
      "\n",
      "1.15 Calculating the Trace of a Matrix\n",
      "\n",
      "Problem\n",
      "You need to calculate the trace of a matrix.\n",
      "\n",
      "Solution\n",
      "Use trace:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 2, 3],\n",
      "                   [2, 4, 6],\n",
      "                   [3, 8, 9]])\n",
      "\n",
      "# Return trace\n",
      "matrix.trace()\n",
      "\n",
      "14\n",
      "\n",
      "Discussion\n",
      "The trace of a matrix is the sum of the diagonal elements and is often used under the\n",
      "hood in machine learning methods. Given a NumPy multidimensional array, we can\n",
      "calculate the trace using trace. We can also return the diagonal of a matrix and calcu‐\n",
      "late its sum:\n",
      "\n",
      "# Return diagonal and sum elements\n",
      "sum(matrix.diagonal())\n",
      "\n",
      "14\n",
      "\n",
      "14 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• The Trace of a Square Matrix\n",
      "\n",
      "1.16 Finding Eigenvalues and Eigenvectors\n",
      "\n",
      "Problem\n",
      "You need to find the eigenvalues and eigenvectors of a square matrix.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s linalg.eig:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, -1, 3],\n",
      "                   [1, 1, 6],\n",
      "                   [3, 8, 9]])\n",
      "\n",
      "# Calculate eigenvalues and eigenvectors\n",
      "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
      "\n",
      "# View eigenvalues\n",
      "eigenvalues\n",
      "\n",
      "array([ 13.55075847,   0.74003145,  -3.29078992])\n",
      "\n",
      "# View eigenvectors\n",
      "eigenvectors\n",
      "\n",
      "array([[-0.17622017, -0.96677403, -0.53373322],\n",
      "       [-0.435951  ,  0.2053623 , -0.64324848],\n",
      "       [-0.88254925,  0.15223105,  0.54896288]])\n",
      "\n",
      "Discussion\n",
      "Eigenvectors are widely used in machine learning libraries. Intuitively, given a linear\n",
      "transformation  represented  by  a  matrix,  A,  eigenvectors  are  vectors  that,  when  that\n",
      "transformation is applied, change only in scale (not direction). More formally:\n",
      "\n",
      "Av = λv\n",
      "\n",
      "where A is a square matrix, λ contains the eigenvalues and v contains the eigenvec‐\n",
      "tors.  In  NumPy’s  linear  algebra  toolset,  eig  lets  us  calculate  the  eigenvalues,  and\n",
      "eigenvectors of any square matrix.\n",
      "\n",
      "1.16 Finding Eigenvalues and Eigenvectors \n",
      "\n",
      "| \n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Eigenvectors and Eigenvalues Explained Visually, Setosa.io\n",
      "\n",
      "• Eigenvectors  and  eigenvalues \n",
      "\n",
      "|  Essence  of \n",
      "\n",
      "linear  algebra,  Chapter  10,\n",
      "\n",
      "3Blue1Brown\n",
      "\n",
      "1.17 Calculating Dot Products\n",
      "\n",
      "Problem\n",
      "You need to calculate the dot product of two vectors.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s dot:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create two vectors\n",
      "vector_a = np.array([1,2,3])\n",
      "vector_b = np.array([4,5,6])\n",
      "\n",
      "# Calculate dot product\n",
      "np.dot(vector_a, vector_b)\n",
      "\n",
      "32\n",
      "\n",
      "Discussion\n",
      "The dot product of two vectors, a and b, is defined as:\n",
      "\n",
      "n\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "aibi\n",
      "\n",
      "where ai is the ith element of vector a. We can use NumPy’s dot class to calculate the\n",
      "dot product. Alternatively, in Python 3.5+ we can use the new @ operator:\n",
      "\n",
      "# Calculate dot product\n",
      "vector_a @ vector_b\n",
      "\n",
      "32\n",
      "\n",
      "See Also\n",
      "\n",
      "• Vector dot product and vector length, Khan Academy\n",
      "\n",
      "16 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "• Dot Product, Paul’s Online Math Notes\n",
      "\n",
      "1.18 Adding and Subtracting Matrices\n",
      "\n",
      "Problem\n",
      "You want to add or subtract two matrices.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s add and subtract:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix_a = np.array([[1, 1, 1],\n",
      "                     [1, 1, 1],\n",
      "                     [1, 1, 2]])\n",
      "\n",
      "# Create matrix\n",
      "matrix_b = np.array([[1, 3, 1],\n",
      "                     [1, 3, 1],\n",
      "                     [1, 3, 8]])\n",
      "\n",
      "# Add two matrices\n",
      "np.add(matrix_a, matrix_b)\n",
      "\n",
      "array([[ 2,  4,  2],\n",
      "       [ 2,  4,  2],\n",
      "       [ 2,  4, 10]])\n",
      "\n",
      "# Subtract two matrices\n",
      "np.subtract(matrix_a, matrix_b)\n",
      "\n",
      "array([[ 0, -2,  0],\n",
      "       [ 0, -2,  0],\n",
      "       [ 0, -2, -6]])\n",
      "\n",
      "Discussion\n",
      "Alternatively, we can simply use the + and - operators:\n",
      "\n",
      "# Add two matrices\n",
      "matrix_a + matrix_b\n",
      "\n",
      "array([[ 2,  4,  2],\n",
      "       [ 2,  4,  2],\n",
      "       [ 2,  4, 10]])\n",
      "\n",
      "1.18 Adding and Subtracting Matrices \n",
      "\n",
      "| \n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "1.19 Multiplying Matrices\n",
      "\n",
      "Problem\n",
      "You want to multiply two matrices.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s dot:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix_a = np.array([[1, 1],\n",
      "                     [1, 2]])\n",
      "\n",
      "# Create matrix\n",
      "matrix_b = np.array([[1, 3],\n",
      "                     [1, 2]])\n",
      "\n",
      "# Multiply two matrices\n",
      "np.dot(matrix_a, matrix_b)\n",
      "\n",
      "array([[2, 5],\n",
      "       [3, 7]])\n",
      "\n",
      "Discussion\n",
      "Alternatively, in Python 3.5+ we can use the @ operator:\n",
      "\n",
      "# Multiply two matrices\n",
      "matrix_a @ matrix_b\n",
      "\n",
      "array([[2, 5],\n",
      "       [3, 7]])\n",
      "\n",
      "If we want to do element-wise multiplication, we can use the * operator:\n",
      "\n",
      "# Multiply two matrices element-wise\n",
      "matrix_a * matrix_b\n",
      "\n",
      "array([[1, 3],\n",
      "       [1, 4]])\n",
      "\n",
      "See Also\n",
      "\n",
      "• Array vs. Matrix Operations, MathWorks\n",
      "\n",
      "18 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "1.20 Inverting a Matrix\n",
      "\n",
      "Problem\n",
      "You want to calculate the inverse of a square matrix.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s linear algebra inv method:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create matrix\n",
      "matrix = np.array([[1, 4],\n",
      "                   [2, 5]])\n",
      "\n",
      "# Calculate inverse of matrix\n",
      "np.linalg.inv(matrix)\n",
      "\n",
      "array([[-1.66666667,  1.33333333],\n",
      "       [ 0.66666667, -0.33333333]])\n",
      "\n",
      "Discussion\n",
      "The inverse of a square matrix, A, is a second matrix A–1, such that:\n",
      "\n",
      "AA−1 = I\n",
      "\n",
      "where I is the identity matrix. In NumPy we can use linalg.inv to calculate A–1 if it\n",
      "exists. To see this in action, we can multiply a matrix by its inverse and the result is\n",
      "the identity matrix:\n",
      "\n",
      "# Multiply matrix and its inverse\n",
      "matrix @ np.linalg.inv(matrix)\n",
      "\n",
      "array([[ 1.,  0.],\n",
      "       [ 0.,  1.]])\n",
      "\n",
      "See Also\n",
      "\n",
      "• Inverse of a Matrix\n",
      "\n",
      "1.20 Inverting a Matrix \n",
      "\n",
      "| \n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "1.21 Generating Random Values\n",
      "\n",
      "Problem\n",
      "You want to generate pseudorandom values.\n",
      "\n",
      "Solution\n",
      "Use NumPy’s random:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Set seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate three random floats between 0.0 and 1.0\n",
      "np.random.random(3)\n",
      "\n",
      "array([ 0.5488135 ,  0.71518937,  0.60276338])\n",
      "\n",
      "Discussion\n",
      "NumPy offers a wide variety of means to generate random numbers, many more than\n",
      "can be covered here. In our solution we generated floats; however, it is also common\n",
      "to generate integers:\n",
      "\n",
      "# Generate three random integers between 1 and 10\n",
      "np.random.randint(0, 11, 3)\n",
      "\n",
      "array([3, 7, 9])\n",
      "\n",
      "Alternatively, we can generate numbers by drawing them from a distribution:\n",
      "\n",
      "# Draw three numbers from a normal distribution with mean 0.0\n",
      "# and standard deviation of 1.0\n",
      "np.random.normal(0.0, 1.0, 3)\n",
      "\n",
      "array([-1.42232584,  1.52006949, -0.29139398])\n",
      "\n",
      "# Draw three numbers from a logistic distribution with mean 0.0 and scale of 1.0\n",
      "np.random.logistic(0.0, 1.0, 3)\n",
      "\n",
      "array([-0.98118713, -0.08939902,  1.46416405])\n",
      "\n",
      "# Draw three numbers greater than or equal to 1.0 and less than 2.0\n",
      "np.random.uniform(1.0, 2.0, 3)\n",
      "\n",
      "array([ 1.47997717,  1.3927848 ,  1.83607876])\n",
      "\n",
      "Finally,  it  can  sometimes  be  useful  to  return  the  same  random  numbers  multiple\n",
      "times to get predictable, repeatable results. We can do this by setting the “seed” (an\n",
      "integer) of the pseudorandom generator. Random processes with the same seed will\n",
      "\n",
      "20 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: Vectors, Matrices, and Arrays\n",
      "\n",
      "\f",
      "always produce the same output. We will use seeds throughout this book so that the\n",
      "code you see in the book and the code you run on your computer produces the same\n",
      "results.\n",
      "\n",
      "1.21 Generating Random Values \n",
      "\n",
      "| \n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 2\n",
      "Loading Data\n",
      "\n",
      "2.0 Introduction\n",
      "The first step in any machine learning endeavor is to get the raw data into our system.\n",
      "The raw data might be a logfile, dataset file, or database. Furthermore, often we will\n",
      "want to retrieve data from multiple sources. The recipes in this chapter look at meth‐\n",
      "ods of loading data from a variety of sources, including CSV files and SQL databases.\n",
      "We  also  cover  methods  of  generating  simulated  data  with  desirable  properties  for\n",
      "experimentation. Finally, while there are many ways to load data in the Python eco‐\n",
      "system, we will focus on using the pandas library’s extensive set of methods for load‐\n",
      "ing external data, and using scikit-learn—an open source machine learning library in\n",
      "Python—for generating simulated data.\n",
      "\n",
      "2.1 Loading a Sample Dataset\n",
      "\n",
      "Problem\n",
      "You want to load a preexisting sample dataset.\n",
      "\n",
      "Solution\n",
      "scikit-learn comes with a number of popular datasets for you to use:\n",
      "\n",
      "# Load scikit-learn's datasets\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load digits dataset\n",
      "digits = datasets.load_digits()\n",
      "\n",
      "# Create features matrix\n",
      "features = digits.data\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "# Create target vector\n",
      "target = digits.target\n",
      "\n",
      "# View first observation\n",
      "features[0]\n",
      "\n",
      "array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13.,\n",
      "        15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,\n",
      "         8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,\n",
      "         5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,   0.,\n",
      "         1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,\n",
      "         0.,   0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.])\n",
      "\n",
      "Discussion\n",
      "Often we do not want to go through the work of loading, transforming, and cleaning\n",
      "a  real-world  dataset  before  we  can  explore  some  machine  learning  algorithm  or\n",
      "method. Luckily, scikit-learn comes with some common datasets we can quickly load.\n",
      "These datasets are often called “toy” datasets because they are far smaller and cleaner\n",
      "than a dataset we would see in the real world. Some popular sample datasets in scikit-\n",
      "learn are:\n",
      "\n",
      "load_boston\n",
      "\n",
      "Contains  503  observations  on  Boston  housing  prices.  It  is  a  good  dataset  for\n",
      "exploring regression algorithms.\n",
      "\n",
      "load_iris\n",
      "\n",
      "Contains 150 observations on the measurements of Iris flowers. It is a good data‐\n",
      "set for exploring classification algorithms.\n",
      "\n",
      "load_digits\n",
      "\n",
      "Contains 1,797 observations from images of handwritten digits. It is a good data‐\n",
      "set for teaching image classification.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn toy datasets\n",
      "\n",
      "• The Digit Dataset\n",
      "\n",
      "2.2 Creating a Simulated Dataset\n",
      "\n",
      "Problem\n",
      "You need to generate a dataset of simulated data.\n",
      "\n",
      "24 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: Loading Data\n",
      "\n",
      "\f",
      "Solution\n",
      "scikit-learn offers many methods for creating simulated data. Of those, three methods\n",
      "are particularly useful.\n",
      "\n",
      "When we want a dataset designed to be used with linear regression, make_regression\n",
      "is a good choice:\n",
      "\n",
      "# Load library\n",
      "from sklearn.datasets import make_regression\n",
      "\n",
      "# Generate features matrix, target vector, and the true coefficients\n",
      "features, target, coefficients = make_regression(n_samples = 100,\n",
      "                                                 n_features = 3,\n",
      "                                                 n_informative = 3,\n",
      "                                                 n_targets = 1,\n",
      "                                                 noise = 0.0,\n",
      "                                                 coef = True,\n",
      "                                                 random_state = 1)\n",
      "\n",
      "# View feature matrix and target vector\n",
      "print('Feature Matrix\\n', features[:3])\n",
      "print('Target Vector\\n', target[:3])\n",
      "\n",
      "Feature Matrix\n",
      " [[ 1.29322588 -0.61736206 -0.11044703]\n",
      " [-2.793085    0.36633201  1.93752881]\n",
      " [ 0.80186103 -0.18656977  0.0465673 ]]\n",
      "Target Vector\n",
      " [-10.37865986  25.5124503   19.67705609]\n",
      "\n",
      "If  we  are  interested  in  creating  a  simulated  dataset  for  classification,  we  can  use\n",
      "make_classification:\n",
      "\n",
      "# Load library\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_classification(n_samples = 100,\n",
      "                                       n_features = 3,\n",
      "                                       n_informative = 3,\n",
      "                                       n_redundant = 0,\n",
      "                                       n_classes = 2,\n",
      "                                       weights = [.25, .75],\n",
      "                                       random_state = 1)\n",
      "\n",
      "# View feature matrix and target vector\n",
      "print('Feature Matrix\\n', features[:3])\n",
      "print('Target Vector\\n', target[:3])\n",
      "\n",
      "Feature Matrix\n",
      " [[ 1.06354768 -1.42632219  1.02163151]\n",
      " [ 0.23156977  1.49535261  0.33251578]\n",
      "\n",
      "2.2 Creating a Simulated Dataset \n",
      "\n",
      "| \n",
      "\n",
      "25\n",
      "\n",
      "\f",
      " [ 0.15972951  0.83533515 -0.40869554]]\n",
      "Target Vector\n",
      " [1 0 0]\n",
      "\n",
      "Finally, if we want a dataset designed to work well with clustering techniques, scikit-\n",
      "learn offers make_blobs:\n",
      "\n",
      "# Load library\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Generate feature matrix and target vector\n",
      "features, target = make_blobs(n_samples = 100,\n",
      "                              n_features = 2,\n",
      "                              centers = 3,\n",
      "                              cluster_std = 0.5,\n",
      "                              shuffle = True,\n",
      "                              random_state = 1)\n",
      "\n",
      "# View feature matrix and target vector\n",
      "print('Feature Matrix\\n', features[:3])\n",
      "print('Target Vector\\n', target[:3])\n",
      "\n",
      "Feature Matrix\n",
      " [[ -1.22685609   3.25572052]\n",
      " [ -9.57463218  -4.38310652]\n",
      " [-10.71976941  -4.20558148]]\n",
      "Target Vector\n",
      " [0 1 1]\n",
      "\n",
      "Discussion\n",
      "As might be apparent from the solutions, make_regression returns a feature matrix\n",
      "of  float  values  and  a  target  vector  of  float  values,  while  make_classification  and\n",
      "make_blobs return a feature matrix of float values and a target vector of integers rep‐\n",
      "resenting membership in a class.\n",
      "\n",
      "scikit-learn’s  simulated  datasets  offer  extensive  options  to  control  the  type  of  data\n",
      "generated. scikit-learn’s documentation contains a full description of all the parame‐\n",
      "ters, but a few are worth noting.\n",
      "\n",
      "In  make_regression  and  make_classification,  n_informative  determines  the\n",
      "number of features that are used to generate the target vector. If n_informative is less\n",
      "than the total number of features (n_features), the resulting dataset will have redun‐\n",
      "dant features that can be identified through feature selection techniques.\n",
      "\n",
      "In  addition,  make_classification  contains  a  weights  parameter  that  allows  us  to\n",
      "simulate  datasets  with  imbalanced  classes.  For  example,  weights  =  [.25,  .75]\n",
      "would return a dataset with 25% of observations belonging to one class and 75% of\n",
      "observations belonging to a second class.\n",
      "\n",
      "26 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: Loading Data\n",
      "\n",
      "\f",
      "For make_blobs, the centers parameter determines the number of clusters generated.\n",
      "Using the matplotlib visualization library, we can visualize the clusters generated by\n",
      "make_blobs:\n",
      "\n",
      "# Load library\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# View scatterplot\n",
      "plt.scatter(features[:,0], features[:,1], c=target)\n",
      "plt.show()\n",
      "\n",
      "See Also\n",
      "\n",
      "• make_regression documentation\n",
      "• make_classification documentation\n",
      "• make_blobs documentation\n",
      "\n",
      "2.3 Loading a CSV File\n",
      "\n",
      "Problem\n",
      "You need to import a comma-separated values (CSV) file.\n",
      "\n",
      "2.3 Loading a CSV File \n",
      "\n",
      "| \n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Solution\n",
      "Use the pandas library’s read_csv to load a local or hosted CSV file:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/simulated_data'\n",
      "\n",
      "# Load dataset\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# View first two rows\n",
      "dataframe.head(2)\n",
      "\n",
      "integer datetime\n",
      "\n",
      "0 5\n",
      "1 5\n",
      "\n",
      "2015-01-01 00:00:00\n",
      "2015-01-01 00:00:01\n",
      "\n",
      "category\n",
      "0\n",
      "0\n",
      "\n",
      "Discussion\n",
      "There are two things to note about loading CSV files. First, it is often useful to take a\n",
      "quick look at the contents of the file before loading. It can be very helpful to see how a\n",
      "dataset  is  structured  beforehand  and  what  parameters  we  need  to  set  to  load  in  the\n",
      "file. Second, read_csv has over 30 parameters and therefore the documentation can\n",
      "be  daunting.  Fortunately,  those  parameters  are  mostly  there  to  allow  it  to  handle  a\n",
      "wide  variety  of  CSV  formats.  For  example,  CSV  files  get  their  names  from  the  fact\n",
      "that  the  values  are  literally  separated  by  commas  (e.g.,  one  row  might  be\n",
      "2,\"2015-01-01  00:00:00\",0);  however,  it  is  common  for  “CSV”  files  to  use  other\n",
      "characters  as  separators,  like  tabs.  pandas’  sep  parameter  allows  us  to  define  the\n",
      "delimiter  used  in  the  file.  Although  it  is  not  always  the  case,  a  common  formatting\n",
      "issue with CSV files is that the first line of the file is used to define column headers\n",
      "(e.g., integer, datetime, category in our solution). The header parameter allows\n",
      "us  to  specify  if  or  where  a  header  row  exists.  If  a  header  row  does  not  exist,  we  set\n",
      "header=None.\n",
      "\n",
      "2.4 Loading an Excel File\n",
      "\n",
      "Problem\n",
      "You need to import an Excel spreadsheet.\n",
      "\n",
      "Solution\n",
      "Use the pandas library’s read_excel to load an Excel spreadsheet:\n",
      "\n",
      "28 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: Loading Data\n",
      "\n",
      "\f",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/simulated_excel'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_excel(url, sheetname=0, header=1)\n",
      "\n",
      "# View the first two rows\n",
      "dataframe.head(2)\n",
      "\n",
      "5 2015-01-01 00:00:00 0\n",
      "0\n",
      "2015-01-01 00:00:01\n",
      "0\n",
      "2015-01-01 00:00:02\n",
      "\n",
      "0 5\n",
      "1 9\n",
      "\n",
      "Discussion\n",
      "This solution is similar to our solution for reading CSV files. The main difference is\n",
      "the  additional  parameter,  sheetname,  that  specifies  which  sheet  in  the  Excel  file  we\n",
      "wish to load. sheetname can accept both strings containing the name of the sheet and\n",
      "integers pointing to sheet positions (zero-indexed). If we need to load multiple sheets,\n",
      "include  them  as  a  list.  For  example,  sheetname=[0,1,2,  \"Monthly  Sales\"]  will\n",
      "return  a  dictionary  of  pandas  DataFrames  containing  the  first,  second,  and  third\n",
      "sheets and the sheet named Monthly Sales.\n",
      "\n",
      "2.5 Loading a JSON File\n",
      "\n",
      "Problem\n",
      "You need to load a JSON file for data preprocessing.\n",
      "\n",
      "Solution\n",
      "The pandas library provides read_json to convert a JSON file into a pandas object:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/simulated_json'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_json(url, orient='columns')\n",
      "\n",
      "# View the first two rows\n",
      "dataframe.head(2)\n",
      "\n",
      "2.5 Loading a JSON File \n",
      "\n",
      "| \n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "category\n",
      "datetime\n",
      "integer\n",
      "0\n",
      "0\n",
      "2015-01-01 00:00:00\n",
      "5\n",
      "1\n",
      "0\n",
      "2015-01-01 00:00:01\n",
      "5\n",
      "\n",
      "Discussion\n",
      "Importing JSON files into pandas is similar to the last few recipes we have seen. The\n",
      "key difference is the orient parameter, which indicates to pandas how the JSON file\n",
      "is structured. However, it might take some experimenting to figure out which argu‐\n",
      "ment (split, records, index, columns, and values) is the right one. Another helpful\n",
      "tool pandas offers is json_normalize, which can help convert semistructured JSON\n",
      "data into a pandas DataFrame.\n",
      "\n",
      "See Also\n",
      "\n",
      "• json_normalize documentation\n",
      "\n",
      "2.6 Querying a SQL Database\n",
      "\n",
      "Problem\n",
      "You need to load data from a database using the structured query language (SQL).\n",
      "\n",
      "Solution\n",
      "pandas’ read_sql_query allows us to make a SQL query to a database and load it:\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "# Create a connection to the database\n",
      "database_connection = create_engine('sqlite:///sample.db')\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_sql_query('SELECT * FROM data', database_connection)\n",
      "\n",
      "# View first two rows\n",
      "dataframe.head(2)\n",
      "\n",
      "30 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: Loading Data\n",
      "\n",
      "\f",
      "first_name\n",
      "\n",
      "0 Jason\n",
      "1 Molly\n",
      "\n",
      "last_name\n",
      "Miller\n",
      "Jacobson\n",
      "\n",
      "age preTestScore postTestScore\n",
      "42\n",
      "52\n",
      "\n",
      "4\n",
      "24\n",
      "\n",
      "25\n",
      "94\n",
      "\n",
      "Discussion\n",
      "Out of all of the recipes presented in this chapter, this recipe is probably the one we\n",
      "will use most in the real world. SQL is the lingua franca for pulling data from data‐\n",
      "bases. In this recipe we first use create_engine to define a connection to a SQL data‐\n",
      "base engine called SQLite. Next we use pandas’ read_sql_query to query that data‐\n",
      "base using SQL and put the results in a DataFrame.\n",
      "\n",
      "SQL is a language in its own right and, while beyond the scope of this book, it is cer‐\n",
      "tainly worth knowing for anyone wanting to learn machine learning. Our SQL query,\n",
      "SELECT  *  FROM  data,  asks  the  database  to  give  us  all  columns  (*)  from  the  table \n",
      "called data.\n",
      "\n",
      "See Also\n",
      "\n",
      "• SQLite\n",
      "\n",
      "• W3Schools SQL Tutorial\n",
      "\n",
      "2.6 Querying a SQL Database \n",
      "\n",
      "| \n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 3\n",
      "Data Wrangling\n",
      "\n",
      "3.0 Introduction\n",
      "Data  wrangling  is  a  broad  term  used,  often  informally,  to  describe  the  process  of\n",
      "transforming  raw  data  to  a  clean  and  organized  format  ready  for  use.  For  us,  data\n",
      "wrangling is only one step in preprocessing our data, but it is an important step.\n",
      "\n",
      "The most common data structure used to “wrangle” data is the data frame, which can\n",
      "be both intuitive and incredibly versatile. Data frames are tabular, meaning that they\n",
      "are  based  on  rows  and  columns  like  you  would  see  in  a  spreadsheet.  Here  is  a  data\n",
      "frame created from data about passengers on the Titanic:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data as a dataframe\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Show first 5 rows\n",
      "dataframe.head(5)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton\n",
      "1 Allison, Miss Helen Loraine\n",
      "2 Allison, Mr Hudson Joshua Creighton\n",
      "3 Allison, Mrs Hudson JC (Bessie Waldo Daniels)\n",
      "4 Allison, Master Hudson Trevor\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "PClass Age\n",
      "29.00\n",
      "1st\n",
      "2.00\n",
      "1st\n",
      "30.00 male\n",
      "1st\n",
      "25.00\n",
      "1st\n",
      "0.92 male\n",
      "1st\n",
      "\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "There are three important things to notice in this data frame.\n",
      "\n",
      "First, in a data frame each row corresponds to one observation (e.g., a passenger) and\n",
      "each column corresponds to one feature (gender, age, etc.). For example, by looking\n",
      "at  the  first  observation  we  can  see  that  Miss  Elisabeth  Walton  Allen  stayed  in  first\n",
      "class, was 29 years old, was female, and survived the disaster.\n",
      "\n",
      "Second, each column contains a name (e.g., Name, PClass, Age) and each row contains\n",
      "an index number (e.g., 0 for the lucky Miss Elisabeth Walton Allen). We will use these\n",
      "to select and manipulate observations and features.\n",
      "\n",
      "Third, two columns, Sex and SexCode, contain the same information in different for‐\n",
      "mats. In Sex, a woman is indicated by the string female, while in SexCode, a woman\n",
      "is  indicated  by  using  the  integer  1.  We  will  want  all  our  features  to  be  unique,  and\n",
      "therefore we will need to remove one of these columns.\n",
      "\n",
      "In this chapter, we will cover a wide variety of techniques to manipulate data frames\n",
      "using the pandas library with the goal of creating a clean, well-structured set of obser‐\n",
      "vations for further preprocessing.\n",
      "\n",
      "3.1 Creating a Data Frame\n",
      "\n",
      "Problem\n",
      "You want to create a new data frame.\n",
      "\n",
      "Solution\n",
      "pandas has many methods of creating a new DataFrame object. One easy method is\n",
      "to  create  an  empty  data  frame  using  DataFrame  and  then  define  each  column  sepa‐\n",
      "rately:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create DataFrame\n",
      "dataframe = pd.DataFrame()\n",
      "\n",
      "# Add columns\n",
      "dataframe['Name'] = ['Jacky Jackson', 'Steven Stevenson']\n",
      "dataframe['Age'] = [38, 25]\n",
      "dataframe['Driver'] = [True, False]\n",
      "\n",
      "# Show DataFrame\n",
      "dataframe\n",
      "\n",
      "34 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Name\n",
      "\n",
      "Age Driver\n",
      "0 Jacky Jackson\n",
      "38\n",
      "1 Steven Stevenson 25\n",
      "\n",
      "True\n",
      "False\n",
      "\n",
      "Alternatively, once we have created a DataFrame object, we can append new rows to\n",
      "the bottom:\n",
      "\n",
      "# Create row\n",
      "new_person = pd.Series(['Molly Mooney', 40, True], index=['Name','Age','Driver'])\n",
      "\n",
      "# Append row\n",
      "dataframe.append(new_person, ignore_index=True)\n",
      "\n",
      "Name\n",
      "\n",
      "Age Driver\n",
      "0 Jacky Jackson\n",
      "38\n",
      "1 Steven Stevenson 25\n",
      "40\n",
      "2 Molly Mooney\n",
      "\n",
      "True\n",
      "False\n",
      "True\n",
      "\n",
      "Discussion\n",
      "pandas offers what can feel like an infinite number of ways to create a DataFrame. In\n",
      "the real world, creating an empty DataFrame and then populating it will almost never\n",
      "happen. Instead, our DataFrames will be created from real data we have loading from\n",
      "other sources (e.g., a CSV file or database).\n",
      "\n",
      "3.2 Describing the Data\n",
      "\n",
      "Problem\n",
      "You want to view some characteristics of a DataFrame.\n",
      "\n",
      "Solution\n",
      "One of the easiest things we can do after loading the data is view the first few rows \n",
      "using head:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Show two rows\n",
      "dataframe.head(2)\n",
      "\n",
      "3.2 Describing the Data \n",
      "\n",
      "| \n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "We can also take a look at the number of rows and columns:\n",
      "\n",
      "# Show dimensions\n",
      "dataframe.shape\n",
      "\n",
      "(1313, 6)\n",
      "\n",
      "Additionally,  we  can  get  descriptive  statistics  for  any  numeric  columns  using\n",
      "describe:\n",
      "\n",
      "# Show statistics\n",
      "dataframe.describe()\n",
      "\n",
      "Age\n",
      "756.000000\n",
      "count\n",
      "mean 30.397989\n",
      "14.259049\n",
      "std\n",
      "min\n",
      "0.170000\n",
      "25% 21.000000\n",
      "50% 28.000000\n",
      "75% 39.000000\n",
      "71.000000\n",
      "max\n",
      "\n",
      "Survived\n",
      "1313.000000\n",
      "0.342727\n",
      "0.474802\n",
      "0.000000\n",
      "0.000000\n",
      "0.000000\n",
      "1.000000\n",
      "1.000000\n",
      "\n",
      "SexCode\n",
      "1313.000000\n",
      "0.351866\n",
      "0.477734\n",
      "0.000000\n",
      "0.000000\n",
      "0.000000\n",
      "1.000000\n",
      "1.000000\n",
      "\n",
      "Discussion\n",
      "After we load some data, it is a good idea to understand how it is structured and what\n",
      "kind of information it contains. Ideally, we would view the full data directly. But with\n",
      "most  real-world  cases,  the  data  could  have  thousands  to  hundreds  of  thousands  to\n",
      "millions  of  rows  and  columns.  Instead,  we  have  to  rely  on  pulling  samples  to  view\n",
      "small slices and calculating summary statistics of the data.\n",
      "\n",
      "In our solution, we are using a toy dataset of the passengers of the Titanic on her last\n",
      "voyage.  Using  head  we  can  take  a  look  at  the  first  few  rows  (five  by  default)  of  the\n",
      "data. Alternatively, we can use tail to view the last few rows. With shape we can see\n",
      "how many rows and columns our DataFrame contains. And finally, with describe we\n",
      "can see some basic descriptive statistics for any numerical column.\n",
      "\n",
      "It is worth noting that summary statistics do not always tell the full story. For exam‐\n",
      "ple,  pandas  treats  the  columns  Survived  and  SexCode  as  numeric  columns  because\n",
      "they  contain  1s  and  0s.  However,  in  this  case  the  numerical  values  represent  cate‐\n",
      "gories. For example, if Survived equals 1, it indicates that the passenger survived the\n",
      "disaster. For this reason, some of the summary statistics provided don’t make sense,\n",
      "\n",
      "36 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "such as the standard deviation of the SexCode column (an indicator of the passenger’s\n",
      "gender).\n",
      "\n",
      "3.3 Navigating DataFrames\n",
      "\n",
      "Problem\n",
      "You need to select individual data or slices of a DataFrame.\n",
      "\n",
      "Solution\n",
      "Use loc or iloc to select one or more rows or values:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com//titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Select first row\n",
      "dataframe.iloc[0]\n",
      "\n",
      "Name        Allen, Miss Elisabeth Walton\n",
      "PClass                               1st\n",
      "Age                                   29\n",
      "Sex                               female\n",
      "Survived                               1\n",
      "SexCode                                1\n",
      "Name: 0, dtype: object\n",
      "\n",
      "We  can  use  :  to  define  a  slice  of  rows  we  want,  such  as  selecting  the  second,  third,\n",
      "and fourth rows:\n",
      "\n",
      "# Select three rows\n",
      "dataframe.iloc[1:4]\n",
      "\n",
      "Name\n",
      "\n",
      "1 Allison, Miss Helen Loraine\n",
      "2 Allison, Mr Hudson Joshua Creighton\n",
      "3 Allison, Mrs Hudson JC (Bessie Waldo Daniels)\n",
      "\n",
      "Sex\n",
      "female\n",
      "\n",
      "PClass Age\n",
      "2.0\n",
      "1st\n",
      "30.0 male\n",
      "1st\n",
      "25.0\n",
      "1st\n",
      "\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "We can even use it to get all rows up to a point, such as all rows up to and including\n",
      "the fourth row:\n",
      "\n",
      "# Select three rows\n",
      "dataframe.iloc[:4]\n",
      "\n",
      "3.3 Navigating DataFrames \n",
      "\n",
      "| \n",
      "\n",
      "37\n",
      "\n",
      "\f",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton\n",
      "1 Allison, Miss Helen Loraine\n",
      "2 Allison, Mr Hudson Joshua Creighton\n",
      "3 Allison, Mrs Hudson JC (Bessie Waldo Daniels)\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "1st\n",
      "2.0\n",
      "1st\n",
      "30.0 male\n",
      "1st\n",
      "25.0\n",
      "1st\n",
      "\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "DataFrames do not need to be numerically indexed. We can set the index of a Data‐\n",
      "Frame to any value where the value is unique to each row. For example, we can set the\n",
      "index to be passenger names and then select rows using a name:\n",
      "\n",
      "# Set index\n",
      "dataframe = dataframe.set_index(dataframe['Name'])\n",
      "\n",
      "# Show row\n",
      "dataframe.loc['Allen, Miss Elisabeth Walton']\n",
      "\n",
      "Name        Allen, Miss Elisabeth Walton\n",
      "PClass                               1st\n",
      "Age                                   29\n",
      "Sex                               female\n",
      "Survived                               1\n",
      "SexCode                                1\n",
      "Name: Allen, Miss Elisabeth Walton, dtype: object\n",
      "\n",
      "Discussion\n",
      "All rows in a pandas DataFrame have a unique index value. By default, this index is\n",
      "an integer indicating the row position in the DataFrame; however, it does not have to\n",
      "be.  DataFrame  indexes  can  be  set  to  be  unique  alphanumeric  strings  or  customer\n",
      "numbers. To select individual rows and slices of rows, pandas provides two methods:\n",
      "\n",
      "• loc is useful when the index of the DataFrame is a label (e.g., a string).\n",
      "• iloc works by looking for the position in the DataFrame. For example, iloc[0]\n",
      "will return the first row regardless of whether the index is an integer or a label.\n",
      "\n",
      "It  is  useful  to  be  comfortable  with  both  loc  and  iloc  since  they  will  come  up  a  lot\n",
      "during data cleaning.\n",
      "\n",
      "3.4 Selecting Rows Based on Conditionals\n",
      "\n",
      "Problem\n",
      "You want to select DataFrame rows based on some condition.\n",
      "\n",
      "38 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Solution\n",
      "This can be easily done in pandas. For example, if we wanted to select all the women\n",
      "on the Titanic:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Show top two rows where column 'sex' is 'female'\n",
      "dataframe[dataframe['Sex'] == 'female'].head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "Take  a  second  and  look  at  the  format  of  this  solution.  dataframe['Sex']  ==\n",
      "'female' is our conditional statement; by wrapping that in dataframe[] we are tell‐\n",
      "ing  pandas  to  “select  all  the  rows  in  the  DataFrame  where  the  value  of  data\n",
      "frame['Sex'] is 'female'.\n",
      "\n",
      "Multiple conditions are easy as well. For example, here we select all the rows where\n",
      "the passenger is a female 65 or older:\n",
      "\n",
      "# Filter rows\n",
      "dataframe[(dataframe['Sex'] == 'female') & (dataframe['Age'] >= 65)]\n",
      "\n",
      "Name\n",
      "\n",
      "73 Crosby, Mrs Edward Gifford (Catherine Elizabet...\n",
      "\n",
      "PClass Age\n",
      "69.0\n",
      "1st\n",
      "\n",
      "Sex\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "Discussion\n",
      "Conditionally  selecting  and  filtering  data  is  one  of  the  most  common  tasks  in  data\n",
      "wrangling. You rarely want all the raw data from the source; instead, you are interes‐\n",
      "ted in only some subsection of it. For example, you might only be interested in stores\n",
      "in certain states or the records of patients over a certain age.\n",
      "\n",
      "3.5 Replacing Values\n",
      "\n",
      "Problem\n",
      "You need to replace values in a DataFrame.\n",
      "\n",
      "3.5 Replacing Values \n",
      "\n",
      "| \n",
      "\n",
      "39\n",
      "\n",
      "\f",
      "Solution\n",
      "pandas’  replace  is  an  easy  way  to  find  and  replace  values.  For  example,  we  can\n",
      "replace any instance of \"female\" in the Sex column with \"Woman\":\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Replace values, show two rows\n",
      "dataframe['Sex'].replace(\"female\", \"Woman\").head(2)\n",
      "\n",
      "0    Woman\n",
      "1    Woman\n",
      "Name: Sex, dtype: object\n",
      "\n",
      "We can also replace multiple values at the same time:\n",
      "\n",
      "# Replace \"female\" and \"male with \"Woman\" and \"Man\"\n",
      "dataframe['Sex'].replace([\"female\", \"male\"], [\"Woman\", \"Man\"]).head(5)\n",
      "\n",
      "0    Woman\n",
      "1    Woman\n",
      "2      Man\n",
      "3    Woman\n",
      "4      Man\n",
      "Name: Sex, dtype: object\n",
      "\n",
      "We  can  also  find  and  replace  across  the  entire  DataFrame  object  by  specifying  the\n",
      "whole data frame instead of a single column:\n",
      "\n",
      "# Replace values, show two rows\n",
      "dataframe.replace(1, \"One\").head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Survived SexCode\n",
      "\n",
      "PClass Age\n",
      "29\n",
      "2\n",
      "\n",
      "Sex\n",
      "female One\n",
      "female\n",
      "\n",
      "0\n",
      "\n",
      "One\n",
      "One\n",
      "\n",
      "replace also accepts regular expressions:\n",
      "\n",
      "# Replace values, show two rows\n",
      "dataframe.replace(r\"1st\", \"First\", regex=True).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton First\n",
      "First\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "40 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Discussion\n",
      "replace is a tool we use to replace values that is simple and yet has the powerful abil‐\n",
      "ity to accept regular expressions.\n",
      "\n",
      "3.6 Renaming Columns\n",
      "\n",
      "Problem\n",
      "You want to rename a column in a pandas DataFrame.\n",
      "\n",
      "Solution\n",
      "Rename columns using the rename method:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Rename column, show two rows\n",
      "dataframe.rename(columns={'PClass': 'Passenger Class'}).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Passenger Class Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "Notice that the rename method can accept a dictionary as a parameter. We can use the\n",
      "dictionary to change multiple column names at once:\n",
      "\n",
      "# Rename columns, show two rows\n",
      "dataframe.rename(columns={'PClass': 'Passenger Class', 'Sex': 'Gender'}).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Passenger Class Age Gender\n",
      "female\n",
      "female\n",
      "\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "Discussion\n",
      "Using rename with a dictionary as an argument to the columns parameter is my pre‐\n",
      "ferred way to rename columns because it works with any number of columns. If we\n",
      "want to rename all columns at once, this helpful snippet of code creates a dictionary\n",
      "with the old column names as keys and empty strings as values:\n",
      "\n",
      "3.6 Renaming Columns \n",
      "\n",
      "| \n",
      "\n",
      "41\n",
      "\n",
      "\f",
      "# Load library\n",
      "import collections\n",
      "\n",
      "# Create dictionary\n",
      "column_names = collections.defaultdict(str)\n",
      "\n",
      "# Create keys\n",
      "for name in dataframe.columns:\n",
      "    column_names[name]\n",
      "\n",
      "# Show dictionary\n",
      "column_names\n",
      "\n",
      "defaultdict(str,\n",
      "            {'Age': '',\n",
      "             'Name': '',\n",
      "             'PClass': '',\n",
      "             'Sex': '',\n",
      "             'SexCode': '',\n",
      "             'Survived': ''})\n",
      "\n",
      "3.7 Finding the Minimum, Maximum, Sum, Average, and\n",
      "Count\n",
      "\n",
      "Problem\n",
      "You want to find the min, max, sum, average, or count of a numeric column.\n",
      "\n",
      "Solution\n",
      "pandas comes with some built-in methods for commonly used descriptive statistics:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Calculate statistics\n",
      "print('Maximum:', dataframe['Age'].max())\n",
      "print('Minimum:', dataframe['Age'].min())\n",
      "print('Mean:', dataframe['Age'].mean())\n",
      "print('Sum:', dataframe['Age'].sum())\n",
      "print('Count:', dataframe['Age'].count())\n",
      "\n",
      "Maximum: 71.0\n",
      "Minimum: 0.17\n",
      "Mean: 30.397989417989415\n",
      "\n",
      "42 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Sum: 22980.879999999997\n",
      "Count: 756\n",
      "\n",
      "Discussion\n",
      "In addition to the statistics used in the solution, pandas offers variance (var), stan‐\n",
      "dard  deviation  (std),  kurtosis  (kurt),  skewness  (skew),  standard  error  of  the  mean\n",
      "(sem), mode (mode), median (median), and a number of others.\n",
      "\n",
      "Furthermore, we can also apply these methods to the whole DataFrame:\n",
      "\n",
      "# Show counts\n",
      "dataframe.count()\n",
      "\n",
      "Name        1313\n",
      "PClass      1313\n",
      "Age          756\n",
      "Sex         1313\n",
      "Survived    1313\n",
      "SexCode     1313\n",
      "dtype: int64\n",
      "\n",
      "3.8 Finding Unique Values\n",
      "\n",
      "Problem\n",
      "You want to select all unique values in a column.\n",
      "\n",
      "Solution\n",
      "Use unique to view an array of all unique values in a column:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Select unique values\n",
      "dataframe['Sex'].unique()\n",
      "\n",
      "array(['female', 'male'], dtype=object)\n",
      "\n",
      "Alternatively, value_counts will display all unique values with the number of times\n",
      "each value appears:\n",
      "\n",
      "# Show counts\n",
      "dataframe['Sex'].value_counts()\n",
      "\n",
      "3.8 Finding Unique Values \n",
      "\n",
      "| \n",
      "\n",
      "43\n",
      "\n",
      "\f",
      "male      851\n",
      "female    462\n",
      "Name: Sex, dtype: int64\n",
      "\n",
      "Discussion\n",
      "Both unique and value_counts are useful for manipulating and exploring categorical\n",
      "columns. Very often in categorical columns there will be classes that need to be han‐\n",
      "dled in the data wrangling phase. For example, in the Titanic dataset, PClass is a col‐\n",
      "umn  indicating  the  class  of  a  passenger’s  ticket.  There  were  three  classes  on  the\n",
      "Titanic; however, if we use value_counts we can see a problem:\n",
      "\n",
      "# Show counts\n",
      "dataframe['PClass'].value_counts()\n",
      "\n",
      "3rd    711\n",
      "1st    322\n",
      "2nd    279\n",
      "*        1\n",
      "Name: PClass, dtype: int64\n",
      "\n",
      "While almost all passengers belong to one of three classes as expected, a single pas‐\n",
      "senger has the class *. There are a number of strategies for handling this type of issue,\n",
      "which  we  will  address  in  Chapter  5,  but  for  now  just  realize  that  “extra”  classes  are\n",
      "common in categorical data and should not be ignored.\n",
      "\n",
      "Finally, if we simply want to count the number of unique values, we can use nunique:\n",
      "\n",
      "# Show number of unique values\n",
      "dataframe['PClass'].nunique()\n",
      "\n",
      "4\n",
      "\n",
      "3.9 Handling Missing Values\n",
      "\n",
      "Problem\n",
      "You want to select missing values in a DataFrame.\n",
      "\n",
      "Solution\n",
      "isnull and notnull return booleans indicating whether a value is missing:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "\n",
      "44 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "## Select missing values, show two rows\n",
      "dataframe[dataframe['Age'].isnull()].head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "12 Aubert, Mrs Leontine Pauline\n",
      "13 Barkworth, Mr Algernon H\n",
      "\n",
      "Sex\n",
      "\n",
      "PClass Age\n",
      "1st\n",
      "1st\n",
      "\n",
      "NaN female\n",
      "NaN male\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "1\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Discussion\n",
      "Missing values are a ubiquitous problem in data wrangling, yet many underestimate\n",
      "the difficulty of working with missing data. pandas uses NumPy’s NaN (“Not A Num‐\n",
      "ber”) value to denote missing values, but it is important to note that NaN is not fully\n",
      "implemented natively in pandas. For example, if we wanted to replace all strings con‐\n",
      "taining male with missing values, we return an error:\n",
      "\n",
      "# Attempt to replace values with NaN\n",
      "dataframe['Sex'] = dataframe['Sex'].replace('male', NaN)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "NameError                                 Traceback (most recent call last)\n",
      "\n",
      "<ipython-input-7-5682d714f87d> in <module>()\n",
      "      1 # Attempt to replace values with NaN\n",
      "----> 2 dataframe['Sex'] = dataframe['Sex'].replace('male', NaN)\n",
      "\n",
      "NameError: name 'NaN' is not defined\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "To have full functionality with NaN we need to import the NumPy library first:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Replace values with NaN\n",
      "dataframe['Sex'] = dataframe['Sex'].replace('male', np.nan)\n",
      "\n",
      "Oftentimes  a  dataset  uses  a  specific  value  to  denote  a  missing  observation,  such  as\n",
      "NONE,  -999,  or  ..  pandas’  read_csv  includes  a  parameter  allowing  us  to  specify  the\n",
      "values used to indicate missing values:\n",
      "\n",
      "# Load data, set missing values\n",
      "dataframe = pd.read_csv(url, na_values=[np.nan, 'NONE', -999])\n",
      "\n",
      "3.9 Handling Missing Values \n",
      "\n",
      "| \n",
      "\n",
      "45\n",
      "\n",
      "\f",
      "3.10 Deleting a Column\n",
      "\n",
      "Problem\n",
      "You want to delete a column from your DataFrame.\n",
      "\n",
      "Solution\n",
      "The best way to delete a column is to use drop with the parameter  axis=1 (i.e., the\n",
      "column axis):\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Delete column\n",
      "dataframe.drop('Age', axis=1).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "PClass\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "You can also use a list of column names as the main argument to drop multiple col‐\n",
      "umns at once:\n",
      "\n",
      "# Drop columns\n",
      "dataframe.drop(['Age', 'Sex'], axis=1).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "PClass\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "If a column does not have a name (which can sometimes happen), you can drop it by\n",
      "its column index using dataframe.columns:\n",
      "\n",
      "# Drop column\n",
      "dataframe.drop(dataframe.columns[1], axis=1).head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "Age\n",
      "0 Allen, Miss Elisabeth Walton 29.0\n",
      "2.0\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "46 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Discussion\n",
      "drop  is  the  idiomatic  method  of  deleting  a  column.  An  alternative  method  is  del\n",
      "dataframe['Age'], which works most of the time but is not recommended because\n",
      "of  how  it  is  called  within  pandas  (the  details  of  which  are  outside  the  scope  of  this\n",
      "book).\n",
      "\n",
      "One  habit  I  recommend  learning  is  to  never  use  pandas’  inplace=True  argument.\n",
      "Many  pandas  methods  include  an  inplace  parameter,  which  when  True  edits  the\n",
      "DataFrame directly. This can lead to problems in more complex data processing pipe‐\n",
      "lines because we are treating the DataFrames as mutable objects (which they techni‐\n",
      "cally are). I recommend treating DataFrames as immutable objects. For example:\n",
      "\n",
      "# Create a new DataFrame\n",
      "dataframe_name_dropped = dataframe.drop(dataframe.columns[0], axis=1)\n",
      "\n",
      "In this example, we are not mutating the DataFrame dataframe but instead are mak‐\n",
      "ing  a  new  DataFrame  that  is  an  altered  version  of  dataframe  called  data\n",
      "frame_name_dropped.  If  you  treat  your  DataFrames  as  immutable  objects,  you  will\n",
      "save yourself a lot of headaches down the road.\n",
      "\n",
      "3.11 Deleting a Row\n",
      "\n",
      "Problem\n",
      "You want to delete one or more rows from a DataFrame.\n",
      "\n",
      "Solution\n",
      "Use a boolean condition to create a new DataFrame excluding the rows you want to\n",
      "delete:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Delete rows, show first two rows of output\n",
      "dataframe[dataframe['Sex'] != 'male'].head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "3.11 Deleting a Row \n",
      "\n",
      "| \n",
      "\n",
      "47\n",
      "\n",
      "\f",
      "Discussion\n",
      "While  technically  you  can  use  the  drop  method  (for  example,  df.drop([0,  1],\n",
      "axis=0)  to  drop  the  first  two  rows),  a  more  practical  method  is  simply  to  wrap  a\n",
      "boolean condition inside df[]. The reason is because we can use the power of condi‐\n",
      "tionals to delete either a single row or (far more likely) many rows at once.\n",
      "\n",
      "We  can  use  boolean  conditions  to  easily  delete  single  rows  by  matching  a  unique\n",
      "value:\n",
      "\n",
      "# Delete row, show first two rows of output\n",
      "dataframe[dataframe['Name'] != 'Allison, Miss Helen Loraine'].head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "0 Allen, Miss Elisabeth Walton\n",
      "1st\n",
      "30.0 male\n",
      "2 Allison, Mr Hudson Joshua Creighton 1st\n",
      "\n",
      "Sex\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "And we can even use it to delete a single row by row index:\n",
      "\n",
      "# Delete row, show first two rows of output\n",
      "dataframe[dataframe.index != 0].head(2)\n",
      "\n",
      "Name\n",
      "\n",
      "PClass Age\n",
      "2.0\n",
      "1 Allison, Miss Helen Loraine\n",
      "1st\n",
      "30.0 male\n",
      "2 Allison, Mr Hudson Joshua Creighton 1st\n",
      "\n",
      "Sex\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "3.12 Dropping Duplicate Rows\n",
      "\n",
      "Problem\n",
      "You want to drop duplicate rows from your DataFrame.\n",
      "\n",
      "Solution\n",
      "Use drop_duplicates, but be mindful of the parameters:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Drop duplicates, show first two rows of output\n",
      "dataframe.drop_duplicates().head(2)\n",
      "\n",
      "48 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "Name\n",
      "\n",
      "0 Allen, Miss Elisabeth Walton 1st\n",
      "1st\n",
      "1 Allison, Miss Helen Loraine\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "2.0\n",
      "\n",
      "Sex\n",
      "female\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "Discussion\n",
      "A keen reader will notice that the solution didn’t actually drop any rows:\n",
      "\n",
      "# Show number of rows\n",
      "print(\"Number Of Rows In The Original DataFrame:\", len(dataframe))\n",
      "print(\"Number Of Rows After Deduping:\", len(dataframe.drop_duplicates()))\n",
      "\n",
      "Number Of Rows In The Original DataFrame: 1313\n",
      "Number Of Rows After Deduping: 1313\n",
      "\n",
      "The  reason  is  because  drop_duplicates  defaults  to  only  dropping  rows  that  match\n",
      "perfectly across all columns. Under this condition, every row in our DataFrame, data\n",
      "frame,  is  actually  unique.  However,  often  we  want  to  consider  only  a  subset  of  col‐\n",
      "umns to check for duplicate rows. We can accomplish this using the subset parame‐\n",
      "ter:\n",
      "\n",
      "# Drop duplicates\n",
      "dataframe.drop_duplicates(subset=['Sex'])\n",
      "\n",
      "Name\n",
      "\n",
      "PClass Age\n",
      "29.0\n",
      "0 Allen, Miss Elisabeth Walton\n",
      "1st\n",
      "30.0 male\n",
      "2 Allison, Mr Hudson Joshua Creighton 1st\n",
      "\n",
      "Sex\n",
      "female\n",
      "\n",
      "Survived SexCode\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Take a close look at the preceding output: we told drop_duplicates to only consider\n",
      "any two rows with the same value for Sex to be duplicates and to drop them. Now we\n",
      "are left with a DataFrame of only two rows: one man and one woman. You might be\n",
      "asking why drop_duplicates decided to keep these two rows instead of two different\n",
      "rows. The answer is that drop_duplicates defaults to keeping the first occurrence of\n",
      "a duplicated row and dropping the rest. We can control this behavior using the keep\n",
      "parameter:\n",
      "\n",
      "# Drop duplicates\n",
      "dataframe.drop_duplicates(subset=['Sex'], keep='last')\n",
      "\n",
      "Name\n",
      "\n",
      "1307 Zabour, Miss Tamini\n",
      "1312 Zimmerman, Leo\n",
      "\n",
      "Sex\n",
      "\n",
      "PClass Age\n",
      "3rd\n",
      "3rd\n",
      "\n",
      "NaN female\n",
      "29.0 male\n",
      "\n",
      "Survived SexCode\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "A related method is duplicated, which returns a boolean series denoting if a row is a\n",
      "duplicate or not. This is a good option if you don’t want to simply drop duplicates.\n",
      "\n",
      "3.12 Dropping Duplicate Rows \n",
      "\n",
      "| \n",
      "\n",
      "49\n",
      "\n",
      "\f",
      "3.13 Grouping Rows by Values\n",
      "\n",
      "Problem\n",
      "You want to group individual rows according to some shared value.\n",
      "\n",
      "Solution\n",
      "groupby is one of the most powerful features in pandas:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Group rows by the values of the column 'Sex', calculate mean\n",
      "# of each group\n",
      "dataframe.groupby('Sex').mean()\n",
      "\n",
      "Age\n",
      "\n",
      "Survived SexCode\n",
      "\n",
      "Sex\n",
      "female 29.396424\n",
      "31.014338\n",
      "male\n",
      "\n",
      "0.666667\n",
      "0.166863\n",
      "\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Discussion\n",
      "groupby is where data wrangling really starts to take shape. It is very common to have\n",
      "a  DataFrame  where  each  row  is  a  person  or  an  event  and  we  want  to  group  them\n",
      "according to some criterion and then calculate a statistic. For example, you can imag‐\n",
      "ine a DataFrame where each row is an individual sale at a national restaurant chain\n",
      "and we want the total sales per restaurant. We can accomplish this by grouping rows\n",
      "by individual resturants and then calculating the sum of each group.\n",
      "\n",
      "Users new to groupby often write a line like this and are confused by what is returned:\n",
      "\n",
      "# Group rows\n",
      "dataframe.groupby('Sex')\n",
      "\n",
      "<pandas.core.groupby.DataFrameGroupBy object at 0x10efacf28>\n",
      "\n",
      "Why didn’t it return something more useful? The reason is because groupby needs to\n",
      "be paired with some operation we want to apply to each group, such as calculating an\n",
      "aggregate statistic (e.g., mean, median, sum). When talking about grouping we often\n",
      "use shorthand and say “group by gender,” but that is incomplete. For grouping to be\n",
      "\n",
      "50 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "useful,  we  need  to  group  by  something  and  then  apply  a  function  to  each  of  those\n",
      "groups:\n",
      "\n",
      "# Group rows, count rows\n",
      "dataframe.groupby('Survived')['Name'].count()\n",
      "\n",
      "Survived\n",
      "0    863\n",
      "1    450\n",
      "Name: Name, dtype: int64\n",
      "\n",
      "Notice  Name  added  after  the  groupby?  That  is  because  particular  summary  statistics\n",
      "are only meaningful to certain types of data. For example, while calculating the aver‐\n",
      "age age by gender makes sense, calculating the total age by gender does not. In this\n",
      "case  we  group  the  data  into  survived  or  not,  then  count  the  number  of  names  (i.e.,\n",
      "passengers) in each group.\n",
      "\n",
      "We can also group by a first column, then group that grouping by a second column:\n",
      "\n",
      "# Group rows, calculate mean\n",
      "dataframe.groupby(['Sex','Survived'])['Age'].mean()\n",
      "\n",
      "Sex     Survived\n",
      "female  0           24.901408\n",
      "        1           30.867143\n",
      "male    0           32.320780\n",
      "        1           25.951875\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "3.14 Grouping Rows by Time\n",
      "\n",
      "Problem\n",
      "You need to group individual rows by time periods.\n",
      "\n",
      "Solution\n",
      "Use resample to group rows by chunks of time:\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Create date range\n",
      "time_index = pd.date_range('06/06/2017', periods=100000, freq='30S')\n",
      "\n",
      "# Create DataFrame\n",
      "dataframe = pd.DataFrame(index=time_index)\n",
      "\n",
      "# Create column of random values\n",
      "dataframe['Sale_Amount'] = np.random.randint(1, 10, 100000)\n",
      "\n",
      "3.14 Grouping Rows by Time \n",
      "\n",
      "| \n",
      "\n",
      "51\n",
      "\n",
      "\f",
      "# Group rows by week, calculate sum per week\n",
      "dataframe.resample('W').sum()\n",
      "\n",
      "Sale_Amount\n",
      "\n",
      "2017-06-11 86423\n",
      "2017-06-18 101045\n",
      "2017-06-25 100867\n",
      "2017-07-02 100894\n",
      "2017-07-09 100438\n",
      "2017-07-16 10297\n",
      "\n",
      "Discussion\n",
      "Our standard Titanic dataset does not contain a datetime column, so for this recipe\n",
      "we have generated a simple DataFrame where each row is an individual sale. For each\n",
      "sale we know its date and time and its dollar amount (this data isn’t realistic because\n",
      "every sale takes place precisely 30 seconds apart and is an exact dollar amount, but\n",
      "for the sake of simplicity let us pretend).\n",
      "\n",
      "The raw data looks like this:\n",
      "\n",
      "# Show three rows\n",
      "dataframe.head(3)\n",
      "\n",
      "Sale_Amount\n",
      "\n",
      "2017-06-06 00:00:00 7\n",
      "2017-06-06 00:00:30 2\n",
      "2017-06-06 00:01:00 7\n",
      "\n",
      "Notice  that  the  date  and  time  of  each  sale  is  the  index  of  the  DataFrame;  this  is\n",
      "because resample requires the index to be datetime-like values.\n",
      "\n",
      "Using resample we can group the rows by a wide array of time periods (offsets) and\n",
      "then we can calculate some statistic on each time group:\n",
      "\n",
      "# Group by two weeks, calculate mean\n",
      "dataframe.resample('2W').mean()\n",
      "\n",
      "Sale_Amount\n",
      "\n",
      "2017-06-11 5.001331\n",
      "2017-06-25 5.007738\n",
      "2017-07-09 4.993353\n",
      "2017-07-23 4.950481\n",
      "\n",
      "52 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "# Group by month, count rows\n",
      "dataframe.resample('M').count()\n",
      "\n",
      "Sale_Amount\n",
      "\n",
      "2017-06-30 72000\n",
      "2017-07-31 28000\n",
      "\n",
      "You might notice that in the two outputs the datetime index is a date despite the fact\n",
      "that  we  are  grouping  by  weeks  and  months,  respectively.  The  reason  is  because  by\n",
      "default resample returns the label of the right “edge” (the last label) of the time group.\n",
      "We can control this behavior using the label parameter:\n",
      "\n",
      "# Group by month, count rows\n",
      "dataframe.resample('M', label='left').count()\n",
      "\n",
      "Sale_Amount\n",
      "\n",
      "2017-05-31 72000\n",
      "2017-06-30 28000\n",
      "\n",
      "See Also\n",
      "\n",
      "• List of pandas time offset aliases\n",
      "\n",
      "3.15 Looping Over a Column\n",
      "\n",
      "Problem\n",
      "You want to iterate over every element in a column and apply some action.\n",
      "\n",
      "Solution\n",
      "You can treat a pandas column like any other sequence in Python:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Print first two names uppercased\n",
      "for name in dataframe['Name'][0:2]:\n",
      "    print(name.upper())\n",
      "\n",
      "3.15 Looping Over a Column \n",
      "\n",
      "| \n",
      "\n",
      "53\n",
      "\n",
      "\f",
      "ALLEN, MISS ELISABETH WALTON\n",
      "ALLISON, MISS HELEN LORAINE\n",
      "\n",
      "Discussion\n",
      "In addition to loops (often called for loops), we can also use list comprehensions:\n",
      "\n",
      "# Show first two names uppercased\n",
      "[name.upper() for name in dataframe['Name'][0:2]]\n",
      "\n",
      "['ALLEN, MISS ELISABETH WALTON', 'ALLISON, MISS HELEN LORAINE']\n",
      "\n",
      "Despite the temptation to fall back on for loops, a more Pythonic solution would use\n",
      "pandas’ apply method, described in the next recipe.\n",
      "\n",
      "3.16 Applying a Function Over All Elements in a Column\n",
      "\n",
      "Problem\n",
      "You want to apply some function over all elements in a column.\n",
      "\n",
      "Solution\n",
      "Use apply to apply a built-in or custom function on every element in a column:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Create function\n",
      "def uppercase(x):\n",
      "    return x.upper()\n",
      "\n",
      "# Apply function, show two rows\n",
      "dataframe['Name'].apply(uppercase)[0:2]\n",
      "\n",
      "0    ALLEN, MISS ELISABETH WALTON\n",
      "1     ALLISON, MISS HELEN LORAINE\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Discussion\n",
      "apply is a great way to do data cleaning and wrangling. It is common to write a func‐\n",
      "tion to perform some useful operation (separate first and last names, convert strings\n",
      "to floats, etc.) and then map that function to every element in a column.\n",
      "\n",
      "54 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "3.17 Applying a Function to Groups\n",
      "\n",
      "Problem\n",
      "You have grouped rows using groupby and want to apply a function to each group.\n",
      "\n",
      "Solution\n",
      "Combine groupby and apply:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create URL\n",
      "url = 'https://tinyurl.com/titanic-csv'\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.read_csv(url)\n",
      "\n",
      "# Group rows, apply function to groups\n",
      "dataframe.groupby('Sex').apply(lambda x: x.count())\n",
      "\n",
      "Name PClass Age\n",
      "\n",
      "Sex\n",
      "\n",
      "Survived SexCode\n",
      "\n",
      "Sex\n",
      "female 462\n",
      "851\n",
      "male\n",
      "\n",
      "462\n",
      "851\n",
      "\n",
      "288\n",
      "468\n",
      "\n",
      "462\n",
      "851\n",
      "\n",
      "462\n",
      "851\n",
      "\n",
      "462\n",
      "851\n",
      "\n",
      "Discussion\n",
      "In  Recipe  3.16  I  mentioned  apply.  apply  is  particularly  useful  when  you  want  to\n",
      "apply a function to groups. By combining groupby and apply we can calculate cus‐\n",
      "tom statistics or apply any function to each group separately.\n",
      "\n",
      "3.18 Concatenating DataFrames\n",
      "\n",
      "Problem\n",
      "You want to concatenate two DataFrames.\n",
      "\n",
      "Solution\n",
      "Use concat with axis=0 to concatenate along the row axis:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create DataFrame\n",
      "\n",
      "3.17 Applying a Function to Groups \n",
      "\n",
      "| \n",
      "\n",
      "55\n",
      "\n",
      "\f",
      "data_a = {'id': ['1', '2', '3'],\n",
      "          'first': ['Alex', 'Amy', 'Allen'],\n",
      "          'last': ['Anderson', 'Ackerman', 'Ali']}\n",
      "dataframe_a = pd.DataFrame(data_a, columns = ['id', 'first', 'last'])\n",
      "\n",
      "# Create DataFrame\n",
      "data_b = {'id': ['4', '5', '6'],\n",
      "          'first': ['Billy', 'Brian', 'Bran'],\n",
      "          'last': ['Bonder', 'Black', 'Balwner']}\n",
      "dataframe_b = pd.DataFrame(data_b, columns = ['id', 'first', 'last'])\n",
      "\n",
      "# Concatenate DataFrames by rows\n",
      "pd.concat([dataframe_a, dataframe_b], axis=0)\n",
      "\n",
      "last\n",
      "Anderson\n",
      "Ackerman\n",
      "\n",
      "id first\n",
      "Alex\n",
      "Amy\n",
      "Allen Ali\n",
      "Billy\n",
      "Brian Black\n",
      "Bran\n",
      "\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "0 4\n",
      "1 5\n",
      "2 6\n",
      "\n",
      "Balwner\n",
      "\n",
      "Bonder\n",
      "\n",
      "You can use axis=1 to concatenate along the column axis:\n",
      "\n",
      "# Concatenate DataFrames by columns\n",
      "pd.concat([dataframe_a, dataframe_b], axis=1)\n",
      "\n",
      "id first\n",
      "Alex\n",
      "Amy\n",
      "Allen Ali\n",
      "\n",
      "id first\n",
      "last\n",
      "Billy\n",
      "Anderson\n",
      "4\n",
      "Brian Black\n",
      "Ackerman 5\n",
      "Bran\n",
      "6\n",
      "\n",
      "last\n",
      "Bonder\n",
      "\n",
      "Balwner\n",
      "\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "\n",
      "Discussion\n",
      "Concatenating  is  not  a  word  you  hear  much  outside  of  computer  science  and  pro‐\n",
      "gramming, so if you have not heard it before, do not worry. The informal definition\n",
      "of concatenate is to glue two objects together. In the solution we glued together two\n",
      "small DataFrames using the axis parameter to indicate whether we wanted to stack\n",
      "the two DataFrames on top of each other or place them side by side.\n",
      "\n",
      "Alternatively we can use append to add a new row to a DataFrame:\n",
      "\n",
      "# Create row\n",
      "row = pd.Series([10, 'Chris', 'Chillon'], index=['id', 'first', 'last'])\n",
      "\n",
      "# Append row\n",
      "dataframe_a.append(row, ignore_index=True)\n",
      "\n",
      "56 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "id\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 10\n",
      "\n",
      "last\n",
      "Anderson\n",
      "Ackerman\n",
      "\n",
      "first\n",
      "Alex\n",
      "Amy\n",
      "Allen Ali\n",
      "Chris\n",
      "\n",
      "Chillon\n",
      "\n",
      "3.19 Merging DataFrames\n",
      "\n",
      "Problem\n",
      "You want to merge two DataFrames.\n",
      "\n",
      "Solution\n",
      "To inner join, use merge with the on parameter to specify the column to merge on:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create DataFrame\n",
      "employee_data = {'employee_id': ['1', '2', '3', '4'],\n",
      "                 'name': ['Amy Jones', 'Allen Keys', 'Alice Bees',\n",
      "                 'Tim Horton']}\n",
      "dataframe_employees = pd.DataFrame(employee_data, columns = ['employee_id',\n",
      "                                                              'name'])\n",
      "\n",
      "# Create DataFrame\n",
      "sales_data = {'employee_id': ['3', '4', '5', '6'],\n",
      "              'total_sales': [23456, 2512, 2345, 1455]}\n",
      "dataframe_sales = pd.DataFrame(sales_data, columns = ['employee_id',\n",
      "                                                      'total_sales'])\n",
      "\n",
      "# Merge DataFrames\n",
      "pd.merge(dataframe_employees, dataframe_sales, on='employee_id')\n",
      "\n",
      "employee_id name\n",
      "\n",
      "total_sales\n",
      "Alice Bees\n",
      "23456\n",
      "Tim Horton 2512\n",
      "\n",
      "0 3\n",
      "1 4\n",
      "\n",
      "merge defaults to inner joins. If we want to do an outer join, we can specify that with\n",
      "the how parameter:\n",
      "\n",
      "3.19 Merging DataFrames \n",
      "\n",
      "| \n",
      "\n",
      "57\n",
      "\n",
      "\f",
      "# Merge DataFrames\n",
      "pd.merge(dataframe_employees, dataframe_sales, on='employee_id', how='outer')\n",
      "\n",
      "employee_id name\n",
      "\n",
      "total_sales\n",
      "NaN\n",
      "Amy Jones\n",
      "NaN\n",
      "Allen Keys\n",
      "Alice Bees\n",
      "23456.0\n",
      "Tim Horton 2512.0\n",
      "2345.0\n",
      "NaN\n",
      "1455.0\n",
      "NaN\n",
      "\n",
      "employee_id name\n",
      "\n",
      "total_sales\n",
      "NaN\n",
      "Amy Jones\n",
      "NaN\n",
      "Allen Keys\n",
      "Alice Bees\n",
      "23456.0\n",
      "Tim Horton 2512.0\n",
      "\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "5 6\n",
      "\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "\n",
      "The same parameter can be used to specify left and right joins:\n",
      "\n",
      "# Merge DataFrames\n",
      "pd.merge(dataframe_employees, dataframe_sales, on='employee_id', how='left')\n",
      "\n",
      "We can also specify the column name in each DataFrame to merge on:\n",
      "\n",
      "# Merge DataFrames\n",
      "pd.merge(dataframe_employees,\n",
      "         dataframe_sales,\n",
      "         left_on='employee_id',\n",
      "         right_on='employee_id')\n",
      "\n",
      "employee_id name\n",
      "\n",
      "total_sales\n",
      "Alice Bees\n",
      "23456\n",
      "Tim Horton 2512\n",
      "\n",
      "0 3\n",
      "1 4\n",
      "\n",
      "If instead of merging on two columns we want to merge on the indexes of each Data‐\n",
      "Frame, we can replace the left_on and right_on parameters with right_index=True\n",
      "and left_index=True.\n",
      "\n",
      "Discussion\n",
      "Oftentimes, the data we need to use is complex; it doesn’t always come in one piece.\n",
      "Instead  in  the  real  world,  we’re  usually  faced  with  disparate  datasets,  from  multiple\n",
      "database  queries  or  files.  To  get  all  that  data  into  one  place,  we  can  load  each  data\n",
      "query  or  data  file  into  pandas  as  individual  DataFrames  and  then  merge  them\n",
      "together into a single DataFrame.\n",
      "\n",
      "58 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "This process might be familiar to anyone who has used SQL, a popular language for\n",
      "doing merging operations (called joins). While the exact parameters used by pandas\n",
      "will  be  different,  they  follow  the  same  general  patterns  used  by  other  software  lan‐\n",
      "guages and tools.\n",
      "\n",
      "There are three aspects to specify with any merge operation. First, we have to specify\n",
      "the two DataFrames we want to merge together. In the solution we named them data\n",
      "frame_employees and  dataframe_sales. Second, we have to specify the name(s) of\n",
      "the columns to merge on—that is, the columns whose values are shared between the\n",
      "two  DataFrames.  For  example,  in  our  solution  both  DataFrames  have  a  column\n",
      "named  employee_id. To merge the two DataFrames we will match up the values in\n",
      "each DataFrame’s employee_id column with each other. If these two columns use the\n",
      "same name, we can use the on parameter. However, if they have different names we\n",
      "can use left_on and right_on.\n",
      "\n",
      "What is the left and right DataFrame? The simple answer is that the left DataFrame is\n",
      "the first one we specified in merge and the right DataFrame is the second one. This\n",
      "language comes up again in the next sets of parameters we will need.\n",
      "\n",
      "The last aspect, and most difficult for some people to grasp, is the type of merge oper‐\n",
      "ation we want to conduct. This is specified by the how parameter. merge supports the\n",
      "four main types of joins:\n",
      "\n",
      "Inner\n",
      "\n",
      "Return only the rows that match in both DataFrames (e.g., return any row with\n",
      "an  employee_id  value  appearing  in  both  dataframe_employees  and  data\n",
      "frame_sales).\n",
      "\n",
      "Outer\n",
      "\n",
      "Return all rows in both DataFrames. If a row exists in one DataFrame but not in\n",
      "the other DataFrame, fill NaN values for the missing values (e.g., return all rows\n",
      "in both employee_id and dataframe_sales).\n",
      "\n",
      "Left\n",
      "\n",
      "Return all rows from the left DataFrame but only rows from the right DataFrame\n",
      "that matched with the left DataFrame. Fill NaN values for the missing values (e.g.,\n",
      "return  all  rows  from  dataframe_employees  but  only  rows  from  data\n",
      "frame_sales  that  have  a  value  for  employee_id  that  appears  in  data\n",
      "frame_employees).\n",
      "\n",
      "Right\n",
      "\n",
      "Return all rows from the right DataFrame but only rows from the left DataFrame\n",
      "that  matched  with  the  right  DataFrame.  Fill  NaN  values  for  the  missing  values\n",
      "(e.g.,  return  all  rows  from  dataframe_sales  but  only  rows  from  data\n",
      "\n",
      "3.19 Merging DataFrames \n",
      "\n",
      "| \n",
      "\n",
      "59\n",
      "\n",
      "\f",
      "frame_employees  that  have  a  value  for  employee_id  that  appears  in  data\n",
      "frame_sales).\n",
      "\n",
      "If you did not understand all of that right now, I encourage you to play around with\n",
      "the how parameter in your code and see how it affects what merge returns.\n",
      "\n",
      "See Also\n",
      "\n",
      "• A Visual Explanation of SQL Joins\n",
      "\n",
      "• pandas documentation on merging\n",
      "\n",
      "60 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Data Wrangling\n",
      "\n",
      "\f",
      "CHAPTER 4\n",
      "Handling Numerical Data\n",
      "\n",
      "4.0 Introduction\n",
      "Quantitative  data  is  the  measurement  of  something—whether  class  size,  monthly\n",
      "sales, or student scores. The natural way to represent these quantities is numerically\n",
      "(e.g., 29 students, $529,392 in sales). In this chapter, we will cover numerous strate‐\n",
      "gies  for  transforming  raw  numerical  data  into  features  purpose-built  for  machine\n",
      "learning algorithms.\n",
      "\n",
      "4.1 Rescaling a Feature\n",
      "\n",
      "Problem\n",
      "You need to rescale the values of a numerical feature to be between two values.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s MinMaxScaler to rescale a feature array:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "\n",
      "# Create feature\n",
      "feature = np.array([[-500.5],\n",
      "                    [-100.1],\n",
      "                    [0],\n",
      "                    [100.1],\n",
      "                    [900.9]])\n",
      "\n",
      "# Create scaler\n",
      "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
      "\n",
      "61\n",
      "\n",
      "\f",
      "# Scale feature\n",
      "scaled_feature = minmax_scale.fit_transform(feature)\n",
      "\n",
      "# Show feature\n",
      "scaled_feature\n",
      "\n",
      "array([[ 0.        ],\n",
      "       [ 0.28571429],\n",
      "       [ 0.35714286],\n",
      "       [ 0.42857143],\n",
      "       [ 1.        ]])\n",
      "\n",
      "Discussion\n",
      "Rescaling  is  a  common  preprocessing  task  in  machine  learning.  Many  of  the  algo‐\n",
      "rithms described later in this book will assume all features are on the same scale, typi‐\n",
      "cally  0  to  1  or  –1  to  1.  There  are  a  number  of  rescaling  techniques,  but  one  of  the\n",
      "simplest  is  called  min-max  scaling.  Min-max  scaling  uses  the  minimum  and  maxi‐\n",
      "mum values of a feature to rescale values to within a range. Specifically, min-max cal‐\n",
      "culates:\n",
      "\n",
      "′ =\n",
      "xi\n",
      "\n",
      "xi − min x\n",
      "max x − min x\n",
      "\n",
      "where  x  is  the  feature  vector,  x’i  is  an  individual  element  of  feature  x,  and  x’i  is  the\n",
      "rescaled element. In our example, we can see from the outputted array that the fea‐\n",
      "ture has been successfully rescaled to between 0 and 1:\n",
      "\n",
      "array([[ 0.        ],\n",
      "      [ 0.28571429],\n",
      "      [ 0.35714286],\n",
      "      [ 0.42857143],\n",
      "      [ 1.        ]])\n",
      "\n",
      "scikit-learn’s  MinMaxScaler  offers  two  options  to  rescale  a  feature.  One  option  is  to\n",
      "use fit to calculate the minimum and maximum values of the feature, then use trans\n",
      "form  to  rescale  the  feature.  The  second  option  is  to  use  fit_transform  to  do  both\n",
      "operations at once. There is no mathematical difference between the two options, but\n",
      "there  is  sometimes  a  practical  benefit  to  keeping  the  operations  separate  because  it\n",
      "allows us to apply the same transformation to different sets of the data.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Feature scaling, Wikipedia\n",
      "\n",
      "• About Feature Scaling and Normalization, Sebastian Raschka\n",
      "\n",
      "62 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "4.2 Standardizing a Feature\n",
      "\n",
      "Problem\n",
      "You want to transform a feature to have a mean of 0 and a standard deviation of 1.\n",
      "\n",
      "Solution\n",
      "scikit-learn’s StandardScaler performs both transformations:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "\n",
      "# Create feature\n",
      "x = np.array([[-1000.1],\n",
      "              [-200.2],\n",
      "              [500.5],\n",
      "              [600.6],\n",
      "              [9000.9]])\n",
      "\n",
      "# Create scaler\n",
      "scaler = preprocessing.StandardScaler()\n",
      "\n",
      "# Transform the feature\n",
      "standardized = scaler.fit_transform(x)\n",
      "\n",
      "# Show feature\n",
      "standardized\n",
      "\n",
      "array([[-0.76058269],\n",
      "       [-0.54177196],\n",
      "       [-0.35009716],\n",
      "       [-0.32271504],\n",
      "       [ 1.97516685]])\n",
      "\n",
      "Discussion\n",
      "A common alternative to min-max scaling discussed in Recipe 4.1 is rescaling of fea‐\n",
      "tures  to  be  approximately  standard  normally  distributed.  To  achieve  this,  we  use \n",
      "standardization to transform the data such that it has a mean, x̄, of 0 and a standard\n",
      "deviation, σ, of 1. Specifically, each element in the feature is transformed so that:\n",
      "\n",
      "′ =\n",
      "xi\n",
      "\n",
      "xi − x\n",
      "σ\n",
      "\n",
      "4.2 Standardizing a Feature \n",
      "\n",
      "| \n",
      "\n",
      "63\n",
      "\n",
      "\f",
      "where x’i is our standardized form of xi. The transformed feature represents the num‐\n",
      "ber  of  standard  deviations  the  original  value  is  away  from  the  feature’s  mean  value\n",
      "(also called a z-score in statistics).\n",
      "\n",
      "Standardization is a common go-to scaling method for machine learning preprocess‐\n",
      "ing and in my experience is used more than min-max scaling. However, it depends on\n",
      "the learning algorithm. For example, principal component analysis often works better\n",
      "using standardization, while min-max scaling is often recommended for neural net‐\n",
      "works (both algorithms are discussed later in this book). As a general rule, I’d recom‐\n",
      "mend defaulting to standardization unless you have a specific reason to use an alter‐\n",
      "native.\n",
      "\n",
      "We can see the effect of standardization by looking at the mean and standard devia‐\n",
      "tion of our solution’s output:\n",
      "\n",
      "# Print mean and standard deviation\n",
      "print(\"Mean:\", round(standardized.mean()))\n",
      "print(\"Standard deviation:\", standardized.std())\n",
      "\n",
      "Mean: 0.0\n",
      "Standard deviation: 1.0\n",
      "\n",
      "If  our  data  has  significant  outliers,  it  can  negatively  impact  our  standardization  by\n",
      "affecting the feature’s mean and variance. In this scenario, it is often helpful to instead\n",
      "rescale  the  feature  using  the  median  and  quartile  range.  In  scikit-learn,  we  do  this\n",
      "using the RobustScaler method:\n",
      "\n",
      "# Create scaler\n",
      "robust_scaler = preprocessing.RobustScaler()\n",
      "\n",
      "# Transform feature\n",
      "robust_scaler.fit_transform(x)\n",
      "\n",
      "array([[ -1.87387612],\n",
      "       [ -0.875     ],\n",
      "       [  0.        ],\n",
      "       [  0.125     ],\n",
      "       [ 10.61488511]])\n",
      "\n",
      "4.3 Normalizing Observations\n",
      "\n",
      "Problem\n",
      "You  want  to  rescale  the  feature  values  of  observations  to  have  unit  norm  (a  total\n",
      "length of 1).\n",
      "\n",
      "Solution\n",
      "Use Normalizer with a norm argument:\n",
      "\n",
      "64 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Normalizer\n",
      "\n",
      "# Create feature matrix\n",
      "features = np.array([[0.5, 0.5],\n",
      "                     [1.1, 3.4],\n",
      "                     [1.5, 20.2],\n",
      "                     [1.63, 34.4],\n",
      "                     [10.9, 3.3]])\n",
      "\n",
      "# Create normalizer\n",
      "normalizer = Normalizer(norm=\"l2\")\n",
      "\n",
      "# Transform feature matrix\n",
      "normalizer.transform(features)\n",
      "\n",
      "array([[ 0.70710678,  0.70710678],\n",
      "       [ 0.30782029,  0.95144452],\n",
      "       [ 0.07405353,  0.99725427],\n",
      "       [ 0.04733062,  0.99887928],\n",
      "       [ 0.95709822,  0.28976368]])\n",
      "\n",
      "Discussion\n",
      "Many rescaling methods (e.g., min-max scaling and standardization) operate on fea‐\n",
      "tures;  however,  we  can  also  rescale  across  individual  observations.  Normalizer\n",
      "rescales  the  values  on  individual  observations  to  have  unit  norm  (the  sum  of  their\n",
      "lengths is 1). This type of rescaling is often used when we have many equivalent fea‐\n",
      "tures (e.g., text classification when every word or n-word group is a feature).\n",
      "\n",
      "Normalizer  provides  three  norm  options  with  Euclidean  norm  (often  called  L2)\n",
      "being the default argument:\n",
      "\n",
      "∥ x ∥2 = x1\n",
      "\n",
      "2 + x2\n",
      "\n",
      "2\n",
      "2 + ⋯ + xn\n",
      "\n",
      "where x is an individual observation and xn is that observation’s value for the nth fea‐\n",
      "ture.\n",
      "\n",
      "# Transform feature matrix\n",
      "features_l2_norm = Normalizer(norm=\"l2\").transform(features)\n",
      "\n",
      "# Show feature matrix\n",
      "features_l2_norm\n",
      "\n",
      "array([[ 0.70710678,  0.70710678],\n",
      "       [ 0.30782029,  0.95144452],\n",
      "       [ 0.07405353,  0.99725427],\n",
      "       [ 0.04733062,  0.99887928],\n",
      "       [ 0.95709822,  0.28976368]])\n",
      "\n",
      "4.3 Normalizing Observations \n",
      "\n",
      "| \n",
      "\n",
      "65\n",
      "\n",
      "\f",
      "Alternatively, we can specify Manhattan norm (L1):\n",
      "\n",
      "n\n",
      "∥ x ∥1 = ∑\n",
      "i = 1\n",
      "\n",
      "xi .\n",
      "\n",
      "# Transform feature matrix\n",
      "features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n",
      "\n",
      "# Show feature matrix\n",
      "features_l1_norm\n",
      "\n",
      "array([[ 0.5       ,  0.5       ],\n",
      "       [ 0.24444444,  0.75555556],\n",
      "       [ 0.06912442,  0.93087558],\n",
      "       [ 0.04524008,  0.95475992],\n",
      "       [ 0.76760563,  0.23239437]])\n",
      "\n",
      "Intuitively,  L2  norm  can  be  thought  of  as  the  distance  between  two  points  in  New\n",
      "York for a bird (i.e., a straight line), while L1 can be thought of as the distance for a\n",
      "human walking on the street (walk north one block, east one block, north one block,\n",
      "east one block, etc.), which is why it is called “Manhattan norm” or “Taxicab norm.”\n",
      "\n",
      "Practically,  notice  that  norm='l1'  rescales  an  observation’s  values  so  they  sum  to  1,\n",
      "which can sometimes be a desirable quality:\n",
      "\n",
      "# Print sum\n",
      "print(\"Sum of the first observation\\'s values:\",\n",
      "   features_l1_norm[0, 0] + features_l1_norm[0, 1])\n",
      "\n",
      "Sum of the first observation's values: 1.0\n",
      "\n",
      "4.4 Generating Polynomial and Interaction Features\n",
      "\n",
      "Problem\n",
      "You want to create polynominal and interaction features.\n",
      "\n",
      "Solution\n",
      "Even  though  some  choose  to  create  polynomial  and  interaction  features  manually,\n",
      "scikit-learn offers a built-in method:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "# Create feature matrix\n",
      "features = np.array([[2, 3],\n",
      "                     [2, 3],\n",
      "\n",
      "66 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "                     [2, 3]])\n",
      "\n",
      "# Create PolynomialFeatures object\n",
      "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
      "\n",
      "# Create polynomial features\n",
      "polynomial_interaction.fit_transform(features)\n",
      "\n",
      "array([[ 2.,  3.,  4.,  6.,  9.],\n",
      "       [ 2.,  3.,  4.,  6.,  9.],\n",
      "       [ 2.,  3.,  4.,  6.,  9.]])\n",
      "\n",
      "The  degree  parameter  determines  the  maximum  degree  of  the  polynomial.  For\n",
      "example, degree=2 will create new features raised to the second power:\n",
      "\n",
      "x1, x2, x1\n",
      "\n",
      "2\n",
      "2, x2\n",
      "\n",
      "while degree=3 will create new features raised to the second and third power:\n",
      "\n",
      "x1, x2, x1\n",
      "\n",
      "2, x2\n",
      "\n",
      "2, x1\n",
      "\n",
      "3\n",
      "3, x2\n",
      "\n",
      "Furthermore, by default PolynomialFeatures includes interaction features:\n",
      "\n",
      "x1x2\n",
      "\n",
      "We  can  restrict  the  features  created  to  only  interaction  features  by  setting  interac\n",
      "tion_only to True:\n",
      "\n",
      "interaction = PolynomialFeatures(degree=2,\n",
      "              interaction_only=True, include_bias=False)\n",
      "              interaction.fit_transform(features)\n",
      "\n",
      "array([[ 2.,  3.,  6.],\n",
      "       [ 2.,  3.,  6.],\n",
      "       [ 2.,  3.,  6.]])\n",
      "\n",
      "Discussion\n",
      "Polynomial features are often created when we want to include the notion that there\n",
      "exists a nonlinear relationship between the features and the target. For example, we\n",
      "might suspect that the effect of age on the probability of having a major medical con‐\n",
      "dition  is  not  constant  over  time  but  increases  as  age  increases.  We  can  encode  that\n",
      "nonconstant effect in a feature, x, by generating that feature’s higher-order forms (x2,\n",
      "x3, etc.).\n",
      "\n",
      "4.4 Generating Polynomial and Interaction Features \n",
      "\n",
      "| \n",
      "\n",
      "67\n",
      "\n",
      "\f",
      "Additionally, often we run into situations where the effect of one feature is dependent\n",
      "on another feature. A simple example would be if we were trying to predict whether\n",
      "or not our coffee was sweet and we had two features: 1) whether or not the coffee was\n",
      "stirred  and  2)  if  we  added  sugar.  Individually,  each  feature  does  not  predict  coffee\n",
      "sweetness, but the combination of their effects does. That is, a coffee would only be\n",
      "sweet if the coffee had sugar and was stirred. The effects of each feature on the target\n",
      "(sweetness) are dependent on each other. We can encode that relationship by includ‐\n",
      "ing an interaction feature that is the product of the individual features.\n",
      "\n",
      "4.5 Transforming Features\n",
      "\n",
      "Problem\n",
      "You want to make a custom transformation to one or more features.\n",
      "\n",
      "Solution\n",
      "In scikit-learn, use FunctionTransformer to apply a function to a set of features:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "\n",
      "# Create feature matrix\n",
      "features = np.array([[2, 3],\n",
      "                     [2, 3],\n",
      "                     [2, 3]])\n",
      "\n",
      "# Define a simple function\n",
      "def add_ten(x):\n",
      "    return x + 10\n",
      "\n",
      "# Create transformer\n",
      "ten_transformer = FunctionTransformer(add_ten)\n",
      "\n",
      "# Transform feature matrix\n",
      "ten_transformer.transform(features)\n",
      "\n",
      "array([[12, 13],\n",
      "       [12, 13],\n",
      "       [12, 13]])\n",
      "\n",
      "We can create the same transformation in pandas using apply:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create DataFrame\n",
      "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
      "\n",
      "68 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "# Apply function\n",
      "df.apply(add_ten)\n",
      "\n",
      "feature_1 feature_2\n",
      "\n",
      "0 12\n",
      "1 12\n",
      "2 12\n",
      "\n",
      "13\n",
      "13\n",
      "13\n",
      "\n",
      "Discussion\n",
      "It is common to want to make some custom transformations to one or more features.\n",
      "For example, we might want to create a feature that is the natural log of the values of\n",
      "the  different  feature.  We  can  do  this  by  creating  a  function  and  then  mapping  it  to\n",
      "features using either scikit-learn’s FunctionTransformer or pandas’ apply. In the sol‐\n",
      "ution we created a very simple function, add_ten, which added 10 to each input, but\n",
      "there is no reason we could not define a much more complex function.\n",
      "\n",
      "4.6 Detecting Outliers\n",
      "\n",
      "Problem\n",
      "You want to identify extreme observations.\n",
      "\n",
      "Solution\n",
      "Detecting outliers is unfortunately more of an art than a science. However, a common\n",
      "method is to assume the data is normally distributed and based on that assumption\n",
      "“draw” an ellipse around the data, classifying any observation inside the ellipse as an\n",
      "inlier (labeled as  1) and any observation outside the ellipse as an outlier (labeled as\n",
      "-1):\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.covariance import EllipticEnvelope\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Create simulated data\n",
      "features, _ = make_blobs(n_samples = 10,\n",
      "                         n_features = 2,\n",
      "                         centers = 1,\n",
      "                         random_state = 1)\n",
      "\n",
      "# Replace the first observation's values with extreme values\n",
      "features[0,0] = 10000\n",
      "features[0,1] = 10000\n",
      "\n",
      "# Create detector\n",
      "\n",
      "4.6 Detecting Outliers \n",
      "\n",
      "| \n",
      "\n",
      "69\n",
      "\n",
      "\f",
      "outlier_detector = EllipticEnvelope(contamination=.1)\n",
      "\n",
      "# Fit detector\n",
      "outlier_detector.fit(features)\n",
      "\n",
      "# Predict outliers\n",
      "outlier_detector.predict(features)\n",
      "\n",
      "array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "\n",
      "A major limitation of this approach is the need to specify a contamination parame‐\n",
      "ter,  which  is  the  proportion  of  observations  that  are  outliers—a  value  that  we  don’t\n",
      "know.  Think  of  contamination  as  our  estimate  of  the  cleanliness  of  our  data.  If  we\n",
      "expect our data to have few outliers, we can set contamination to something small.\n",
      "However, if we believe that the data is very likely to have outliers, we can set it to a\n",
      "higher value.\n",
      "\n",
      "Instead of looking at observations as a whole, we can instead look at individual fea‐\n",
      "tures and identify extreme values in those features using interquartile range (IQR):\n",
      "\n",
      "# Create one feature\n",
      "feature = features[:,0]\n",
      "\n",
      "# Create a function to return index of outliers\n",
      "def indicies_of_outliers(x):\n",
      "    q1, q3 = np.percentile(x, [25, 75])\n",
      "    iqr = q3 - q1\n",
      "    lower_bound = q1 - (iqr * 1.5)\n",
      "    upper_bound = q3 + (iqr * 1.5)\n",
      "    return np.where((x > upper_bound) | (x < lower_bound))\n",
      "\n",
      "# Run function\n",
      "indicies_of_outliers(feature)\n",
      "\n",
      "(array([0]),)\n",
      "\n",
      "IQR  is  the  difference  between  the  first  and  third  quartile  of  a  set  of  data.  You  can\n",
      "think of IQR as the spread of the bulk of the data, with outliers being observations far\n",
      "from the main concentration of data. Outliers are commonly defined as any value 1.5\n",
      "IQRs less than the first quartile or 1.5 IQRs greater than the third quartile.\n",
      "\n",
      "Discussion\n",
      "There is no single best technique for detecting outliers. Instead, we have a collection\n",
      "of  techniques  all  with  their  own  advantages  and  disadvantages.  Our  best  strategy  is\n",
      "often trying multiple techniques (e.g., both EllipticEnvelope and IQR-based detec‐\n",
      "tion) and looking at the results as a whole.\n",
      "\n",
      "If at all possible, we should take a look at observations we detect as outliers and try to\n",
      "understand them. For example, if we have a dataset of houses and one feature is num‐\n",
      "\n",
      "70 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "ber of rooms, is an outlier with 100 rooms really a house or is it actually a hotel that\n",
      "has been misclassified?\n",
      "\n",
      "See Also\n",
      "\n",
      "• Three  ways  to  detect  outliers  (and  the  source  of  the  IQR  function  used  in  this\n",
      "\n",
      "recipe)\n",
      "\n",
      "4.7 Handling Outliers\n",
      "\n",
      "Problem\n",
      "You have outliers.\n",
      "\n",
      "Solution\n",
      "Typically  we  have  three  strategies  we  can  use  to  handle  outliers.  First,  we  can  drop\n",
      "them:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create DataFrame\n",
      "houses = pd.DataFrame()\n",
      "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
      "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
      "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
      "\n",
      "# Filter observations\n",
      "houses[houses['Bathrooms'] < 20]\n",
      "\n",
      "Price\n",
      "0 534433\n",
      "1 392333\n",
      "2 293222\n",
      "\n",
      "Bathrooms\n",
      "2.0\n",
      "3.5\n",
      "2.0\n",
      "\n",
      "Square_Feet\n",
      "1500\n",
      "2500\n",
      "1500\n",
      "\n",
      "Second, we can mark them as outliers and include it as a feature:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create feature based on boolean condition\n",
      "houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n",
      "\n",
      "# Show data\n",
      "houses\n",
      "\n",
      "4.7 Handling Outliers \n",
      "\n",
      "| \n",
      "\n",
      "71\n",
      "\n",
      "\f",
      "Price\n",
      "0 534433\n",
      "1 392333\n",
      "2 293222\n",
      "3 4322032\n",
      "\n",
      "Bathrooms\n",
      "2.0\n",
      "3.5\n",
      "2.0\n",
      "116.0\n",
      "\n",
      "Square_Feet Outlier\n",
      "1500\n",
      "2500\n",
      "1500\n",
      "48000\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "Finally, we can transform the feature to dampen the effect of the outlier:\n",
      "\n",
      "# Log feature\n",
      "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n",
      "\n",
      "# Show data\n",
      "houses\n",
      "\n",
      "Price\n",
      "0 534433\n",
      "1 392333\n",
      "2 293222\n",
      "3 4322032\n",
      "\n",
      "Bathrooms\n",
      "2.0\n",
      "3.5\n",
      "2.0\n",
      "116.0\n",
      "\n",
      "Square_Feet Outlier\n",
      "1500\n",
      "2500\n",
      "1500\n",
      "48000\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "Log_Of_Square_Feet\n",
      "7.313220\n",
      "7.824046\n",
      "7.313220\n",
      "10.778956\n",
      "\n",
      "Discussion\n",
      "Similar to detecting outliers, there is no hard-and-fast rule for handling them. How\n",
      "we  handle  them  should  be  based  on  two  aspects.  First,  we  should  consider  what\n",
      "makes them an outlier. If we believe they are errors in the data such as from a broken\n",
      "sensor  or  a  miscoded  value,  then  we  might  drop  the  observation  or  replace  outlier\n",
      "values with NaN since we can’t believe those values. However, if we believe the outliers\n",
      "are genuine extreme values (e.g., a house [mansion] with 200 bathrooms), then mark‐\n",
      "ing them as outliers or transforming their values is more appropriate.\n",
      "\n",
      "Second,  how  we  handle  outliers  should  be  based  on  our  goal  for  machine  learning.\n",
      "For  example,  if  we  want  to  predict  house  prices  based  on  features  of  the  house,  we\n",
      "might reasonably assume the price for mansions with over 100 bathrooms is driven\n",
      "by a different dynamic than regular family homes. Furthermore, if we are training a\n",
      "model to use as part of an online home loan web application, we might assume that\n",
      "our potential users will not include billionaires looking to buy a mansion.\n",
      "\n",
      "So what should we do if we have outliers? Think about why they are outliers, have an\n",
      "end goal in mind for the data, and, most importantly, remember that not making a\n",
      "decision to address outliers is itself a decision with implications.\n",
      "\n",
      "One additional point: if you do have outliers standardization might not be appropri‐\n",
      "ate because the mean and variance might be highly influenced by the outliers. In this\n",
      "case, use a rescaling method more robust against outliers like RobustScaler.\n",
      "\n",
      "72 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• RobustScaler documentation\n",
      "\n",
      "4.8 Discretizating Features\n",
      "\n",
      "Problem\n",
      "You have a numerical feature and want to break it up into discrete bins.\n",
      "\n",
      "Solution\n",
      "Depending  on  how  we  want  to  break  up  the  data,  there  are  two  techniques  we  can\n",
      "use. First, we can binarize the feature according to some threshold:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Binarizer\n",
      "\n",
      "# Create feature\n",
      "age = np.array([[6],\n",
      "                [12],\n",
      "                [20],\n",
      "                [36],\n",
      "                [65]])\n",
      "\n",
      "# Create binarizer\n",
      "binarizer = Binarizer(18)\n",
      "\n",
      "# Transform feature\n",
      "binarizer.fit_transform(age)\n",
      "\n",
      "array([[0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]])\n",
      "\n",
      "Second, we can break up numerical features according to multiple thresholds:\n",
      "\n",
      "# Bin feature\n",
      "np.digitize(age, bins=[20,30,64])\n",
      "\n",
      "array([[0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [3]])\n",
      "\n",
      "4.8 Discretizating Features \n",
      "\n",
      "| \n",
      "\n",
      "73\n",
      "\n",
      "\f",
      "Note that the arguments for the bins parameter denote the left edge of each bin. For\n",
      "example, the 20 argument does not include the element with the value of 20, only the\n",
      "two  values  smaller  than  20.  We  can  switch  this  behavior  by  setting  the  parameter\n",
      "right to True:\n",
      "\n",
      "# Bin feature\n",
      "np.digitize(age, bins=[20,30,64], right=True)\n",
      "\n",
      "array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [3]])\n",
      "\n",
      "Discussion\n",
      "Discretization can be a fruitful strategy when we have reason to believe that a numeri‐\n",
      "cal  feature  should  behave  more  like  a  categorical  feature.  For  example,  we  might\n",
      "believe there is very little difference in the spending habits of 19- and 20-year-olds,\n",
      "but a significant difference between 20- and 21-year-olds (the age in the United States\n",
      "when young adults can consume alcohol). In that example, it could be useful to break\n",
      "up individuals in our data into those who can drink alcohol and those who cannot.\n",
      "Similarly, in other cases it might be useful to discretize our data into three or more\n",
      "bins.\n",
      "\n",
      "In  the  solution,  we  saw  two  methods  of  discretization—scikit-learn’s  Binarizer  for\n",
      "two  bins  and  NumPy’s  digitize  for  three  or  more  bins—however,  we  can  also  use\n",
      "digitize to binarize features like Binarizer by only specifying a single threshold:\n",
      "\n",
      "# Bin feature\n",
      "np.digitize(age, bins=[18])\n",
      "\n",
      "array([[0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]])\n",
      "\n",
      "See Also\n",
      "\n",
      "• digitize documentation\n",
      "\n",
      "4.9 Grouping Observations Using Clustering\n",
      "\n",
      "Problem\n",
      "You want to cluster observations so that similar observations are grouped together.\n",
      "\n",
      "74 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "Solution\n",
      "If you know that you have k groups, you can use k-means clustering to group similar\n",
      "observations and output a new feature containing each observation’s group member‐\n",
      "ship:\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Make simulated feature matrix\n",
      "features, _ = make_blobs(n_samples = 50,\n",
      "                         n_features = 2,\n",
      "                         centers = 3,\n",
      "                         random_state = 1)\n",
      "\n",
      "# Create DataFrame\n",
      "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
      "\n",
      "# Make k-means clusterer\n",
      "clusterer = KMeans(3, random_state=0)\n",
      "\n",
      "# Fit clusterer\n",
      "clusterer.fit(features)\n",
      "\n",
      "# Predict values\n",
      "dataframe[\"group\"] = clusterer.predict(features)\n",
      "\n",
      "# View first few observations\n",
      "dataframe.head(5)\n",
      "\n",
      "feature_1\n",
      "\n",
      "feature_2\n",
      "0 –9.877554 –3.336145\n",
      "1 –7.287210 –8.353986\n",
      "2 –6.943061 –7.023744\n",
      "3 –7.440167 –8.791959\n",
      "4 –6.641388 –8.075888\n",
      "\n",
      "group\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "Discussion\n",
      "We are jumping ahead of ourselves a bit and will go much more in depth about clus‐\n",
      "tering algorithms later in the book. However, I wanted to point out that we can use\n",
      "clustering  as  a  preprocessing  step.  Specifically,  we  use  unsupervised  learning  algo‐\n",
      "rithms like k-means to cluster observations into groups. The end result is a categori‐\n",
      "cal feature with similar observations being members of the same group.\n",
      "\n",
      "4.9 Grouping Observations Using Clustering \n",
      "\n",
      "| \n",
      "\n",
      "75\n",
      "\n",
      "\f",
      "Don’t worry if you did not understand all of that right now: just file away the idea that\n",
      "clustering can be used in preprocessing. And if you really can’t wait, feel free to flip to\n",
      "Chapter 19 now.\n",
      "\n",
      "4.10 Deleting Observations with Missing Values\n",
      "\n",
      "Problem\n",
      "You need to delete observations containing missing values.\n",
      "\n",
      "Solution\n",
      "Deleting observations with missing values is easy with a clever line of NumPy:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "\n",
      "# Create feature matrix\n",
      "features = np.array([[1.1, 11.1],\n",
      "                     [2.2, 22.2],\n",
      "                     [3.3, 33.3],\n",
      "                     [4.4, 44.4],\n",
      "                     [np.nan, 55]])\n",
      "\n",
      "# Keep only observations that are not (denoted by ~) missing\n",
      "features[~np.isnan(features).any(axis=1)]\n",
      "\n",
      "array([[  1.1,  11.1],\n",
      "       [  2.2,  22.2],\n",
      "       [  3.3,  33.3],\n",
      "       [  4.4,  44.4]])\n",
      "\n",
      "Alternatively, we can drop missing observations using pandas:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Load data\n",
      "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
      "\n",
      "# Remove observations with missing values\n",
      "dataframe.dropna()\n",
      "\n",
      "feature_1 feature_2\n",
      "\n",
      "0 1.1\n",
      "1 2.2\n",
      "2 3.3\n",
      "3 4.4\n",
      "\n",
      "11.1\n",
      "22.2\n",
      "33.3\n",
      "44.4\n",
      "\n",
      "76 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "Discussion\n",
      "Most machine learning algorithms cannot handle any missing values in the target and\n",
      "feature arrays. For this reason, we cannot ignore missing values in our data and must\n",
      "address the issue during preprocessing.\n",
      "\n",
      "The simplest solution is to delete every observation that contains one or more miss‐\n",
      "ing values, a task quickly and easily accomplished using NumPy or pandas.\n",
      "\n",
      "That  said,  we  should  be  very  reluctant  to  delete  observations  with  missing  values.\n",
      "Deleting them is the nuclear option, since our algorithm loses access to the informa‐\n",
      "tion contained in the observation’s non-missing values.\n",
      "\n",
      "Just as important, depending on the cause of the missing values, deleting observations\n",
      "can introduce bias into our data. There are three types of missing data:\n",
      "\n",
      "Missing Completely At Random (MCAR)\n",
      "\n",
      "The probability that a value is missing is independent of everything. For example,\n",
      "a survey respondent rolls a die before answering a question: if she rolls a six, she\n",
      "skips that question.\n",
      "\n",
      "Missing At Random (MAR)\n",
      "\n",
      "The probability that a value is missing is not completely random, but depends on\n",
      "the information captured in other features. For example, a survey asks about gen‐\n",
      "der identity and annual salary and women are more likely to skip the salary ques‐\n",
      "tion; however, their nonresponse depends only on information we have captured\n",
      "in our gender identity feature.\n",
      "\n",
      "Missing Not At Random (MNAR)\n",
      "\n",
      "The probability that a value is missing is not random and depends on informa‐\n",
      "tion not captured in our features. For example, a survey asks about gender iden‐\n",
      "tity and women are more likely to skip the salary question, and we do not have a\n",
      "gender identity feature in our data.\n",
      "\n",
      "It  is  sometimes  acceptable  to  delete  observations  if  they  are  MCAR  or  MAR.  How‐\n",
      "ever, if the value is MNAR, the fact that a value is missing is itself information. Delet‐\n",
      "ing MNAR observations can inject bias into our data because we are removing obser‐\n",
      "vations produced by some unobserved systematic effect.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Identifying the Three Types of Missing Data\n",
      "\n",
      "• Missing-Data Imputation\n",
      "\n",
      "4.10 Deleting Observations with Missing Values \n",
      "\n",
      "| \n",
      "\n",
      "77\n",
      "\n",
      "\f",
      "4.11 Imputing Missing Values\n",
      "\n",
      "Problem\n",
      "You have missing values in your data and want to fill in or predict their values.\n",
      "\n",
      "Solution\n",
      "If you have a small amount of data, predict the missing values using k-nearest neigh‐\n",
      "bors (KNN):\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from fancyimpute import KNN\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Make a simulated feature matrix\n",
      "features, _ = make_blobs(n_samples = 1000,\n",
      "                         n_features = 2,\n",
      "                         random_state = 1)\n",
      "\n",
      "# Standardize the features\n",
      "scaler = StandardScaler()\n",
      "standardized_features = scaler.fit_transform(features)\n",
      "\n",
      "# Replace the first feature's first value with a missing value\n",
      "true_value = standardized_features[0,0]\n",
      "standardized_features[0,0] = np.nan\n",
      "\n",
      "# Predict the missing values in the feature matrix\n",
      "features_knn_imputed = KNN(k=5, verbose=0).complete(standardized_features)\n",
      "\n",
      "# Compare true and imputed values\n",
      "print(\"True Value:\", true_value)\n",
      "print(\"Imputed Value:\", features_knn_imputed[0,0])\n",
      "\n",
      "True Value: 0.8730186114\n",
      "Imputed Value: 1.09553327131\n",
      "\n",
      "Alternatively, we can use scikit-learn’s Imputer module to fill in missing values with\n",
      "the  feature’s  mean,  median,  or  most  frequent  value.  However,  we  will  typically  get\n",
      "worse results than KNN:\n",
      "\n",
      "# Load library\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "# Create imputer\n",
      "mean_imputer = Imputer(strategy=\"mean\", axis=0)\n",
      "\n",
      "# Impute values\n",
      "\n",
      "78 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Handling Numerical Data\n",
      "\n",
      "\f",
      "features_mean_imputed = mean_imputer.fit_transform(features)\n",
      "\n",
      "# Compare true and imputed values\n",
      "print(\"True Value:\", true_value)\n",
      "print(\"Imputed Value:\", features_mean_imputed[0,0])\n",
      "\n",
      "True Value: 0.8730186114\n",
      "Imputed Value: -3.05837272461\n",
      "\n",
      "Discussion\n",
      "There are two main strategies for replacing missing data with substitute values, each\n",
      "of which has strengths and weaknesses. First, we can use machine learning to predict\n",
      "the values of the missing data. To do this we treat the feature with missing values as a\n",
      "target  vector  and  use  the  remaining  subset  of  features  to  predict  missing  values.\n",
      "While  we  can  use  a  wide  range  of  machine  learning  algorithms  to  impute  values,  a\n",
      "popular choice is KNN. KNN is addressed in depth later in Chapter 14, but the short\n",
      "explanation is that the algorithm uses the k nearest observations (according to some\n",
      "distance metric) to predict the missing value. In our solution we predicted the miss‐\n",
      "ing value using the five closest observations.\n",
      "\n",
      "The downside to KNN is that in order to know which observations are the closest to\n",
      "the  missing  value,  it  needs  to  calculate  the  distance  between  the  missing  value  and\n",
      "every single observation. This is reasonable in smaller datasets, but quickly becomes\n",
      "problematic if a dataset has millions of observations.\n",
      "\n",
      "An  alternative  and  more  scalable  strategy  is  to  fill  in  all  missing  values  with  some\n",
      "average value. For example, in our solution we used scikit-learn to fill in missing val‐\n",
      "ues  with  a  feature’s  mean  value.  The  imputed  value  is  often  not  as  close  to  the  true\n",
      "value  as  when  we  used  KNN,  but  we  can  scale  mean-filling  to  data  containing  mil‐\n",
      "lions of observations easily.\n",
      "\n",
      "If we use imputation, it is a good idea to create a binary feature indicating whether or\n",
      "not the observation contains an imputed value.\n",
      "\n",
      "See Also\n",
      "\n",
      "• A Study of K-Nearest Neighbour as an Imputation Method\n",
      "\n",
      "4.11 Imputing Missing Values \n",
      "\n",
      "| \n",
      "\n",
      "79\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 5\n",
      "Handling Categorical Data\n",
      "\n",
      "5.0 Introduction\n",
      "It  is  often  useful  to  measure  objects  not  in  terms  of  their  quantity  but  in  terms  of\n",
      "some quality. We frequently represent this qualitative information as an observation’s\n",
      "membership in a discrete category such as gender, colors, or brand of car. However,\n",
      "not  all  categorical  data  is  the  same.  Sets  of  categories  with  no  intrinsic  ordering  is\n",
      "called nominal. Examples of nominal categories include:\n",
      "\n",
      "• Blue, Red, Green\n",
      "\n",
      "• Man, Woman\n",
      "\n",
      "• Banana, Strawberry, Apple\n",
      "\n",
      "In contrast, when a set of categories has some natural ordering we refer to it as ordi‐\n",
      "nal. For example:\n",
      "\n",
      "• Low, Medium, High\n",
      "\n",
      "• Young, Old\n",
      "\n",
      "• Agree, Neutral, Disagree\n",
      "\n",
      "Furthermore, categorical information is often represented in data as a vector or col‐\n",
      "umn  of  strings  (e.g.,  \"Maine\",  \"Texas\",  \"Delaware\").  The  problem  is  that  most\n",
      "machine learning algorithms require inputs be numerical values.\n",
      "\n",
      "The k-nearest neighbor algorithm provides a simple example. One step in the algo‐\n",
      "rithm  is  calculating  the  distances  between  observations—often  using  Euclidean\n",
      "distance:\n",
      "\n",
      "81\n",
      "\n",
      "\f",
      "n\n",
      "∑i = 1\n",
      "\n",
      "2\n",
      "\n",
      "xi − yi\n",
      "\n",
      "where x and y are two observations and subscript i denotes the value for the observa‐\n",
      "tions’  ith  feature.  However,  the  distance  calculation  obviously  is  impossible  if  the\n",
      "value of xi is a string (e.g., \"Texas\"). Instead, we need to convert the string into some\n",
      "numerical format so that it can be inputted into the Euclidean distance equation. Our\n",
      "goal is to make a transformation that properly conveys the information in the cate‐\n",
      "gories  (ordinality,  relative  intervals  between  categories,  etc.).  In  this  chapter  we  will\n",
      "cover  techniques  for  making  this  transformation  as  well  as  overcoming  other  chal‐\n",
      "lenges often encountered when handling categorical data.\n",
      "\n",
      "5.1 Encoding Nominal Categorical Features\n",
      "\n",
      "Problem\n",
      "You  have  a  feature  with  nominal  classes  that  has  no  intrinsic  ordering  (e.g.,  apple,\n",
      "pear, banana).\n",
      "\n",
      "Solution\n",
      "One-hot encode the feature using scikit-learn’s LabelBinarizer:\n",
      "\n",
      "# Import libraries\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
      "\n",
      "# Create feature\n",
      "feature = np.array([[\"Texas\"],\n",
      "                    [\"California\"],\n",
      "                    [\"Texas\"],\n",
      "                    [\"Delaware\"],\n",
      "                    [\"Texas\"]])\n",
      "\n",
      "# Create one-hot encoder\n",
      "one_hot = LabelBinarizer()\n",
      "\n",
      "# One-hot encode feature\n",
      "one_hot.fit_transform(feature)\n",
      "\n",
      "array([[0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 1, 0],\n",
      "       [0, 0, 1]])\n",
      "\n",
      "We can use the classes_ method to output the classes:\n",
      "\n",
      "82 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "# View feature classes\n",
      "one_hot.classes_\n",
      "\n",
      "array(['California', 'Delaware', 'Texas'],\n",
      "      dtype='<U10')\n",
      "\n",
      "If we want to reverse the one-hot encoding, we can use inverse_transform:\n",
      "\n",
      "# Reverse one-hot encoding\n",
      "one_hot.inverse_transform(one_hot.transform(feature))\n",
      "\n",
      "array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'],\n",
      "      dtype='<U10')\n",
      "\n",
      "We can even use pandas to one-hot encode the feature:\n",
      "\n",
      "# Import library\n",
      "import pandas as pd\n",
      "\n",
      "# Create dummy variables from feature\n",
      "pd.get_dummies(feature[:,0])\n",
      "\n",
      "California Delaware\n",
      "\n",
      "0 0\n",
      "1 1\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "Texas\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "One helpful ability of scikit-learn is to handle a situation where each observation lists\n",
      "multiple classes:\n",
      "\n",
      "# Create multiclass feature\n",
      "multiclass_feature = [(\"Texas\", \"Florida\"),\n",
      "                      (\"California\", \"Alabama\"),\n",
      "                      (\"Texas\", \"Florida\"),\n",
      "                      (\"Delware\", \"Florida\"),\n",
      "                      (\"Texas\", \"Alabama\")]\n",
      "\n",
      "# Create multiclass one-hot encoder\n",
      "one_hot_multiclass = MultiLabelBinarizer()\n",
      "\n",
      "# One-hot encode multiclass feature\n",
      "one_hot_multiclass.fit_transform(multiclass_feature)\n",
      "\n",
      "array([[0, 0, 0, 1, 1],\n",
      "       [1, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 1],\n",
      "       [0, 0, 1, 1, 0],\n",
      "       [1, 0, 0, 0, 1]])\n",
      "\n",
      "Once again, we can see the classes with the classes_ method:\n",
      "\n",
      "5.1 Encoding Nominal Categorical Features \n",
      "\n",
      "| \n",
      "\n",
      "83\n",
      "\n",
      "\f",
      "# View classes\n",
      "one_hot_multiclass.classes_\n",
      "\n",
      "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'], dtype=object)\n",
      "\n",
      "Discussion\n",
      "We  might  think  the  proper  strategy  is  to  assign  each  class  a  numerical  value  (e.g.,\n",
      "Texas = 1, California = 2). However, when our classes have no intrinsic ordering (e.g.,\n",
      "Texas isn’t “less” than California), our numerical values erroneously create an order‐\n",
      "ing that is not present.\n",
      "\n",
      "The proper strategy is to create a binary feature for each class in the original feature.\n",
      "This is often called one-hot encoding (in machine learning literature) or dummying (in\n",
      "statistical and research literature). Our solution’s feature was a vector containing three\n",
      "classes  (i.e.,  Texas,  California,  and  Delaware).  In  one-hot  encoding,  each  class\n",
      "becomes its own feature with 1s when the class appears and 0s otherwise. Because our\n",
      "feature  had  three  classes,  one-hot  encoding  returned  three  binary  features  (one  for\n",
      "each class). By using one-hot encoding we can capture the membership of an obser‐\n",
      "vation in a class while preserving the notion that the class lacks any sort of hierarchy.\n",
      "\n",
      "Finally,  it  is  worthwhile  to  note  that  it  is  often  recommended  that  after  one-hot\n",
      "encoding  a  feature,  we  drop  one  of  the  one-hot  encoded  features  in  the  resulting\n",
      "matrix to avoid linear dependence.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Dummy Variable Trap, Algosome\n",
      "\n",
      "• Dropping one of the columns when using one-hot encoding, CrossValidated\n",
      "\n",
      "5.2 Encoding Ordinal Categorical Features\n",
      "\n",
      "Problem\n",
      "You have an ordinal categorical feature (e.g., high, medium, low).\n",
      "\n",
      "Solution\n",
      "Use  pandas  DataFrame’s  replace  method  to  transform  string  labels  to  numerical\n",
      "equivalents:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create features\n",
      "dataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})\n",
      "\n",
      "84 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "# Create mapper\n",
      "scale_mapper = {\"Low\":1,\n",
      "                \"Medium\":2,\n",
      "                \"High\":3}\n",
      "\n",
      "# Replace feature values with scale\n",
      "dataframe[\"Score\"].replace(scale_mapper)\n",
      "\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: Score, dtype: int64\n",
      "\n",
      "Discussion\n",
      "Often  we  have  a  feature  with  classes  that  have  some  kind  of  natural  ordering.  A\n",
      "famous example is the Likert scale:\n",
      "\n",
      "• Strongly Agree\n",
      "\n",
      "• Agree\n",
      "\n",
      "• Neutral\n",
      "\n",
      "• Disagree\n",
      "\n",
      "• Strongly Disagree\n",
      "\n",
      "When  encoding  the  feature  for  use  in  machine  learning,  we  need  to  transform  the\n",
      "ordinal classes into numerical values that maintain the notion of ordering. The most\n",
      "common approach is to create a dictionary that maps the string label of the class to a\n",
      "number and then apply that map to the feature.\n",
      "\n",
      "It is important that our choice of numeric values is based on our prior information on\n",
      "the ordinal classes. In our solution, high is literally three times larger than low. This\n",
      "is  fine  in  any  instances,  but  can  break  down  if  the  assumed  intervals  between  the\n",
      "classes are not equal:\n",
      "\n",
      "dataframe = pd.DataFrame({\"Score\": [\"Low\",\n",
      "                                    \"Low\",\n",
      "                                    \"Medium\",\n",
      "                                    \"Medium\",\n",
      "                                    \"High\",\n",
      "                                    \"Barely More Than Medium\"]})\n",
      "\n",
      "scale_mapper = {\"Low\":1,\n",
      "                \"Medium\":2,\n",
      "                \"Barely More Than Medium\": 3,\n",
      "                \"High\":4}\n",
      "\n",
      "5.2 Encoding Ordinal Categorical Features \n",
      "\n",
      "| \n",
      "\n",
      "85\n",
      "\n",
      "\f",
      "dataframe[\"Score\"].replace(scale_mapper)\n",
      "\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    4\n",
      "5    3\n",
      "Name: Score, dtype: int64\n",
      "\n",
      "In  this  example,  the  distance  between  Low  and  Medium  is  the  same  as  the  distance\n",
      "between Medium and Barely More Than Medium, which is almost certainly not accu‐\n",
      "rate.  The  best  approach  is  to  be  conscious  about  the  numerical  values  mapped  to \n",
      "classes:\n",
      "\n",
      "scale_mapper = {\"Low\":1,\n",
      "                \"Medium\":2,\n",
      "                \"Barely More Than Medium\": 2.1,\n",
      "                \"High\":3}\n",
      "\n",
      "dataframe[\"Score\"].replace(scale_mapper)\n",
      "\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    2.0\n",
      "3    2.0\n",
      "4    3.0\n",
      "5    2.1\n",
      "Name: Score, dtype: float64\n",
      "\n",
      "5.3 Encoding Dictionaries of Features\n",
      "\n",
      "Problem\n",
      "You have a dictionary and want to convert it into a feature matrix.\n",
      "\n",
      "Solution\n",
      "Use DictVectorizer:\n",
      "\n",
      "# Import library\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "# Create dictionary\n",
      "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
      "             {\"Red\": 4, \"Blue\": 3},\n",
      "             {\"Red\": 1, \"Yellow\": 2},\n",
      "             {\"Red\": 2, \"Yellow\": 2}]\n",
      "\n",
      "# Create dictionary vectorizer\n",
      "\n",
      "86 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "dictvectorizer = DictVectorizer(sparse=False)\n",
      "\n",
      "# Convert dictionary to feature matrix\n",
      "features = dictvectorizer.fit_transform(data_dict)\n",
      "\n",
      "# View feature matrix\n",
      "features\n",
      "\n",
      "array([[ 4.,  2.,  0.],\n",
      "       [ 3.,  4.,  0.],\n",
      "       [ 0.,  1.,  2.],\n",
      "       [ 0.,  2.,  2.]])\n",
      "\n",
      "By default DictVectorizer outputs a sparse matrix that only stores elements with a\n",
      "value  other  than  0.  This  can  be  very  helpful  when  we  have  massive  matrices  (often\n",
      "encountered  in  natural  language  processing)  and  want  to  minimize  the  memory\n",
      "requirements.  We  can  force  DictVectorizer  to  output  a  dense  matrix  using\n",
      "sparse=False.\n",
      "\n",
      "We  can  get  the  names  of  each  generated  feature  using  the  get_feature_names \n",
      "method:\n",
      "\n",
      "# Get feature names\n",
      "feature_names = dictvectorizer.get_feature_names()\n",
      "\n",
      "# View feature names\n",
      "feature_names\n",
      "\n",
      "['Blue', 'Red', 'Yellow']\n",
      "\n",
      "While not necessary, for the sake of illustration we can create a pandas DataFrame to\n",
      "view the output better:\n",
      "\n",
      "# Import library\n",
      "import pandas as pd\n",
      "\n",
      "# Create dataframe from features\n",
      "pd.DataFrame(features, columns=feature_names)\n",
      "\n",
      "Blue Red Yellow\n",
      "\n",
      "0 4.0\n",
      "1 3.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "\n",
      "2.0\n",
      "4.0\n",
      "1.0\n",
      "2.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "\n",
      "Discussion\n",
      "A dictionary is a popular data structure used by many programming languages; how‐\n",
      "ever, machine learning algorithms expect the data to be in the form of a matrix. We\n",
      "can accomplish this using scikit-learn’s dictvectorizer.\n",
      "\n",
      "5.3 Encoding Dictionaries of Features \n",
      "\n",
      "| \n",
      "\n",
      "87\n",
      "\n",
      "\f",
      "This  is  a  common  situation  when  working  with  natural  language  processing.  For\n",
      "example, we might have a collection of documents and for each document we have a\n",
      "dictionary  containing  the  number  of  times  every  word  appears  in  the  document.\n",
      "Using  dictvectorizer,  we  can  easily  create  a  feature  matrix  where  every  feature  is\n",
      "the number of times a word appears in each document:\n",
      "\n",
      "# Create word counts dictionaries for four documents\n",
      "doc_1_word_count = {\"Red\": 2, \"Blue\": 4}\n",
      "doc_2_word_count = {\"Red\": 4, \"Blue\": 3}\n",
      "doc_3_word_count = {\"Red\": 1, \"Yellow\": 2}\n",
      "doc_4_word_count = {\"Red\": 2, \"Yellow\": 2}\n",
      "\n",
      "# Create list\n",
      "doc_word_counts = [doc_1_word_count,\n",
      "                   doc_2_word_count,\n",
      "                   doc_3_word_count,\n",
      "                   doc_4_word_count]\n",
      "\n",
      "# Convert list of word count dictionaries into feature matrix\n",
      "dictvectorizer.fit_transform(doc_word_counts)\n",
      "\n",
      "array([[ 4.,  2.,  0.],\n",
      "       [ 3.,  4.,  0.],\n",
      "       [ 0.,  1.,  2.],\n",
      "       [ 0.,  2.,  2.]])\n",
      "\n",
      "In our toy example there are only three unique words (Red, Yellow, Blue) so there are\n",
      "only  three  features  in  our  matrix;  however,  you  can  imagine  that  if  each  document\n",
      "was actually a book in a university library our feature matrix would be very large (and\n",
      "then we would want to set spare to True).\n",
      "\n",
      "See Also\n",
      "\n",
      "• How to use dictionaries in Python\n",
      "\n",
      "• SciPy Sparse Matrices\n",
      "\n",
      "5.4 Imputing Missing Class Values\n",
      "\n",
      "Problem\n",
      "You have a categorical feature containing missing values that you want to replace with\n",
      "predicted values.\n",
      "\n",
      "88 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "Solution\n",
      "The  ideal  solution  is  to  train  a  machine  learning  classifier  algorithm  to  predict  the\n",
      "missing values, commonly a k-nearest neighbors (KNN) classifier:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "# Create feature matrix with categorical feature\n",
      "X = np.array([[0, 2.10, 1.45],\n",
      "              [1, 1.18, 1.33],\n",
      "              [0, 1.22, 1.27],\n",
      "              [1, -0.21, -1.19]])\n",
      "\n",
      "# Create feature matrix with missing values in the categorical feature\n",
      "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
      "                       [np.nan, -0.67, -0.22]])\n",
      "\n",
      "# Train KNN learner\n",
      "clf = KNeighborsClassifier(3, weights='distance')\n",
      "trained_model = clf.fit(X[:,1:], X[:,0])\n",
      "\n",
      "# Predict missing values' class\n",
      "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
      "\n",
      "# Join column of predicted class with their other features\n",
      "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
      "\n",
      "# Join two feature matrices\n",
      "np.vstack((X_with_imputed, X))\n",
      "\n",
      "array([[ 0.  ,  0.87,  1.31],\n",
      "       [ 1.  , -0.67, -0.22],\n",
      "       [ 0.  ,  2.1 ,  1.45],\n",
      "       [ 1.  ,  1.18,  1.33],\n",
      "       [ 0.  ,  1.22,  1.27],\n",
      "       [ 1.  , -0.21, -1.19]])\n",
      "\n",
      "An  alternative  solution  is  to  fill  in  missing  values  with  the  feature’s  most  frequent\n",
      "value:\n",
      "\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "# Join the two feature matrices\n",
      "X_complete = np.vstack((X_with_nan, X))\n",
      "\n",
      "imputer = Imputer(strategy='most_frequent', axis=0)\n",
      "\n",
      "imputer.fit_transform(X_complete)\n",
      "\n",
      "array([[ 0.  ,  0.87,  1.31],\n",
      "       [ 0.  , -0.67, -0.22],\n",
      "\n",
      "5.4 Imputing Missing Class Values \n",
      "\n",
      "| \n",
      "\n",
      "89\n",
      "\n",
      "\f",
      "       [ 0.  ,  2.1 ,  1.45],\n",
      "       [ 1.  ,  1.18,  1.33],\n",
      "       [ 0.  ,  1.22,  1.27],\n",
      "       [ 1.  , -0.21, -1.19]])\n",
      "\n",
      "Discussion\n",
      "When we have missing values in a categorical feature, our best solution is to open our\n",
      "toolbox of machine learning algorithms to predict the values of the missing observa‐\n",
      "tions. We can accomplish this by treating the feature with the missing values as the\n",
      "target  vector  and  the  other  features  as  the  feature  matrix.  A  commonly  used  algo‐\n",
      "rithm  is  KNN  (discussed  in  depth  later  in  this  book),  which  assigns  to  the  missing\n",
      "value the median class of the k nearest observations.\n",
      "\n",
      "Alternatively, we can fill in missing values with the most frequent class of the feature.\n",
      "While less sophisticated than KNN, it is much more scalable to larger data. In either\n",
      "case, it is advisable to include a binary feature indicating which observations contain\n",
      "imputed values.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Overcoming Missing Values in a Random Forest Classifier\n",
      "\n",
      "• A Study of K-Nearest Neighbour as an Imputation Method\n",
      "\n",
      "5.5 Handling Imbalanced Classes\n",
      "\n",
      "Problem\n",
      "You have a target vector with highly imbalanced classes.\n",
      "\n",
      "Solution\n",
      "Collect  more  data.  If  that  isn’t  possible,  change  the  metrics  used  to  evaluate  your\n",
      "model. If that doesn’t work, consider using a model’s built-in class weight parameters\n",
      "(if  available),  downsampling,  or  upsampling.  We  cover  evaluation  metrics  in  a  later\n",
      "chapter,  so  for  now  let  us  focus  on  class  weight  parameters,  downsampling,  and\n",
      "upsampling.\n",
      "\n",
      "To demonstrate our solutions, we need to create some data with imbalanced classes.\n",
      "Fisher’s Iris dataset contains three balanced classes of 50 observations, each indicating\n",
      "the  species  of  flower  (Iris  setosa,  Iris  virginica,  and  Iris  versicolor).  To  unbalance  the\n",
      "dataset, we remove 40 of the 50 Iris setosa observations and then merge the Iris virgin‐\n",
      "ica and Iris versicolor classes. The end result is a binary target vector indicating if an\n",
      "\n",
      "90 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "observation is an Iris setosa flower or not. The result is 10 observations of Iris setosa\n",
      "(class 0) and 100 observations of not Iris setosa (class 1):\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# Load iris data\n",
      "iris = load_iris()\n",
      "\n",
      "# Create feature matrix\n",
      "features = iris.data\n",
      "\n",
      "# Create target vector\n",
      "target = iris.target\n",
      "\n",
      "# Remove first 40 observations\n",
      "features = features[40:,:]\n",
      "target = target[40:]\n",
      "\n",
      "# Create binary target vector indicating if class 0\n",
      "target = np.where((target == 0), 0, 1)\n",
      "\n",
      "# Look at the imbalanced target vector\n",
      "target\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Many algorithms in scikit-learn offer a parameter to weight classes during training to\n",
      "counteract the effect of their imbalance. While we have not covered it yet, RandomFor\n",
      "estClassifier  is  a  popular  classification  algorithm  and  includes  a  class_weight\n",
      "parameter. You can pass an argument specifying the desired class weights explicitly:\n",
      "\n",
      "# Create weights\n",
      "weights = {0: .9, 1: 0.1}\n",
      "\n",
      "# Create random forest classifier with weights\n",
      "RandomForestClassifier(class_weight=weights)\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight={0: 0.9, 1: 0.1},\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\n",
      "5.5 Handling Imbalanced Classes \n",
      "\n",
      "| \n",
      "\n",
      "91\n",
      "\n",
      "\f",
      "Or  you  can  pass  balanced,  which  automatically  creates  weights  inversely  propor‐\n",
      "tional to class frequencies:\n",
      "\n",
      "# Train a random forest with balanced class weights\n",
      "RandomForestClassifier(class_weight=\"balanced\")\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\n",
      "Alternatively, we can downsample the majority class or upsample the minority class.\n",
      "In downsampling, we randomly sample without replacement from the majority class\n",
      "(i.e., the class with more observations) to create a new subset of observations equal in\n",
      "size to the minority class. For example, if the minority class has 10 observations, we\n",
      "will randomly select 10 observations from the majority class and use those 20 obser‐\n",
      "vations as our data. Here we do exactly that using our unbalanced Iris data:\n",
      "\n",
      "# Indicies of each class' observations\n",
      "i_class0 = np.where(target == 0)[0]\n",
      "i_class1 = np.where(target == 1)[0]\n",
      "\n",
      "# Number of observations in each class\n",
      "n_class0 = len(i_class0)\n",
      "n_class1 = len(i_class1)\n",
      "\n",
      "# For every observation of class 0, randomly sample\n",
      "# from class 1 without replacement\n",
      "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)\n",
      "\n",
      "# Join together class 0's target vector with the\n",
      "# downsampled class 1's target vector\n",
      "np.hstack((target[i_class0], target[i_class1_downsampled]))\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "# Join together class 0's feature matrix with the\n",
      "# downsampled class 1's feature matrix\n",
      "np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]\n",
      "\n",
      "array([[ 5. ,  3.5,  1.3,  0.3],\n",
      "       [ 4.5,  2.3,  1.3,  0.3],\n",
      "       [ 4.4,  3.2,  1.3,  0.2],\n",
      "       [ 5. ,  3.5,  1.6,  0.6],\n",
      "       [ 5.1,  3.8,  1.9,  0.4]])\n",
      "\n",
      "Our other option is to upsample the minority class. In upsampling, for every observa‐\n",
      "tion in the majority class, we randomly select an observation from the minority class\n",
      "with  replacement.  The  end  result  is  the  same  number  of  observations  from  the\n",
      "\n",
      "92 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "minority  and  majority  classes.  Upsampling  is  implemented  very  similarly  to  down‐\n",
      "sampling, just in reverse:\n",
      "\n",
      "# For every observation in class 1, randomly sample from class 0 with replacement\n",
      "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)\n",
      "\n",
      "# Join together class 0's upsampled target vector with class 1's target vector\n",
      "np.concatenate((target[i_class0_upsampled], target[i_class1]))\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "# Join together class 0's upsampled feature matrix with class 1's feature matrix\n",
      "np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]\n",
      "\n",
      "array([[ 5. ,  3.5,  1.6,  0.6],\n",
      "       [ 5. ,  3.5,  1.6,  0.6],\n",
      "       [ 5. ,  3.3,  1.4,  0.2],\n",
      "       [ 4.5,  2.3,  1.3,  0.3],\n",
      "       [ 4.8,  3. ,  1.4,  0.3]])\n",
      "\n",
      "Discussion\n",
      "In  the  real  world,  imbalanced  classes  are  everywhere—most  visitors  don’t  click  the\n",
      "buy  button  and  many  types  of  cancer  are  thankfully  rare.  For  this  reason,  handling\n",
      "imbalanced classes is a common activity in machine learning.\n",
      "\n",
      "Our  best  strategy  is  simply  to  collect  more  observations—especially  observations\n",
      "from the minority class. However, this is often just not possible, so we have to resort\n",
      "to other options.\n",
      "\n",
      "A  second  strategy  is  to  use  a  model  evaluation  metric  better  suited  to  imbalanced\n",
      "classes. Accuracy is often used as a metric for evaluating the performance of a model,\n",
      "but  when  imbalanced  classes  are  present  accuracy  can  be  ill  suited.  For  example,  if\n",
      "only  0.5%  of  observations  have  some  rare  cancer,  then  even  a  naive  model  that\n",
      "predicts nobody has cancer will be 99.5% accurate. Clearly this is not ideal. Some bet‐\n",
      "ter  metrics  we  discuss  in  later  chapters  are  confusion  matrices,  precision,  recall,  F1\n",
      "scores, and ROC curves.\n",
      "\n",
      "A third strategy is to use the class weighing parameters included in implementations\n",
      "of some models. This allows us to have the algorithm adjust for imbalanced classes.\n",
      "Fortunately, many scikit-learn classifiers have a class_weight parameter, making it a\n",
      "good option.\n",
      "\n",
      "5.5 Handling Imbalanced Classes \n",
      "\n",
      "| \n",
      "\n",
      "93\n",
      "\n",
      "\f",
      "The fourth and fifth strategies are related: downsampling and upsampling. In down‐\n",
      "sampling we create a random subset of the majority class of equal size to the minority\n",
      "class. In upsampling we repeatedly sample with replacement from the minority class\n",
      "to make it of equal size as the majority class. The decision between using downsam‐\n",
      "pling  and  upsampling  is  context-specific,  and  in  general  we  should  try  both  to  see\n",
      "which produces better results.\n",
      "\n",
      "94 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Handling Categorical Data\n",
      "\n",
      "\f",
      "CHAPTER 6\n",
      "Handling Text\n",
      "\n",
      "6.0 Introduction\n",
      "Unstructured text data, like the contents of a book or a tweet, is both one of the most\n",
      "interesting sources of features and one of the most complex to handle. In this chapter,\n",
      "we  will  cover  strategies  for  transforming  text  into  information-rich  features.  This  is\n",
      "not  to  say  that  the  recipes  covered  here  are  comprehensive.  There  exist  entire  aca‐\n",
      "demic disciplines focused on handling this and similar types of data, and the contents\n",
      "of  all  their  techniques  would  fill  a  small  library.  Despite  this,  there  are  some  com‐\n",
      "monly used techniques, and a knowledge of these will add valuable tools to our pre‐\n",
      "processing toolbox.\n",
      "\n",
      "6.1 Cleaning Text\n",
      "\n",
      "Problem\n",
      "You have some unstructured text data and want to complete some basic cleaning.\n",
      "\n",
      "Solution\n",
      "Most  basic  text  cleaning  operations  should  only  replace  Python’s  core  string  opera‐\n",
      "tions, in particular strip, replace, and split:\n",
      "\n",
      "# Create text\n",
      "text_data = [\"   Interrobang. By Aishwarya Henriette     \",\n",
      "             \"Parking And Going. By Karl Gautier\",\n",
      "             \"    Today Is The night. By Jarek Prakash   \"]\n",
      "\n",
      "# Strip whitespaces\n",
      "strip_whitespace = [string.strip() for string in text_data]\n",
      "\n",
      "95\n",
      "\n",
      "\f",
      "# Show text\n",
      "strip_whitespace\n",
      "\n",
      "['Interrobang. By Aishwarya Henriette',\n",
      " 'Parking And Going. By Karl Gautier',\n",
      " 'Today Is The night. By Jarek Prakash']\n",
      "\n",
      "# Remove periods\n",
      "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
      "\n",
      "# Show text\n",
      "remove_periods\n",
      "\n",
      "['Interrobang By Aishwarya Henriette',\n",
      " 'Parking And Going By Karl Gautier',\n",
      " 'Today Is The night By Jarek Prakash']\n",
      "\n",
      "We also create and apply a custom transformation function:\n",
      "\n",
      "# Create function\n",
      "def capitalizer(string: str) -> str:\n",
      "    return string.upper()\n",
      "\n",
      "# Apply function\n",
      "[capitalizer(string) for string in remove_periods]\n",
      "\n",
      "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
      " 'PARKING AND GOING BY KARL GAUTIER',\n",
      " 'TODAY IS THE NIGHT BY JAREK PRAKASH']\n",
      "\n",
      "Finally, we can use regular expressions to make powerful string operations:\n",
      "\n",
      "# Import library\n",
      "import re\n",
      "\n",
      "# Create function\n",
      "def replace_letters_with_X(string: str) -> str:\n",
      "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
      "\n",
      "# Apply function\n",
      "[replace_letters_with_X(string) for string in remove_periods]\n",
      "\n",
      "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
      " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
      " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']\n",
      "\n",
      "Discussion\n",
      "Most  text  data  will  need  to  be  cleaned  before  we  can  use  it  to  build  features.  Most\n",
      "basic text cleaning can be completed using Python’s standard string operations. In the\n",
      "real world we will most likely define a custom cleaning function (e.g., capitalizer)\n",
      "combining some cleaning tasks and apply that to the text data.\n",
      "\n",
      "96 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Beginners Tutorial for Regular Expressions in Python\n",
      "\n",
      "6.2 Parsing and Cleaning HTML\n",
      "\n",
      "Problem\n",
      "You have text data with HTML elements and want to extract just the text.\n",
      "\n",
      "Solution\n",
      "Use Beautiful Soup’s extensive set of options to parse and extract from HTML:\n",
      "\n",
      "# Load library\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Create some HTML code\n",
      "html = \"\"\"\n",
      "       <div class='full_name'><span style='font-weight:bold'>\n",
      "       Masego</span> Azra</div>\"\n",
      "       \"\"\"\n",
      "\n",
      "# Parse html\n",
      "soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "# Find the div with the class \"full_name\", show text\n",
      "soup.find(\"div\", { \"class\" : \"full_name\" }).text\n",
      "\n",
      "'Masego Azra'\n",
      "\n",
      "Discussion\n",
      "Despite the strange name, Beautiful Soup is a powerful Python library designed for\n",
      "scraping HTML. Typically Beautiful Soup is used scrape live websites, but we can just\n",
      "as  easily  use  it  to  extract  text  data  embedded  in  HTML.  The  full  range  of  Beautiful\n",
      "Soup operations is beyond the scope of this book, but even the few methods used in\n",
      "our solution show how easily we can parse HTML code to extract the data we want.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Beautiful Soup\n",
      "\n",
      "6.2 Parsing and Cleaning HTML \n",
      "\n",
      "| \n",
      "\n",
      "97\n",
      "\n",
      "\f",
      "6.3 Removing Punctuation\n",
      "\n",
      "Problem\n",
      "You have a feature of text data and want to remove punctuation.\n",
      "\n",
      "Solution\n",
      "Define a function that uses translate with a dictionary of punctuation characters:\n",
      "\n",
      "# Load libraries\n",
      "import unicodedata\n",
      "import sys\n",
      "\n",
      "# Create text\n",
      "text_data = ['Hi!!!! I. Love. This. Song....',\n",
      "             '10000% Agree!!!! #LoveIT',\n",
      "             'Right?!?!']\n",
      "\n",
      "# Create a dictionary of punctuation characters\n",
      "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
      "                            if unicodedata.category(chr(i)).startswith('P'))\n",
      "\n",
      "# For each string, remove any punctuation characters\n",
      "[string.translate(punctuation) for string in text_data]\n",
      "\n",
      "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']\n",
      "\n",
      "Discussion\n",
      "translate is a Python method popular due to its blazing speed. In our solution, first\n",
      "we  created  a  dictionary,  punctuation,  with  all  punctuation  characters  according  to\n",
      "Unicode  as  its  keys  and  None  as  its  values.  Next  we  translated  all  characters  in  the\n",
      "string that are in punctuation into None, effectively removing them. There are more\n",
      "readable  ways  to  remove  punctuation,  but  this  somewhat  hacky  solution  has  the\n",
      "advantage of being far faster than alternatives.\n",
      "\n",
      "It is important to be conscious of the fact that punctuation contains information (e.g.,\n",
      "“Right?”  versus  “Right!”).  Removing  punctuation  is  often  a  necessary  evil  to  create\n",
      "features; however, if the punctuation is important we should make sure to take that\n",
      "into account.\n",
      "\n",
      "6.4 Tokenizing Text\n",
      "\n",
      "Problem\n",
      "You have text and want to break it up into individual words.\n",
      "\n",
      "98 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "Solution\n",
      "Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation\n",
      "operations, including word tokenizing:\n",
      "\n",
      "# Load library\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "# Create text\n",
      "string = \"The science of today is the technology of tomorrow\"\n",
      "\n",
      "# Tokenize words\n",
      "word_tokenize(string)\n",
      "\n",
      "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']\n",
      "\n",
      "We can also tokenize into sentences:\n",
      "\n",
      "# Load library\n",
      "from nltk.tokenize import sent_tokenize\n",
      "\n",
      "# Create text\n",
      "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
      "\n",
      "# Tokenize sentences\n",
      "sent_tokenize(string)\n",
      "\n",
      "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']\n",
      "\n",
      "Discussion\n",
      "Tokenization, especially word tokenization, is a common task after cleaning text data\n",
      "because it is the first step in the process of turning the text into data we will use to\n",
      "construct useful features.\n",
      "\n",
      "6.5 Removing Stop Words\n",
      "\n",
      "Problem\n",
      "Given tokenized text data, you want to remove extremely common words (e.g., a, is,\n",
      "of, on) that contain little informational value.\n",
      "\n",
      "Solution\n",
      "Use NLTK’s stopwords:\n",
      "\n",
      "# Load library\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# You will have to download the set of stop words the first time\n",
      "# import nltk\n",
      "\n",
      "6.5 Removing Stop Words \n",
      "\n",
      "| \n",
      "\n",
      "99\n",
      "\n",
      "\f",
      "# nltk.download('stopwords')\n",
      "\n",
      "# Create word tokens\n",
      "tokenized_words = ['i',\n",
      "                   'am',\n",
      "                   'going',\n",
      "                   'to',\n",
      "                   'go',\n",
      "                   'to',\n",
      "                   'the',\n",
      "                   'store',\n",
      "                   'and',\n",
      "                   'park']\n",
      "\n",
      "# Load stop words\n",
      "stop_words = stopwords.words('english')\n",
      "\n",
      "# Remove stop words\n",
      "[word for word in tokenized_words if word not in stop_words]\n",
      "\n",
      "['going', 'go', 'store', 'park']\n",
      "\n",
      "Discussion\n",
      "While “stop words” can refer to any set of words we want to remove before process‐\n",
      "ing, frequently the term refers to extremely common words that themselves contain\n",
      "little  information  value.  NLTK  has  a  list  of  common  stop  words  that  we  can  use  to\n",
      "find and remove stop words in our tokenized words:\n",
      "\n",
      "# Show stop words\n",
      "stop_words[:5]\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we']\n",
      "\n",
      "Note that NLTK’s stopwords assumes the tokenized words are all lowercased.\n",
      "\n",
      "6.6 Stemming Words\n",
      "\n",
      "Problem\n",
      "You have tokenized words and want to convert them into their root forms.\n",
      "\n",
      "Solution\n",
      "Use NLTK’s PorterStemmer:\n",
      "\n",
      "# Load library\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "\n",
      "# Create word tokens\n",
      "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
      "\n",
      "100 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "# Create stemmer\n",
      "porter = PorterStemmer()\n",
      "\n",
      "# Apply stemmer\n",
      "[porter.stem(word) for word in tokenized_words]\n",
      "\n",
      "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']\n",
      "\n",
      "Discussion\n",
      "Stemming reduces a word to its stem by identifying and removing affixes (e.g., ger‐\n",
      "unds) while keeping the root meaning of the word. For example, both “tradition” and\n",
      "“traditional” have “tradit” as their stem, indicating that while they are different words\n",
      "they represent the same general concept. By stemming our text data, we transform it\n",
      "to something less readable, but closer to its base meaning and thus more suitable for\n",
      "comparison across observations. NLTK’s PorterStemmer implements the widely used\n",
      "Porter  stemming  algorithm  to  remove  or  replace  common  suffixes  to  produce  the\n",
      "word stem.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Porter Stemming Algorithm\n",
      "\n",
      "6.7 Tagging Parts of Speech\n",
      "\n",
      "Problem\n",
      "You have text data and want to tag each word or character with its part of speech.\n",
      "\n",
      "Solution\n",
      "Use NLTK’s pre-trained parts-of-speech tagger:\n",
      "\n",
      "# Load libraries\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "\n",
      "# Create text\n",
      "text_data = \"Chris loved outdoor running\"\n",
      "\n",
      "# Use pre-trained part of speech tagger\n",
      "text_tagged = pos_tag(word_tokenize(text_data))\n",
      "\n",
      "# Show parts of speech\n",
      "text_tagged\n",
      "\n",
      "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]\n",
      "\n",
      "6.7 Tagging Parts of Speech \n",
      "\n",
      "| \n",
      "\n",
      "101\n",
      "\n",
      "\f",
      "The output is a list of tuples with the word and the tag of the part of speech. NLTK\n",
      "uses the Penn Treebank parts for speech tags. Some examples of the Penn Treebank\n",
      "tags are:\n",
      "\n",
      "Tag Part of speech\n",
      "Proper noun, singular\n",
      "NNP\n",
      "Noun, singular or mass\n",
      "NN\n",
      "Adverb\n",
      "RB\n",
      "VBD Verb, past tense\n",
      "VBG Verb, gerund or present participle\n",
      "JJ\n",
      "PRP\n",
      "\n",
      "Adjective\n",
      "Personal pronoun\n",
      "\n",
      "Once the text has been tagged, we can use the tags to find certain parts of speech. For\n",
      "example, here are all nouns:\n",
      "\n",
      "# Filter words\n",
      "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]\n",
      "\n",
      "['Chris']\n",
      "\n",
      "A more realistic situation would be that we have data where every observation con‐\n",
      "tains a tweet and we want to convert those sentences into features for individual parts\n",
      "of speech (e.g., a feature with 1 if a proper noun is present, and 0 otherwise):\n",
      "\n",
      "# Create text\n",
      "tweets = [\"I am eating a burrito for breakfast\",\n",
      "          \"Political science is an amazing field\",\n",
      "          \"San Francisco is an awesome city\"]\n",
      "\n",
      "# Create list\n",
      "tagged_tweets = []\n",
      "\n",
      "# Tag each word and each tweet\n",
      "for tweet in tweets:\n",
      "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
      "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
      "\n",
      "# Use one-hot encoding to convert the tags into features\n",
      "one_hot_multi = MultiLabelBinarizer()\n",
      "one_hot_multi.fit_transform(tagged_tweets)\n",
      "\n",
      "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
      "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
      "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])\n",
      "\n",
      "Using classes_ we can see that each feature is a part-of-speech tag:\n",
      "\n",
      "# Show feature names\n",
      "one_hot_multi.classes_\n",
      "\n",
      "102 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'], dtype=object)\n",
      "\n",
      "Discussion\n",
      "If our text is English and not on a specialized topic (e.g., medicine) the simplest solu‐\n",
      "tion is to use NLTK’s pre-trained parts-of-speech tagger. However, if pos_tag is not\n",
      "very  accurate,  NLTK  also  gives  us  the  ability  to  train  our  own  tagger.  The  major\n",
      "downside of training a tagger is that we need a large corpus of text where the tag of\n",
      "each word is known. Constructing this tagged corpus is obviously labor intensive and\n",
      "is probably going to be a last resort.\n",
      "\n",
      "All that said, if we had a tagged corpus and wanted to train a tagger, the following is\n",
      "an example of how we could do it. The corpus we are using is the Brown Corpus, one\n",
      "of  the  most  popular  sources  of  tagged  text.  Here  we  use  a  backoff  n-gram  tagger,\n",
      "where  n  is  the  number  of  previous  words  we  take  into  account  when  predicting  a\n",
      "word’s  part-of-speech  tag.  First  we  take  into  account  the  previous  two  words  using\n",
      "TrigramTagger; if two words are not present, we “back off ” and take into account the\n",
      "tag of the previous one word using BigramTagger, and finally if that fails we only look\n",
      "at  the  word  itself  using  UnigramTagger.  To  examine  the  accuracy  of  our  tagger,  we\n",
      "split our text data into two parts, train our tagger on one part, and test how well it\n",
      "predicts the tags of the second part:\n",
      "\n",
      "# Load library\n",
      "from nltk.corpus import brown\n",
      "from nltk.tag import UnigramTagger\n",
      "from nltk.tag import BigramTagger\n",
      "from nltk.tag import TrigramTagger\n",
      "\n",
      "# Get some text from the Brown Corpus, broken into sentences\n",
      "sentences = brown.tagged_sents(categories='news')\n",
      "\n",
      "# Split into 4000 sentences for training and 623 for testing\n",
      "train = sentences[:4000]\n",
      "test = sentences[4000:]\n",
      "\n",
      "# Create backoff tagger\n",
      "unigram = UnigramTagger(train)\n",
      "bigram = BigramTagger(train, backoff=unigram)\n",
      "trigram = TrigramTagger(train, backoff=bigram)\n",
      "\n",
      "# Show accuracy\n",
      "trigram.evaluate(test)\n",
      "\n",
      "0.8179229731754832\n",
      "\n",
      "See Also\n",
      "\n",
      "• Penn Treebank\n",
      "\n",
      "6.7 Tagging Parts of Speech \n",
      "\n",
      "| \n",
      "\n",
      "103\n",
      "\n",
      "\f",
      "• Brown Corpus\n",
      "\n",
      "6.8 Encoding Text as a Bag of Words\n",
      "\n",
      "Problem\n",
      "You have text data and want to create a set of features indicating the number of times\n",
      "an observation’s text contains a particular word.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s CountVectorizer:\n",
      "\n",
      "# Load library\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create text\n",
      "text_data = np.array(['I love Brazil. Brazil!',\n",
      "                      'Sweden is best',\n",
      "                      'Germany beats both'])\n",
      "\n",
      "# Create the bag of words feature matrix\n",
      "count = CountVectorizer()\n",
      "bag_of_words = count.fit_transform(text_data)\n",
      "\n",
      "# Show feature matrix\n",
      "bag_of_words\n",
      "\n",
      "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
      "    with 8 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "This output is a sparse array, which is often necessary when we have a large amount\n",
      "of  text.  However,  in  our  toy  example  we  can  use  toarray  to  view  a  matrix  of  word\n",
      "counts for each observation:\n",
      "\n",
      "bag_of_words.toarray()\n",
      "\n",
      "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
      "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
      "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)\n",
      "\n",
      "We can use the vocabulary_ method to view the word associated with each feature:\n",
      "\n",
      "# Show feature names\n",
      "count.get_feature_names()\n",
      "\n",
      "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']\n",
      "\n",
      "104 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "This  might  be  confusing,  so  for  the  sake  of  clarity  here  is  what  the  feature  matrix\n",
      "looks like with the words as column names (each row is one observation):\n",
      "\n",
      "beats best both brazil germany\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "0\n",
      "0\n",
      "\n",
      "is\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "love\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "sweden\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "Discussion\n",
      "One  of  the  most  common  methods  of  transforming  text  into  features  is  by  using  a\n",
      "bag-of-words model. Bag-of-words models output a feature for every unique word in\n",
      "text  data,  with  each  feature  containing  a  count  of  occurrences  in  observations.  For\n",
      "example, in our solution the sentence I love Brazil. Brazil! has a value of 2 in\n",
      "the “brazil” feature because the word brazil appears two times.\n",
      "\n",
      "The text data in our solution was purposely small. In the real world, a single observa‐\n",
      "tion  of  text  data  could  be  the  contents  of  an  entire  book!  Since  our  bag-of-words\n",
      "model  creates  a  feature  for  every  unique  word  in  the  data,  the  resulting  matrix  can\n",
      "contain thousands of features. This means that the size of the matrix can sometimes\n",
      "become very large in memory. However, luckily we can exploit a common character‐\n",
      "istic of bag-of-words feature matrices to reduce the amount of data we need to store.\n",
      "\n",
      "Most words likely do not occur in most observations, and therefore bag-of-words fea‐\n",
      "ture matrices will contain mostly 0s as values. We call these types of matrices “sparse.”\n",
      "Instead of storing all values of the matrix, we can only store nonzero values and then\n",
      "assume all other values are 0. This will save us memory when we have large feature\n",
      "matrices. One of the nice features of CountVectorizer is that the output is a sparse\n",
      "matrix by default.\n",
      "\n",
      "CountVectorizer comes with a number of useful parameters to make creating bag-\n",
      "of-words  feature  matrices  easy.  First,  while  by  default  every  feature  is  a  word,  that\n",
      "does not have to be the case. Instead we can set every feature to be the combination of\n",
      "two  words  (called  a  2-gram)  or  even  three  words  (3-gram).  ngram_range  sets  the\n",
      "minimum and maximum size of our n-grams. For example, (2,3) will return all 2-\n",
      "grams and 3-grams. Second, we can easily remove low-information filler words using\n",
      "stop_words  either  with  a  built-in  list  or  a  custom  list.  Finally,  we  can  restrict  the\n",
      "words or phrases we want to consider to a certain list of words using vocabulary. For\n",
      "example, we could create a bag-of-words feature matrix for only occurrences of coun‐\n",
      "try names:\n",
      "\n",
      "# Create feature matrix with arguments\n",
      "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
      "                              stop_words=\"english\",\n",
      "                              vocabulary=['brazil'])\n",
      "\n",
      "6.8 Encoding Text as a Bag of Words \n",
      "\n",
      "| \n",
      "\n",
      "105\n",
      "\n",
      "\f",
      "bag = count_2gram.fit_transform(text_data)\n",
      "\n",
      "# View feature matrix\n",
      "bag.toarray()\n",
      "\n",
      "array([[2],\n",
      "       [0],\n",
      "       [0]])\n",
      "\n",
      "# View the 1-grams and 2-grams\n",
      "count_2gram.vocabulary_\n",
      "\n",
      "{'brazil': 0}\n",
      "\n",
      "See Also\n",
      "\n",
      "• n-gram\n",
      "\n",
      "• Bag of Words Meets Bags of Popcorn\n",
      "\n",
      "6.9 Weighting Word Importance\n",
      "\n",
      "Problem\n",
      "You want a bag of words, but with words weighted by their importance to an observa‐\n",
      "tion.\n",
      "\n",
      "Solution\n",
      "Compare  the  frequency  of  the  word  in  a  document  (a  tweet,  movie  review,  speech\n",
      "transcript,  etc.)  with  the  frequency  of  the  word  in  all  other  documents  using  term\n",
      "frequency-inverse  document  frequency  (tf-idf).  scikit-learn  makes  this  easy  with\n",
      "TfidfVectorizer:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Create text\n",
      "text_data = np.array(['I love Brazil. Brazil!',\n",
      "                      'Sweden is best',\n",
      "                      'Germany beats both'])\n",
      "\n",
      "# Create the tf-idf feature matrix\n",
      "tfidf = TfidfVectorizer()\n",
      "feature_matrix = tfidf.fit_transform(text_data)\n",
      "\n",
      "# Show tf-idf feature matrix\n",
      "feature_matrix\n",
      "\n",
      "106 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
      "    with 8 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "Just  as  in  Recipe  6.8,  the  output  is  a  spare  matrix.  However,  if  we  want  to  view  the\n",
      "output as a dense matrix, we can use .toarray:\n",
      "\n",
      "# Show tf-idf feature matrix as dense matrix\n",
      "feature_matrix.toarray()\n",
      "\n",
      "array([[ 0.        ,  0.        ,  0.        ,  0.89442719,  0.        ,\n",
      "         0.        ,  0.4472136 ,  0.        ],\n",
      "       [ 0.        ,  0.57735027,  0.        ,  0.        ,  0.        ,\n",
      "         0.57735027,  0.        ,  0.57735027],\n",
      "       [ 0.57735027,  0.        ,  0.57735027,  0.        ,  0.57735027,\n",
      "         0.        ,  0.        ,  0.        ]])\n",
      "\n",
      "vocabulary_ shows us the word of each feature:\n",
      "\n",
      "# Show feature names\n",
      "tfidf.vocabulary_\n",
      "\n",
      "{'beats': 0,\n",
      " 'best': 1,\n",
      " 'both': 2,\n",
      " 'brazil': 3,\n",
      " 'germany': 4,\n",
      " 'is': 5,\n",
      " 'love': 6,\n",
      " 'sweden': 7}\n",
      "\n",
      "Discussion\n",
      "The more a word appears in a document, the more likely it is important to that docu‐\n",
      "ment.  For  example,  if  the  word  economy  appears  frequently,  it  is  evidence  that  the\n",
      "document might be about economics. We call this term frequency (tf).\n",
      "\n",
      "In  contrast,  if  a  word  appears  in  many  documents,  it  is  likely  less  important  to  any\n",
      "individual document. For example, if every document in some text data contains the\n",
      "word after then it is probably an unimportant word. We call this document frequency\n",
      "(df).\n",
      "\n",
      "By combining these two statistics, we can assign a score to every word representing\n",
      "how important that word is in a document. Specifically, we multiply tf to the inverse\n",
      "of document frequency (idf):\n",
      "\n",
      "tf‐idf t, d = t f t, d × id f t\n",
      "\n",
      "where t is a word and d is a document. There are a number of variations in how tf and\n",
      "idf are calculated. In scikit-learn, tf is simply the number of times a word appears in\n",
      "the document and idf is calculated as:\n",
      "\n",
      "6.9 Weighting Word Importance \n",
      "\n",
      "| \n",
      "\n",
      "107\n",
      "\n",
      "\f",
      "idf t = log\n",
      "\n",
      "1 + nd\n",
      "1 + df d, t\n",
      "\n",
      "+ 1\n",
      "\n",
      "where nd is the number of documents and df(d,t) is term, t’s document frequency (i.e.,\n",
      "number of documents where the term appears).\n",
      "\n",
      "By  default,  scikit-learn  then  normalizes  the  tf-idf  vectors  using  the  Euclidean  norm\n",
      "(L2 norm). The higher the resulting value, the more important the word is to a docu‐\n",
      "ment.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: tf–idf term weighting\n",
      "\n",
      "108 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Handling Text\n",
      "\n",
      "\f",
      "CHAPTER 7\n",
      "Handling Dates and Times\n",
      "\n",
      "7.0 Introduction\n",
      "Dates  and  times  (datetimes)  are  frequently  encountered  during  preprocessing  for\n",
      "machine  learning,  whether  the  time  of  a  particular  sale  or  the  year  of  some  public\n",
      "health statistic. In this chapter, we will build a toolbox of strategies for handling time\n",
      "series  data  including  tackling  time  zones  and  creating  lagged  time  features.  Specifi‐\n",
      "cally, we will focus on the time series tools in the pandas library, which centralizes the\n",
      "functionality of many other libraries.\n",
      "\n",
      "7.1 Converting Strings to Dates\n",
      "\n",
      "Problem\n",
      "Given a vector of strings representing dates and times, you want to transform them\n",
      "into time series data.\n",
      "\n",
      "Solution\n",
      "Use  pandas’  to_datetime  with  the  format  of  the  date  and/or  time  specified  in  the\n",
      "format parameter:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Create strings\n",
      "date_strings = np.array(['03-04-2005 11:35 PM',\n",
      "                         '23-05-2010 12:01 AM',\n",
      "                         '04-09-2009 09:09 PM'])\n",
      "\n",
      "109\n",
      "\n",
      "\f",
      "# Convert to datetimes\n",
      "[pd.to_datetime(date, format='%d-%m-%Y %I:%M %p') for date in date_strings]\n",
      "\n",
      "[Timestamp('2005-04-03 23:35:00'),\n",
      " Timestamp('2010-05-23 00:01:00'),\n",
      " Timestamp('2009-09-04 21:09:00')]\n",
      "\n",
      "We might also want to add an argument to the errors parameter to handle problems:\n",
      "\n",
      "# Convert to datetimes\n",
      "[pd.to_datetime(date, format=\"%d-%m-%Y %I:%M %p\", errors=\"coerce\")\n",
      "for date in date_strings]\n",
      "\n",
      "[Timestamp('2005-04-03 23:35:00'),\n",
      " Timestamp('2010-05-23 00:01:00'),\n",
      " Timestamp('2009-09-04 21:09:00')]\n",
      "\n",
      "If errors=\"coerce\", then any problem that occurs will not raise an error (the default\n",
      "behavior) but instead will set the value causing the error to NaT (i.e., a missing value).\n",
      "\n",
      "Discussion\n",
      "When  dates  and  times  come  as  strings,  we  need  to  convert  them  into  a  data  type\n",
      "Python  can  understand.  While  there  are  a  number  of  Python  tools  for  converting\n",
      "strings to datetimes, following our use of pandas in other recipes we can use to_date\n",
      "time  to  conduct  the  transformation.  One  obstacle  to  strings  representing  dates  and\n",
      "times is that the format of the strings can vary significantly between data sources. For\n",
      "example, one vector of dates might represent March 23rd, 2015 as “03-23-15” while\n",
      "another might use “3|23|2015”. We can use the format parameter to specify the exact\n",
      "format of the string. Here are some common date and time formatting codes:\n",
      "\n",
      "Example\n",
      "2001\n",
      "\n",
      "Code Description\n",
      "%Y\n",
      "\n",
      "04\n",
      "\n",
      "Full year\n",
      "Month w/ zero padding\n",
      "Day of the month w/ zero padding 09\n",
      "Hour (12hr clock) w/ zero padding 02\n",
      "AM\n",
      "AM or PM\n",
      "Minute w/ zero padding\n",
      "Second w/ zero padding\n",
      "\n",
      "05\n",
      "\n",
      "09\n",
      "\n",
      "%m\n",
      "\n",
      "%d\n",
      "\n",
      "%I\n",
      "\n",
      "%p\n",
      "\n",
      "%M\n",
      "\n",
      "%S\n",
      "\n",
      "See Also\n",
      "\n",
      "• Complete List of Python String Time Codes\n",
      "\n",
      "110 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "7.2 Handling Time Zones\n",
      "\n",
      "Problem\n",
      "You have time series data and want to add or change time zone information.\n",
      "\n",
      "Solution\n",
      "If not specified, pandas objects have no time zone. However, we can add a time zone \n",
      "using tz during creation:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create datetime\n",
      "pd.Timestamp('2017-05-01 06:00:00', tz='Europe/London')\n",
      "\n",
      "Timestamp('2017-05-01 06:00:00+0100', tz='Europe/London')\n",
      "\n",
      "We can add a time zone to a previously created datetime using tz_localize:\n",
      "\n",
      "# Create datetime\n",
      "date = pd.Timestamp('2017-05-01 06:00:00')\n",
      "\n",
      "# Set time zone\n",
      "date_in_london = date.tz_localize('Europe/London')\n",
      "\n",
      "# Show datetime\n",
      "date_in_london\n",
      "\n",
      "Timestamp('2017-05-01 06:00:00+0100', tz='Europe/London')\n",
      "\n",
      "We can also convert to a different time zone:\n",
      "\n",
      "# Change time zone\n",
      "date_in_london.tz_convert('Africa/Abidjan')\n",
      "\n",
      "Timestamp('2017-05-01 05:00:00+0000', tz='Africa/Abidjan')\n",
      "\n",
      "Finally,  pandas’  Series  objects  can  apply  tz_localize  and  tz_convert  to  every  ele‐\n",
      "ment:\n",
      "\n",
      "# Create three dates\n",
      "dates = pd.Series(pd.date_range('2/2/2002', periods=3, freq='M'))\n",
      "\n",
      "# Set time zone\n",
      "dates.dt.tz_localize('Africa/Abidjan')\n",
      "\n",
      "0   2002-02-28 00:00:00+00:00\n",
      "1   2002-03-31 00:00:00+00:00\n",
      "2   2002-04-30 00:00:00+00:00\n",
      "dtype: datetime64[ns, Africa/Abidjan]\n",
      "\n",
      "7.2 Handling Time Zones \n",
      "\n",
      "| \n",
      "\n",
      "111\n",
      "\n",
      "\f",
      "Discussion\n",
      "pandas supports two sets of strings representing timezones; however, I suggest using\n",
      "pytz  library’s  strings.  We  can  see  all  the  strings  used  to  represent  time  zones  by\n",
      "importing all_timezones:\n",
      "\n",
      "# Load library\n",
      "from pytz import all_timezones\n",
      "\n",
      "# Show two time zones\n",
      "all_timezones[0:2]\n",
      "\n",
      "['Africa/Abidjan', 'Africa/Accra']\n",
      "\n",
      "7.3 Selecting Dates and Times\n",
      "\n",
      "Problem\n",
      "You have a vector of dates and you want to select one or more.\n",
      "\n",
      "Solution\n",
      "Use two boolean conditions as the start and end dates:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create data frame\n",
      "dataframe = pd.DataFrame()\n",
      "\n",
      "# Create datetimes\n",
      "dataframe['date'] = pd.date_range('1/1/2001', periods=100000, freq='H')\n",
      "\n",
      "# Select observations between two datetimes\n",
      "dataframe[(dataframe['date'] > '2002-1-1 01:00:00') &\n",
      "          (dataframe['date'] <= '2002-1-1 04:00:00')]\n",
      "\n",
      "date\n",
      "\n",
      "8762 2002-01-01 02:00:00\n",
      "8763 2002-01-01 03:00:00\n",
      "8764 2002-01-01 04:00:00\n",
      "\n",
      "Alternatively,  we  can  set  the  date  column  as  the  DataFrame’s  index  and  then  slice\n",
      "using loc:\n",
      "\n",
      "# Set index\n",
      "dataframe = dataframe.set_index(dataframe['date'])\n",
      "\n",
      "112 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "# Select observations between two datetimes\n",
      "dataframe.loc['2002-1-1 01:00:00':'2002-1-1 04:00:00']\n",
      "\n",
      "date\n",
      "date\n",
      "\n",
      "2002-01-01 01:00:00 2002-01-01 01:00:00\n",
      "2002-01-01 02:00:00 2002-01-01 02:00:00\n",
      "2002-01-01 03:00:00 2002-01-01 03:00:00\n",
      "2002-01-01 04:00:00 2002-01-01 04:00:00\n",
      "\n",
      "Discussion\n",
      "Whether  we  use  boolean  conditions  or  index  slicing  is  situation  dependent.  If  we\n",
      "wanted to do some complex time series manipulation, it might be worth the overhead\n",
      "of  setting  the  date  column  as  the  index  of  the  DataFrame,  but  if  we  wanted  to  do\n",
      "some simple data wrangling, the boolean conditions might be easier.\n",
      "\n",
      "7.4 Breaking Up Date Data into Multiple Features\n",
      "\n",
      "Problem\n",
      "You  have  a  column  of  dates  and  times  and  you  want  to  create  features  for  year,\n",
      "month, day, hour, and minute.\n",
      "\n",
      "Solution\n",
      "Use pandas Series.dt’s time properties:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create data frame\n",
      "dataframe = pd.DataFrame()\n",
      "\n",
      "# Create five dates\n",
      "dataframe['date'] = pd.date_range('1/1/2001', periods=150, freq='W')\n",
      "\n",
      "# Create features for year, month, day, hour, and minute\n",
      "dataframe['year'] = dataframe['date'].dt.year\n",
      "dataframe['month'] = dataframe['date'].dt.month\n",
      "dataframe['day'] = dataframe['date'].dt.day\n",
      "dataframe['hour'] = dataframe['date'].dt.hour\n",
      "dataframe['minute'] = dataframe['date'].dt.minute\n",
      "\n",
      "# Show three rows\n",
      "dataframe.head(3)\n",
      "\n",
      "7.4 Breaking Up Date Data into Multiple Features \n",
      "\n",
      "| \n",
      "\n",
      "113\n",
      "\n",
      "\f",
      "date\n",
      "0 2001-01-07\n",
      "1 2001-01-14\n",
      "2 2001-01-21\n",
      "\n",
      "year month day hour minute\n",
      "7\n",
      "2001\n",
      "14\n",
      "2001\n",
      "21\n",
      "2001\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "Discussion\n",
      "Sometimes  it  can  be  useful  to  break  up  a  column  of  dates  into  components.  For\n",
      "example, we might want a feature that just includes the year of the observation or we\n",
      "might want only to consider the month of some observation so we can compare them\n",
      "regardless of year.\n",
      "\n",
      "7.5 Calculating the Difference Between Dates\n",
      "\n",
      "Problem\n",
      "You have two datetime features and want to calculate the time between them for each\n",
      "observation.\n",
      "\n",
      "Solution\n",
      "Subtract the two date features using pandas:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create data frame\n",
      "dataframe = pd.DataFrame()\n",
      "\n",
      "# Create two datetime features\n",
      "dataframe['Arrived'] = [pd.Timestamp('01-01-2017'), pd.Timestamp('01-04-2017')]\n",
      "dataframe['Left'] = [pd.Timestamp('01-01-2017'), pd.Timestamp('01-06-2017')]\n",
      "\n",
      "# Calculate duration between features\n",
      "dataframe['Left'] - dataframe['Arrived']\n",
      "\n",
      "0   0 days\n",
      "1   2 days\n",
      "dtype: timedelta64[ns]\n",
      "\n",
      "Often we will want to remove the days output and keep only the numerical value:\n",
      "\n",
      "# Calculate duration between features\n",
      "pd.Series(delta.days for delta in (dataframe['Left'] - dataframe['Arrived']))\n",
      "\n",
      "0    0\n",
      "1    2\n",
      "dtype: int64\n",
      "\n",
      "114 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "Discussion\n",
      "There are times when the feature we want is the change (delta) between two points in\n",
      "time. For example, we might have the dates a customer checks in and checks out of a\n",
      "hotel, but the feature we want is the duration of his stay. pandas makes this calcula‐\n",
      "tion easy using the TimeDelta data type.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Pandas documentation: Time Deltas\n",
      "\n",
      "7.6 Encoding Days of the Week\n",
      "\n",
      "Problem\n",
      "You have a vector of dates and want to know the day of the week for each date.\n",
      "\n",
      "Solution\n",
      "Use pandas’ Series.dt property weekday_name:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create dates\n",
      "dates = pd.Series(pd.date_range(\"2/2/2002\", periods=3, freq=\"M\"))\n",
      "\n",
      "# Show days of the week\n",
      "dates.dt.weekday_name\n",
      "\n",
      "0    Thursday\n",
      "1      Sunday\n",
      "2     Tuesday\n",
      "dtype: object\n",
      "\n",
      "If we want the output to be a numerical value and therefore more usable as a machine\n",
      "learning feature, we can use weekday where the days of the week are represented as an\n",
      "integer (Monday is 0):\n",
      "\n",
      "# Show days of the week\n",
      "dates.dt.weekday\n",
      "\n",
      "0    3\n",
      "1    6\n",
      "2    1\n",
      "dtype: int64\n",
      "\n",
      "7.6 Encoding Days of the Week \n",
      "\n",
      "| \n",
      "\n",
      "115\n",
      "\n",
      "\f",
      "Discussion\n",
      "Knowing the weekday can be helpful if, for instance, we wanted to compare total sales\n",
      "on Sundays for the past three years. pandas makes creating a feature vector contain‐\n",
      "ing weekday information easy.\n",
      "\n",
      "See Also\n",
      "\n",
      "• pandas Series datetimelike properties\n",
      "\n",
      "7.7 Creating a Lagged Feature\n",
      "\n",
      "Problem\n",
      "You want to create a feature that is lagged n time periods.\n",
      "\n",
      "Solution\n",
      "Use pandas’ shift:\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create data frame\n",
      "dataframe = pd.DataFrame()\n",
      "\n",
      "# Create data\n",
      "dataframe[\"dates\"] = pd.date_range(\"1/1/2001\", periods=5, freq=\"D\")\n",
      "dataframe[\"stock_price\"] = [1.1,2.2,3.3,4.4,5.5]\n",
      "\n",
      "# Lagged values by one row\n",
      "dataframe[\"previous_days_stock_price\"] = dataframe[\"stock_price\"].shift(1)\n",
      "\n",
      "# Show data frame\n",
      "dataframe\n",
      "\n",
      "dates\n",
      "0 2001-01-01\n",
      "1 2001-01-02\n",
      "2 2001-01-03\n",
      "3 2001-01-04\n",
      "4 2001-01-05\n",
      "\n",
      "stock_price previous_days_stock_price\n",
      "1.1\n",
      "2.2\n",
      "3.3\n",
      "4.4\n",
      "5.5\n",
      "\n",
      "NaN\n",
      "1.1\n",
      "2.2\n",
      "3.3\n",
      "4.4\n",
      "\n",
      "116 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "Discussion\n",
      "Very often data is based on regularly spaced time periods (e.g., every day, every hour,\n",
      "every three hours) and we are interested in using values in the past to make predic‐\n",
      "tions (this is often called lagging a feature). For example, we might want to predict a\n",
      "stock’s price using the price it was the day before. With pandas we can use shift to\n",
      "lag values by one row, creating a new feature containing past values.\n",
      "\n",
      "In  our  solution,  the  first  row  for  previous_days_stock_price  is  a  missing  value\n",
      "because there is no previous stock_price value.\n",
      "\n",
      "7.8 Using Rolling Time Windows\n",
      "\n",
      "Problem\n",
      "Given time series data, you want to calculate some statistic for a rolling time.\n",
      "\n",
      "Solution\n",
      "\n",
      "# Load library\n",
      "import pandas as pd\n",
      "\n",
      "# Create datetimes\n",
      "time_index = pd.date_range(\"01/01/2010\", periods=5, freq=\"M\")\n",
      "\n",
      "# Create data frame, set index\n",
      "dataframe = pd.DataFrame(index=time_index)\n",
      "\n",
      "# Create feature\n",
      "dataframe[\"Stock_Price\"] = [1,2,3,4,5]\n",
      "\n",
      "# Calculate rolling mean\n",
      "dataframe.rolling(window=2).mean()\n",
      "\n",
      "Stock_Price\n",
      "\n",
      "2010-01-31 NaN\n",
      "2010-02-28 1.5\n",
      "2010-03-31 2.5\n",
      "2010-04-30 3.5\n",
      "2010-05-31 4.5\n",
      "\n",
      "Discussion\n",
      "Rolling (also called moving) time windows are conceptually simple but can be diffi‐\n",
      "cult to understand at first. Imagine we have monthly observations for a stock’s price.\n",
      "It  is  often  useful  to  have  a  time  window  of  a  certain  number  of  months  and  then\n",
      "\n",
      "7.8 Using Rolling Time Windows \n",
      "\n",
      "| \n",
      "\n",
      "117\n",
      "\n",
      "\f",
      "move over the observations calculating a statistic for all observations in the time win‐\n",
      "dow.\n",
      "\n",
      "For example, if we have a time window of three months and we want a rolling mean,\n",
      "we would calculate:\n",
      "\n",
      "1. mean(January, February, March)\n",
      "\n",
      "2. mean(February, March, April)\n",
      "\n",
      "3. mean(March, April, May)\n",
      "\n",
      "4. etc.\n",
      "\n",
      "Another way to put it: our three-month time window “walks” over the observations,\n",
      "calculating the window’s mean at each step.\n",
      "\n",
      "pandas’  rolling allows us to specify the size of the window using  window and then\n",
      "quickly  calculate  some  common  statistics,  including  the  max  value  (max()),  mean\n",
      "value (mean()), count of values (count()), and rolling correlation (corr()).\n",
      "\n",
      "Rolling means are often used to smooth out time series data because using the mean\n",
      "of the entire time window dampens the effect of short-term fluctuations.\n",
      "\n",
      "See Also\n",
      "\n",
      "• pandas documentation: Rolling Windows\n",
      "\n",
      "• What are Moving Average or Smoothing Techniques?\n",
      "\n",
      "7.9 Handling Missing Data in Time Series\n",
      "\n",
      "Problem\n",
      "You have missing values in time series data.\n",
      "\n",
      "Solution\n",
      "In  addition  to  the  missing  data  strategies  previously  discussed,  when  we  have  time\n",
      "series data we can use interpolation to fill in gaps caused by missing values:\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Create date\n",
      "time_index = pd.date_range(\"01/01/2010\", periods=5, freq=\"M\")\n",
      "\n",
      "# Create data frame, set index\n",
      "\n",
      "118 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "dataframe = pd.DataFrame(index=time_index)\n",
      "\n",
      "# Create feature with a gap of missing values\n",
      "dataframe[\"Sales\"] = [1.0,2.0,np.nan,np.nan,5.0]\n",
      "\n",
      "# Interpolate missing values\n",
      "dataframe.interpolate()\n",
      "\n",
      "Sales\n",
      "\n",
      "2010-01-31 1.0\n",
      "2010-02-28 2.0\n",
      "2010-03-31 3.0\n",
      "2010-04-30 4.0\n",
      "2010-05-31 5.0\n",
      "\n",
      "Alternatively, we can replace missing values with the last known value (i.e., forward-\n",
      "filling):\n",
      "\n",
      "# Forward-fill\n",
      "dataframe.ffill()\n",
      "\n",
      "Sales\n",
      "\n",
      "2010-01-31 1.0\n",
      "2010-02-28 2.0\n",
      "2010-03-31 2.0\n",
      "2010-04-30 2.0\n",
      "2010-05-31 5.0\n",
      "\n",
      "We can also replace missing values with the latest known value (i.e., back-filling):\n",
      "\n",
      "# Back-fill\n",
      "dataframe.bfill()\n",
      "\n",
      "Sales\n",
      "\n",
      "2010-01-31 1.0\n",
      "2010-02-28 2.0\n",
      "2010-03-31 5.0\n",
      "2010-04-30 5.0\n",
      "2010-05-31 5.0\n",
      "\n",
      "Discussion\n",
      "Interpolation is a technique for filling in gaps caused by missing values by, in effect,\n",
      "drawing a line or curve between the known values bordering the gap and using that\n",
      "line  or  curve  to  predict  reasonable  values.  Interpolation  can  be  particularly  useful\n",
      "\n",
      "7.9 Handling Missing Data in Time Series \n",
      "\n",
      "| \n",
      "\n",
      "119\n",
      "\n",
      "\f",
      "when the time intervals between are constant, the data is not prone to noisy fluctua‐\n",
      "tions, and the gaps caused by missing values are small. For example, in our solution a\n",
      "gap of two missing values was bordered by 2.0 and 5.0. By fitting a line starting at\n",
      "2.0 and ending at 5.0, we can make reasonable guesses for the two missing values in\n",
      "between of 3.0 and 4.0.\n",
      "\n",
      "If we believe the line between the two known points is nonlinear, we can use interpo\n",
      "late’s method to specify the interpolation method:\n",
      "\n",
      "# Interpolate missing values\n",
      "dataframe.interpolate(method=\"quadratic\")\n",
      "\n",
      "Sales\n",
      "\n",
      "2010-01-31 1.000000\n",
      "2010-02-28 2.000000\n",
      "2010-03-31 3.059808\n",
      "2010-04-30 4.038069\n",
      "2010-05-31 5.000000\n",
      "\n",
      "Finally, there might be cases when we have large gaps of missing values and do not\n",
      "want  to  interpolate  values  across  the  entire  gap.  In  these  cases  we  can  use  limit  to\n",
      "restrict  the  number  of  interpolated  values  and  limit_direction  to  set  whether  to\n",
      "interpolate values forward from at the last known value before the gap or vice versa:\n",
      "\n",
      "# Interpolate missing values\n",
      "dataframe.interpolate(limit=1, limit_direction=\"forward\")\n",
      "\n",
      "Sales\n",
      "\n",
      "2010-01-31 1.0\n",
      "2010-02-28 2.0\n",
      "2010-03-31 3.0\n",
      "2010-04-30 NaN\n",
      "2010-05-31 5.0\n",
      "\n",
      "Back-filling  and  forward-filling  can  be  thought  of  as  a  form  of  naive  interpolation,\n",
      "where we draw a flat line from a known value and use it to fill in missing values. One\n",
      "(minor) advantage back- and forward-filling have over interpolation is the lack of the\n",
      "need for known values on both sides of missing value(s).\n",
      "\n",
      "120 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Handling Dates and Times\n",
      "\n",
      "\f",
      "CHAPTER 8\n",
      "Handling Images\n",
      "\n",
      "8.0 Introduction\n",
      "Image classification is one of the most exciting areas of machine learning. The ability\n",
      "of computers to recognize patterns and objects from images is an incredibly powerful\n",
      "tool  in  our  toolkit.  However,  before  we  can  apply  machine  learning  to  images,  we\n",
      "often first need to transform the raw images to features usable by our learning algo‐\n",
      "rithms.\n",
      "\n",
      "To  work  with  images,  we  will  use  the  Open  Source  Computer  Vision  Library\n",
      "(OpenCV).  While  there  are  a  number  of  good  libraries  out  there,  OpenCV  is  the\n",
      "most popular and documented library for handling images. One of the biggest hur‐\n",
      "dles to using OpenCV is installing it. However, fortunately if we are using Python 3\n",
      "(at  the  time  of  publication  OpenCV  does  not  work  with  Python  3.6+),  we  can  use\n",
      "Anaconda’s package manager tool conda to install OpenCV in a single line of code in\n",
      "our terminal:\n",
      "\n",
      "conda install --channel https://conda.anaconda.org/menpo opencv3\n",
      "\n",
      "Afterward, we can check the installation by opening a notebook, importing OpenCV,\n",
      "and checking the version number (3.1.0):\n",
      "\n",
      "import cv2\n",
      "\n",
      "cv2.__version__\n",
      "\n",
      "If installing OpenCV using conda does not work, there are many guides online.\n",
      "\n",
      "Finally,  throughout  this  chapter  we  will  use  a  set  of  images  as  examples,  which  are\n",
      "available to download on GitHub.\n",
      "\n",
      "121\n",
      "\n",
      "\f",
      "8.1 Loading Images\n",
      "\n",
      "Problem\n",
      "You want to load an image for preprocessing.\n",
      "\n",
      "Solution\n",
      "Use OpenCV’s imread:\n",
      "\n",
      "# Load library\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "If we want to view the image, we can use the Python plotting library Matplotlib:\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "122 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "Discussion\n",
      "Fundamentally, images are data and when we use imread we convert that data into a\n",
      "data type we are very familiar with—a NumPy array:\n",
      "\n",
      "# Show data type\n",
      "type(image)\n",
      "\n",
      "numpy.ndarray\n",
      "\n",
      "We have transformed the image into a matrix whose elements correspond to individ‐\n",
      "ual pixels. We can even take a look at the actual values of the matrix:\n",
      "\n",
      "# Show image data\n",
      "image\n",
      "\n",
      "array([[140, 136, 146, ..., 132, 139, 134],\n",
      "       [144, 136, 149, ..., 142, 124, 126],\n",
      "       [152, 139, 144, ..., 121, 127, 134],\n",
      "       ...,\n",
      "       [156, 146, 144, ..., 157, 154, 151],\n",
      "       [146, 150, 147, ..., 156, 158, 157],\n",
      "       [143, 138, 147, ..., 156, 157, 157]], dtype=uint8)\n",
      "\n",
      "The resolution of our image was 3600 × 2270, the exact dimensions of our matrix:\n",
      "\n",
      "# Show dimensions\n",
      "image.shape\n",
      "\n",
      "(2270, 3600)\n",
      "\n",
      "What  does  each  element  in  the  matrix  actually  represent?  In  grayscale  images,  the\n",
      "value of an individual element is the pixel intensity. Intensity values range from black\n",
      "(0) to white (255). For example, the intensity of the top-rightmost pixel in our image\n",
      "has a value of 140:\n",
      "\n",
      "# Show first pixel\n",
      "image[0,0]\n",
      "\n",
      "140\n",
      "\n",
      "In the matrix, each element contains three values corresponding to blue, green, red\n",
      "values (BGR):\n",
      "\n",
      "# Load image in color\n",
      "image_bgr = cv2.imread(\"images/plane.jpg\", cv2.IMREAD_COLOR)\n",
      "\n",
      "# Show pixel\n",
      "image_bgr[0,0]\n",
      "\n",
      "array([195, 144, 111], dtype=uint8)\n",
      "\n",
      "One  small  caveat:  by  default  OpenCV  uses  BGR,  but  many  image  applications—\n",
      "including  Matplotlib—use  red,  green,  blue  (RGB),  meaning  the  red  and  the  blue\n",
      "\n",
      "8.1 Loading Images \n",
      "\n",
      "| \n",
      "\n",
      "123\n",
      "\n",
      "\f",
      "values are swapped. To properly display OpenCV color images in Matplotlib, we need\n",
      "to first convert the color to RGB (apologies to hardcopy readers):\n",
      "\n",
      "# Convert to RGB\n",
      "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_rgb), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "See Also\n",
      "\n",
      "• Difference between RGB and BGR\n",
      "\n",
      "• RGB color model\n",
      "\n",
      "8.2 Saving Images\n",
      "\n",
      "Problem\n",
      "You want to save an image for preprocessing.\n",
      "\n",
      "Solution\n",
      "Use OpenCV’s imwrite:\n",
      "\n",
      "124 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Save image\n",
      "cv2.imwrite(\"images/plane_new.jpg\", image)\n",
      "\n",
      "True\n",
      "\n",
      "Discussion\n",
      "OpenCV’s imwrite saves images to the filepath specified. The format of the image is\n",
      "defined by the filename’s extension (.jpg, .png, etc.). One behavior to be careful about:\n",
      "imwrite will overwrite existing files without outputting an error or asking for confir‐\n",
      "mation.\n",
      "\n",
      "8.3 Resizing Images\n",
      "\n",
      "Problem\n",
      "You want to resize an image for further preprocessing.\n",
      "\n",
      "Solution\n",
      "Use resize to change the size of an image:\n",
      "\n",
      "# Load image\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Resize image to 50 pixels by 50 pixels\n",
      "image_50x50 = cv2.resize(image, (50, 50))\n",
      "\n",
      "# View image\n",
      "plt.imshow(image_50x50, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "8.3 Resizing Images \n",
      "\n",
      "| \n",
      "\n",
      "125\n",
      "\n",
      "\f",
      "Discussion\n",
      "Resizing  images  is  a  common  task  in  image  preprocessing  for  two  reasons.  First,\n",
      "images come in all shapes and sizes, and to be usable as features, images must have\n",
      "the same dimensions. This standardization of image size does come with costs, how‐\n",
      "ever; images are matrices of information and when we reduce the size of the image\n",
      "we  are  reducing  the  size  of  that  matrix  and  the  information  it  contains.  Second,\n",
      "machine learning can require thousands or hundreds of thousands of images. When\n",
      "those images are very large they can take up a lot of memory, and by resizing them we\n",
      "can  dramatically  reduce  memory  usage.  Some  common  image  sizes  for  machine\n",
      "learning are 32 × 32, 64 × 64, 96 × 96, and 256 × 256.\n",
      "\n",
      "8.4 Cropping Images\n",
      "\n",
      "Problem\n",
      "You want to remove the outer portion of the image to change its dimensions.\n",
      "\n",
      "126 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "Solution\n",
      "The image is encoded as a two-dimensional NumPy array, so we can crop the image\n",
      "easily by slicing the array:\n",
      "\n",
      "# Load image\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image in grayscale\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Select first half of the columns and all rows\n",
      "image_cropped = image[:,:128]\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_cropped, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "Since OpenCV represents images as a matrix of elements, by selecting the rows and\n",
      "columns we want to keep we are able to easily crop the image. Cropping can be par‐\n",
      "ticularly useful if we know that we only want to keep a certain part of every image.\n",
      "\n",
      "8.4 Cropping Images \n",
      "\n",
      "| \n",
      "\n",
      "127\n",
      "\n",
      "\f",
      "For example, if  our images come from a stationary security camera we can crop all\n",
      "the images so they only contain the area of interest.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Slicing NumPy Arrays\n",
      "\n",
      "8.5 Blurring Images\n",
      "\n",
      "Problem\n",
      "You want to smooth out an image.\n",
      "\n",
      "Solution\n",
      "To blur an image, each pixel is transformed to be the average value of its neighbors.\n",
      "This neighbor and the operation performed are mathematically represented as a ker‐\n",
      "nel  (don’t  worry  if  you  don’t  know  what  a  kernel  is).  The  size  of  this  kernel  deter‐\n",
      "mines the amount of blurring, with larger kernels producing smoother images. Here\n",
      "we blur an image by averaging the values of a 5 × 5 kernel around each pixel:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Blur image\n",
      "image_blurry = cv2.blur(image, (5,5))\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_blurry, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "128 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "To  highlight  the  effect  of  kernel  size,  here  is  the  same  blurring  with  a  100  ×  100\n",
      "kernel:\n",
      "\n",
      "# Blur image\n",
      "image_very_blurry = cv2.blur(image, (100,100))\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_very_blurry, cmap=\"gray\"), plt.xticks([]), plt.yticks([])\n",
      "plt.show()\n",
      "\n",
      "8.5 Blurring Images \n",
      "\n",
      "| \n",
      "\n",
      "129\n",
      "\n",
      "\f",
      "Discussion\n",
      "Kernels  are  widely  used  in  image  processing  to  do  everything  from  sharpening  to\n",
      "edge detection, and will come up repeatedly in this chapter. The blurring kernel we\n",
      "used looks like this:\n",
      "\n",
      "# Create kernel\n",
      "kernel = np.ones((5,5)) / 25.0\n",
      "\n",
      "# Show kernel\n",
      "kernel\n",
      "\n",
      "array([[ 0.04,  0.04,  0.04,  0.04,  0.04],\n",
      "       [ 0.04,  0.04,  0.04,  0.04,  0.04],\n",
      "       [ 0.04,  0.04,  0.04,  0.04,  0.04],\n",
      "       [ 0.04,  0.04,  0.04,  0.04,  0.04],\n",
      "       [ 0.04,  0.04,  0.04,  0.04,  0.04]])\n",
      "\n",
      "The center element in the kernel is the pixel being examined, while the remaining ele‐\n",
      "ments are its neighbors. Since all elements have the same value (normalized to add up\n",
      "to 1), each has an equal say in the resulting value of the pixel of interest. We can man‐\n",
      "ually apply a kernel to an image using filter2D to produce a similar blurring effect:\n",
      "\n",
      "# Apply kernel\n",
      "image_kernel = cv2.filter2D(image, -1, kernel)\n",
      "\n",
      "# Show image\n",
      "\n",
      "130 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "plt.imshow(image_kernel, cmap=\"gray\"), plt.xticks([]), plt.yticks([])\n",
      "plt.show()\n",
      "\n",
      "See Also\n",
      "\n",
      "• Image Kernels Explained Visually\n",
      "\n",
      "• Common Image Kernels\n",
      "\n",
      "8.6 Sharpening Images\n",
      "\n",
      "Problem\n",
      "You want to sharpen an image.\n",
      "\n",
      "Solution\n",
      "Create a kernel that highlights the target pixel. Then apply it to the image using fil\n",
      "ter2D:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "8.6 Sharpening Images \n",
      "\n",
      "| \n",
      "\n",
      "131\n",
      "\n",
      "\f",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Create kernel\n",
      "kernel = np.array([[0, -1, 0],\n",
      "                   [-1, 5,-1],\n",
      "                   [0, -1, 0]])\n",
      "\n",
      "# Sharpen image\n",
      "image_sharp = cv2.filter2D(image, -1, kernel)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_sharp, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "Sharpening  works  similarly  to  blurring,  except  instead  of  using  a  kernel  to  average\n",
      "the  neighboring  values,  we  constructed  a  kernel  to  highlight  the  pixel  itself.  The\n",
      "resulting effect makes contrasts in edges stand out more in the image.\n",
      "\n",
      "132 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "8.7 Enhancing Contrast\n",
      "\n",
      "Problem\n",
      "We want to increase the contrast between pixels in an image.\n",
      "\n",
      "Solution\n",
      "Histogram  equalization  is  a  tool  for  image  processing  that  can  make  objects  and\n",
      "shapes stand out. When we have a grayscale image, we can apply OpenCV’s equali\n",
      "zeHist directly on the image:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Enhance image\n",
      "image_enhanced = cv2.equalizeHist(image)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_enhanced, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "8.7 Enhancing Contrast \n",
      "\n",
      "| \n",
      "\n",
      "133\n",
      "\n",
      "\f",
      "However, when we have a color image, we first need to convert the image to the YUV\n",
      "color format. The Y is the luma, or brightness, and U and V denote the color. After\n",
      "the conversion, we can apply equalizeHist to the image and then convert it back to\n",
      "BGR or RGB:\n",
      "\n",
      "# Load image\n",
      "image_bgr = cv2.imread(\"images/plane.jpg\")\n",
      "\n",
      "# Convert to YUV\n",
      "image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV)\n",
      "\n",
      "# Apply histogram equalization\n",
      "image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])\n",
      "\n",
      "# Convert to RGB\n",
      "image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_rgb), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "134 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "Discussion\n",
      "While  a  detailed  explanation  of  how  histogram  equalization  works  is  beyond  the\n",
      "scope of this book, the short explanation is that it transforms the image so that it uses\n",
      "a wider range of pixel intensities.\n",
      "\n",
      "While the resulting image often does not look “realistic,” we need to remember that\n",
      "the image is just a visual representation of the underlying data. If histogram equaliza‐\n",
      "tion  is  able  to  make  objects  of  interest  more  distinguishable  from  other  objects  or\n",
      "backgrounds (which is not always the case), then it can be a valuable addition to our\n",
      "image preprocessing pipeline.\n",
      "\n",
      "8.8 Isolating Colors\n",
      "\n",
      "Problem\n",
      "You want to isolate a color in an image.\n",
      "\n",
      "Solution\n",
      "Define a range of colors and then apply a mask to the image:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "\n",
      "8.8 Isolating Colors \n",
      "\n",
      "| \n",
      "\n",
      "135\n",
      "\n",
      "\f",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image\n",
      "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
      "\n",
      "# Convert BGR to HSV\n",
      "image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "# Define range of blue values in HSV\n",
      "lower_blue = np.array([50,100,50])\n",
      "upper_blue = np.array([130,255,255])\n",
      "\n",
      "# Create mask\n",
      "mask = cv2.inRange(image_hsv, lower_blue, upper_blue)\n",
      "\n",
      "# Mask image\n",
      "image_bgr_masked = cv2.bitwise_and(image_bgr, image_bgr, mask=mask)\n",
      "\n",
      "# Convert BGR to RGB\n",
      "image_rgb = cv2.cvtColor(image_bgr_masked, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_rgb), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "136 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "Discussion\n",
      "Isolating colors in OpenCV is straightforward. First we convert an image into HSV\n",
      "(hue, saturation, and value). Second, we define a range of values we want to isolate,\n",
      "which  is  probably  the  most  difficult  and  time-consuming  part.  Third,  we  create  a\n",
      "mask for the image (we will only keep the white areas):\n",
      "\n",
      "# Show image\n",
      "plt.imshow(mask, cmap='gray'), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Finally,  we  apply  the  mask  to  the  image  using  bitwise_and  and  convert  to  our\n",
      "desired output format.\n",
      "\n",
      "8.9 Binarizing Images\n",
      "\n",
      "Problem\n",
      "Given an image, you want to output a simplified version.\n",
      "\n",
      "8.9 Binarizing Images \n",
      "\n",
      "| \n",
      "\n",
      "137\n",
      "\n",
      "\f",
      "Solution\n",
      "Thresholding is the process of setting pixels with intensity greater than some value to\n",
      "be white and less than the value to be black. A more advanced technique is adaptive\n",
      "thresholding, where the threshold value for a pixel is determined by the pixel intensi‐\n",
      "ties of its neighbors. This can be helpful when lighting conditions change over differ‐\n",
      "ent regions in an image:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image_grey = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Apply adaptive thresholding\n",
      "max_output_value = 255\n",
      "neighborhood_size = 99\n",
      "subtract_from_mean = 10\n",
      "image_binarized = cv2.adaptiveThreshold(image_grey,\n",
      "                                        max_output_value,\n",
      "                                        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
      "                                        cv2.THRESH_BINARY,\n",
      "                                        neighborhood_size,\n",
      "                                        subtract_from_mean)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_binarized, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "138 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "Discussion\n",
      "Our  solution  has  four  important  arguments  in  adaptiveThreshold.  max_out\n",
      "put_value simply determines the maximum intensity of the output pixel intensities.\n",
      "cv2.ADAPTIVE_THRESH_GAUSSIAN_C  sets  a  pixel’s  threshold  to  be  a  weighted  sum  of\n",
      "the neighboring pixel intensities. The weights are determined by a Gaussian window.\n",
      "Alternatively we could set the threshold to simply the mean of the neighboring pixels\n",
      "with cv2.ADAPTIVE_THRESH_MEAN_C:\n",
      "\n",
      "# Apply cv2.ADAPTIVE_THRESH_MEAN_C\n",
      "image_mean_threshold = cv2.adaptiveThreshold(image_grey,\n",
      "                                             max_output_value,\n",
      "                                             cv2.ADAPTIVE_THRESH_MEAN_C,\n",
      "                                             cv2.THRESH_BINARY,\n",
      "                                             neighborhood_size,\n",
      "                                             subtract_from_mean)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_mean_threshold, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "8.9 Binarizing Images \n",
      "\n",
      "| \n",
      "\n",
      "139\n",
      "\n",
      "\f",
      "The  last  two  parameters  are  the  block  size  (the  size  of  the  neighborhood  used  to\n",
      "determine  a  pixel’s  threshold)  and  a  constant  subtracted  from  the  calculated  thres‐\n",
      "hold (used to manually fine-tune the threshold).\n",
      "\n",
      "A major benefit of thresholding is denoising an image—keeping only the most impor‐\n",
      "tant elements. For example, thresholding is often applied to photos of printed text to\n",
      "isolate the letters from the page.\n",
      "\n",
      "8.10 Removing Backgrounds\n",
      "\n",
      "Problem\n",
      "You want to isolate the foreground of an image.\n",
      "\n",
      "Solution\n",
      "Mark a rectangle around the desired foreground, then run the GrabCut algorithm:\n",
      "\n",
      "# Load library\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "140 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "# Load image and convert to RGB\n",
      "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
      "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "# Rectangle values: start x, start y, width, height\n",
      "rectangle = (0, 56, 256, 150)\n",
      "\n",
      "# Create initial mask\n",
      "mask = np.zeros(image_rgb.shape[:2], np.uint8)\n",
      "\n",
      "# Create temporary arrays used by grabCut\n",
      "bgdModel = np.zeros((1, 65), np.float64)\n",
      "fgdModel = np.zeros((1, 65), np.float64)\n",
      "\n",
      "# Run grabCut\n",
      "cv2.grabCut(image_rgb, # Our image\n",
      "            mask, # The Mask\n",
      "            rectangle, # Our rectangle\n",
      "            bgdModel, # Temporary array for background\n",
      "            fgdModel, # Temporary array for background\n",
      "            5, # Number of iterations\n",
      "            cv2.GC_INIT_WITH_RECT) # Initiative using our rectangle\n",
      "\n",
      "# Create mask where sure and likely backgrounds set to 0, otherwise 1\n",
      "mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\n",
      "\n",
      "# Multiply image with new mask to subtract background\n",
      "image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_rgb_nobg), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "8.10 Removing Backgrounds \n",
      "\n",
      "| \n",
      "\n",
      "141\n",
      "\n",
      "\f",
      "Discussion\n",
      "The first thing we notice is that even though GrabCut did a pretty good job, there are\n",
      "still  areas  of  background  left  in  the  image.  We  could  go  back  and  manually  mark\n",
      "those  areas  as  background,  but  in  the  real  world  we  have  thousands  of  images  and\n",
      "manually  fixing  them  individually  is  not  feasible.  Therefore,  we  would  do  well  by\n",
      "simply accepting that the image data will still contain some background noise.\n",
      "\n",
      "In our solution, we start out by marking a rectangle around the area that contains the\n",
      "foreground.  GrabCut  assumes  everything  outside  this  rectangle  to  be  background\n",
      "and  uses  that  information  to  figure  out  what  is  likely  background  inside  the  square\n",
      "(to learn how the algorithm does this, check out the external resources at the end of\n",
      "this solution). Then a mask is created that denotes the different definitely/likely back‐\n",
      "ground/foreground regions:\n",
      "\n",
      "# Show mask\n",
      "plt.imshow(mask, cmap='gray'), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "142 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "The  black  region  is  the  area  outside  our  rectangle  that  is  assumed  to  be  definitely\n",
      "background. The gray area is what GrabCut considered likely background, while the\n",
      "white area is likely foreground.\n",
      "\n",
      "This  mask  is  then  used  to  create  a  second  mask  that  merges  the  black  and  gray\n",
      "regions:\n",
      "\n",
      "# Show mask\n",
      "plt.imshow(mask_2, cmap='gray'), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "8.10 Removing Backgrounds \n",
      "\n",
      "| \n",
      "\n",
      "143\n",
      "\n",
      "\f",
      "The second mask is then applied to the image so that only the foreground remains.\n",
      "\n",
      "8.11 Detecting Edges\n",
      "\n",
      "Problem\n",
      "You want to find the edges in an image.\n",
      "\n",
      "Solution\n",
      "Use an edge detection technique like the Canny edge detector:\n",
      "\n",
      "# Load library\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image_gray = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Calculate median intensity\n",
      "median_intensity = np.median(image_gray)\n",
      "\n",
      "# Set thresholds to be one standard deviation above and below median intensity\n",
      "\n",
      "144 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "lower_threshold = int(max(0, (1.0 - 0.33) * median_intensity))\n",
      "upper_threshold = int(min(255, (1.0 + 0.33) * median_intensity))\n",
      "\n",
      "# Apply canny edge detector\n",
      "image_canny = cv2.Canny(image_gray, lower_threshold, upper_threshold)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_canny, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "Edge  detection  is  a  major  topic  of  interest  in  computer  vision.  Edges  are  important\n",
      "because they are areas of high information. For example, in our image one patch of\n",
      "sky  looks  very  much  like  another  and  is  unlikely  to  contain  unique  or  interesting\n",
      "information. However, patches where the background sky meets the airplane contain\n",
      "a lot of information (e.g., an object’s shape). Edge detection allows us to remove low-\n",
      "information areas and isolate the areas of images containing the most information.\n",
      "\n",
      "There  are  many  edge  detection  techniques  (Sobel  filters,  Laplacian  edge  detector,\n",
      "etc.). However, our solution uses the commonly used Canny edge detector. How the\n",
      "Canny detector works is too detailed for this book, but there is one point that we need\n",
      "to address. The Canny detector requires two parameters denoting low and high gra‐\n",
      "dient threshold values. Potential edge pixels between the low and high thresholds are\n",
      "\n",
      "8.11 Detecting Edges \n",
      "\n",
      "| \n",
      "\n",
      "145\n",
      "\n",
      "\f",
      "considered  weak  edge  pixels,  while  those  above  the  high  threshold  are  considered\n",
      "strong edge pixels. OpenCV’s Canny method includes the low and high thresholds as\n",
      "required parameters. In our solution, we set the lower and upper thresholds to be one\n",
      "standard  deviation  below  and  above  the  image’s  median  pixel  intensity.  However,\n",
      "there are often cases when we might get better results if we used a good pair of low\n",
      "and high threshold values through manual trial and error using a few images before\n",
      "running Canny on our entire collection of images.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Canny Edge Detector\n",
      "\n",
      "• Canny Edge Detection Auto Thresholding\n",
      "\n",
      "8.12 Detecting Corners\n",
      "\n",
      "Problem\n",
      "You want to detect the corners in an image.\n",
      "\n",
      "Solution\n",
      "Use OpenCV’s implementation of the Harris corner detector, cornerHarris:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image_bgr = cv2.imread(\"images/plane_256x256.jpg\")\n",
      "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
      "image_gray = np.float32(image_gray)\n",
      "\n",
      "# Set corner detector parameters\n",
      "block_size = 2\n",
      "aperture = 29\n",
      "free_parameter = 0.04\n",
      "\n",
      "# Detect corners\n",
      "detector_responses = cv2.cornerHarris(image_gray,\n",
      "                                      block_size,\n",
      "                                      aperture,\n",
      "                                      free_parameter)\n",
      "\n",
      "# Large corner markers\n",
      "detector_responses = cv2.dilate(detector_responses, None)\n",
      "\n",
      "146 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "# Only keep detector responses greater than threshold, mark as white\n",
      "threshold = 0.02\n",
      "image_bgr[detector_responses >\n",
      "          threshold *\n",
      "          detector_responses.max()] = [255,255,255]\n",
      "\n",
      "# Convert to grayscale\n",
      "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_gray, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "The Harris corner detector is a commonly used method of detecting the intersection\n",
      "of two edges. Our interest in detecting corners is motivated by the same reason as for\n",
      "deleting edges: corners are points of high information. A complete explanation of the\n",
      "Harris corner detector is available in the external resources at the end of this recipe,\n",
      "but a simplified explanation is that it looks for windows (also called neighborhoods or\n",
      "patches) where small movements of the window (imagine shaking the window) cre‐\n",
      "ates big changes in the contents of the pixels inside the window. cornerHarris con‐\n",
      "tains three important parameters that we can use to control the edges detected. First,\n",
      "block_size is the size of the neighbor around each pixel used for corner detection.\n",
      "\n",
      "8.12 Detecting Corners \n",
      "\n",
      "| \n",
      "\n",
      "147\n",
      "\n",
      "\f",
      "Second, aperture is the size of the Sobel kernel used (don’t worry if you don’t know\n",
      "what that is), and finally there is a free parameter where larger values correspond to\n",
      "identifying softer corners.\n",
      "\n",
      "The output is a grayscale image depicting potential corners:\n",
      "\n",
      "# Show potential corners\n",
      "plt.imshow(detector_responses, cmap='gray'), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "We  then  apply  thresholding  to  keep  only  the  most  likely  corners.  Alternatively,  we\n",
      "can use a similar detector, the Shi-Tomasi corner detector, which works in a similar\n",
      "way  to  the  Harris  detector  (goodFeaturesToTrack)  to  identify  a  fixed  number  of\n",
      "strong corners. goodFeaturesToTrack takes three major parameters—the number of\n",
      "corners  to  detect,  the  minimum  quality  of  the  corner  (0  to  1),  and  the  minimum\n",
      "Euclidean distance between corners:\n",
      "\n",
      "# Load images\n",
      "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
      "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "# Number of corners to detect\n",
      "corners_to_detect = 10\n",
      "minimum_quality_score = 0.05\n",
      "minimum_distance = 25\n",
      "\n",
      "148 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "# Detect corners\n",
      "corners = cv2.goodFeaturesToTrack(image_gray,\n",
      "                                  corners_to_detect,\n",
      "                                  minimum_quality_score,\n",
      "                                  minimum_distance)\n",
      "corners = np.float32(corners)\n",
      "\n",
      "# Draw white circle at each corner\n",
      "for corner in corners:\n",
      "    x, y = corner[0]\n",
      "    cv2.circle(image_bgr, (x,y), 10, (255,255,255), -1)\n",
      "\n",
      "# Convert to grayscale\n",
      "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "# Show image\n",
      "plt.imshow(image_rgb, cmap='gray'), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "See Also\n",
      "\n",
      "• OpenCV’s cornerHarris\n",
      "\n",
      "• OpenCV’s goodFeaturesToTrack\n",
      "\n",
      "8.12 Detecting Corners \n",
      "\n",
      "| \n",
      "\n",
      "149\n",
      "\n",
      "\f",
      "8.13 Creating Features for Machine Learning\n",
      "\n",
      "Problem\n",
      "You want to convert an image into an observation for machine learning.\n",
      "\n",
      "Solution\n",
      "Use  NumPy’s  flatten  to  convert  the  multidimensional  array  containing  an  image’s\n",
      "data into a vector containing the observation’s values:\n",
      "\n",
      "# Load image\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as grayscale\n",
      "image = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Resize image to 10 pixels by 10 pixels\n",
      "image_10x10 = cv2.resize(image, (10, 10))\n",
      "\n",
      "# Convert image data to one-dimensional vector\n",
      "image_10x10.flatten()\n",
      "\n",
      "array([133, 130, 130, 129, 130, 129, 129, 128, 128, 127, 135, 131, 131,\n",
      "       131, 130, 130, 129, 128, 128, 128, 134, 132, 131, 131, 130, 129,\n",
      "       129, 128, 130, 133, 132, 158, 130, 133, 130,  46,  97,  26, 132,\n",
      "       143, 141,  36,  54,  91,   9,   9,  49, 144, 179,  41, 142,  95,\n",
      "        32,  36,  29,  43, 113, 141, 179, 187, 141, 124,  26,  25, 132,\n",
      "       135, 151, 175, 174, 184, 143, 151,  38, 133, 134, 139, 174, 177,\n",
      "       169, 174, 155, 141, 135, 137, 137, 152, 169, 168, 168, 179, 152,\n",
      "       139, 136, 135, 137, 143, 159, 166, 171, 175], dtype=uint8)\n",
      "\n",
      "Discussion\n",
      "Images are presented as a grid of pixels. If an image is in grayscale, each pixel is pre‐\n",
      "sented by one value (i.e., pixel intensity: 1 if white, 0 if black). For example, imagine\n",
      "we have a 10 × 10–pixel image:\n",
      "\n",
      "plt.imshow(image_10x10, cmap=\"gray\"), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "150 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "In this case the dimensions of the images data will be 10 × 10:\n",
      "\n",
      "image_10x10.shape\n",
      "\n",
      "(10, 10)\n",
      "\n",
      "And if we flatten the array, we get a vector of length 100 (10 multiplied by 10):\n",
      "\n",
      "image_10x10.flatten().shape\n",
      "\n",
      "(100,)\n",
      "\n",
      "This is the feature data for our image that can be joined with the vectors from other\n",
      "images to create the data we will feed to our machine learning algorithms.\n",
      "\n",
      "If the image is in color, instead of each pixel being represented by one value, it is rep‐\n",
      "resented by multiple values (most often three) representing the channels (red, green,\n",
      "blue, etc.) that blend to make the final color of that pixel. For this reason, if our 10 ×\n",
      "10 image is in color, we will have 300 feature values for each observation:\n",
      "\n",
      "# Load image in color\n",
      "image_color = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
      "\n",
      "# Resize image to 10 pixels by 10 pixels\n",
      "image_color_10x10 = cv2.resize(image_color, (10, 10))\n",
      "\n",
      "8.13 Creating Features for Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "151\n",
      "\n",
      "\f",
      "# Convert image data to one-dimensional vector, show dimensions\n",
      "image_color_10x10.flatten().shape\n",
      "\n",
      "(300,)\n",
      "\n",
      "One  of  the  major  challenges  of  image  processing  and  computer  vision  is  that  since\n",
      "every pixel location in a collection of images is a feature, as the images get larger, the\n",
      "number of features explodes:\n",
      "\n",
      "# Load image in grayscale\n",
      "image_256x256_gray = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "# Convert image data to one-dimensional vector, show dimensions\n",
      "image_256x256_gray.flatten().shape\n",
      "\n",
      "(65536,)\n",
      "\n",
      "And the number of features only intensifies when the image is in color:\n",
      "\n",
      "# Load image in color\n",
      "image_256x256_color = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
      "\n",
      "# Convert image data to one-dimensional vector, show dimensions\n",
      "image_256x256_color.flatten().shape\n",
      "\n",
      "(196608,)\n",
      "\n",
      "As the output shows, even a small color image has almost 200,000 features, which can\n",
      "cause  problems  when  we  are  training  our  models  because  the  number  of  features\n",
      "might far exceed the number of observations.\n",
      "\n",
      "This  problem  will  motivate  dimensionality  strategies  discussed  in  a  later  chapter,\n",
      "which attempt to reduce the number of features while not losing an excessive amount\n",
      "of information contained in the data.\n",
      "\n",
      "8.14 Encoding Mean Color as a Feature\n",
      "\n",
      "Problem\n",
      "You want a feature based on the colors of an image.\n",
      "\n",
      "Solution\n",
      "Each pixel in an image is represented by the combination of multiple color channels\n",
      "(often three: red, green, and blue). Calculate the mean red, green, and blue channel\n",
      "values  for  an  image  to  make  three  color  features  representing  the  average  colors  in\n",
      "that image:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "\n",
      "152 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image as BGR\n",
      "image_bgr = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
      "\n",
      "# Calculate the mean of each channel\n",
      "channels = cv2.mean(image_bgr)\n",
      "\n",
      "# Swap blue and red values (making it RGB, not BGR)\n",
      "observation = np.array([(channels[2], channels[1], channels[0])])\n",
      "\n",
      "# Show mean channel values\n",
      "observation\n",
      "\n",
      "array([[  90.53204346,  133.11735535,  169.03074646]])\n",
      "\n",
      "We can view the mean channel values directly (apologies to printed book readers):\n",
      "\n",
      "# Show image\n",
      "plt.imshow(observation), plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "The output is three feature values for an observation, one for each color channel in\n",
      "the image. These features can be used like any other features in learning algorithms to\n",
      "classify images according to their colors.\n",
      "\n",
      "8.15 Encoding Color Histograms as Features\n",
      "\n",
      "Problem\n",
      "You want to create a set of features representing the colors appearing in an image.\n",
      "\n",
      "8.15 Encoding Color Histograms as Features \n",
      "\n",
      "| \n",
      "\n",
      "153\n",
      "\n",
      "\f",
      "Solution\n",
      "Compute the histograms for each color channel:\n",
      "\n",
      "# Load libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Load image\n",
      "image_bgr = cv2.imread(\"images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
      "\n",
      "# Convert to RGB\n",
      "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "# Create a list for feature values\n",
      "features = []\n",
      "\n",
      "# Calculate the histogram for each color channel\n",
      "colors = (\"r\",\"g\",\"b\")\n",
      "\n",
      "# For each channel: calculate histogram and add to feature value list\n",
      "for i, channel in enumerate(colors):\n",
      "    histogram = cv2.calcHist([image_rgb], # Image\n",
      "                        [i], # Index of channel\n",
      "                        None, # No mask\n",
      "                        [256], # Histogram size\n",
      "                        [0,256]) # Range\n",
      "    features.extend(histogram)\n",
      "\n",
      "# Create a vector for an observation's feature values\n",
      "observation = np.array(features).flatten()\n",
      "\n",
      "# Show the observation's value for the first five features\n",
      "observation[0:5]\n",
      "\n",
      "array([ 1008.,   217.,   184.,   165.,   116.], dtype=float32)\n",
      "\n",
      "Discussion\n",
      "In the RGB color model, each color is the combination of three color channels (i.e.,\n",
      "red, green, blue). In turn, each channel can take on one of 256 values (represented by\n",
      "an integer between 0 and 255). For example, the top-leftmost pixel in our image has\n",
      "the following channel values:\n",
      "\n",
      "# Show RGB channel values\n",
      "image_rgb[0,0]\n",
      "\n",
      "array([107, 163, 212], dtype=uint8)\n",
      "\n",
      "A histogram is a representation of the distribution of values in data. Here is a simple\n",
      "example:\n",
      "\n",
      "154 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "# Import pandas\n",
      "import pandas as pd\n",
      "\n",
      "# Create some data\n",
      "data = pd.Series([1, 1, 2, 2, 3, 3, 3, 4, 5])\n",
      "\n",
      "# Show the histogram\n",
      "data.hist(grid=False)\n",
      "plt.show()\n",
      "\n",
      "In this example, we have some data with two 1s, two 2s, three 3s, one 4, and one 5. In\n",
      "the histogram, each bar represents the number of times each value (1, 2, etc.) appears\n",
      "in our data.\n",
      "\n",
      "We can apply this same technique to each of the color channels, but instead of five\n",
      "possible values, we have 256 (the range of possible values for a channel value). The x-\n",
      "axis represents the 256 possible channel values, and the y-axis represents the number\n",
      "of times a particular channel value appears across all pixels in an image:\n",
      "\n",
      "# Calculate the histogram for each color channel\n",
      "colors = (\"r\",\"g\",\"b\")\n",
      "\n",
      "# For each channel: calculate histogram, make plot\n",
      "for i, channel in enumerate(colors):\n",
      "    histogram = cv2.calcHist([image_rgb], # Image\n",
      "                        [i], # Index of channel\n",
      "                        None, # No mask\n",
      "\n",
      "8.15 Encoding Color Histograms as Features \n",
      "\n",
      "| \n",
      "\n",
      "155\n",
      "\n",
      "\f",
      "                        [256], # Histogram size\n",
      "                        [0,256]) # Range\n",
      "    plt.plot(histogram, color = channel)\n",
      "    plt.xlim([0,256])\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "As  we  can  see  in  the  histogram,  barely  any  pixels  contain  the  blue  channel  values\n",
      "between  0  and  ~180,  while  many  pixels  contain  blue  channel  values  between  ~190\n",
      "and ~210. This distribution of channel values is shown for all three channels. The his‐\n",
      "togram, however, is not simply a visualization; it is 256 features for each color chan‐\n",
      "nel, making for 768 total features representing the distribution of colors in an image.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Histogram\n",
      "\n",
      "• pandas Histogram documentation\n",
      "\n",
      "• OpenCV Histogram tutorial\n",
      "\n",
      "156 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Handling Images\n",
      "\n",
      "\f",
      "CHAPTER 9\n",
      "Dimensionality Reduction\n",
      "Using Feature Extraction\n",
      "\n",
      "9.0 Introduction\n",
      "It  is  common  to  have  access  to  thousands  and  even  hundreds  of  thousands  of  fea‐\n",
      "tures. For example, in Chapter 8 we transformed a 256 × 256–pixel color image into\n",
      "196,608 features. Furthermore, because each of these pixels can take one of 256 possi‐\n",
      "ble values, there ends up being 256196608 different configurations our observation can\n",
      "take. This is problematic because we will practically never be able to collect enough\n",
      "observations to cover even a small fraction of those configurations and our learning\n",
      "algorithms do not have enough data to operate correctly.\n",
      "\n",
      "Fortunately,  not  all  features  are  created  equal  and  the  goal  of  feature  extraction  for\n",
      "dimensionality reduction is to transform our set of features, poriginal, such that we end\n",
      "up with a new set, pnew, where poriginal > pnew, while still keeping much of the underlying\n",
      "information.  Put  another  way,  we  reduce  the  number  of  features  with  only  a  small\n",
      "loss in our data’s ability to generate high-quality predictions. In this chapter, we will\n",
      "cover a number of feature extraction techniques to do just this.\n",
      "\n",
      "One downside of the feature extraction techniques we discuss is that the new features\n",
      "we generate will not be interpretable by humans. They will contain as much or nearly\n",
      "as much ability to train our models, but will appear to the human eye as a collection\n",
      "of  random  numbers.  If  we  wanted  to  maintain  our  ability  to  interpret  our  models,\n",
      "dimensionality reduction through feature selection is a better option.\n",
      "\n",
      "157\n",
      "\n",
      "\f",
      "9.1 Reducing Features Using Principal Components\n",
      "\n",
      "Problem\n",
      "Given a set of features, you want to reduce the number of features while retaining the\n",
      "variance in the data.\n",
      "\n",
      "Solution\n",
      "Use principal component analysis with scikit’s PCA:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load the data\n",
      "digits = datasets.load_digits()\n",
      "\n",
      "# Standardize the feature matrix\n",
      "features = StandardScaler().fit_transform(digits.data)\n",
      "\n",
      "# Create a PCA that will retain 99% of variance\n",
      "pca = PCA(n_components=0.99, whiten=True)\n",
      "\n",
      "# Conduct PCA\n",
      "features_pca = pca.fit_transform(features)\n",
      "\n",
      "# Show results\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_pca.shape[1])\n",
      "\n",
      "Original number of features: 64\n",
      "Reduced number of features: 54\n",
      "\n",
      "Discussion\n",
      "Principal  component  analysis  (PCA)  is  a  popular  linear  dimensionality  reduction\n",
      "technique.  PCA  projects  observations  onto  the  (hopefully  fewer)  principal  compo‐\n",
      "nents  of  the  feature  matrix  that  retain  the  most  variance.  PCA  is  an  unsupervised\n",
      "technique, meaning that it does not use the information from the target vector and\n",
      "instead only considers the feature matrix.\n",
      "\n",
      "For a mathematical description of how PCA works, see the external resources listed at\n",
      "the end of this recipe. However, we can understand the intuition behind PCA using a\n",
      "simple  example.  In  the  following  figure,  our  data  contains  two  features,  x1  and  x2.\n",
      "Looking at the visualization, it should be clear that observations are spread out like a\n",
      "cigar, with a lot of length and very little height. More specifically, we can say that the\n",
      "variance of the “length” is significantly greater than the “height.” Instead of length and\n",
      "\n",
      "158 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "height, we refer to the “directions” with the most variance as the first principal com‐\n",
      "ponent  and  the  “direction”  with  the  second-most  variance  as  the  second  principal\n",
      "component (and so on).\n",
      "\n",
      "If we wanted to reduce our features, one strategy would be to project all observations\n",
      "in  our  2D  space  onto  the  1D  principal  component.  We  would  lose  the  information\n",
      "captured in the second principal component, but in some situations that would be an\n",
      "acceptable trade-off. This is PCA.\n",
      "\n",
      "PCA  is  implemented  in  scikit-learn  using  the  pca  method.  n_components  has  two\n",
      "operations, depending on the argument provided. If the argument is greater than 1,\n",
      "n_components  will  return  that  many  features.  This  leads  to  the  question  of  how  to\n",
      "select  the  number  of  features  that  is  optimal.  Fortunately  for  us,  if  the  argument  to\n",
      "n_components is between 0 and 1, pca returns the minimum amount of features that\n",
      "retain that much variance. It is common to use values of 0.95 and 0.99, meaning 95%\n",
      "and  99%  of  the  variance  of  the  original  features  has  been  retained,  respectively.\n",
      "whiten=True  transforms  the  values  of  each  principal  component  so  that  they  have\n",
      "zero  mean \n",
      "is\n",
      "svd_solver=\"randomized\", which implements a stochastic algorithm to find the first\n",
      "principal components in often significantly less time.\n",
      "\n",
      "and  unit  variance.  Another  parameter \n",
      "\n",
      "argument \n",
      "\n",
      "and \n",
      "\n",
      "The  output  of  our  solution  shows  that  PCA  let  us  reduce  our  dimensionality  by  10\n",
      "features while still retaining 99% of the information (variance) in the feature matrix.\n",
      "\n",
      "9.1 Reducing Features Using Principal Components \n",
      "\n",
      "| \n",
      "\n",
      "159\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• scikit-learn documentation on PCA\n",
      "\n",
      "• Choosing the Number of Principal Components\n",
      "\n",
      "• Principal component analysis with linear algebra\n",
      "\n",
      "9.2 Reducing Features When Data Is Linearly Inseparable\n",
      "\n",
      "Problem\n",
      "You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
      "\n",
      "Solution\n",
      "Use an extension of principal component analysis that uses kernels to allow for non-\n",
      "linear dimensionality reduction:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.decomposition import PCA, KernelPCA\n",
      "from sklearn.datasets import make_circles\n",
      "\n",
      "# Create linearly inseparable data\n",
      "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
      "\n",
      "# Apply kernal PCA with radius basis function (RBF) kernel\n",
      "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
      "features_kpca = kpca.fit_transform(features)\n",
      "\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_kpca.shape[1])\n",
      "\n",
      "Original number of features: 2\n",
      "Reduced number of features: 1\n",
      "\n",
      "Discussion\n",
      "PCA is able to reduce the dimensionality of our feature matrix (e.g., the number of\n",
      "features).  Standard  PCA  uses  linear  projection  to  reduce  the  features.  If  the  data  is\n",
      "linearly separable (i.e., you can draw a straight line or hyperplane between different\n",
      "classes) then PCA works well. However, if your data is not linearly separable (e.g., you\n",
      "can only separate classes using a curved decision boundary), the linear transforma‐\n",
      "tion will not work as well. In our solution we used scikit-learn’s make_circles to gen‐\n",
      "erate  a  simulated  dataset  with  a  target  vector  of  two  classes  and  two  features.\n",
      "make_circles  makes  linearly  inseparable  data;  specifically,  one  class  is  surrounded\n",
      "on all sides by the other class.\n",
      "\n",
      "160 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "If we used linear PCA to reduce the dimensions of our data, the two classes would be\n",
      "linearly  projected  onto  the  first  principal  component  such  that  they  would  become\n",
      "intertwined.\n",
      "\n",
      "Ideally, we would want a transformation that would both reduce the dimensions and\n",
      "also make the data linearly separable. Kernel PCA can do both.\n",
      "\n",
      "Kernels  allow  us  to  project  the  linearly  inseparable  data  into  a  higher  dimension\n",
      "where it is linearly separable; this is called the kernel trick. Don’t worry if you don’t\n",
      "understand  the  details  of  the  kernel  trick;  just  think  of  kernels  as  different  ways  of\n",
      "projecting  the  data.  There  are  a  number  of  kernels  we  can  use  in  scikit-learn’s  ker\n",
      "nelPCA, specified using the kernel parameter. A common kernel to use is the Gaus‐\n",
      "sian  radial  basis  function  kernel  rbf,  but  other  options  are  the  polynomial  kernel\n",
      "(poly)  and  sigmoid  kernel  (sigmoid).  We  can  even  specify  a  linear  projection  (lin\n",
      "ear), which will produce the same results as standard PCA.\n",
      "\n",
      "9.2 Reducing Features When Data Is Linearly Inseparable \n",
      "\n",
      "| \n",
      "\n",
      "161\n",
      "\n",
      "\f",
      "One  downside  of  kernel  PCA  is  that  there  are  a  number  of  parameters  we  need  to\n",
      "specify. For example, in Recipe 9.1 we set n_components to 0.99 to make PCA select\n",
      "the number of components to retain 99% of the variance. We don’t have this option in\n",
      "kernel  PCA.  Instead  we  have  to  define  the  number  of  parameters  (e.g.,  n_compo\n",
      "nents=1).  Furthermore,  kernels  come  with  their  own  hyperparameters  that  we  will\n",
      "have to set; for example, the radial basis function requires a gamma value.\n",
      "\n",
      "So how do we know which values to use? Through trial and error. Specifically we can\n",
      "train our machine learning model multiple times, each time with a different kernel or\n",
      "different  value  of  the  parameter.  Once  we  find  the  combination  of  values  that  pro‐\n",
      "duces the highest quality predicted values, we are done. We will learn about this strat‐\n",
      "egy in depth in Chapter 12.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation on Kernel PCA\n",
      "\n",
      "• Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA\n",
      "\n",
      "9.3 Reducing Features by Maximizing Class Separability\n",
      "\n",
      "Problem\n",
      "You want to reduce the features to be used by a classifier.\n",
      "\n",
      "Solution\n",
      "Try linear discriminant analysis (LDA) to project the features onto component axes\n",
      "that maximize the separation of classes:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "\n",
      "# Load Iris flower dataset:\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create and run an LDA, then use it to transform the features\n",
      "lda = LinearDiscriminantAnalysis(n_components=1)\n",
      "features_lda = lda.fit(features, target).transform(features)\n",
      "\n",
      "# Print the number of features\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_lda.shape[1])\n",
      "\n",
      "162 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "Original number of features: 4\n",
      "Reduced number of features: 1\n",
      "\n",
      "We can use  explained_variance_ratio_ to view the amount of variance explained\n",
      "by each component. In our solution the single component explained over 99% of the\n",
      "variance:\n",
      "\n",
      "lda.explained_variance_ratio_\n",
      "\n",
      "array([ 0.99147248])\n",
      "\n",
      "Discussion\n",
      "LDA is a classification that is also a popular technique for dimensionality reduction.\n",
      "LDA  works  similarly  to  principal  component  analysis  (PCA)  in  that  it  projects  our\n",
      "feature space onto a lower-dimensional space. However, in PCA we were only interes‐\n",
      "ted in the component axes that maximize the variance in the data, while in LDA we\n",
      "have  the  additional  goal  of  maximizing  the  differences  between  classes.  In  this  pic‐\n",
      "tured  example,  we  have  data  comprising  two  target  classes  and  two  features.  If  we\n",
      "project  the  data  onto  the  y-axis,  the  two  classes  are  not  easily  separable  (i.e.,  they\n",
      "overlap), while if we project the data onto the x-axis, we are left with a feature vector\n",
      "(i.e., we reduced our dimensionality by one) that still preserves class separability. In\n",
      "the real world, of course, the relationship between the classes will be more complex\n",
      "and the dimensionality will be higher, but the concept remains the same.\n",
      "\n",
      "9.3 Reducing Features by Maximizing Class Separability \n",
      "\n",
      "| \n",
      "\n",
      "163\n",
      "\n",
      "\f",
      "In  scikit-learn,  LDA  is  implemented  using  LinearDiscriminantAnalysis,  which\n",
      "includes  a  parameter,  n_components,  indicating  the  number  of  features  we  want\n",
      "returned.  To  figure  out  what  argument  value  to  use  with  n_components  (e.g.,  how\n",
      "many  parameters  to  keep),  we  can  take  advantage  of  the  fact  that  explained_var\n",
      "iance_ratio_ tells us the variance explained by each outputted feature and is a sor‐\n",
      "ted array. For example:\n",
      "\n",
      "lda.explained_variance_ratio_\n",
      "\n",
      "array([ 0.99147248])\n",
      "\n",
      "Specifically,  we  can  run  LinearDiscriminantAnalysis  with  n_components  set  to\n",
      "None to return the ratio of variance explained by every component feature, then cal‐\n",
      "culate how many components are required to get above some threshold of variance \n",
      "explained (often 0.95 or 0.99):\n",
      "\n",
      "# Create and run LDA\n",
      "lda = LinearDiscriminantAnalysis(n_components=None)\n",
      "features_lda = lda.fit(features, target)\n",
      "\n",
      "# Create array of explained variance ratios\n",
      "lda_var_ratios = lda.explained_variance_ratio_\n",
      "\n",
      "# Create function\n",
      "def select_n_components(var_ratio, goal_var: float) -> int:\n",
      "    # Set initial variance explained so far\n",
      "    total_variance = 0.0\n",
      "\n",
      "    # Set initial number of features\n",
      "    n_components = 0\n",
      "\n",
      "    # For the explained variance of each feature:\n",
      "    for explained_variance in var_ratio:\n",
      "\n",
      "        # Add the explained variance to the total\n",
      "        total_variance += explained_variance\n",
      "\n",
      "        # Add one to the number of components\n",
      "        n_components += 1\n",
      "\n",
      "        # If we reach our goal level of explained variance\n",
      "        if total_variance >= goal_var:\n",
      "            # End the loop\n",
      "            break\n",
      "\n",
      "    # Return the number of components\n",
      "    return n_components\n",
      "\n",
      "# Run function\n",
      "select_n_components(lda_var_ratios, 0.95)\n",
      "\n",
      "1\n",
      "\n",
      "164 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Comparison of LDA and PCA 2D projection of Iris dataset\n",
      "\n",
      "• Linear Discriminant Analysis\n",
      "\n",
      "9.4 Reducing Features Using Matrix Factorization\n",
      "\n",
      "Problem\n",
      "You have a feature matrix of nonnegative values and want to reduce the dimensional‐\n",
      "ity.\n",
      "\n",
      "Solution\n",
      "Use non-negative matrix factorization (NMF) to reduce the dimensionality of the fea‐\n",
      "ture matrix:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.decomposition import NMF\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load the data\n",
      "digits = datasets.load_digits()\n",
      "\n",
      "# Load feature matrix\n",
      "features = digits.data\n",
      "\n",
      "# Create, fit, and apply NMF\n",
      "nmf = NMF(n_components=10, random_state=1)\n",
      "features_nmf = nmf.fit_transform(features)\n",
      "\n",
      "# Show results\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_nmf.shape[1])\n",
      "\n",
      "Original number of features: 64\n",
      "Reduced number of features: 10\n",
      "\n",
      "Discussion\n",
      "NMF is an unsupervised technique for linear dimensionality reduction that factorizes\n",
      "(i.e.,  breaks  up  into  multiple  matrices  whose  product  approximates  the  original\n",
      "matrix) the feature matrix into matrices representing the latent relationship between\n",
      "observations and their features. Intuitively, NMF can reduce dimensionality because\n",
      "in matrix multiplication, the two factors (matrices being multiplied) can have signifi‐\n",
      "cantly fewer dimensions than the product matrix. Formally, given a desired number\n",
      "of returned features, r, NMF factorizes our feature matrix such that:\n",
      "\n",
      "9.4 Reducing Features Using Matrix Factorization \n",
      "\n",
      "| \n",
      "\n",
      "165\n",
      "\n",
      "\f",
      "V ≈ WH\n",
      "\n",
      "where V is our d × _n feature matrix (i.e., d features, n observations), W is a d × r,\n",
      "and H is an r × n matrix. By adjusting the value of r we can set the amount of dimen‐\n",
      "sionality reduction desired.\n",
      "\n",
      "One major requirement of NMA is that, as the name implies, the feature matrix can‐\n",
      "not contain negative values. Additionally, unlike PCA and other techniques we have\n",
      "examined,  NMA  does  not  provide  us  with  the  explained  variance  of  the  outputted\n",
      "features. Thus, the best way for us to find the optimum value of n_components is by\n",
      "trying a range of values to find the one that produces the best result in our end model\n",
      "(see Chapter 12).\n",
      "\n",
      "See Also\n",
      "\n",
      "• Non-Negative Matrix Factorization (NMF)\n",
      "\n",
      "9.5 Reducing Features on Sparse Data\n",
      "\n",
      "Problem\n",
      "You have a sparse feature matrix and want to reduce the dimensionality.\n",
      "\n",
      "Solution\n",
      "Use Truncated Singular Value Decomposition (TSVD):\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "\n",
      "# Load the data\n",
      "digits = datasets.load_digits()\n",
      "\n",
      "# Standardize feature matrix\n",
      "features = StandardScaler().fit_transform(digits.data)\n",
      "\n",
      "# Make sparse matrix\n",
      "features_sparse = csr_matrix(features)\n",
      "\n",
      "# Create a TSVD\n",
      "tsvd = TruncatedSVD(n_components=10)\n",
      "\n",
      "# Conduct TSVD on sparse matrix\n",
      "\n",
      "166 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n",
      "\n",
      "# Show results\n",
      "print(\"Original number of features:\", features_sparse.shape[1])\n",
      "print('\"Reduced number of features:\", features_sparse_tsvd.shape[1])\n",
      "\n",
      "Original number of features: 64\n",
      "Reduced number of features: 10\n",
      "\n",
      "Discussion\n",
      "TSVD is similar to PCA and in fact, PCA actually often uses non-truncated Singular\n",
      "Value  Decomposition  (SVD)  in  one  of  its  steps.  In  regular  SVD,  given  d  features,\n",
      "SVD will create factor matrices that are d × d, whereas TSVD will return factors that\n",
      "are n × n, where n is previously specified by a parameter. The practical advantage of\n",
      "TSVD is that unlike PCA, it works on sparse feature matrices.\n",
      "\n",
      "One issue with TSVD is that because of how it uses a random number generator, the\n",
      "signs of the output can flip between fittings. An easy workaround is to use fit only\n",
      "once per preprocessing pipeline, then use transform multiple times.\n",
      "\n",
      "As with linear discriminant analysis, we have to specify the number of features (com‐\n",
      "ponents) we want outputted. This is done with the n_components parameter. A natu‐\n",
      "ral question is then: what is the optimum number of components? One strategy is to \n",
      "include n_components as a hyperparameter to optimize during model selection (i.e.,\n",
      "choose  the  value  for  n_components  that  produces  the  best  trained  model).  Alterna‐\n",
      "tively,  because  TSVD  provides  us  with  the  ratio  of  the  original  feature  matrix’s  var‐\n",
      "iance  explained  by  each  component,  we  can  select  the  number  of  components  that\n",
      "explain a desired amount of variance (95% or 99% are common values). For example,\n",
      "in  our  solution  the  first  three  outputted  components  explain  approximately  30%  of\n",
      "the original data’s variance:\n",
      "\n",
      "# Sum of first three components' explained variance ratios\n",
      "tsvd.explained_variance_ratio_[0:3].sum()\n",
      "\n",
      "0.30039385386597783\n",
      "\n",
      "We  can  automate  the  process  by  creating  a  function  that  runs  TSVD  with  n_compo\n",
      "nents set to one less than the number of original features and then calculate the num‐\n",
      "ber of components that explain a desired amount of the original data’s variance:\n",
      "\n",
      "# Create and run an TSVD with one less than number of features\n",
      "tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n",
      "features_tsvd = tsvd.fit(features)\n",
      "\n",
      "# List of explained variances\n",
      "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
      "\n",
      "# Create a function\n",
      "def select_n_components(var_ratio, goal_var):\n",
      "\n",
      "9.5 Reducing Features on Sparse Data \n",
      "\n",
      "| \n",
      "\n",
      "167\n",
      "\n",
      "\f",
      "    # Set initial variance explained so far\n",
      "    total_variance = 0.0\n",
      "\n",
      "    # Set initial number of features\n",
      "    n_components = 0\n",
      "\n",
      "    # For the explained variance of each feature:\n",
      "    for explained_variance in var_ratio:\n",
      "\n",
      "        # Add the explained variance to the total\n",
      "        total_variance += explained_variance\n",
      "\n",
      "        # Add one to the number of components\n",
      "        n_components += 1\n",
      "\n",
      "        # If we reach our goal level of explained variance\n",
      "        if total_variance >= goal_var:\n",
      "            # End the loop\n",
      "            break\n",
      "\n",
      "    # Return the number of components\n",
      "    return n_components\n",
      "\n",
      "# Run function\n",
      "select_n_components(tsvd_var_ratios, 0.95)\n",
      "\n",
      "40\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation TruncatedSVD\n",
      "\n",
      "168 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Dimensionality Reduction Using Feature Extraction\n",
      "\n",
      "\f",
      "CHAPTER 10\n",
      "Dimensionality Reduction\n",
      "Using Feature Selection\n",
      "\n",
      "10.0 Introduction\n",
      "In Chapter 9, we discussed how to reduce the dimensionality of our feature matrix by\n",
      "creating  new  features  with  (ideally)  similar  ability  to  train  quality  models  but  with\n",
      "significantly  fewer  dimensions.  This  is  called  feature  extraction.  In  this  chapter  we\n",
      "will  cover  an  alternative  approach:  selecting  high-quality,  informative  features  and\n",
      "dropping less useful features. This is called feature selection.\n",
      "\n",
      "There are three types of feature selection methods: filter, wrapper, and embedded. Fil‐\n",
      "ter methods select the best features by examining their statistical properties. Wrapper\n",
      "methods use trial and error to find the subset of features that produce models with\n",
      "the  highest  quality  predictions.  Finally,  embedded  methods  select  the  best  feature\n",
      "subset as part or as an extension of a learning algorithm’s training process.\n",
      "\n",
      "Ideally,  we’d  describe  all  three  methods  in  this  chapter.  However,  since  embedded\n",
      "methods are closely intertwined with specific learning algorithms, they are difficult to\n",
      "explain prior to a deeper dive into the algorithms themselves. Therefore, in this chap‐\n",
      "ter we cover only filter and wrapper feature selection methods, leaving the discussion\n",
      "of particular embedded methods until the chapters where those learning algorithms\n",
      "are discussed in depth.\n",
      "\n",
      "169\n",
      "\n",
      "\f",
      "10.1 Thresholding Numerical Feature Variance\n",
      "\n",
      "Problem\n",
      "You have a set of numerical features and want to remove those with low variance (i.e.,\n",
      "likely containing little information).\n",
      "\n",
      "Solution\n",
      "Select a subset of features with variances above a given threshold:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "# import some data to play with\n",
      "iris = datasets.load_iris()\n",
      "\n",
      "# Create features and target\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create thresholder\n",
      "thresholder = VarianceThreshold(threshold=.5)\n",
      "\n",
      "# Create high variance feature matrix\n",
      "features_high_variance = thresholder.fit_transform(features)\n",
      "\n",
      "# View high variance feature matrix\n",
      "features_high_variance[0:3]\n",
      "\n",
      "array([[ 5.1,  1.4,  0.2],\n",
      "       [ 4.9,  1.4,  0.2],\n",
      "       [ 4.7,  1.3,  0.2]])\n",
      "\n",
      "Discussion\n",
      "Variance thresholding (VT) is one of the most basic approaches to feature selection. It\n",
      "is motivated by the idea that features with low variance are likely less interesting (and\n",
      "useful)  than  features  with  high  variance.  VT  first  calculates  the  variance  of  each\n",
      "feature:\n",
      "\n",
      "operatornameVar x =\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "n ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "xi − μ 2\n",
      "\n",
      "where x is the feature vector, xi is an individual feature value, and μ is that feature’s\n",
      "mean value. Next, it drops all features whose variance does not meet that threshold.\n",
      "\n",
      "170 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Dimensionality Reduction Using Feature Selection\n",
      "\n",
      "\f",
      "There are two things to keep in mind when employing VT. First, the variance is not\n",
      "centered; that is, it is in the squared unit of the feature itself. Therefore, the VT will\n",
      "not work when feature sets contain different units (e.g., one feature is in years while a\n",
      "different feature is in dollars). Second, the variance threshold is selected manually, so\n",
      "we have to use our own judgment for a good value to select (or use a model selection\n",
      "technique  described  in  Chapter  12).  We  can  see  the  variance  for  each  feature  using\n",
      "variances_:\n",
      "\n",
      "# View variances\n",
      "thresholder.fit(features).variances_\n",
      "\n",
      "array([ 0.68112222,  0.18675067,  3.09242489,  0.57853156])\n",
      "\n",
      "Finally, if the features have been standardized (to mean zero and unit variance), then\n",
      "for obvious reasons variance thresholding will not work correctly:\n",
      "\n",
      "# Load library\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Standardize feature matrix\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "# Caculate variance of each feature\n",
      "selector = VarianceThreshold()\n",
      "selector.fit(features_std).variances_\n",
      "\n",
      "array([ 1.,  1.,  1.,  1.])\n",
      "\n",
      "10.2 Thresholding Binary Feature Variance\n",
      "\n",
      "Problem\n",
      "You have a set of binary categorical features and want to remove those with low var‐\n",
      "iance (i.e., likely containing little information).\n",
      "\n",
      "Solution\n",
      "Select  a  subset  of  features  with  a  Bernoulli  random  variable  variance  above  a  given\n",
      "threshold:\n",
      "\n",
      "# Load library\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "\n",
      "# Create feature matrix with:\n",
      "# Feature 0: 80% class 0\n",
      "# Feature 1: 80% class 1\n",
      "# Feature 2: 60% class 0, 40% class 1\n",
      "features = [[0, 1, 0],\n",
      "            [0, 1, 1],\n",
      "\n",
      "10.2 Thresholding Binary Feature Variance \n",
      "\n",
      "| \n",
      "\n",
      "171\n",
      "\n",
      "\f",
      "            [0, 1, 0],\n",
      "            [0, 1, 1],\n",
      "            [1, 0, 0]]\n",
      "\n",
      "# Run threshold by variance\n",
      "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
      "thresholder.fit_transform(features)\n",
      "\n",
      "array([[0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0]])\n",
      "\n",
      "Discussion\n",
      "Just like with numerical features, one strategy for selecting highly informative catego‐\n",
      "rical features is to examine their variances. In binary features (i.e., Bernoulli random\n",
      "variables), variance is calculated as:\n",
      "\n",
      "Var x = p 1 − p\n",
      "\n",
      "where p is the proportion of observations of class 1. Therefore, by setting p, we can\n",
      "remove features where the vast majority of observations are one class.\n",
      "\n",
      "10.3 Handling Highly Correlated Features\n",
      "\n",
      "Problem\n",
      "You have a feature matrix and suspect some features are highly correlated.\n",
      "\n",
      "Solution\n",
      "Use a correlation matrix to check for highly correlated features. If highly correlated\n",
      "features exist, consider dropping one of the correlated features:\n",
      "\n",
      "# Load libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Create feature matrix with two highly correlated features\n",
      "features = np.array([[1, 1, 1],\n",
      "                     [2, 2, 0],\n",
      "                     [3, 3, 1],\n",
      "                     [4, 4, 0],\n",
      "                     [5, 5, 1],\n",
      "                     [6, 6, 0],\n",
      "                     [7, 7, 1],\n",
      "                     [8, 7, 0],\n",
      "\n",
      "172 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Dimensionality Reduction Using Feature Selection\n",
      "\n",
      "\f",
      "                     [9, 7, 1]])\n",
      "\n",
      "# Convert feature matrix into DataFrame\n",
      "dataframe = pd.DataFrame(features)\n",
      "\n",
      "# Create correlation matrix\n",
      "corr_matrix = dataframe.corr().abs()\n",
      "\n",
      "# Select upper triangle of correlation matrix\n",
      "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
      "                          k=1).astype(np.bool))\n",
      "\n",
      "# Find index of feature columns with correlation greater than 0.95\n",
      "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
      "\n",
      "# Drop features\n",
      "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)\n",
      "\n",
      "0 2\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "\n",
      "Discussion\n",
      "One problem we often run into in machine learning is highly correlated features. If\n",
      "two features are highly correlated, then the information they contain is very similar,\n",
      "and it is likely redundant to include both features. The solution to highly correlated\n",
      "features is simple: remove one of them from the feature set.\n",
      "\n",
      "In our solution, first we create a correlation matrix of all features:\n",
      "\n",
      "# Correlation matrix\n",
      "dataframe.corr()\n",
      "\n",
      "0\n",
      "\n",
      "0 1.000000\n",
      "1 0.976103\n",
      "2 0.000000\n",
      "\n",
      "1\n",
      "0.976103\n",
      "1.000000\n",
      "-0.034503\n",
      "\n",
      "2\n",
      "0.000000\n",
      "-0.034503\n",
      "1.000000\n",
      "\n",
      "Second,  we  look  at  the  upper  triangle  of  the  correlation  matrix  to  identify  pairs  of\n",
      "highly correlated features:\n",
      "\n",
      "# Upper triangle of correlation matrix\n",
      "upper\n",
      "\n",
      "10.3 Handling Highly Correlated Features \n",
      "\n",
      "| \n",
      "\n",
      "173\n",
      "\n",
      "\f",
      "0\n",
      "\n",
      "1\n",
      "0 NaN 0.976103\n",
      "1 NaN NaN\n",
      "2 NaN NaN\n",
      "\n",
      "2\n",
      "0.000000\n",
      "0.034503\n",
      "NaN\n",
      "\n",
      "Third, we remove one feature from each of those pairs from the feature set.\n",
      "\n",
      "10.4 Removing Irrelevant Features for Classification\n",
      "\n",
      "Problem\n",
      "You have a categorical target vector and want to remove uninformative features.\n",
      "\n",
      "Solution\n",
      "If the features are categorical, calculate a chi-square (χ2) statistic between each feature\n",
      "and the target vector:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2, f_classif\n",
      "\n",
      "# Load data\n",
      "iris = load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Convert to categorical data by converting data to integers\n",
      "features = features.astype(int)\n",
      "\n",
      "# Select two features with highest chi-squared statistics\n",
      "chi2_selector = SelectKBest(chi2, k=2)\n",
      "features_kbest = chi2_selector.fit_transform(features, target)\n",
      "\n",
      "# Show results\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
      "\n",
      "Original number of features: 4\n",
      "Reduced number of features: 2\n",
      "\n",
      "If  the  features  are  quantitative,  compute  the  ANOVA  F-value  between  each  feature\n",
      "and the target vector:\n",
      "\n",
      "# Select two features with highest F-values\n",
      "fvalue_selector = SelectKBest(f_classif, k=2)\n",
      "features_kbest = fvalue_selector.fit_transform(features, target)\n",
      "\n",
      "174 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Dimensionality Reduction Using Feature Selection\n",
      "\n",
      "\f",
      "# Show results\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
      "\n",
      "Original number of features: 4\n",
      "Reduced number of features: 2\n",
      "\n",
      "Instead of selecting a specific number of features, we can also use SelectPercentile\n",
      "to select the top n percent of features:\n",
      "\n",
      "# Load library\n",
      "from sklearn.feature_selection import SelectPercentile\n",
      "\n",
      "# Select top 75% of features with highest F-values\n",
      "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
      "features_kbest = fvalue_selector.fit_transform(features, target)\n",
      "\n",
      "# Show results\n",
      "print(\"Original number of features:\", features.shape[1])\n",
      "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
      "\n",
      "Original number of features: 4\n",
      "Reduced number of features: 3\n",
      "\n",
      "Discussion\n",
      "Chi-square  statistics  examines  the  independence  of  two  categorical  vectors.  That  is,\n",
      "the  statistic  is  the  difference  between  the  observed  number  of  observations  in  each\n",
      "class of a categorical feature and what we would expect if that feature was independ‐\n",
      "ent (i.e., no relationship) with the target vector:\n",
      "\n",
      "2\n",
      "\n",
      "n Oi − Ei\n",
      "χ2 = ∑\n",
      "i = 1\n",
      "\n",
      "Ei\n",
      "\n",
      "where Oi is the number of observations in class i and Ei is the number of observations\n",
      "in class i we would expect if there is no relationship between the feature and target\n",
      "vector.\n",
      "\n",
      "A  chi-squared  statistic  is  a  single  number  that  tells  you  how  much  difference  exists\n",
      "between your observed counts and the counts you would expect if there were no rela‐\n",
      "tionship  at  all  in  the  population.  By  calculating  the  chi-squared  statistic  between  a\n",
      "feature and the target vector, we obtain a measurement of the independence between\n",
      "the two. If the target is independent of the feature variable, then it is irrelevant for our\n",
      "purposes  because  it  contains  no  information  we  can  use  for  classification.  On  the\n",
      "other hand, if the two features are highly dependent, they likely are very informative\n",
      "for training our model.\n",
      "\n",
      "10.4 Removing Irrelevant Features for Classification \n",
      "\n",
      "| \n",
      "\n",
      "175\n",
      "\n",
      "\f",
      "To use chi-squared in feature selection, we calculate the chi-squared statistic between\n",
      "each  feature  and  the  target  vector,  then  select  the  features  with  the  best  chi-square\n",
      "statistics. In scikit-learn, we can use SelectKBest to select the features with the best\n",
      "statistics. The parameter k determines the number of features we want to keep.\n",
      "\n",
      "It  is  important  to  note  that  chi-square  statistics  can  only  be  calculated  between  two\n",
      "categorical  vectors.  For  this  reason,  chi-squared  for  feature  selection  requires  that\n",
      "both the target vector and the features are categorical. However, if we have a numeri‐\n",
      "cal feature we can use the chi-squared technique by first transforming the quantita‐\n",
      "tive feature into a categorical feature. Finally, to use our chi-squared approach, all val‐\n",
      "ues need to be non-negative.\n",
      "\n",
      "Alternatively,  if  we  have  a  numerical  feature  we  can  use  f_classif  to  calculate  the\n",
      "ANOVA F-value statistic with each feature and the target vector. F-value scores exam‐\n",
      "ine if, when we group the numerical feature by the target vector, the means for each\n",
      "group are significantly different. For example, if we had a binary target vector, gender,\n",
      "and a quantitative feature, test scores, the F-value score would tell us if the mean test\n",
      "score for men is different than the mean test score for women. If it is not, then test\n",
      "score doesn’t help us predict gender and therefore the feature is irrelevant.\n",
      "\n",
      "10.5 Recursively Eliminating Features\n",
      "\n",
      "Problem\n",
      "You want to automatically select the best features to keep.\n",
      "\n",
      "Solution\n",
      "Use  scikit-learn’s  RFECV  to  conduct  recursive  feature  elimination  (RFE)  using  cross-\n",
      "validation (CV). That is, repeatedly train a model, each time removing a feature until\n",
      "model  performance  (e.g.,  accuracy)  becomes  worse.  The  remaining  features  are  the\n",
      "best:\n",
      "\n",
      "# Load libraries\n",
      "import warnings\n",
      "from sklearn.datasets import make_regression\n",
      "from sklearn.feature_selection import RFECV\n",
      "from sklearn import datasets, linear_model\n",
      "\n",
      "# Suppress an annoying but harmless warning\n",
      "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n",
      "                        message=\"^internal gelsd\")\n",
      "\n",
      "# Generate features matrix, target vector, and the true coefficients\n",
      "features, target = make_regression(n_samples = 10000,\n",
      "                                   n_features = 100,\n",
      "                                   n_informative = 2,\n",
      "\n",
      "176 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Dimensionality Reduction Using Feature Selection\n",
      "\n",
      "\f",
      "                                   random_state = 1)\n",
      "\n",
      "# Create a linear regression\n",
      "ols = linear_model.LinearRegression()\n",
      "\n",
      "# Recursively eliminate features\n",
      "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
      "rfecv.fit(features, target)\n",
      "rfecv.transform(features)\n",
      "\n",
      "array([[ 0.00850799,  0.7031277 , -1.2416911 , -0.25651883, -0.10738769],\n",
      "       [-1.07500204,  2.56148527,  0.5540926 , -0.72602474, -0.91773159],\n",
      "       [ 1.37940721, -1.77039484, -0.59609275,  0.51485979, -1.17442094],\n",
      "       ...,\n",
      "       [-0.80331656, -1.60648007,  0.37195763,  0.78006511, -0.20756972],\n",
      "       [ 0.39508844, -1.34564911, -0.9639982 ,  1.7983361 , -0.61308782],\n",
      "       [-0.55383035,  0.82880112,  0.24597833, -1.71411248,  0.3816852 ]])\n",
      "\n",
      "Once we have conducted RFE, we can see the number of features we should keep:\n",
      "\n",
      "# Number of best features\n",
      "rfecv.n_features_\n",
      "\n",
      "5\n",
      "\n",
      "We can also see which of those features we should keep:\n",
      "\n",
      "# Which categories are best\n",
      "rfecv.support_\n",
      "\n",
      "array([False, False, False, False, False,  True, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False,  True, False, False,\n",
      "       False, False, False, False, False,  True, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False], dtype=bool)\n",
      "\n",
      "We can even view the rankings of the features:\n",
      "\n",
      "# Rank features best (1) to worst\n",
      "rfecv.ranking_\n",
      "\n",
      "array([11, 92, 96, 87, 46,  1, 48, 23, 16,  2, 66, 83, 33, 27, 70, 75, 29,\n",
      "       84, 54, 88, 37, 42, 85, 62, 74, 50, 80, 10, 38, 59, 79, 57, 44,  8,\n",
      "       82, 45, 89, 69, 94,  1, 35, 47, 39,  1, 34, 72, 19,  4, 17, 91, 90,\n",
      "       24, 32, 13, 49, 26, 12, 71, 68, 40,  1, 43, 63, 28, 73, 58, 21, 67,\n",
      "        1, 95, 77, 93, 22, 52, 30, 60, 81, 14, 86, 18, 15, 41,  7, 53, 65,\n",
      "       51, 64,  6,  9, 20,  5, 55, 56, 25, 36, 61, 78, 31,  3, 76])\n",
      "\n",
      "10.5 Recursively Eliminating Features \n",
      "\n",
      "| \n",
      "\n",
      "177\n",
      "\n",
      "\f",
      "Discussion\n",
      "This  is  likely  the  most  advanced  recipe  in  this  book  up  to  this  point,  combining  a\n",
      "number of topics we have yet to address in detail. However, the intuition is straight‐\n",
      "forward enough that we can address it here rather than holding off until a later chap‐\n",
      "ter.  The  idea  behind  RFE  is  to  train  a  model  that  contains  some  parameters  (also\n",
      "called weights or coefficients) like linear regression or support vector machines repeat‐\n",
      "edly. The first time we train the model, we include all the features. Then, we find the\n",
      "feature  with  the  smallest  parameter  (notice  that  this  assumes  the  features  are  either\n",
      "rescaled or standardized), meaning it is less important, and remove the feature from\n",
      "the feature set.\n",
      "\n",
      "The obvious question then is: how many features should we keep? We can (hypotheti‐\n",
      "cally) repeat this loop until we only have one feature left. A better approach requires\n",
      "that  we  include  a  new  concept  called  cross-validation  (CV).  We  will  discuss  cross-\n",
      "validation in detail in the next chapter, but here is the general idea.\n",
      "\n",
      "Given data containing 1) a target we want to predict and 2) a feature matrix, first we\n",
      "split the data into two groups: a training set and a test set. Second, we train our model\n",
      "using the training set. Third, we pretend that we do not know the target of the test\n",
      "set, and apply our model to the test set’s features in order to predict the values of the\n",
      "test set. Finally, we compare our predicted target values with the true target values to\n",
      "evaluate our model.\n",
      "\n",
      "We can use CV to find the optimum number of features to keep during RFE. Specifi‐\n",
      "cally,  in  RFE  with  CV  after  every  iteration,  we  use  cross-validation  to  evaluate  our\n",
      "model. If CV shows that our model improved after we eliminated a feature, then we\n",
      "continue on to the next loop. However, if CV shows that our model got worse after\n",
      "we eliminated a feature, we put that feature back into the feature set and select those\n",
      "features as the best.\n",
      "\n",
      "In scikit-learn, RFE with CV is implemented using RFECV and contains a number of\n",
      "important  parameters.  The  estimator  parameter  determines  the  type  of  model  we\n",
      "want to train (e.g., linear regression). The step regression sets the number or propor‐\n",
      "tion of features to drop during each loop. The scoring parameter sets the metric of\n",
      "quality we use to evaluate our model during cross-validation.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Recursive feature elimination with cross-validation\n",
      "\n",
      "178 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Dimensionality Reduction Using Feature Selection\n",
      "\n",
      "\f",
      "CHAPTER 11\n",
      "Model Evaluation\n",
      "\n",
      "11.0 Introduction\n",
      "In this chapter we will examine strategies for evaluating the quality of models created\n",
      "through our learning algorithms. It might appear strange to discuss model evaluation\n",
      "before discussing how to create them, but there is a method to our madness. Models\n",
      "are only as useful as the quality of their predictions, and thus fundamentally our goal\n",
      "is  not  to  create  models  (which  is  easy)  but  to  create  high-quality  models  (which  is\n",
      "hard).  Therefore,  before  we  explore  the  myriad  learning  algorithms,  we  first  set  up\n",
      "how we can evaluate the models they produce.\n",
      "\n",
      "11.1 Cross-Validating Models\n",
      "\n",
      "Problem\n",
      "You want to evaluate how well your model will work in the real world.\n",
      "\n",
      "Solution\n",
      "Create  a  pipeline  that  preprocesses  the  data,  trains  the  model,  and  then  evaluates  it\n",
      "using cross-validation:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import KFold, cross_val_score\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load digits dataset\n",
      "\n",
      "179\n",
      "\n",
      "\f",
      "digits = datasets.load_digits()\n",
      "\n",
      "# Create features matrix\n",
      "features = digits.data\n",
      "\n",
      "# Create target vector\n",
      "target = digits.target\n",
      "\n",
      "# Create standardizer\n",
      "standardizer = StandardScaler()\n",
      "\n",
      "# Create logistic regression object\n",
      "logit = LogisticRegression()\n",
      "\n",
      "# Create a pipeline that standardizes, then runs logistic regression\n",
      "pipeline = make_pipeline(standardizer, logit)\n",
      "\n",
      "# Create k-Fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
      "\n",
      "# Conduct k-fold cross-validation\n",
      "cv_results = cross_val_score(pipeline, # Pipeline\n",
      "                             features, # Feature matrix\n",
      "                             target, # Target vector\n",
      "                             cv=kf, # Cross-validation technique\n",
      "                             scoring=\"accuracy\", # Loss function\n",
      "                             n_jobs=-1) # Use all CPU scores\n",
      "\n",
      "# Calculate mean\n",
      "cv_results.mean()\n",
      "\n",
      "0.96493171942892597\n",
      "\n",
      "Discussion\n",
      "At first consideration, evaluating supervised-learning models might appear straight‐\n",
      "forward:  train  a  model  and  then  calculate  how  well  it  did  using  some  performance\n",
      "metric  (accuracy,  squared  errors,  etc.).  However,  this  approach  is  fundamentally\n",
      "flawed. If we train a model using our data, and then evaluate how well it did on that\n",
      "data, we are not achieving our desired goal. Our goal is not to evaluate how well the\n",
      "model does on our training data, but how well it does on data it has never seen before\n",
      "(e.g.,  a  new  customer,  a  new  crime,  a  new  image).  For  this  reason,  our  method  of\n",
      "evaluation should help us understand how well models are able to make predictions\n",
      "from data they have never seen before.\n",
      "\n",
      "One strategy might be to hold off a slice of data for testing. This is called validation\n",
      "(or hold-out). In validation our observations (features and targets) are split into two\n",
      "sets, traditionally called the training set and the test set. We take the test set and put it\n",
      "off to the side, pretending that we have never seen it before. Next we train our model\n",
      "using our training set, using the features and target vector to teach the model how to\n",
      "\n",
      "180 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "make the best prediction. Finally, we simulate having never before seen external data\n",
      "by  evaluating  how  our  model  trained  on  our  training  set  performs  on  our  test  set.\n",
      "However, the validation approach has two major weaknesses. First, the performance\n",
      "of  the  model  can  be  highly  dependent  on  which  few  observations  were  selected  for\n",
      "the  test  set.  Second,  the  model  is  not  being  trained  using  all  the  available  data,  and\n",
      "not being evaluated on all the available data.\n",
      "\n",
      "A better strategy, which overcomes these weaknesses, is called k-fold cross-validation\n",
      "(KFCV).  In  KFCV,  we  split  the  data  into  k  parts  called  “folds.”  The  model  is  then\n",
      "trained using k – 1 folds—combined into one training set—and then the last fold is\n",
      "used as a test set. We repeat this k times, each time using a different fold as the test\n",
      "set.  The  performance  on  the  model  for  each  of  the  k  iterations  is  then  averaged  to\n",
      "produce an overall measurement.\n",
      "\n",
      "In  our  solution,  we  conducted  k-fold  cross-validation  using  10  folds  and  outputted\n",
      "the evaluation scores to cv_results:\n",
      "\n",
      "# View score for all 10 folds\n",
      "cv_results\n",
      "\n",
      "array([ 0.97222222,  0.97777778,  0.95555556,  0.95      ,  0.95555556,\n",
      "        0.98333333,  0.97777778,  0.96648045,  0.96089385,  0.94972067])\n",
      "\n",
      "There are three important points to consider when we are using KFCV. First, KFCV\n",
      "assumes that each observation was created independent from the other (i.e., the data\n",
      "is  independent  identically  distributed  [IID]).  If  the  data  is  IID,  it  is  a  good  idea  to\n",
      "shuffle observations when assigning to folds. In scikit-learn we can set shuffle=True\n",
      "to perform shuffling.\n",
      "\n",
      "Second, when we are using KFCV to evaluate a classifier, it is often beneficial to have\n",
      "folds containing roughly the same percentage of observations from each of the differ‐\n",
      "ent target classes (called stratified k-fold). For example, if our target vector contained\n",
      "gender  and  80%  of  the  observations  were  male,  then  each  fold  would  contain  80%\n",
      "male  and  20%  female  observations.  In  scikit-learn,  we  can  conduct  stratified  k-fold\n",
      "cross-validation by replacing the KFold class with StratifiedKFold.\n",
      "\n",
      "Finally, when we are using validation sets or cross-validation, it is important to pre‐\n",
      "process data based on the training set and then apply those transformations to both\n",
      "the training and test set. For example, when we fit our standardization object, stand\n",
      "ardizer, we calculate the mean and variance of only the training set. Then we apply\n",
      "that transformation (using transform) to both the training and test sets:\n",
      "\n",
      "# Import library\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create training and test sets\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features, target, test_size=0.1, random_state=1)\n",
      "\n",
      "11.1 Cross-Validating Models \n",
      "\n",
      "| \n",
      "\n",
      "181\n",
      "\n",
      "\f",
      "# Fit standardizer to training set\n",
      "standardizer.fit(features_train)\n",
      "\n",
      "# Apply to both training and test sets\n",
      "features_train_std = standardizer.transform(features_train)\n",
      "features_test_std = standardizer.transform(features_test)\n",
      "\n",
      "The reason for this is because we are pretending that the test set is unknown data. If\n",
      "we  fit  both  our  preprocessors  using  observations  from  both  training  and  test  sets,\n",
      "some of the information from the test set leaks into our training set. This rule applies\n",
      "for any preprocessing step such as feature selection.\n",
      "\n",
      "scikit-learn’s pipeline package makes this easy to do while using cross-validation tech‐\n",
      "niques. We first create a pipeline that preprocesses the data (e.g., standardizer) and\n",
      "then trains a model (logistic regression, logit):\n",
      "\n",
      "# Create a pipeline\n",
      "pipeline = make_pipeline(standardizer, logit)\n",
      "\n",
      "Then we run KFCV using that pipeline and scikit does all the work for us:\n",
      "\n",
      "# Do k-fold cross-validation\n",
      "cv_results = cross_val_score(pipeline, # Pipeline\n",
      "                             features, # Feature matrix\n",
      "                             target, # Target vector\n",
      "                             cv=kf, # Cross-validation technique\n",
      "                             scoring=\"accuracy\", # Loss function\n",
      "                             n_jobs=-1) # Use all CPU scores\n",
      "\n",
      "cross_val_score  comes  with  three  parameters  that  we  have  not  discussed  that  are\n",
      "worth noting. cv determines our cross-validation technique. K-fold is the most com‐\n",
      "mon by far, but there are others, like leave-one-out-cross-validation where the num‐\n",
      "ber of folds k equals the number of observations. The scoring parameter defines our\n",
      "metric for success, a number of which are discussed in other recipes in this chapter.\n",
      "Finally, n_jobs=-1 tells scikit-learn to use every core available. For example, if your\n",
      "computer has four cores (a common number for laptops), then scikit-learn will use\n",
      "all four cores at once to speed up the operation.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Why every statistician should know about cross-validation\n",
      "\n",
      "• Cross-Validation Gone Wrong\n",
      "\n",
      "182 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "11.2 Creating a Baseline Regression Model\n",
      "\n",
      "Problem\n",
      "You want a simple baseline regression model to compare against your model.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s DummyRegressor to create a simple model to use as a baseline:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.dummy import DummyRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load data\n",
      "boston = load_boston()\n",
      "\n",
      "# Create features\n",
      "features, target = boston.data, boston.target\n",
      "\n",
      "# Make test and training split\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features, target, random_state=0)\n",
      "\n",
      "# Create a dummy regressor\n",
      "dummy = DummyRegressor(strategy='mean')\n",
      "\n",
      "# \"Train\" dummy regressor\n",
      "dummy.fit(features_train, target_train)\n",
      "\n",
      "# Get R-squared score\n",
      "dummy.score(features_test, target_test)\n",
      "\n",
      "-0.0011193592039553391\n",
      "\n",
      "To compare, we train our model and evaluate the performance score:\n",
      "\n",
      "# Load library\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Train simple linear regression model\n",
      "ols = LinearRegression()\n",
      "ols.fit(features_train, target_train)\n",
      "\n",
      "# Get R-squared score\n",
      "ols.score(features_test, target_test)\n",
      "\n",
      "0.63536207866746675\n",
      "\n",
      "11.2 Creating a Baseline Regression Model \n",
      "\n",
      "| \n",
      "\n",
      "183\n",
      "\n",
      "\f",
      "Discussion\n",
      "DummyRegressor allows us to create a very simple model that we can use as a baseline\n",
      "to compare against our actual model. This can often be useful to simulate a “naive”\n",
      "existing  prediction  process  in  a  product  or  system.  For  example,  a  product  might\n",
      "have been originally hardcoded to assume that all new users will spend $100 in the\n",
      "first month, regardless of their features. If we encode that assumption into a baseline\n",
      "model,  we  are  able  to  concretely  state  the  benefits  of  using  a  machine  learning\n",
      "approach.\n",
      "\n",
      "DummyRegressor  uses  the  strategy  parameter  to  set  the  method  of  making  predic‐\n",
      "tions, including the mean or median value in the training set. Furthermore, if we set\n",
      "strategy  to  constant  and  use  the  constant  parameter,  we  can  set  the  dummy\n",
      "regressor to predict some constant value for every observation:\n",
      "\n",
      "# Create dummy regressor that predicts 20's for everything\n",
      "clf = DummyRegressor(strategy='constant', constant=20)\n",
      "clf.fit(features_train, target_train)\n",
      "\n",
      "# Evaluate score\n",
      "clf.score(features_test, target_test)\n",
      "\n",
      "-0.065105020293257265\n",
      "\n",
      "One small note regarding score. By default, score returns the coefficient of determi‐\n",
      "nation (R-squared, R2) score:\n",
      "\n",
      "R2 = 1 −\n",
      "\n",
      "2\n",
      "\n",
      "∑i yi − yi\n",
      "∑i yi − y 2\n",
      "\n",
      "where yi is the true value of the target observation,  yi is the predicted value, and ȳ is\n",
      "the mean value for the target vector.\n",
      "\n",
      "The closer R2 is to 1, the more of the variance in the target vector that is explained by\n",
      "the features.\n",
      "\n",
      "11.3 Creating a Baseline Classification Model\n",
      "\n",
      "Problem\n",
      "You want a simple baseline classifier to compare against your model.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s DummyClassifier:\n",
      "\n",
      "184 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.dummy import DummyClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load data\n",
      "iris = load_iris()\n",
      "\n",
      "# Create target vector and feature matrix\n",
      "features, target = iris.data, iris.target\n",
      "\n",
      "# Split into training and test set\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "features, target, random_state=0)\n",
      "\n",
      "# Create dummy classifier\n",
      "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
      "\n",
      "# \"Train\" model\n",
      "dummy.fit(features_train, target_train)\n",
      "\n",
      "# Get accuracy score\n",
      "dummy.score(features_test, target_test)\n",
      "\n",
      "0.42105263157894735\n",
      "\n",
      "By comparing the baseline classifier to our trained classifier, we can see the improve‐\n",
      "ment:\n",
      "\n",
      "# Load library\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Create classifier\n",
      "classifier = RandomForestClassifier()\n",
      "\n",
      "# Train model\n",
      "classifier.fit(features_train, target_train)\n",
      "\n",
      "# Get accuracy score\n",
      "classifier.score(features_test, target_test)\n",
      "\n",
      "0.94736842105263153\n",
      "\n",
      "Discussion\n",
      "A common measure of a classifier’s performance is how much better it is than ran‐\n",
      "dom guessing. scikit-learn’s DummyClassifier makes this comparison easy. The strat\n",
      "egy parameter gives us a number of options for generating values. There are two par‐\n",
      "ticularly useful strategies. First, stratified makes predictions that are proportional\n",
      "to the training set’s target vector’s class proportions (i.e., if 20% of the observations in\n",
      "the training data are women, then DummyClassifier will predict women 20% of the\n",
      "\n",
      "11.3 Creating a Baseline Classification Model \n",
      "\n",
      "| \n",
      "\n",
      "185\n",
      "\n",
      "\f",
      "time).  Second,  uniform  will  generate  predictions  uniformly  at  random  between  the\n",
      "different classes. For example, if 20% of observations are women and 80% are men,\n",
      "uniform will produce predictions that are 50% women and 50% men.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: DummyClassifier\n",
      "\n",
      "11.4 Evaluating Binary Classifier Predictions\n",
      "\n",
      "Problem\n",
      "Given a trained classification model, you want to evaluate its quality.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s cross_val_score to conduct cross-validation while using the scor\n",
      "ing parameter to define one of a number of performance metrics, including accuracy,\n",
      "precision, recall, and F1.\n",
      "\n",
      "Accuracy  is  a  common  performance  metric.  It  is  simply  the  proportion  of  observa‐\n",
      "tions predicted correctly:\n",
      "\n",
      "Accuracy =\n",
      "\n",
      "TP + TN\n",
      "TP + TN + FP + FN\n",
      "\n",
      "where:\n",
      "\n",
      "• TP is the number of true positives. Observations that are part of the positive class\n",
      "(has the disease, purchased the product, etc.) and that we predicted correctly.\n",
      "\n",
      "• TN  is  the  number  of  true  negatives.  Observations  that  are  part  of  the  negative\n",
      "class (does not have the disease, did not purchase the product, etc.) and that we\n",
      "predicted correctly.\n",
      "\n",
      "• FP is the number of false positives. Also called a Type I error. Observations pre‐\n",
      "dicted to be part of the positive class that are actually part of the negative class.\n",
      "\n",
      "• FN is the number of false negatives. Also called a Type II error. Observations pre‐\n",
      "dicted to be part of the negative class that are actually part of the positive class.\n",
      "\n",
      "We can measure accuracy in three-fold (the default number of folds) cross-validation\n",
      "by setting scoring=\"accuracy\":\n",
      "\n",
      "186 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "X, y = make_classification(n_samples = 10000,\n",
      "                           n_features = 3,\n",
      "                           n_informative = 3,\n",
      "                           n_redundant = 0,\n",
      "                           n_classes = 2,\n",
      "                           random_state = 1)\n",
      "\n",
      "# Create logistic regression\n",
      "logit = LogisticRegression()\n",
      "\n",
      "# Cross-validate model using accuracy\n",
      "cross_val_score(logit, X, y, scoring=\"accuracy\")\n",
      "\n",
      "array([ 0.95170966,  0.9580084 ,  0.95558223])\n",
      "\n",
      "The appeal of accuracy is that it has an intuitive and plain English explanation: pro‐\n",
      "portion of observations predicted correctly. However, in the real world, often our data\n",
      "has imbalanced classes (e.g., the 99.9% of observations are of class 1 and only 0.1%\n",
      "are class 2). When in the presence of imbalanced classes, accuracy suffers from a par‐\n",
      "adox where a model is highly accurate but lacks predictive power. For example, imag‐\n",
      "ine we are trying to predict the presence of a very rare cancer that occurs in 0.1% of\n",
      "the  population.  After  training  our  model,  we  find  the  accuracy  is  at  95%.  However,\n",
      "99.9% of people do not have the cancer: if we simply created a model that “predicted”\n",
      "that nobody had that form of cancer, our naive model would be 4.9% more accurate,\n",
      "but clearly is not able to predict anything. For this reason, we are often motivated to\n",
      "use other metrics like precision, recall, and the F1 score.\n",
      "\n",
      "Precision  is  the  proportion  of  every  observation  predicted  to  be  positive  that  is\n",
      "actually positive. We can think about it as a measurement noise in our predictions—\n",
      "that is, when we predict something is positive, how likely we are to be right. Models\n",
      "with high precision are pessimistic in that they only predict an observation is of the\n",
      "positive class when they are very certain about it. Formally, precision is:\n",
      "\n",
      "Precision =\n",
      "\n",
      "TP\n",
      "TP + FP\n",
      "\n",
      "# Cross-validate model using precision\n",
      "cross_val_score(logit, X, y, scoring=\"precision\")\n",
      "\n",
      "array([ 0.95252404,  0.96583282,  0.95558223])\n",
      "\n",
      "Recall  is  the  proportion  of  every  positive  observation  that  is  truly  positive.  Recall\n",
      "measures  the  model’s  ability  to  identify  an  observation  of  the  positive  class.  Models\n",
      "\n",
      "11.4 Evaluating Binary Classifier Predictions \n",
      "\n",
      "| \n",
      "\n",
      "187\n",
      "\n",
      "\f",
      "with high recall are optimistic in that they have a low bar for predicting that an obser‐\n",
      "vation is in the positive class:\n",
      "\n",
      "Recall =\n",
      "\n",
      "TP\n",
      "TP + FN\n",
      "\n",
      "# Cross-validate model using recall\n",
      "cross_val_score(logit, X, y, scoring=\"recall\")\n",
      "\n",
      "array([ 0.95080984,  0.94961008,  0.95558223])\n",
      "\n",
      "If this is the first time you have encountered precision and recall, it is understandable\n",
      "if it takes you a little while to fully understand them. This is one of the downsides to\n",
      "accuracy; precision and recall are less intuitive. Almost always we want some kind of\n",
      "balance  between  precision  and  recall,  and  this  role  is  filled  by  the  F1  score.  The  F1\n",
      "score is the harmonic mean (a kind of average used for ratios):\n",
      "\n",
      "F1 = 2 ×\n",
      "\n",
      "Precision × Recall\n",
      "Precision + Recall\n",
      "\n",
      "It is a measure of correctness achieved in positive prediction—that is, of observations\n",
      "labeled as positive, how many are actually positive:\n",
      "\n",
      "# Cross-validate model using f1\n",
      "cross_val_score(logit, X, y, scoring=\"f1\")\n",
      "\n",
      "array([ 0.95166617,  0.95765275,  0.95558223])\n",
      "\n",
      "Discussion\n",
      "As an evaluation metric, accuracy has some valuable properties, especially its simple\n",
      "intuition. However, better metrics often involve using some balance of precision and\n",
      "recall—that is, a trade-off between the optimism and pessimism of our model. F1 rep‐\n",
      "resents a balance between the recall and precision, where the relative contributions of\n",
      "both are equal.\n",
      "\n",
      "Alternatively to using cross_val_score, if we already have the true y values and the\n",
      "predicted y values, we can calculate metrics like accuracy and recall directly:\n",
      "\n",
      "# Load library\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Create training and test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,\n",
      "                                                    y,\n",
      "                                                    test_size=0.1,\n",
      "                                                    random_state=1)\n",
      "\n",
      "188 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "# Predict values for training target vector\n",
      "y_hat = logit.fit(X_train, y_train).predict(X_test)\n",
      "\n",
      "# Calculate accuracy\n",
      "accuracy_score(y_test, y_hat)\n",
      "\n",
      "0.94699999999999995\n",
      "\n",
      "See Also\n",
      "\n",
      "• Accuracy paradox\n",
      "\n",
      "11.5 Evaluating Binary Classifier Thresholds\n",
      "\n",
      "Problem\n",
      "You want to evaluate a binary classifier and various probability thresholds.\n",
      "\n",
      "Solution\n",
      "The Receiving Operating Characteristic (ROC) curve is a common method for evalu‐\n",
      "ating the quality of a binary classifier. ROC compares the presence of true positives\n",
      "and  false  positives  at  every  probability  threshold  (i.e.,  the  probability  at  which  an\n",
      "observation is predicted to be a class). By plotting the ROC curve, we can see how the\n",
      "model performs. A classifier that predicts every observation correctly would look like\n",
      "the solid light gray line in the following chart, going straight up to the top immedi‐\n",
      "ately. A classifier that predicts at random will appear as the diagonal line. The better\n",
      "the model, the closer it is to the solid line. In scikit-learn, we can use roc_curve to\n",
      "calculate the true and false positives at each threshold, then plot them:\n",
      "\n",
      "# Load libraries\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Create feature matrix and target vector\n",
      "features, target = make_classification(n_samples=10000,\n",
      "                                       n_features=10,\n",
      "                                       n_classes=2,\n",
      "                                       n_informative=3,\n",
      "                                       random_state=3)\n",
      "\n",
      "# Split into training and test sets\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features, target, test_size=0.1, random_state=1)\n",
      "\n",
      "11.5 Evaluating Binary Classifier Thresholds \n",
      "\n",
      "| \n",
      "\n",
      "189\n",
      "\n",
      "\f",
      "# Create classifier\n",
      "logit = LogisticRegression()\n",
      "\n",
      "# Train model\n",
      "logit.fit(features_train, target_train)\n",
      "\n",
      "# Get predicted probabilities\n",
      "target_probabilities = logit.predict_proba(features_test)[:,1]\n",
      "\n",
      "# Create true and false positive rates\n",
      "false_positive_rate, true_positive_rate, threshold = roc_curve(target_test,\n",
      "                                                               target_probabilities)\n",
      "\n",
      "# Plot ROC curve\n",
      "plt.title(\"Receiver Operating Characteristic\")\n",
      "plt.plot(false_positive_rate, true_positive_rate)\n",
      "plt.plot([0, 1], ls=\"--\")\n",
      "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
      "plt.ylabel(\"True Positive Rate\")\n",
      "plt.xlabel(\"False Positive Rate\")\n",
      "plt.show()\n",
      "\n",
      "190 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "Discussion\n",
      "Up until now we have only examined models based on the values they predict. How‐\n",
      "ever, in many learning algorithms those predicted values are based off of probability\n",
      "estimates.  That  is,  each  observation  is  given  an  explicit  probability  of  belonging  in\n",
      "each class. In our solution, we can use predict_proba to see the predicted probabili‐\n",
      "ties for the first observation:\n",
      "\n",
      "# Get predicted probabilities\n",
      "logit.predict_proba(features_test)[0:1]\n",
      "\n",
      "array([[ 0.8688938,  0.1311062]])\n",
      "\n",
      "We can see the classes using classes_:\n",
      "\n",
      "logit.classes_\n",
      "\n",
      "array([0, 1])\n",
      "\n",
      "In  this  example,  the  first  observation  has  an  ~87%  chance  of  being  in  the  negative\n",
      "class (0) and a 13% chance of being in the positive class (1). By default, scikit-learn\n",
      "predicts an observation is part of the positive class if the probability is greater than\n",
      "0.5 (called the threshold). However, instead of a middle ground, we will often want to\n",
      "explicitly  bias  our  model  to  use  a  different  threshold  for  substantive  reasons.  For\n",
      "example,  if  a  false  positive  is  very  costly  to  our  company,  we  might  prefer  a  model\n",
      "that has a high probability threshold. We fail to predict some positives, but when an\n",
      "observation is predicted to be positive, we can be very confident that the prediction is\n",
      "correct. This trade-off is represented in the true positive rate (TPR) and the false pos‐\n",
      "itive rate (FPR). The true positive rate is the number of observations correctly predic‐\n",
      "ted true divided by all true positive observations:\n",
      "\n",
      "TPR =\n",
      "\n",
      "True Positives\n",
      "True Positives+False Negatives\n",
      "\n",
      "The false positive rate is the number of incorrectly predicted positives divided by all\n",
      "true negative observations:\n",
      "\n",
      "FPR =\n",
      "\n",
      "False Positives\n",
      "False Positives+True Negatives\n",
      "\n",
      "The ROC curve represents the respective TPR and FPR for every probability thres‐\n",
      "hold. For example, in our solution a threshold of roughly 0.50 has a TPR of  \\0.81 and an\n",
      "FPR of \\0.15:\n",
      "\n",
      "print(\"Threshold:\", threshold[116])\n",
      "print(\"True Positive Rate:\", true_positive_rate[116])\n",
      "print(\"False Positive Rate:\", false_positive_rate[116])\n",
      "\n",
      "11.5 Evaluating Binary Classifier Thresholds \n",
      "\n",
      "| \n",
      "\n",
      "191\n",
      "\n",
      "\f",
      "Threshold: 0.528224777887\n",
      "True Positive Rate: 0.810204081633\n",
      "False Positive Rate: 0.154901960784\n",
      "\n",
      "However, if we increase the threshold to ~80% (i.e., increase how certain the model\n",
      "has to be before it predicts an observation as positive) the TPR drops significantly but\n",
      "so does the FPR:\n",
      "\n",
      "print(\"Threshold:\", threshold[45])\n",
      "print(\"True Positive Rate:\", true_positive_rate[45])\n",
      "print(\"False Positive Rate:\", false_positive_rate[45])\n",
      "\n",
      "Threshold: 0.808019566563\n",
      "True Positive Rate: 0.563265306122\n",
      "False Positive Rate: 0.0470588235294\n",
      "\n",
      "This is because our higher requirement for being predicted to be in the positive class\n",
      "has made the model not identify a number of positive observations (the lower TPR),\n",
      "but also reduce the noise from negative observations being predicted as positive (the\n",
      "lower FPR).\n",
      "\n",
      "In addition to being able to visualize the trade-off between TPR and FPR, the ROC\n",
      "curve  can  also  be  used  as  a  general  metric  for  a  model.  The  better  a  model  is,  the\n",
      "higher the curve and thus the greater the area under the curve. For this reason, it is\n",
      "common to calculate the area under the ROC curve (AUCROC) to judge the overall\n",
      "equality of a model at all possible thresholds. The closer the AUCROC is to 1, the bet‐\n",
      "ter the model. In scikit-learn we can calculate the AUCROC using roc_auc_score:\n",
      "\n",
      "# Calculate area under curve\n",
      "roc_auc_score(target_test, target_probabilities)\n",
      "\n",
      "0.90733893557422962\n",
      "\n",
      "See Also\n",
      "\n",
      "• ROC Curves in Python and R\n",
      "\n",
      "• The Area Under an ROC Curve\n",
      "\n",
      "11.6 Evaluating Multiclass Classifier Predictions\n",
      "\n",
      "Problem\n",
      "You have a model that predicts three or more classes and want to evaluate its perfor‐\n",
      "mance.\n",
      "\n",
      "192 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "Solution\n",
      "Use  cross-validation  with  an  evaluation  metric  capable  of  handling  more  than  two\n",
      "classes:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_classification(n_samples = 10000,\n",
      "                           n_features = 3,\n",
      "                           n_informative = 3,\n",
      "                           n_redundant = 0,\n",
      "                           n_classes = 3,\n",
      "                           random_state = 1)\n",
      "\n",
      "# Create logistic regression\n",
      "logit = LogisticRegression()\n",
      "\n",
      "# Cross-validate model using accuracy\n",
      "cross_val_score(logit, features, target, scoring='accuracy')\n",
      "\n",
      "array([ 0.83653269,  0.8259826 ,  0.81308131])\n",
      "\n",
      "Discussion\n",
      "When we have balanced classes (e.g., a roughly equal number of observations in each\n",
      "class of the target vector), accuracy is—just like in the binary class setting—a simple\n",
      "and interpretable choice for an evaluation metric. Accuracy is the number of correct\n",
      "predictions divided by the number of observations and works just as well in the mul‐\n",
      "ticlass as binary setting. However, when we have imbalanced classes (a common sce‐\n",
      "nario), we should be inclined to use other evaluation metrics.\n",
      "\n",
      "Many of scikit-learn’s built-in metrics are for evaluating binary classifiers. However,\n",
      "many of these metrics can be extended for use when we have more than two classes.\n",
      "Precision,  recall,  and  F1  scores  are  useful  metrics  that  we  have  already  covered  in\n",
      "detail in previous recipes. While all of them were originally designed for binary clas‐\n",
      "sifiers, we can apply them to multiclass settings by treating our data as a set of binary\n",
      "classes. Doing so enables us to apply the metrics to each class as if it were the only\n",
      "class in the data, and then aggregate the evaluation scores for all the classes by averag‐\n",
      "ing them:\n",
      "\n",
      "# Cross-validate model using macro averaged F1 score\n",
      "cross_val_score(logit, features, target, scoring='f1_macro')\n",
      "\n",
      "array([ 0.83613125,  0.82562258,  0.81293539])\n",
      "\n",
      "11.6 Evaluating Multiclass Classifier Predictions \n",
      "\n",
      "| \n",
      "\n",
      "193\n",
      "\n",
      "\f",
      "In this code, _macro refers to the method used to average the evaluation scores from\n",
      "the classes:\n",
      "\n",
      "macro\n",
      "\n",
      "Calculate mean of metric scores for each class, weighting each class equally.\n",
      "\n",
      "weighted\n",
      "\n",
      "Calculate mean of metric scores for each class, weighting each class proportional\n",
      "to its size in the data.\n",
      "\n",
      "micro\n",
      "\n",
      "Calculate mean of metric scores for each observation-class combination.\n",
      "\n",
      "11.7 Visualizing a Classifier’s Performance\n",
      "\n",
      "Problem\n",
      "Given predicted classes and true classes of the test data, you want to visually compare\n",
      "the model’s quality.\n",
      "\n",
      "Solution\n",
      "Use a confusion matrix, which compares predicted classes and true classes:\n",
      "\n",
      "# Load libraries\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import pandas as pd\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "\n",
      "# Create feature matrix\n",
      "features = iris.data\n",
      "\n",
      "# Create target vector\n",
      "target = iris.target\n",
      "\n",
      "# Create list of target class names\n",
      "class_names = iris.target_names\n",
      "\n",
      "# Create training and test set\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features, target, random_state=1)\n",
      "\n",
      "194 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "# Create logistic regression\n",
      "classifier = LogisticRegression()\n",
      "\n",
      "# Train model and make predictions\n",
      "target_predicted = classifier.fit(features_train,\n",
      "    target_train).predict(features_test)\n",
      "\n",
      "# Create confusion matrix\n",
      "matrix = confusion_matrix(target_test, target_predicted)\n",
      "\n",
      "# Create pandas dataframe\n",
      "dataframe = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
      "\n",
      "# Create heatmap\n",
      "sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n",
      "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
      "plt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\n",
      "plt.show()\n",
      "\n",
      "Discussion\n",
      "Confusion  matrices  are  an  easy,  effective  visualization  of  a  classifier’s  performance.\n",
      "One of the major benefits of confusion matrices is their interpretability. Each column\n",
      "of the matrix (often visualized as a heatmap) represents predicted classes, while every\n",
      "row shows true classes. The end result is that every cell is one possible combination of\n",
      "predict  and  true  classes.  This  is  probably  best  explained  using  an  example.  In  the\n",
      "\n",
      "11.7 Visualizing a Classifier’s Performance \n",
      "\n",
      "| \n",
      "\n",
      "195\n",
      "\n",
      "\f",
      "solution,  the  top-left  cell  is  the  number  of  observations  predicted  to  be  Iris  setosa\n",
      "(indicated  by  the  column)  that  are  actually  Iris  setosa  (indicated  by  the  row).  This\n",
      "means  the  models  accurately  predicted  all  Iris  setosa  flowers.  However,  the  model\n",
      "does not do as well at predicting Iris virginica. The bottom-right cell indicates that the\n",
      "model  successfully  predicted  nine  observations  were  Iris  virginica,  but  (looking  one\n",
      "cell up) predicted six flowers to be viriginica that were actually Iris versicolor.\n",
      "\n",
      "There are three things worth noting about confusion matrices. First, a perfect model\n",
      "will have values along the diagonal and zeros everywhere else. A bad model will look\n",
      "like  the  observation  counts  will  be  spread  evenly  around  cells.  Second,  a  confusion\n",
      "matrix lets us see not only where the model was wrong, but also how it was wrong.\n",
      "That is, we can look at patterns of misclassification. For example, our model had an\n",
      "easy time differentiating Iris virginica and Iris setosa, but a much more difficult time\n",
      "classifying Iris virginica and Iris versicolor. Finally, confusion matrices work with any\n",
      "number  of  classes  (although  if  we  had  one  million  classes  in  our  target  vector,  the\n",
      "confusion matrix visualization might be difficult to read).\n",
      "\n",
      "See Also\n",
      "\n",
      "• Confusion Matrix\n",
      "\n",
      "• scikit-learn documentation: Confusion Matrix\n",
      "\n",
      "11.8 Evaluating Regression Models\n",
      "\n",
      "Problem\n",
      "You want to evaluate the performance of a regression model.\n",
      "\n",
      "Solution\n",
      "Use mean squared error (MSE):\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.datasets import make_regression\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Generate features matrix, target vector\n",
      "features, target = make_regression(n_samples = 100,\n",
      "                                   n_features = 3,\n",
      "                                   n_informative = 3,\n",
      "                                   n_targets = 1,\n",
      "                                   noise = 50,\n",
      "                                   coef = False,\n",
      "                                   random_state = 1)\n",
      "\n",
      "196 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "# Create a linear regression object\n",
      "ols = LinearRegression()\n",
      "\n",
      "# Cross-validate the linear regression using (negative) MSE\n",
      "cross_val_score(ols, features, target, scoring='neg_mean_squared_error')\n",
      "\n",
      "array([-1718.22817783, -3103.4124284 , -1377.17858823])\n",
      "\n",
      "Another common regression metric is the coefficient of determination, R2:\n",
      "\n",
      "# Cross-validate the linear regression using R-squared\n",
      "cross_val_score(ols, features, target, scoring='r2')\n",
      "\n",
      "array([ 0.87804558,  0.76395862,  0.89154377])\n",
      "\n",
      "Discussion\n",
      "MSE is one of the most common evaluation metrics for regression models. Formally,\n",
      "MSE is:\n",
      "\n",
      "MSE =\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "n ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "yi − yi\n",
      "\n",
      "where n is the number of observations, yi is the true value of the target we are trying\n",
      "to  predict  for  observation  i,  and  yi  is  the  model’s  predicted  value  for  yi.  MSE  is  a\n",
      "measurement of the squared sum of all distances between predicted and true values.\n",
      "The higher the value of MSE, the greater the total squared error and thus the worse\n",
      "the model. There are a number of mathematical benefits to squaring the error term,\n",
      "including that it forces all error values to be positive, but one often unrealized impli‐\n",
      "cation is that squaring penalizes a few large errors more than many small errors, even\n",
      "if the absolute value of the errors is the same. For example, imagine two models, A\n",
      "and B, each with two observations:\n",
      "\n",
      "• Model A has errors of 0 and 10 and thus its MSE is 02 + 102 = 100.\n",
      "• Model B has two errors of 5 each, and thus its MSE is 52 + 52 = 50.\n",
      "\n",
      "Both  models  have  the  same  total  error,  10;  however,  MSE  would  consider  Model  A\n",
      "(MSE = 100) worse than Model B (MSE = 50). In practice this implication is rarely an\n",
      "issue (and indeed can be theoretically beneficial) and MSE works perfectly fine as an\n",
      "evaluation metric.\n",
      "\n",
      "One  important  note:  by  default  in  scikit-learn  arguments  of  the  scoring  parameter\n",
      "assume that higher values are better than lower values. However, this is not the case\n",
      "for MSE, where higher values mean a worse model. For this reason, scikit-learn looks\n",
      "at the negative MSE using the neg_mean_squared_error argument.\n",
      "\n",
      "11.8 Evaluating Regression Models \n",
      "\n",
      "| \n",
      "\n",
      "197\n",
      "\n",
      "\f",
      "A common alternative regression evaluation metric is R2, which measures the amount\n",
      "of variance in the target vector that is explained by the model:\n",
      "\n",
      "R2 = 1 −\n",
      "\n",
      "n\n",
      "∑i = 1\n",
      "n\n",
      "∑i = 1\n",
      "\n",
      "2\n",
      "\n",
      "yi − yi\n",
      "yi − y 2\n",
      "\n",
      "where yi is the true target value of the ith observation, yi is the predicted value for the\n",
      "ith observation, and  y is the mean value of the target vector. The closer to 1.0, the \n",
      "better the model.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Mean squared error\n",
      "\n",
      "• Coefficient of determination\n",
      "\n",
      "11.9 Evaluating Clustering Models\n",
      "\n",
      "Problem\n",
      "You  have  used  an  unsupervised  learning  algorithm  to  cluster  your  data.  Now  you\n",
      "want to know how well it did.\n",
      "\n",
      "Solution\n",
      "The short answer is that you probably can’t, at least not in the way you want.\n",
      "\n",
      "That  said,  one  option  is  to  evaluate  clustering  using  silhouette  coefficients,  which\n",
      "measure the quality of the clusters:\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics import silhouette_score\n",
      "from sklearn import datasets\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Generate feature matrix\n",
      "features, _ = make_blobs(n_samples = 1000,\n",
      "                         n_features = 10,\n",
      "                         centers = 2,\n",
      "                         cluster_std = 0.5,\n",
      "                         shuffle = True,\n",
      "                         random_state = 1)\n",
      "\n",
      "# Cluster data using k-means to predict classes\n",
      "\n",
      "198 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "model = KMeans(n_clusters=2, random_state=1).fit(features)\n",
      "\n",
      "# Get predicted classes\n",
      "target_predicted = model.labels_\n",
      "\n",
      "# Evaluate model\n",
      "silhouette_score(features, target_predicted)\n",
      "\n",
      "0.89162655640721422\n",
      "\n",
      "Discussion\n",
      "Supervised  model  evaluation  compares  predictions  (e.g.,  classes  or  quantitative  val‐\n",
      "ues) with the corresponding true values in the target vector. However, the most com‐\n",
      "mon motivation for using clustering methods is that your data doesn’t have a target\n",
      "vector. There are a number of clustering evaluation metrics that require a target vec‐\n",
      "tor, but again, using unsupervised learning approaches like clustering when you have\n",
      "a target vector available to you is probably handicapping yourself unnecessarily.\n",
      "\n",
      "While we cannot evaluate predictions versus true values if we don’t have a target vec‐\n",
      "tor, we can evaluate the nature of the clusters themselves. Intuitively, we can imagine\n",
      "“good” clusters having very small distances between observations in the same cluster\n",
      "(i.e.,  dense  clusters)  and  large  distances  between  the  different  clusters  (i.e.,  well-\n",
      "separated  clusters).  Silhouette  coefficients  provide  a  single  value  measuring  both\n",
      "traits. Formally, the ith observation’s silhouette coefficient is:\n",
      "\n",
      "si =\n",
      "\n",
      "bi − ai\n",
      "max ai, bi\n",
      "\n",
      "where si is the silhouette coefficient for observation i, ai is the mean distance between\n",
      "i and all observations of the same class, and bi is the mean distance between i and all\n",
      "observations from the closest cluster of a different class. The value returned by sil\n",
      "houette_score is the mean silhouette coefficient for all observations. Silhouette coef‐\n",
      "ficients range between –1 and 1, with 1 indicating dense, well-separated clusters.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: silhouette_score\n",
      "\n",
      "11.10 Creating a Custom Evaluation Metric\n",
      "\n",
      "Problem\n",
      "You want to evaluate a model using a metric you created.\n",
      "\n",
      "11.10 Creating a Custom Evaluation Metric \n",
      "\n",
      "| \n",
      "\n",
      "199\n",
      "\n",
      "\f",
      "Solution\n",
      "Create  the  metric  as  a  function  and  convert  it  into  a  scorer  function  using  scikit-\n",
      "learn’s make_scorer:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.metrics import make_scorer, r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.datasets import make_regression\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_regression(n_samples = 100,\n",
      "                                   n_features = 3,\n",
      "                                   random_state = 1)\n",
      "\n",
      "# Create training set and test set\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "     features, target, test_size=0.10, random_state=1)\n",
      "\n",
      "# Create custom metric\n",
      "def custom_metric(target_test, target_predicted):\n",
      "    # Calculate r-squared score\n",
      "    r2 = r2_score(target_test, target_predicted)\n",
      "    # Return r-squared score\n",
      "    return r2\n",
      "\n",
      "# Make scorer and define that higher scores are better\n",
      "score = make_scorer(custom_metric, greater_is_better=True)\n",
      "\n",
      "# Create ridge regression object\n",
      "classifier = Ridge()\n",
      "\n",
      "# Train ridge regression model\n",
      "model = classifier.fit(features_train, target_train)\n",
      "\n",
      "# Apply custom scorer\n",
      "score(model, features_test, target_test)\n",
      "\n",
      "0.99979061028820582\n",
      "\n",
      "Discussion\n",
      "While  scikit-learn  has  a  number  of  built-in  metrics  for  evaluating  model  perfor‐\n",
      "mance, it is often useful to define our own metrics. scikit-learn makes this easy using\n",
      "make_scorer.  First,  we  define  a  function  that  takes  in  two  arguments—the  ground\n",
      "truth  target  vector  and  our  predicted  values—and  outputs  some  score.  Second,  we\n",
      "use make_scorer to create a scorer object, making sure to specify whether higher or\n",
      "lower scores are desirable (using the greater_is_better parameter).\n",
      "\n",
      "200 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "The custom metric in the solution (custom_metric) is a toy example since it simply\n",
      "wraps  a  built-in  metric  for  calculating  the  R2  score.  In  a  real-world  situation,  we\n",
      "would replace the custom_metric function with whatever custom metric we wanted.\n",
      "However, we can see that the custom metric that calculates R2 does work by compar‐\n",
      "ing the results to scikit-learn’s r2_score built-in method:\n",
      "\n",
      "# Predict values\n",
      "target_predicted = model.predict(features_test)\n",
      "\n",
      "# Calculate r-squared score\n",
      "r2_score(target_test, target_predicted)\n",
      "\n",
      "0.99979061028820582\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: make_scorer\n",
      "\n",
      "11.11 Visualizing the Effect of Training Set Size\n",
      "\n",
      "Problem\n",
      "You want to evaluate the effect of the number of observations in your training set on\n",
      "some metric (accuracy, F1, etc.).\n",
      "\n",
      "Solution\n",
      "Plot the learning curve:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.model_selection import learning_curve\n",
      "\n",
      "# Load data\n",
      "digits = load_digits()\n",
      "\n",
      "# Create feature matrix and target vector\n",
      "features, target = digits.data, digits.target\n",
      "\n",
      "# Create CV training and test scores for various training set sizes\n",
      "train_sizes, train_scores, test_scores = learning_curve(# Classifier\n",
      "                                                        RandomForestClassifier(),\n",
      "                                                        # Feature matrix\n",
      "                                                        features,\n",
      "                                                        # Target vector\n",
      "\n",
      "11.11 Visualizing the Effect of Training Set Size \n",
      "\n",
      "| \n",
      "\n",
      "201\n",
      "\n",
      "\f",
      "                                                        target,\n",
      "                                                        # Number of folds\n",
      "                                                        cv=10,\n",
      "                                                        # Performance metric\n",
      "                                                        scoring='accuracy',\n",
      "                                                        # Use all computer cores\n",
      "                                                        n_jobs=-1,\n",
      "                                                        # Sizes of 50\n",
      "                                                        # training set\n",
      "                                                       train_sizes=np.linspace(\n",
      "                                                       0.01,\n",
      "                                                       1.0,\n",
      "                                                       50))\n",
      "\n",
      "# Create means and standard deviations of training set scores\n",
      "train_mean = np.mean(train_scores, axis=1)\n",
      "train_std = np.std(train_scores, axis=1)\n",
      "\n",
      "# Create means and standard deviations of test set scores\n",
      "test_mean = np.mean(test_scores, axis=1)\n",
      "test_std = np.std(test_scores, axis=1)\n",
      "\n",
      "# Draw lines\n",
      "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
      "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
      "\n",
      "# Draw bands\n",
      "plt.fill_between(train_sizes, train_mean - train_std,\n",
      "                 train_mean + train_std, color=\"#DDDDDD\")\n",
      "plt.fill_between(train_sizes, test_mean - test_std,\n",
      "                 test_mean + test_std, color=\"#DDDDDD\")\n",
      "\n",
      "# Create plot\n",
      "plt.title(\"Learning Curve\")\n",
      "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"),\n",
      "plt.legend(loc=\"best\")\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "202 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "Discussion\n",
      "Learning  curves  visualize  the  performance  (e.g.,  accuracy,  recall)  of  a  model  on  the\n",
      "training set and during cross-validation as the number of observations in the training\n",
      "set increases. They are commonly used to determine if our learning algorithms would\n",
      "benefit from gathering additional training data.\n",
      "\n",
      "In  our  solution,  we  plot  the  accuracy  of  a  random  forest  classifier  at  50  different\n",
      "training set sizes ranging from 1% of observations to 100%. The increasing accuracy\n",
      "score  of  the  cross-validated  models  tell  us  that  we  would  likely  benefit  from  addi‐\n",
      "tional observations (although in practice this might not be feasible).\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: Learning Curve\n",
      "\n",
      "11.12 Creating a Text Report of Evaluation Metrics\n",
      "\n",
      "Problem\n",
      "You want a quick description of a classifier’s performance.\n",
      "\n",
      "11.12 Creating a Text Report of Evaluation Metrics \n",
      "\n",
      "| \n",
      "\n",
      "203\n",
      "\n",
      "\f",
      "Solution\n",
      "Use scikit-learn’s classification_report:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "\n",
      "# Create feature matrix\n",
      "features = iris.data\n",
      "\n",
      "# Create target vector\n",
      "target = iris.target\n",
      "\n",
      "# Create list of target class names\n",
      "class_names = iris.target_names\n",
      "\n",
      "# Create training and test set\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features, target, random_state=1)\n",
      "\n",
      "# Create logistic regression\n",
      "classifier = LogisticRegression()\n",
      "\n",
      "# Train model and make predictions\n",
      "model = classifier.fit(features_train, target_train)\n",
      "target_predicted = model.predict(features_test)\n",
      "\n",
      "# Create a classification report\n",
      "print(classification_report(target_test,\n",
      "                            target_predicted,\n",
      "                            target_names=class_names))\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        13\n",
      " versicolor       1.00      0.62      0.77        16\n",
      "  virginica       0.60      1.00      0.75         9\n",
      "\n",
      "avg / total       0.91      0.84      0.84        38\n",
      "\n",
      "Discussion\n",
      "classification_report provides a quick means for us to see some common evalua‐\n",
      "tion metrics, including precision, recall, and F1-score (described earlier in this chap‐\n",
      "ter). Support refers to the number of observations in each class.\n",
      "\n",
      "204 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Precision and recall\n",
      "\n",
      "11.13 Visualizing the Effect of Hyperparameter Values\n",
      "\n",
      "Problem\n",
      "You  want  to  understand  how  the  performance  of  a  model  changes  as  the  value  of\n",
      "some hyperparameter changes.\n",
      "\n",
      "Solution\n",
      "Plot the validation curve:\n",
      "\n",
      "# Load libraries\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import validation_curve\n",
      "\n",
      "# Load data\n",
      "digits = load_digits()\n",
      "\n",
      "# Create feature matrix and target vector\n",
      "features, target = digits.data, digits.target\n",
      "\n",
      "# Create range of values for parameter\n",
      "param_range = np.arange(1, 250, 2)\n",
      "\n",
      "# Calculate accuracy on training and test set using range of parameter values\n",
      "train_scores, test_scores = validation_curve(\n",
      "    # Classifier\n",
      "    RandomForestClassifier(),\n",
      "    # Feature matrix\n",
      "    features,\n",
      "    # Target vector\n",
      "    target,\n",
      "    # Hyperparameter to examine\n",
      "    param_name=\"n_estimators\",\n",
      "    # Range of hyperparameter's values\n",
      "    param_range=param_range,\n",
      "    # Number of folds\n",
      "    cv=3,\n",
      "    # Performance metric\n",
      "    scoring=\"accuracy\",\n",
      "    # Use all computer cores\n",
      "    n_jobs=-1)\n",
      "\n",
      "11.13 Visualizing the Effect of Hyperparameter Values \n",
      "\n",
      "| \n",
      "\n",
      "205\n",
      "\n",
      "\f",
      "# Calculate mean and standard deviation for training set scores\n",
      "train_mean = np.mean(train_scores, axis=1)\n",
      "train_std = np.std(train_scores, axis=1)\n",
      "\n",
      "# Calculate mean and standard deviation for test set scores\n",
      "test_mean = np.mean(test_scores, axis=1)\n",
      "test_std = np.std(test_scores, axis=1)\n",
      "\n",
      "# Plot mean accuracy scores for training and test sets\n",
      "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
      "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
      "\n",
      "# Plot accurancy bands for training and test sets\n",
      "plt.fill_between(param_range, train_mean - train_std,\n",
      "                 train_mean + train_std, color=\"gray\")\n",
      "plt.fill_between(param_range, test_mean - test_std,\n",
      "                 test_mean + test_std, color=\"gainsboro\")\n",
      "\n",
      "# Create plot\n",
      "plt.title(\"Validation Curve With Random Forest\")\n",
      "plt.xlabel(\"Number Of Trees\")\n",
      "plt.ylabel(\"Accuracy Score\")\n",
      "plt.tight_layout()\n",
      "plt.legend(loc=\"best\")\n",
      "plt.show()\n",
      "\n",
      "206 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Model Evaluation\n",
      "\n",
      "\f",
      "Discussion\n",
      "Most training algorithms (including many covered in this book) contain hyperpara‐\n",
      "meters  that  must  be  chosen  before  the  training  process  begins.  For  example,  a  ran‐\n",
      "dom  forest  classifier  creates  a  “forest”  of  decision  trees,  each  of  which  votes  on  the\n",
      "predicted class of an observation. One hyperparameter in random forest classifiers is\n",
      "the number of trees in the forest. Most often hyperparameter values are selected dur‐\n",
      "ing  model  selection  (see  Chapter  12).  However,  it  is  occasionally  useful  to  visualize\n",
      "how model performance changes as the hyperparameter value changes. In our solu‐\n",
      "tion, we plot the changes in accuracy for a random forest classifier for the training set\n",
      "and during cross-validation as the number of trees increases. When we have a small\n",
      "number of trees, both the training and cross-validation score are low, suggesting the\n",
      "model  is  underfitted.  As  the  number  of  trees  increases  to  250,  the  accuracy  of  both\n",
      "levels off, suggesting there is probably not much value in the computational cost of\n",
      "training a massive forest.\n",
      "\n",
      "In scikit-learn, we can calculate the validation curve using validation_curve, which\n",
      "contains three important parameters:\n",
      "\n",
      "• param_name is the name of the hyperparameter to vary.\n",
      "• param_range is the value of the hyperparameter to use.\n",
      "• scoring is the evaluation metric used to judge to model.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: Validation Curve\n",
      "\n",
      "11.13 Visualizing the Effect of Hyperparameter Values \n",
      "\n",
      "| \n",
      "\n",
      "207\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 12\n",
      "Model Selection\n",
      "\n",
      "12.0 Introduction\n",
      "In machine learning, we use training algorithms to learn the parameters of a model\n",
      "by  minimizing  some  loss  function.  However,  in  addition  many  learning  algorithms\n",
      "(e.g.,  support  vector  classifier  and  random  forests)  also  have  hyperparameters  that\n",
      "must be defined outside of the learning process. For example, random forests are col‐\n",
      "lections  of  decision  trees  (hence  the  word  forest);  however,  the  number  of  decision\n",
      "trees in the forest is not learned by the algorithm and must be set prior to fitting. This\n",
      "is often referred to as hyperparameter tuning, hyperparameter optimization, or model\n",
      "selection. Additionally, often we might want to try multiple learning algorithms (for\n",
      "example, trying both support vector classifier and random forests to see which learn‐\n",
      "ing method produces the best model).\n",
      "\n",
      "While  there  is  widespread  variation  in  the  use  of  terminology  in  this  area,  in  this\n",
      "book we refer to both selecting the best learning algorithm and its best hyperparame‐\n",
      "ters as model selection. The reason is straightforward: imagine we have data and want\n",
      "to  train  a  support  vector  classifier  with  10  candidate  hyperparameter  values  and  a\n",
      "random forest classifier with 10 candidate hyperparameter values. The result is that\n",
      "we are trying to select the best model from a set of 20 candidate models. In this chap‐\n",
      "ter, we will cover techniques to efficiently select the best model from the set of candi‐\n",
      "dates.\n",
      "\n",
      "Throughout  this  chapter  we  will  refer  to  specific  hyperparameters,  such  as  C  (the\n",
      "inverse of regularization strength). Do not worry if you don’t know what the hyper‐\n",
      "parameters are. We will cover them in later chapters. Instead, just treat hyperparame‐\n",
      "ters  like  the  settings  for  the  learning  algorithm  that  we  must  choose  before  starting\n",
      "training.\n",
      "\n",
      "209\n",
      "\n",
      "\f",
      "12.1 Selecting Best Models Using Exhaustive Search\n",
      "\n",
      "Problem\n",
      "You want to select the best model by searching over a range of hyperparameters.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s GridSearchCV:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create logistic regression\n",
      "logistic = linear_model.LogisticRegression()\n",
      "\n",
      "# Create range of candidate penalty hyperparameter values\n",
      "penalty = ['l1', 'l2']\n",
      "\n",
      "# Create range of candidate regularization hyperparameter values\n",
      "C = np.logspace(0, 4, 10)\n",
      "\n",
      "# Create dictionary hyperparameter candidates\n",
      "hyperparameters = dict(C=C, penalty=penalty)\n",
      "\n",
      "# Create grid search\n",
      "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
      "\n",
      "# Fit grid search\n",
      "best_model = gridsearch.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "GridSearchCV  is  a  brute-force  approach  to  model  selection  using  cross-validation.\n",
      "Specifically, a user defines sets of possible values for one or multiple hyperparameters,\n",
      "and then GridSearchCV trains a model using every value and/or combination of val‐\n",
      "ues.  The  model  with  the  best  performance  score  is  selected  as  the  best  model.  For\n",
      "example, in our solution we used logistic regression as our learning algorithm, con‐\n",
      "taining  two  hyperparameters:  C  and  the  regularization  penalty.  Don’t  worry  if  you\n",
      "don’t know what C and regularization mean; we cover them in the next few chapters.\n",
      "Just  realize  that  C  and  the  regularization  penalty  can  take  a  range  of  values,  which\n",
      "have to be specified prior to training. For C, we define 10 possible values:\n",
      "\n",
      "210 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "np.logspace(0, 4, 10)\n",
      "\n",
      "array([  1.00000000e+00,   2.78255940e+00,   7.74263683e+00,\n",
      "         2.15443469e+01,   5.99484250e+01,   1.66810054e+02,\n",
      "         4.64158883e+02,   1.29154967e+03,   3.59381366e+03,\n",
      "         1.00000000e+04])\n",
      "\n",
      "And  similarly  we  define  two  possible  values  for  the  regularization  penalty:  ['l1',\n",
      "'l2'].  For  each  combination  of  C  and  regularization  penalty  values,  we  train  the\n",
      "model and evaluate it using k-fold cross-validation. In our solution, we had 10 possi‐\n",
      "ble values of C, 2 possible values of regularization penalty, and 5 folds. They created\n",
      "10 × 2 × 5 = 100 candidate models from which the best was selected.\n",
      "\n",
      "Once GridSearchCV is complete, we can see the hyperparameters of the best model:\n",
      "\n",
      "# View best hyperparameters\n",
      "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
      "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
      "\n",
      "Best Penalty: l1\n",
      "Best C: 7.74263682681\n",
      "\n",
      "By  default,  after  identifying  the  best  hyperparameters,  GridSearchCV  will  retrain  a\n",
      "model using the best hyperparameters on the entire dataset (rather than leaving a fold\n",
      "out  for  cross-validation).  We  can  use  this  model  to  predict  values  like  any  other\n",
      "scikit-learn model:\n",
      "\n",
      "# Predict target vector\n",
      "best_model.predict(features)\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "One GridSearchCV parameter is worth noting: verbose. While mostly unnecessary, it\n",
      "can  be  reassuring  during  long  searching  processes  to  receive  an  indication  that  the\n",
      "search  is  progressing.  The  verbose  parameter  determines  the  amount  of  messages\n",
      "outputted during the search, with 0 showing no output, and 1 to 3 outputting mes‐\n",
      "sages with increasing detail.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: GridSearchCV\n",
      "\n",
      "12.1 Selecting Best Models Using Exhaustive Search \n",
      "\n",
      "| \n",
      "\n",
      "211\n",
      "\n",
      "\f",
      "12.2 Selecting Best Models Using Randomized Search\n",
      "\n",
      "Problem\n",
      "You want a computationally cheaper method than exhaustive search to select the best\n",
      "model.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s RandomizedSearchCV:\n",
      "\n",
      "# Load libraries\n",
      "from scipy.stats import uniform\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create logistic regression\n",
      "logistic = linear_model.LogisticRegression()\n",
      "\n",
      "# Create range of candidate regularization penalty hyperparameter values\n",
      "penalty = ['l1', 'l2']\n",
      "\n",
      "# Create distribution of candidate regularization hyperparameter values\n",
      "C = uniform(loc=0, scale=4)\n",
      "\n",
      "# Create hyperparameter options\n",
      "hyperparameters = dict(C=C, penalty=penalty)\n",
      "\n",
      "# Create randomized search\n",
      "randomizedsearch = RandomizedSearchCV(\n",
      "    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
      "    n_jobs=-1)\n",
      "\n",
      "# Fit randomized search\n",
      "best_model = randomizedsearch.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "In Recipe 12.1, we used GridSearchCV on a user-defined set of hyperparameter values\n",
      "to search for the best model according to a score function. A more efficient method\n",
      "than GridSearchCV’s brute-force search is to search over a specific number of random\n",
      "combinations  of  hyperparameter  values  from  user-supplied  distributions  (e.g.,  nor‐\n",
      "mal,  uniform).  scikit-learn  implements  this  randomized  search  technique  with  Ran\n",
      "domizedSearchCV.\n",
      "\n",
      "212 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "With  RandomizedSearchCV,  if  we  specify  a  distribution,  scikit-learn  will  randomly\n",
      "sample  without  replacement  hyperparameter  values  from  that  distribution.  As  an\n",
      "example of the general concept, here we randomly sample 10 values from a uniform\n",
      "distribution ranging from 0 to 4:\n",
      "\n",
      "# Define a uniform distribution between 0 and 4, sample 10 values\n",
      "uniform(loc=0, scale=4).rvs(10)\n",
      "\n",
      "array([ 1.97118721,  1.34786898,  3.79867719,  1.15999967,  3.33485021,\n",
      "        1.20394179,  0.88117832,  0.47855329,  0.48521699,  1.5479854 ])\n",
      "\n",
      "Alternatively, if we specify a list of values such as two regularization penalty hyper‐\n",
      "parameter  values,  ['l1',  'l2'],  RandomizedSearchCV  will  randomly  sample  with\n",
      "replacement from the list.\n",
      "\n",
      "Just like with GridSearchCV, we can see the hyperparameter values of the best model:\n",
      "\n",
      "# View best hyperparameters\n",
      "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
      "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
      "\n",
      "Best Penalty: l1\n",
      "Best C: 1.66808801881\n",
      "\n",
      "And  just  like  with  GridSearchCV,  after  the  search  is  complete  RandomizedSearchCV\n",
      "fits a new model using the best hyperparameters on the entire dataset. We can use this\n",
      "model like any other in scikit-learn; for example, to make predictions:\n",
      "\n",
      "# Predict target vector\n",
      "best_model.predict(features)\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "The number of sampled combinations of hyperparameters (i.e., the number of candi‐\n",
      "date models trained) is specified with the n_iter (number of iterations) setting.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: RandomizedSearchCV\n",
      "\n",
      "• Random Search for Hyper-Parameter Optimization\n",
      "\n",
      "12.2 Selecting Best Models Using Randomized Search \n",
      "\n",
      "| \n",
      "\n",
      "213\n",
      "\n",
      "\f",
      "12.3 Selecting Best Models from Multiple Learning\n",
      "Algorithms\n",
      "\n",
      "Problem\n",
      "You  want  to  select  the  best  model  by  searching  over  a  range  of  learning  algorithms\n",
      "and their respective hyperparameters.\n",
      "\n",
      "Solution\n",
      "Create a dictionary of candidate learning algorithms and their hyperparameters:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create a pipeline\n",
      "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
      "\n",
      "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
      "search_space = [{\"classifier\": [LogisticRegression()],\n",
      "                 \"classifier__penalty\": ['l1', 'l2'],\n",
      "                 \"classifier__C\": np.logspace(0, 4, 10)},\n",
      "                {\"classifier\": [RandomForestClassifier()],\n",
      "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
      "                 \"classifier__max_features\": [1, 2, 3]}]\n",
      "\n",
      "# Create grid search\n",
      "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
      "\n",
      "# Fit grid search\n",
      "best_model = gridsearch.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "In  the  previous  two  recipes  we  found  the  best  model  by  searching  over  possible\n",
      "hyperparameter values of a learning algorithm. However, what if we are not certain\n",
      "\n",
      "214 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "which learning algorithm to use? Recent versions of scikit-learn allow us to include\n",
      "learning  algorithms  as  part  of  the  search  space.  In  our  solution  we  define  a  search\n",
      "space  that  includes  two  learning  algorithms:  logistic  regression  and  random  forest\n",
      "classifier. Each learning algorithm has its own hyperparameters, and we define their\n",
      "candidate values using the format classifier__[hyperparameter name]. For exam‐\n",
      "ple, for our logistic regression, to define the set of possible values for regularization\n",
      "hyperparameter space, C, and potential types of regularization penalties, penalty, we\n",
      "create a dictionary:\n",
      "\n",
      "{'classifier': [LogisticRegression()],\n",
      " 'classifier__penalty': ['l1', 'l2'],\n",
      " 'classifier__C': np.logspace(0, 4, 10)}\n",
      "\n",
      "We can also create a similar dictionary for the random forest hyperparameters:\n",
      "\n",
      "{'classifier': [RandomForestClassifier()],\n",
      " 'classifier__n_estimators': [10, 100, 1000],\n",
      " 'classifier__max_features': [1, 2, 3]}\n",
      "\n",
      "After  the  search  is  complete,  we  can  use  best_estimator_  to  view  the  best  model’s\n",
      "learning algorithm and hyperparameters:\n",
      "\n",
      "# View best model\n",
      "best_model.best_estimator_.get_params()[\"classifier\"]\n",
      "\n",
      "LogisticRegression(C=7.7426368268112693, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "Just like with the last two recipes, once we have fit the model selection search, we can\n",
      "use this best model just like any other scikit-learn model:\n",
      "\n",
      "# Predict target vector\n",
      "best_model.predict(features)\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "12.4 Selecting Best Models When Preprocessing\n",
      "\n",
      "Problem\n",
      "You want to include a preprocessing step during model selection.\n",
      "\n",
      "12.4 Selecting Best Models When Preprocessing \n",
      "\n",
      "| \n",
      "\n",
      "215\n",
      "\n",
      "\f",
      "Solution\n",
      "Create a pipeline that includes the preprocessing step and any of its parameters:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create a preprocessing object that includes StandardScaler features and PCA\n",
      "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
      "\n",
      "# Create a pipeline\n",
      "pipe = Pipeline([(\"preprocess\", preprocess),\n",
      "                 (\"classifier\", LogisticRegression())])\n",
      "\n",
      "# Create space of candidate values\n",
      "search_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n",
      "                 \"classifier__penalty\": [\"l1\", \"l2\"],\n",
      "                 \"classifier__C\": np.logspace(0, 4, 10)}]\n",
      "\n",
      "# Create grid search\n",
      "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
      "\n",
      "# Fit grid search\n",
      "best_model = clf.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Very often we will need to preprocess our data before using it to train a model. We\n",
      "have  to  be  careful  to  properly  handle  preprocessing  when  conducting  model  selec‐\n",
      "tion.  First,  GridSearchCV  uses  cross-validation  to  determine  which  model  has  the\n",
      "highest  performance.  However,  in  cross-validation  we  are  in  effect  pretending  that\n",
      "the fold held out, as the test set is not seen, and thus not part of fitting any prepro‐\n",
      "cessing steps (e.g., scaling or standardization). For this reason, we cannot preprocess\n",
      "the data and then run GridSearchCV. Rather, the preprocessing steps must be a part\n",
      "of  the  set  of  actions  taken  by  GridSearchCV.  While  this  might  appear  complex,  the\n",
      "reality is that scikit-learn makes it simple. FeatureUnion allows us to combine multi‐\n",
      "\n",
      "216 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "ple preprocessing actions properly. In our solution we use FeatureUnion to combine\n",
      "two preprocessing steps: standardize the feature values (StandardScaler) and Princi‐\n",
      "pal Component Analysis (PCA). This object is called preprocess and contains both of\n",
      "our preprocessing steps. We then include preprocess into a pipeline with our learn‐\n",
      "ing algorithm. The end result is that this allows us to outsource the proper (and con‐\n",
      "fusing) handling of fitting, transforming, and training the models with combinations\n",
      "of hyperparameters to scikit-learn.\n",
      "\n",
      "Second, some preprocessing methods have their own parameters, which often have to\n",
      "be  supplied  by  the  user.  For  example,  dimensionality  reduction  using  PCA  requires\n",
      "the user to define the number of principal components to use to produce the trans‐\n",
      "formed  feature  set.  Ideally,  we  would  choose  the  number  of  components  that  pro‐\n",
      "duces a model with the greatest performance for some evaluation test metric. Luckily,\n",
      "scikit-learn  makes  this  easy.  When  we  include  candidate  component  values  in  the\n",
      "search space, they are treated like any other hyperparameter to be searched over. In\n",
      "our solution, we defined features__pca__n_components': [1, 2, 3] in the search\n",
      "space  to  indicate  that  we  wanted  to  discover  if  one,  two,  or  three  principal  compo‐\n",
      "nents produced the best model.\n",
      "\n",
      "After  model  selection  is  complete,  we  can  view  the  preprocessing  values  that  pro‐\n",
      "duced the best model. For example, we can see the best number of principal compo‐\n",
      "nents:\n",
      "\n",
      "# View best model\n",
      "best_model.best_estimator_.get_params()['preprocess__pca__n_components']\n",
      "\n",
      "1\n",
      "\n",
      "12.5 Speeding Up Model Selection with Parallelization\n",
      "\n",
      "Problem\n",
      "You need to speed up model selection.\n",
      "\n",
      "Solution\n",
      "Use all the cores in your machine by setting n_jobs=-1:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "12.5 Speeding Up Model Selection with Parallelization \n",
      "\n",
      "| \n",
      "\n",
      "217\n",
      "\n",
      "\f",
      "# Create logistic regression\n",
      "logistic = linear_model.LogisticRegression()\n",
      "\n",
      "# Create range of candidate regularization penalty hyperparameter values\n",
      "penalty = [\"l1\", \"l2\"]\n",
      "\n",
      "# Create range of candidate values for C\n",
      "C = np.logspace(0, 4, 1000)\n",
      "\n",
      "# Create hyperparameter options\n",
      "hyperparameters = dict(C=C, penalty=penalty)\n",
      "\n",
      "# Create grid search\n",
      "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
      "\n",
      "# Fit grid search\n",
      "best_model = gridsearch.fit(features, target)\n",
      "\n",
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Done 1352 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5120 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  1.1min finished\n",
      "\n",
      "Discussion\n",
      "In the recipes of this chapter, we have kept the number of candidate models small to\n",
      "make the code complete quickly. However, in the real world we will often have many\n",
      "thousands or tens of thousands of models to train. The end result is that it can take\n",
      "many hours to find the best model. To speed up the process, scikit-learn lets us train\n",
      "multiple models simultaneously. Without going into too much technical detail, scikit-\n",
      "learn  can  simultaneously  train  models  up  to  the  number  of  cores  on  the  machine.\n",
      "Most modern laptops have four cores, so (assuming you are currently on a laptop) we\n",
      "can potentially train four models at the same time. This will dramatically increase the\n",
      "speed of our model selection process. The parameter  n_jobs defines the number of\n",
      "models to train in parallel.\n",
      "\n",
      "In our solution, we set n_jobs to -1, which tells scikit-learn to use all cores. However,\n",
      "by default n_jobs is set to 1, meaning it only uses one core. To demonstrate this, if we\n",
      "run the same  GridSearch as in the solution, but with  n_jobs=1, we can see it takes\n",
      "significantly longer to find the best model (note that exact time will depend on your\n",
      "computer):\n",
      "\n",
      "# Create grid search using one core\n",
      "clf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n",
      "\n",
      "# Fit grid search\n",
      "best_model = clf.fit(features, target)\n",
      "\n",
      "218 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:  2.8min finished\n",
      "\n",
      "12.6 Speeding Up Model Selection Using Algorithm-\n",
      "Specific Methods\n",
      "\n",
      "Problem\n",
      "You need to speed up model selection.\n",
      "\n",
      "Solution\n",
      "If  you  are  using  a  select  number  of  learning  algorithms,  use  scikit-learn’s  model-\n",
      "specific  cross-validation  hyperparameter  tuning.  For  example,  LogisticRegres\n",
      "sionCV:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import linear_model, datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create cross-validated logistic regression\n",
      "logit = linear_model.LogisticRegressionCV(Cs=100)\n",
      "\n",
      "# Train model\n",
      "logit.fit(features, target)\n",
      "\n",
      "LogisticRegressionCV(Cs=100, class_weight=None, cv=None, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n",
      "\n",
      "Discussion\n",
      "Sometimes the characteristics of a learning algorithm allow us to search for the best\n",
      "hyperparameters  significantly  faster  than  either  brute  force  or  randomized  model\n",
      "search methods. In scikit-learn, many learning algorithms (e.g., ridge, lasso, and elas‐\n",
      "tic net regression) have an algorithm-specific cross-validation method to take advan‐\n",
      "tage of this. For example, LogisticRegression is used to conduct a standard logistic\n",
      "regression  classifier,  while  LogisticRegressionCV  implements  an  efficient  cross-\n",
      "validated  logistic  regression  classifier  that  has  the  ability  to  identify  the  optimum\n",
      "value of the hyperparameter C.\n",
      "\n",
      "12.6 Speeding Up Model Selection Using Algorithm-Specific Methods \n",
      "\n",
      "| \n",
      "\n",
      "219\n",
      "\n",
      "\f",
      "scikit-learn’s  LogisticRegressionCV  method  includes  a  parameter  Cs.  If  supplied  a\n",
      "list, Cs is the candidate hyperparameter values to select from. If supplied an integer,\n",
      "the parameter Cs generates a list of that many candidate values. The candidate values\n",
      "are drawn logarithmically from a range between 0.0001 and 1,0000 (a range of rea‐\n",
      "sonable values for C).\n",
      "\n",
      "However,  a  major  downside  to  LogisticRegressionCV  is  that  it  can  only  search  a\n",
      "range  of  values  for  C.  In  Recipe  12.1  our  possible  hyperparameter  space  included\n",
      "both C and another hyperparameter (the regularization penalty norm). This limita‐\n",
      "tion is common to many of scikit-learn’s model-specific cross-validated approaches.\n",
      "\n",
      "See Also\n",
      "\n",
      "• scikit-learn documentation: LogisticRegressionCV\n",
      "\n",
      "• scikit-learn documentation: Model specific cross-validation\n",
      "\n",
      "12.7 Evaluating Performance After Model Selection\n",
      "\n",
      "Problem\n",
      "You want to evaluate the performance of a model found through model selection.\n",
      "\n",
      "Solution\n",
      "Use nested cross-validation to avoid biased evaluation:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create logistic regression\n",
      "logistic = linear_model.LogisticRegression()\n",
      "\n",
      "# Create range of 20 candidate values for C\n",
      "C = np.logspace(0, 4, 20)\n",
      "\n",
      "# Create hyperparameter options\n",
      "hyperparameters = dict(C=C)\n",
      "\n",
      "# Create grid search\n",
      "\n",
      "220 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
      "\n",
      "# Conduct nested cross-validation and outut the average score\n",
      "cross_val_score(gridsearch, features, target).mean()\n",
      "\n",
      "0.95343137254901966\n",
      "\n",
      "Discussion\n",
      "Nested cross-validation during model selection is a difficult concept for many people\n",
      "to grasp the first time. Remember that in k-fold cross-validation, we train our model\n",
      "on k–1 folds of the data, use this model to make predictions on the remaining fold,\n",
      "and then evaluate our model best on how well our model’s predictions compare to the\n",
      "true values. We then repeat this process k times.\n",
      "\n",
      "In the model selection searches described in this chapter (i.e., GridSearchCV and Ran\n",
      "domizedSearchCV), we used cross-validation to evaluate which hyperparameter values\n",
      "produced the best models. However, a nuanced and generally underappreciated prob‐\n",
      "lem arises: since we used the data to select the best hyperparameter values, we cannot\n",
      "use  that  same  data  to  evaluate  the  model’s  performance.  The  solution?  Wrap  the\n",
      "cross-validation  used  for  model  search  in  another  cross-validation!  In  nested  cross-\n",
      "validation, the “inner” cross-validation selects the best model, while the “outer” cross-\n",
      "validation  provides  us  with  an  unbiased  evaluation  of  the  model’s  performance.  In\n",
      "our  solution,  the  inner  cross-validation  is  our  GridSearchCV  object,  which  we  then\n",
      "wrap in an outer cross-validation using cross_val_score.\n",
      "\n",
      "If you are confused, try a simple experiment. First, set verbose=1 so we can see what\n",
      "is happening:\n",
      "\n",
      "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n",
      "\n",
      "Next, run gridsearch.fit(features, target), which is our inner cross-validation\n",
      "used to find the best model:\n",
      "\n",
      "best_model = gridsearch.fit(features, target)\n",
      "\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "\n",
      "From the output you can see the inner cross-validation trained 20 candidate models\n",
      "five  times,  totaling  100  models.  Next,  nest  clf  inside  a  new  cross-validation,  which\n",
      "defaults to three folds:\n",
      "\n",
      "scores = cross_val_score(gridsearch, features, target)\n",
      "\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "\n",
      "12.7 Evaluating Performance After Model Selection \n",
      "\n",
      "| \n",
      "\n",
      "221\n",
      "\n",
      "\f",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "\n",
      "The output shows that the inner cross-validation trained 20 models five times to find\n",
      "the  best  model,  and  this  model  was  evaluated  using  an  outer  three-fold  cross-\n",
      "validation, creating a total of 300 models trained.\n",
      "\n",
      "222 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Model Selection\n",
      "\n",
      "\f",
      "CHAPTER 13\n",
      "Linear Regression\n",
      "\n",
      "13.0 Introduction\n",
      "Linear regression is one of the simplest supervised learning algorithms in our toolkit.\n",
      "If you have ever taken an introductory statistics course in college, likely the final topic\n",
      "you covered was linear regression. In fact, it is so simple that it is sometimes not con‐\n",
      "sidered machine learning at all! Whatever you believe, the fact is that linear regres‐\n",
      "sion—and its extensions—continues to be a common and useful method of making\n",
      "predictions  when  the  target  vector  is  a  quantitative  value  (e.g.,  home  price,  age).  In\n",
      "this  chapter  we  will  cover  a  variety  of  linear  regression  methods  (and  some  exten‐\n",
      "sions) for creating well-performing prediction models.\n",
      "\n",
      "13.1 Fitting a Line\n",
      "\n",
      "Problem\n",
      "You  want  to  train  a  model  that  represents  a  linear  relationship  between  the  feature\n",
      "and target vector.\n",
      "\n",
      "Solution\n",
      "Use a linear regression (in scikit-learn, LinearRegression):\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "# Load data with only two features\n",
      "boston = load_boston()\n",
      "features = boston.data[:,0:2]\n",
      "target = boston.target\n",
      "\n",
      "223\n",
      "\n",
      "\f",
      "# Create linear regression\n",
      "regression = LinearRegression()\n",
      "\n",
      "# Fit the linear regression\n",
      "model = regression.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Linear  regression  assumes  that  the  relationship  between  the  features  and  the  target\n",
      "vector  is  approximately  linear.  That  is,  the  effect  (also  called  coefficient,  weight,  or\n",
      "parameter)  of  the  features  on  the  target  vector  is  constant.  In  our  solution,  for  the\n",
      "sake of explanation we have trained our model using only two features. This means\n",
      "our linear model will be:\n",
      "\n",
      "y = β0 + β1x1 + β2x2 + \n",
      "\n",
      "where ŷ is our target, xi is the data for a single feature, β1 and β2 are the coefficients\n",
      "identified by fitting the model, and ϵ is the error. After we have fit our model, we can\n",
      "view the value of each parameter. For example, β0, also called the bias or intercept, can\n",
      "be viewed using intercept_:\n",
      "\n",
      "# View the intercept\n",
      "model.intercept_\n",
      "\n",
      "22.46681692105723\n",
      "\n",
      "And β1 and β2 are shown using coef_:\n",
      "# View the feature coefficients\n",
      "model.coef_\n",
      "\n",
      "array([-0.34977589,  0.11642402])\n",
      "\n",
      "In our dataset, the target value is the median value of a Boston home (in the 1970s) in\n",
      "thousands of dollars. Therefore the price of the first home in the dataset is:\n",
      "\n",
      "# First value in the target vector multiplied by 1000\n",
      "target[0]*1000\n",
      "\n",
      "24000.0\n",
      "\n",
      "Using the predict method, we can predict a value for that house:\n",
      "\n",
      "# Predict the target value of the first observation, multiplied by 1000\n",
      "model.predict(features)[0]*1000\n",
      "\n",
      "24560.23872370844\n",
      "\n",
      "Not bad! Our model was only off by $560.24!\n",
      "\n",
      "224 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Linear Regression\n",
      "\n",
      "\f",
      "The major advantage of linear regression is its interpretability, in large part because\n",
      "the coefficients of the model are the effect of a one-unit change on the target vector.\n",
      "For  example,  the  first  feature  in  our  solution  is  the  number  of  crimes  per  resident.\n",
      "Our  model’s  coefficient  of  this  feature  was  ~–0.35,  meaning  that  if  we  multiply  this\n",
      "coefficient by 1,000 (since the target vector is the house price in thousands of dollars),\n",
      "we have the change in house price for each additional one crime per capita:\n",
      "\n",
      "# First coefficient multiplied by 1000\n",
      "model.coef_[0]*1000\n",
      "\n",
      "-349.77588707748947\n",
      "\n",
      "This  says  that  every  single  crime  per  capita  will  decrease  the  price  of  the  house  by \n",
      "approximately $350!\n",
      "\n",
      "13.2 Handling Interactive Effects\n",
      "\n",
      "Problem\n",
      "You have a feature whose effect on the target variable depends on another feature.\n",
      "\n",
      "Solution\n",
      "Create an interaction term to capture that dependence using scikit-learn’s Polynomial\n",
      "Features:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "# Load data with only two features\n",
      "boston = load_boston()\n",
      "features = boston.data[:,0:2]\n",
      "target = boston.target\n",
      "\n",
      "# Create interaction term\n",
      "interaction = PolynomialFeatures(\n",
      "    degree=3, include_bias=False, interaction_only=True)\n",
      "features_interaction = interaction.fit_transform(features)\n",
      "\n",
      "# Create linear regression\n",
      "regression = LinearRegression()\n",
      "\n",
      "# Fit the linear regression\n",
      "model = regression.fit(features_interaction, target)\n",
      "\n",
      "13.2 Handling Interactive Effects \n",
      "\n",
      "| \n",
      "\n",
      "225\n",
      "\n",
      "\f",
      "Discussion\n",
      "Sometimes  a  feature’s  effect  on  our  target  variable  is  at  least  partially  dependent  on\n",
      "another feature. For example, imagine a simple coffee-based example where we have\n",
      "two binary features—the presence of sugar (sugar) and whether or not we have stir‐\n",
      "red (stirred)—and we want to predict if the coffee tastes sweet. Just putting sugar in\n",
      "the coffee (sugar=1, stirred=0) won’t make the coffee taste sweet (all the sugar is at\n",
      "the bottom!) and just stirring the coffee without adding sugar (sugar=0, stirred=1)\n",
      "won’t make it sweet either. Instead it is the interaction of putting sugar in the coffee\n",
      "and stirring the coffee (sugar=1, stirred=1) that will make a coffee taste sweet. The\n",
      "effects of sugar and stir on sweetness are dependent on each other. In this case we\n",
      "say there is an interaction effect between the features sugar and stirred.\n",
      "\n",
      "We  can  account  for  interaction  effects  by  including  a  new  feature  comprising  the\n",
      "product of corresponding values from the interacting features:\n",
      "\n",
      "y = β0 + β1x1 + β2x2 + β3x1x2 + \n",
      "\n",
      "where x1 and x2 are the values of the sugar and stirred, respectively, and x1x2 repre‐\n",
      "sents the interaction between the two.\n",
      "\n",
      "In  our  solution,  we  used  a  dataset  containing  only  two  features.  Here  is  the  first\n",
      "observation’s values for each of those features:\n",
      "\n",
      "# View the feature values for first observation\n",
      "features[0]\n",
      "\n",
      "array([  6.32000000e-03,   1.80000000e+01])\n",
      "\n",
      "To create an interaction term, we simply multiply those two values together for every\n",
      "observation:\n",
      "\n",
      "# Import library\n",
      "import numpy as np\n",
      "\n",
      "# For each observation, multiply the values of the first and second feature\n",
      "interaction_term = np.multiply(features[:, 0], features[:, 1])\n",
      "\n",
      "We can then view the interaction term for the first observation:\n",
      "\n",
      "# View interaction term for first observation\n",
      "interaction_term[0]\n",
      "\n",
      "0.11376\n",
      "\n",
      "However, while often we will have a substantive reason for believing there is an inter‐\n",
      "action between two features, sometimes we will not. In those cases it can be useful to\n",
      "use  scikit-learn’s  PolynomialFeatures  to  create  interaction  terms  for  all  combina‐\n",
      "\n",
      "226 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Linear Regression\n",
      "\n",
      "\f",
      "tions of features. We can then use model selection strategies to identify the combina‐\n",
      "tion of features and interaction terms that produce the best model.\n",
      "\n",
      "To  create  interaction  terms  using  PolynomialFeatures,  there  are  three  important\n",
      "parameters we must set. Most important,  interaction_only=True tells  Polynomial\n",
      "Features  to  only  return  interaction  terms  (and  not  polynomial  features,  which  we\n",
      "will discuss in Recipe 13.3). By default, PolynomialFeatures will add a feature con‐\n",
      "taining ones called a bias. We can prevent that with include_bias=False. Finally, the\n",
      "degree parameter determines the maximum number of features to create interaction\n",
      "terms from (in case we wanted to create an interaction term that is the combination\n",
      "of three features). We can see the output of PolynomialFeatures from our solution\n",
      "by checking to see if the first observation’s feature values and interaction term value\n",
      "match our manually calculated version:\n",
      "\n",
      "# View the values of the first observation\n",
      "features_interaction[0]\n",
      "\n",
      "array([  6.32000000e-03,   1.80000000e+01,   1.13760000e-01])\n",
      "\n",
      "13.3 Fitting a Nonlinear Relationship\n",
      "\n",
      "Problem\n",
      "You want to model a nonlinear relationship.\n",
      "\n",
      "Solution\n",
      "Create  a  polynomial  regression  by  including  polynomial  features  in  a  linear  regres‐\n",
      "sion model:\n",
      "\n",
      "# Load library\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "# Load data with one feature\n",
      "boston = load_boston()\n",
      "features = boston.data[:,0:1]\n",
      "target = boston.target\n",
      "\n",
      "# Create polynomial features x^2 and x^3\n",
      "polynomial = PolynomialFeatures(degree=3, include_bias=False)\n",
      "features_polynomial = polynomial.fit_transform(features)\n",
      "\n",
      "# Create linear regression\n",
      "regression = LinearRegression()\n",
      "\n",
      "# Fit the linear regression\n",
      "model = regression.fit(features_polynomial, target)\n",
      "\n",
      "13.3 Fitting a Nonlinear Relationship \n",
      "\n",
      "| \n",
      "\n",
      "227\n",
      "\n",
      "\f",
      "Discussion\n",
      "So far we have only discussed modeling linear relationships. An example of a linear\n",
      "relationship would be the number of stories a building has and the building’s height.\n",
      "In linear regression, we assume the effect of number of stories and building height is\n",
      "approximately constant, meaning a 20-story building will be roughly twice as high as\n",
      "a 10-story building, which will be roughly twice as high as a 5-story building. Many\n",
      "relationships of interest, however, are not strictly linear.\n",
      "\n",
      "Often  we  want  to  model  a  non-linear  relationship—for  example,  the  relationship\n",
      "between  the  number  of  hours  a  student  studies  and  the  score  she  gets  on  the  test.\n",
      "Intuitively,  we  can  imagine  there  is  a  big  difference  in  test  scores  between  students\n",
      "who  study  for  one  hour  compared  to  students  who  did  not  study  at  all.  However,\n",
      "there is a much smaller difference in test scores between a student who studied for 99\n",
      "hours and a student who studied for 100 hours. The effect one hour of studying has\n",
      "on a student’s test score decreases as the number of hours increases.\n",
      "\n",
      "Polynomial regression is an extension of linear regression to allow us to model non‐\n",
      "linear relationships. To create a polynomial regression, convert the linear function we\n",
      "used in Recipe 13.1:\n",
      "\n",
      "y = β0 + β1x1 + \n",
      "\n",
      "into a polynomial function by adding polynomial features:\n",
      "\n",
      "y = β0 + β1x1 + β2x1\n",
      "\n",
      "2 + . . . + βdxi\n",
      "\n",
      "d + \n",
      "\n",
      "where d is the degree of the polynomial. How are we able to use a linear regression\n",
      "for a nonlinear function? The answer is that we do not change how the linear regres‐\n",
      "sion fits the model, but rather only add polynomial features. That is, the linear regres‐\n",
      "sion does not “know” that the x2 is a quadratic transformation of x. It just considers it\n",
      "one more variable.\n",
      "\n",
      "A more practical description might be in order. To model nonlinear relationships, we\n",
      "can create new features that raise an existing feature, x, up to some power: x2, x3, and\n",
      "so on. The more of these new features we add, the more flexible the “line” created by\n",
      "our model. To make this more explicit, imagine we want to create a polynomial to the\n",
      "third  degree.  For  the  sake  of  simplicity,  we  will  focus  on  only  one  observation  (the\n",
      "first observation in the dataset), x0:\n",
      "\n",
      "# View first observation\n",
      "features[0]\n",
      "\n",
      "array([ 0.00632])\n",
      "\n",
      "228 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Linear Regression\n",
      "\n",
      "\f",
      "To create a polynomial feature, we would raise the first observation’s value to the sec‐\n",
      "ond degree, x1\n",
      "\n",
      "2:\n",
      "\n",
      "# View first observation raised to the second power, x^2\n",
      "features[0]**2\n",
      "\n",
      "array([  3.99424000e-05])\n",
      "\n",
      "This would be our new feature. We would then also raise the first observation’s value\n",
      "to the third degree, x1\n",
      "\n",
      "3:\n",
      "\n",
      "# View first observation raised to the third power, x^3\n",
      "features[0]**3\n",
      "\n",
      "array([  2.52435968e-07])\n",
      "\n",
      "By including all three features (x, x2, and x3) in our feature matrix and then running a\n",
      "linear regression, we have conducted a polynomial regression:\n",
      "\n",
      "# View the first observation's values for x, x^2, and x^3\n",
      "features_polynomial[0]\n",
      "\n",
      "array([  6.32000000e-03,   3.99424000e-05,   2.52435968e-07])\n",
      "\n",
      "PolynomialFeatures  has  two  important  parameters.  First,  degree  determines  the\n",
      "maximum number of degrees for the polynomial features. For example, degree=3 will\n",
      "generate x2 and x3. Finally, by default PolynomialFeatures includes a feature contain‐\n",
      "ing only ones (called a bias). We can remove that by setting include_bias=False.\n",
      "\n",
      "13.4 Reducing Variance with Regularization\n",
      "\n",
      "Problem\n",
      "You want to reduce the variance of your linear regression model.\n",
      "\n",
      "Solution\n",
      "Use a learning algorithm that includes a shrinkage penalty (also called regularization)\n",
      "like ridge regression and lasso regression:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "boston = load_boston()\n",
      "features = boston.data\n",
      "target = boston.target\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "\n",
      "13.4 Reducing Variance with Regularization \n",
      "\n",
      "| \n",
      "\n",
      "229\n",
      "\n",
      "\f",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create ridge regression with an alpha value\n",
      "regression = Ridge(alpha=0.5)\n",
      "\n",
      "# Fit the linear regression\n",
      "model = regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "In standard linear regression the model trains to minimize the sum of squared error\n",
      "between  the  true  (yi)  and  prediction,  (yi)  target  values,  or  residual  sum  of  squares\n",
      "(RSS):\n",
      "\n",
      "n\n",
      "RSS = ∑\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "yi − yi\n",
      "\n",
      "Regularized regression learners are similar, except they attempt to minimize RSS and\n",
      "some  penalty  for  the  total  size  of  the  coefficient  values,  called  a  shrinkage  penalty\n",
      "because it attempts to “shrink” the model. There are two common types of regular‐\n",
      "ized learners for linear regression: ridge regression and the lasso. The only formal dif‐\n",
      "ference is the type of shrinkage penalty used. In ridge regression, the shrinkage pen‐\n",
      "alty is a tuning hyperparameter multiplied by the squared sum of all coefficients:\n",
      "\n",
      "p\n",
      "RSS+α ∑\n",
      "j = 1\n",
      "\n",
      "2\n",
      "β j\n",
      "\n",
      "where β j is the coefficient of the jth of p features and α is a hyperparameter (discussed\n",
      "next).  The  lasso  is  similar,  except  the  shrinkage  penalty  is  a  tuning  hyperparameter\n",
      "multiplied by the sum of the absolute value of all coefficients:\n",
      "\n",
      "1\n",
      "2n\n",
      "\n",
      "p\n",
      "RSS + α ∑\n",
      "j = 1\n",
      "\n",
      "β j\n",
      "\n",
      "where n is the number of observations. So which one should we use? As a very gen‐\n",
      "eral  rule  of  thumb,  ridge  regression  often  produces  slightly  better  predictions  than\n",
      "lasso, but lasso (for reasons we will discuss in Recipe 13.5) produces more interpreta‐\n",
      "ble models. If we want a balance between ridge and lasso’s penalty functions we can\n",
      "use  elastic  net,  which  is  simply  a  regression  model  with  both  penalties  included.\n",
      "Regardless of which one we use, both ridge and lasso regressions can penalize large or\n",
      "complex models by including coefficient values in the loss function we are trying to\n",
      "minimize.\n",
      "\n",
      "230 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Linear Regression\n",
      "\n",
      "\f",
      "The  hyperparameter,  α,  lets  us  control  how  much  we  penalize  the  coefficients,  with\n",
      "higher values of α creating simpler models. The ideal value of α should be tuned like\n",
      "any other hyperparameter. In scikit-learn, α is set using the alpha parameter.\n",
      "\n",
      "scikit-learn includes a RidgeCV method that allows us to select the ideal value for α:\n",
      "\n",
      "# Load library\n",
      "from sklearn.linear_model import RidgeCV\n",
      "\n",
      "# Create ridge regression with three alpha values\n",
      "regr_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
      "\n",
      "# Fit the linear regression\n",
      "model_cv = regr_cv.fit(features_standardized, target)\n",
      "\n",
      "# View coefficients\n",
      "model_cv.coef_\n",
      "\n",
      "array([-0.91215884,  1.0658758 ,  0.11942614,  0.68558782, -2.03231631,\n",
      "        2.67922108,  0.01477326, -3.0777265 ,  2.58814315, -2.00973173,\n",
      "       -2.05390717,  0.85614763, -3.73565106])\n",
      "\n",
      "We can then easily view the best model’s α value:\n",
      "\n",
      "# View alpha\n",
      "model_cv.alpha_\n",
      "\n",
      "1.0\n",
      "\n",
      "One  final  note:  because  in  linear  regression  the  value  of  the  coefficients  is  partially\n",
      "determined by the scale of the feature, and in regularized models all coefficients are\n",
      "summed together, we must make sure to standardize the feature prior to training.\n",
      "\n",
      "13.5 Reducing Features with Lasso Regression\n",
      "\n",
      "Problem\n",
      "You  want  to  simplify  your  linear  regression  model  by  reducing  the  number  of  fea‐\n",
      "tures.\n",
      "\n",
      "Solution\n",
      "Use a lasso regression:\n",
      "\n",
      "# Load library\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "boston = load_boston()\n",
      "\n",
      "13.5 Reducing Features with Lasso Regression \n",
      "\n",
      "| \n",
      "\n",
      "231\n",
      "\n",
      "\f",
      "features = boston.data\n",
      "target = boston.target\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create lasso regression with alpha value\n",
      "regression = Lasso(alpha=0.5)\n",
      "\n",
      "# Fit the linear regression\n",
      "model = regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "One  interesting  characteristic  of  lasso  regression’s  penalty  is  that  it  can  shrink  the\n",
      "coefficients  of  a  model  to  zero,  effectively  reducing  the  number  of  features  in  the\n",
      "model. For example, in our solution we set alpha to 0.5 and we can see that many of\n",
      "the  coefficients  are  0,  meaning  their  corresponding  features  are  not  used  in  the\n",
      "model:\n",
      "\n",
      "# View coefficients\n",
      "model.coef_\n",
      "\n",
      "array([-0.10697735,  0.        , -0.        ,  0.39739898, -0.        ,\n",
      "        2.97332316, -0.        , -0.16937793, -0.        , -0.        ,\n",
      "       -1.59957374,  0.54571511, -3.66888402])\n",
      "\n",
      "However,  if  we  increase  α  to  a  much  higher  value,  we  see  that  literally  none  of  the\n",
      "features are being used:\n",
      "\n",
      "# Create lasso regression with a high alpha\n",
      "regression_a10 = Lasso(alpha=10)\n",
      "model_a10 = regression_a10.fit(features_standardized, target)\n",
      "model_a10.coef_\n",
      "\n",
      "array([-0.,  0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0.])\n",
      "\n",
      "The practical benefit of this effect is that it means that we could include 100 features\n",
      "in our feature matrix and then, through adjusting lasso’s α hyperparameter, produce a\n",
      "model  that  uses  only  10  (for  instance)  of  the  most  important  features.  This  lets  us\n",
      "reduce  variance  while  improving  the  interpretability  of  our  model  (since  fewer  fea‐\n",
      "tures is easier to explain).\n",
      "\n",
      "232 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Linear Regression\n",
      "\n",
      "\f",
      "CHAPTER 14\n",
      "Trees and Forests\n",
      "\n",
      "14.0 Introduction\n",
      "Tree-based  learning  algorithms  are  a  broad  and  popular  family  of  related  non-\n",
      "parametric,  supervised  methods  for  both  classification  and  regression.  The  basis  of\n",
      "tree-based learners is the decision tree wherein a series of decision rules (e.g., “If their\n",
      "gender  is  male…”)  are  chained.  The  result  looks  vaguely  like  an  upside-down  tree,\n",
      "with  the  first  decision  rule  at  the  top  and  subsequent  decision  rules  spreading  out\n",
      "below. In a decision tree, every decision rule occurs at a decision node, with the rule\n",
      "creating branches leading to new nodes. A branch without a decision rule at the end\n",
      "is called a leaf.\n",
      "\n",
      "One  reason  for  the  popularity  of  tree-based  models  is  their  interpretability.  In  fact,\n",
      "decision trees can literally be drawn out in their complete form (see Recipe 14.3) to\n",
      "create a highly intuitive model. From this basic tree system comes a wide variety of\n",
      "extensions  from  random  forests  to  stacking.  In  this  chapter  we  will  cover  how  to\n",
      "train, handle, adjust, visualize, and evaluate a number of tree-based models.\n",
      "\n",
      "14.1 Training a Decision Tree Classifier\n",
      "\n",
      "Problem\n",
      "You need to train a classifier using a decision tree.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s DecisionTreeClassifier:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "233\n",
      "\n",
      "\f",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create decision tree classifier object\n",
      "decisiontree = DecisionTreeClassifier(random_state=0)\n",
      "\n",
      "# Train model\n",
      "model = decisiontree.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Decision  tree  learners  attempt  to  find  a  decision  rule  that  produces  the  greatest\n",
      "decrease in impurity at a node. While there are a number of measurements of impur‐\n",
      "ity, by default DecisionTreeClassifier uses Gini impurity:\n",
      "\n",
      "c\n",
      "G t = 1 − ∑\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "pi\n",
      "\n",
      "where G(t) is the Gini impurity at node t and pi is the proportion of observations of\n",
      "class c at node t. This process of finding the decision rules that create splits to increase\n",
      "impurity  is  repeated  recursively  until  all  leaf  nodes  are  pure  (i.e.,  contain  only  one\n",
      "class) or some arbitrary cut-off is reached.\n",
      "\n",
      "In scikit-learn, DecisionTreeClassifier operates like other learning methods; after\n",
      "the model is trained using fit we can use the model to predict the class of an obser‐\n",
      "vation:\n",
      "\n",
      "# Make new observation\n",
      "observation = [[ 5,  4,  3,  2]]\n",
      "\n",
      "# Predict observation's class\n",
      "model.predict(observation)\n",
      "\n",
      "array([1])\n",
      "\n",
      "We can also see the predicted class probabilities of the observation:\n",
      "\n",
      "# View predicted class probabilities for the three classes\n",
      "model.predict_proba(observation)\n",
      "\n",
      "array([[ 0.,  1.,  0.]])\n",
      "\n",
      "Finally, if we want to use a different impurity measurement we can use the criterion \n",
      "parameter:\n",
      "\n",
      "# Create decision tree classifier object using entropy\n",
      "decisiontree_entropy = DecisionTreeClassifier(\n",
      "\n",
      "234 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "    criterion='entropy', random_state=0)\n",
      "\n",
      "# Train model\n",
      "model_entropy = decisiontree_entropy.fit(features, target)\n",
      "\n",
      "See Also\n",
      "\n",
      "• Decision Tree Learning, Princeton\n",
      "\n",
      "14.2 Training a Decision Tree Regressor\n",
      "\n",
      "Problem\n",
      "You need to train a regression model using a decision tree.\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s DecisionTreeRegressor:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data with only two features\n",
      "boston = datasets.load_boston()\n",
      "features = boston.data[:,0:2]\n",
      "target = boston.target\n",
      "\n",
      "# Create decision tree classifier object\n",
      "decisiontree = DecisionTreeRegressor(random_state=0)\n",
      "\n",
      "# Train model\n",
      "model = decisiontree.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Decision  tree  regression  works  similarly  to  decision  tree  classification;  however,\n",
      "instead of reducing Gini impurity or entropy, potential splits are by default measured\n",
      "on how much they reduce mean squared error (MSE):\n",
      "\n",
      "MSE =\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "n ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "yi − yi\n",
      "\n",
      "where  yi  is  the  true  value  of  the  target  and  yi  is  the  predicted  value.  In  scikit-learn,\n",
      "decision  tree  regression  can  be  conducted  using  DecisionTreeRegressor.  Once  we\n",
      "\n",
      "14.2 Training a Decision Tree Regressor \n",
      "\n",
      "| \n",
      "\n",
      "235\n",
      "\n",
      "\f",
      "have trained a decision tree, we can use it to predict the target value for an observa‐\n",
      "tion:\n",
      "\n",
      "# Make new observation\n",
      "observation = [[0.02, 16]]\n",
      "\n",
      "# Predict observation's value\n",
      "model.predict(observation)\n",
      "\n",
      "array([ 33.])\n",
      "\n",
      "Just like with DecisionTreeClassifier we can use the criterion parameter to select\n",
      "the desired measurement of split quality. For example, we can construct a tree whose\n",
      "splits reduce mean absolute error (MAE):\n",
      "\n",
      "# Create decision tree classifier object using entropy\n",
      "decisiontree_mae = DecisionTreeRegressor(criterion=\"mae\", random_state=0)\n",
      "\n",
      "# Train model\n",
      "model_mae = decisiontree_mae.fit(features, target)\n",
      "\n",
      "See Also\n",
      "\n",
      "• Decision Tree Regression, scikit-learn\n",
      "\n",
      "14.3 Visualizing a Decision Tree Model\n",
      "\n",
      "Problem\n",
      "You need to visualize a model created by a decision tree learning algorithm.\n",
      "\n",
      "Solution\n",
      "Export the decision tree model into DOT format, then visualize:\n",
      "\n",
      "# Load libraries\n",
      "import pydotplus\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn import datasets\n",
      "from IPython.display import Image\n",
      "from sklearn import tree\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create decision tree classifier object\n",
      "decisiontree = DecisionTreeClassifier(random_state=0)\n",
      "\n",
      "236 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "# Train model\n",
      "model = decisiontree.fit(features, target)\n",
      "\n",
      "# Create DOT data\n",
      "dot_data = tree.export_graphviz(decisiontree,\n",
      "                                out_file=None,\n",
      "                                feature_names=iris.feature_names,\n",
      "                                class_names=iris.target_names)\n",
      "\n",
      "# Draw graph\n",
      "graph = pydotplus.graph_from_dot_data(dot_data)\n",
      "\n",
      "# Show graph\n",
      "Image(graph.create_png())\n",
      "\n",
      "Discussion\n",
      "One  of  the  advantages  of  decision  tree  classifiers  is  that  we  can  visualize  the  entire\n",
      "trained  model—making  decision  trees  one  of  the  most  interpretable  models  in\n",
      "machine learning. In our solution, we exported our trained model in DOT format (a\n",
      "graph description language) and then used that to draw the graph.\n",
      "\n",
      "14.3 Visualizing a Decision Tree Model \n",
      "\n",
      "| \n",
      "\n",
      "237\n",
      "\n",
      "\f",
      "If we look at the root node, we can see the decision rule is that if petal widths are less\n",
      "than or equal to 0.8, then go to the left branch; if not, go to the right branch. We can\n",
      "also see the Gini impurity index (0.667), the number of observations (150), the num‐\n",
      "ber of observations in each class ([50,50,50]), and the class the observations would\n",
      "be predicted to be if we stopped at that node (setosa). We can also see that at that\n",
      "node the learner found that a single decision rule (petal width (cm) <= 0.8) was\n",
      "able to perfectly identify all of the setosa class observations. Furthermore, with one\n",
      "more decision rule with the same feature (petal width (cm) <= 1.75) the decision\n",
      "tree is able to correctly classify 144 of 150 observations. This makes petal width a very\n",
      "important feature!\n",
      "\n",
      "If  we  want  to  use  the  decision  tree  in  other  applications  or  reports,  we  can  easily\n",
      "export the visualization into PDF or a PNG image:\n",
      "\n",
      "# Create PDF\n",
      "graph.write_pdf(\"iris.pdf\")\n",
      "\n",
      "True\n",
      "\n",
      "# Create PNG\n",
      "graph.write_png(\"iris.png\")\n",
      "\n",
      "True\n",
      "\n",
      "While this solution visualized a decision tree classifier, it can just as easily be used to\n",
      "visualize a decision tree regressor.\n",
      "\n",
      "Note: macOS users might have to install GraphViz’s executable to run the preceding\n",
      "code. This can be done using Homebrew: brew install graphviz. For Homebrew\n",
      "installation instructions, visit Homebrew’s website.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Homebrew\n",
      "\n",
      "14.4 Training a Random Forest Classifier\n",
      "\n",
      "Problem\n",
      "You want to train a classification model using a “forest” of randomized decision trees.\n",
      "\n",
      "Solution\n",
      "Train  a  random  forest  classification  model  using  scikit-learn’s  RandomForestClassi\n",
      "fier:\n",
      "\n",
      "238 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create random forest classifier object\n",
      "randomforest = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = randomforest.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "A common problem with decision trees is that they tend to fit the training data too\n",
      "closely (i.e., overfitting). This has motivated the widespread use of an ensemble learn‐\n",
      "ing method called random forest. In a random forest, many decision trees are trained,\n",
      "but  each  tree  only  receives  a  bootstrapped  sample  of  observations  (i.e.,  a  random\n",
      "sample of observations with replacement that matches the original number of obser‐\n",
      "vations) and each node only considers a subset of features when determining the best\n",
      "split. This forest of randomized decision trees (hence the name) votes to determine\n",
      "the predicted class.\n",
      "\n",
      "As we can see by comparing this solution to Recipe 14.1, scikit-learn’s RandomForest\n",
      "Classifier works similarly to DecisionTreeClassifier:\n",
      "\n",
      "# Make new observation\n",
      "observation = [[ 5,  4,  3,  2]]\n",
      "\n",
      "# Predict observation's class\n",
      "model.predict(observation)\n",
      "\n",
      "array([1])\n",
      "\n",
      "RandomForestClassifier  also  uses  many  of  the  same  parameters  as  DecisionTree\n",
      "Classifier. For example, we can change the measure of split quality used:\n",
      "\n",
      "# Create random forest classifier object using entropy\n",
      "randomforest_entropy = RandomForestClassifier(\n",
      "    criterion=\"entropy\", random_state=0)\n",
      "\n",
      "# Train model\n",
      "model_entropy = randomforest_entropy.fit(features, target)\n",
      "\n",
      "However, being a forest rather than an individual decision tree, RandomForestClassi\n",
      "fier has certain parameters that are either unique to random forests or particularly\n",
      "important.  First,  the  max_features  parameter  determines  the  maximum  number  of\n",
      "\n",
      "14.4 Training a Random Forest Classifier \n",
      "\n",
      "| \n",
      "\n",
      "239\n",
      "\n",
      "\f",
      "features  to  be  considered  at  each  node  and  takes  a  number  of  arguments  including\n",
      "integers (number of features), floats (percentage of features), and sqrt (square root of\n",
      "the number of features). By default, max_features is set to auto, which acts the same\n",
      "as  sqrt.  Second,  the  bootstrap  parameter  allows  us  to  set  whether  the  subset  of\n",
      "observations  considered  for  a  tree  is  created  using  sampling  with  replacement  (the\n",
      "default  setting)  or  without  replacement.  Third,  n_estimators  sets  the  number  of\n",
      "decision  trees  to  include  in  the  forest.  In  Recipe  10.4  we  treated  n_estimators  as  a\n",
      "hyperparameter  and  visualized  the  effect  of  increasing  the  number  of  trees  on  an\n",
      "evaluation metric. Finally, while not specific to random forest classifiers, because we\n",
      "are effectively training many decision tree models, it is often useful to use all available\n",
      "cores by setting n_jobs=-1.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Random Forests, Berkeley Statistics\n",
      "\n",
      "14.5 Training a Random Forest Regressor\n",
      "\n",
      "Problem\n",
      "You want to train a regression model using a “forest” of randomized decision trees.\n",
      "\n",
      "Solution\n",
      "Train a random forest regression model using scikit-learn’s RandomForestRegressor:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data with only two features\n",
      "boston = datasets.load_boston()\n",
      "features = boston.data[:,0:2]\n",
      "target = boston.target\n",
      "\n",
      "# Create random forest classifier object\n",
      "randomforest = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = randomforest.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Just like how we can make a forest of decision tree classifiers, we can make a forest of\n",
      "decision  tree  regressors  where  each  tree  uses  a  bootstrapped  subset  of  observations\n",
      "\n",
      "240 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "and at each node the decision rule considers only a subset of features. As with Random\n",
      "ForestClassifier we have certain important parameters:\n",
      "\n",
      "• max_features  sets  the  maximum  number  of  features  to  consider  at  each  node.\n",
      "\n",
      "Defaults to  p features, where p is the total number of features.\n",
      "\n",
      "• bootstrap sets whether or not to sample with replacement. Defaults to True.\n",
      "• n_estimators sets the number of decision trees to construct. Defaults to 10.\n",
      "\n",
      "See Also\n",
      "\n",
      "• RandomForestRegressor, scikit-learn\n",
      "\n",
      "14.6 Identifying Important Features in Random Forests\n",
      "\n",
      "Problem\n",
      "You need to know which features are most important in a random forest model.\n",
      "\n",
      "Solution\n",
      "Calculate and visualize the importance of each feature:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create random forest classifier object\n",
      "randomforest = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = randomforest.fit(features, target)\n",
      "\n",
      "# Calculate feature importances\n",
      "importances = model.feature_importances_\n",
      "\n",
      "# Sort feature importances in descending order\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "# Rearrange feature names so they match the sorted feature importances\n",
      "\n",
      "14.6 Identifying Important Features in Random Forests \n",
      "\n",
      "| \n",
      "\n",
      "241\n",
      "\n",
      "\f",
      "names = [iris.feature_names[i] for i in indices]\n",
      "\n",
      "# Create plot\n",
      "plt.figure()\n",
      "\n",
      "# Create plot title\n",
      "plt.title(\"Feature Importance\")\n",
      "\n",
      "# Add bars\n",
      "plt.bar(range(features.shape[1]), importances[indices])\n",
      "\n",
      "# Add feature names as x-axis labels\n",
      "plt.xticks(range(features.shape[1]), names, rotation=90)\n",
      "\n",
      "# Show plot\n",
      "plt.show()\n",
      "\n",
      "242 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "Discussion\n",
      "One of the major benefits of decision trees is interpretability. Specifically, we can visu‐\n",
      "alize  the  entire  model  (see  Recipe  13.3).  However,  a  random  forest  model  is  com‐\n",
      "prised of tens, hundreds, even thousands of decision trees. This makes a simple, intu‐\n",
      "itive  visualization  of  a  random  forest  model  impractical.  That  said,  there  is  another\n",
      "option: we can compare (and visualize) the relative importance of each feature.\n",
      "\n",
      "In  Recipe  13.3,  we  visualized  a  decision  tree  classifier  model  and  saw  that  decision\n",
      "rules  based  only  on  petal  width  were  able  to  classify  many  observations  correctly.\n",
      "Intuitively, we can say that this means that petal width is an important feature in our\n",
      "classifier.  More  formally,  features  with  splits  that  have  the  greater  mean  decrease  in\n",
      "impurity (e.g., Gini impurity or entropy in classifiers and variance in regressors) are\n",
      "considered more important.\n",
      "\n",
      "However,  there  are  two  things  to  keep  in  mind  regarding  feature  importance.  First,\n",
      "scikit-learn  requires  that  we  break  up  nominal  categorical  features  into  multiple\n",
      "binary features. This has the effect of spreading the importance of that feature across\n",
      "all of the binary features and can often make each feature appear to be unimportant\n",
      "even when the original nominal categorical feature is highly important. Second, if two\n",
      "features are highly correlated, one feature will claim much of the importance, making\n",
      "the  other  feature  appear  to  be  far  less  important—which  has  implications  for  inter‐\n",
      "pretation if not considered.\n",
      "\n",
      "In  scikit-learn,  classification  and  regression  decision  trees  and  random  forests  can\n",
      "report  the  relative  importance  of  each  feature  using  the  feature_importances_\n",
      "method:\n",
      "\n",
      "# View feature importances\n",
      "model.feature_importances_\n",
      "\n",
      "array([ 0.11896532,  0.0231668 ,  0.36804744,  0.48982043])\n",
      "\n",
      "The higher the number, the more important the feature (all importance scores sum to\n",
      "1). By plotting these values we can add interpretability to our random forest models.\n",
      "\n",
      "14.7 Selecting Important Features in Random Forests\n",
      "\n",
      "Problem\n",
      "You need to conduct feature selection on a random forest.\n",
      "\n",
      "Solution\n",
      "Identify the importance features and retrain the model using only the most important\n",
      "features:\n",
      "\n",
      "14.7 Selecting Important Features in Random Forests \n",
      "\n",
      "| \n",
      "\n",
      "243\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create random forest classifier\n",
      "randomforest = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
      "\n",
      "# Create object that selects features with importance greater\n",
      "# than or equal to a threshold\n",
      "selector = SelectFromModel(randomforest, threshold=0.3)\n",
      "\n",
      "# Feature new feature matrix using selector\n",
      "features_important = selector.fit_transform(features, target)\n",
      "\n",
      "# Train random forest using most important featres\n",
      "model = randomforest.fit(features_important, target)\n",
      "\n",
      "Discussion\n",
      "There  are  situations  where  we  might  want  to  reduce  the  number  of  features  in  our\n",
      "model. For example, we might want to reduce the model’s variance or we might want\n",
      "to improve interpretability by including only the most important features.\n",
      "\n",
      "In  scikit-learn  we  can  use  a  simple  two-stage  workflow  to  create  a  model  with\n",
      "reduced features. First, we train a random forest model using all features. Then, we\n",
      "use this model to identify the most important features. Next, we create a new feature\n",
      "matrix that includes only these features. In our solution, we used the SelectFromMo\n",
      "del  method  to  create  a  feature  matrix  containing  only  features  with  an  importance\n",
      "greater than or equal to some threshold value. Finally, we created a new model using\n",
      "only those features.\n",
      "\n",
      "It must be noted that there are two caveats to this approach. First, nominal categorical\n",
      "features  that  have  been  one-hot  encoded  will  see  the  feature  importance  diluted\n",
      "across  the  binary  features.  Second,  the  feature  importance  of  highly  correlated  fea‐\n",
      "tures will be effectively assigned to one feature and not evenly distributed across both\n",
      "features.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Variable  selection  using  Random  Forests,  Robin  Genuer,  Jean-Michel  Poggi,\n",
      "\n",
      "Christine Tuleau-Malot\n",
      "\n",
      "244 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "14.8 Handling Imbalanced Classes\n",
      "\n",
      "Problem\n",
      "You have a target vector with highly imbalanced classes and want to train a random\n",
      "forest model.\n",
      "\n",
      "Solution\n",
      "Train a decision tree or random forest model with class_weight=\"balanced\":\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Make class highly imbalanced by removing first 40 observations\n",
      "features = features[40:,:]\n",
      "target = target[40:]\n",
      "\n",
      "# Create target vector indicating if class 0, otherwise 1\n",
      "target = np.where((target == 0), 0, 1)\n",
      "\n",
      "# Create random forest classifier object\n",
      "randomforest = RandomForestClassifier(\n",
      "    random_state=0, n_jobs=-1, class_weight=\"balanced\")\n",
      "\n",
      "# Train model\n",
      "model = randomforest.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Imbalanced classes are a common problem when we are doing machine learning in\n",
      "the real world. Left unaddressed, the presence of imbalanced classes can reduce the\n",
      "performance  of  our  model.  We  already  talked  about  a  few  strategies  for  handling\n",
      "imbalanced  classes  during  preprocessing  in  Recipe  17.5.  However,  many  learning\n",
      "algorithms in scikit-learn come with built-in methods for correcting for imbalanced\n",
      "classes. We can set RandomForestClassifier to correct for imbalanced classes using\n",
      "the class_weight parameter. If supplied with a dictionary in the form of class names\n",
      "and  respective  desired  weights  (e.g.,  {\"male\":  0.2,  \"female\":  0.8}),  RandomFor\n",
      "estClassifier  will  weight  the  classes  accordingly.  However,  often  a  more  useful\n",
      "argument is balanced, wherein classes are automatically weighted inversely propor‐\n",
      "tional to how frequently they appear in the data:\n",
      "\n",
      "14.8 Handling Imbalanced Classes \n",
      "\n",
      "| \n",
      "\n",
      "245\n",
      "\n",
      "\f",
      "w j =\n",
      "\n",
      "n\n",
      "kn j\n",
      "\n",
      "where wj is the weight to class j, n is the number of observations, nj is the number of\n",
      "observations in class j, and k is the total number of classes. For example, in our solu‐\n",
      "tion we have 2 classes (k), 110 observations (n), and 10 and 100 observations in each\n",
      "class, respectively (nj). If we weight the classes using class_weight=\"balanced\", then\n",
      "the smaller class is weighted more:\n",
      "\n",
      "# Calculate weight for small class\n",
      "110/(2*10)\n",
      "\n",
      "5.5\n",
      "\n",
      "while the larger class is weighted less:\n",
      "\n",
      "# Calculate weight for large class\n",
      "110/(2*100)\n",
      "\n",
      "0.55\n",
      "\n",
      "14.9 Controlling Tree Size\n",
      "\n",
      "Problem\n",
      "You want to manually determine the structure and size of a decision tree.\n",
      "\n",
      "Solution\n",
      "Use the tree structure parameters in scikit-learn tree-based learning algorithms:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create decision tree classifier object\n",
      "decisiontree = DecisionTreeClassifier(random_state=0,\n",
      "                                      max_depth=None,\n",
      "                                      min_samples_split=2,\n",
      "                                      min_samples_leaf=1,\n",
      "                                      min_weight_fraction_leaf=0,\n",
      "                                      max_leaf_nodes=None,\n",
      "                                      min_impurity_decrease=0)\n",
      "\n",
      "246 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "# Train model\n",
      "model = decisiontree.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "scikit-learn’s tree-based learning algorithms have a variety of techniques for control‐\n",
      "ling the size of the decision tree(s). These are accessed through parameters:\n",
      "\n",
      "max_depth\n",
      "\n",
      "Maximum depth of the tree. If None, the tree is grown until all leaves are pure. If\n",
      "an integer, the tree is effectively “pruned” to that depth.\n",
      "\n",
      "min_samples_split\n",
      "\n",
      "Minimum number of observations at a node before that node is split. If an inte‐\n",
      "ger is supplied as an argument it determines the raw minimum, while if a float is\n",
      "supplied the minimum is the percent of total observations.\n",
      "\n",
      "min_samples_leaf\n",
      "\n",
      "Minimum number of observations required to be at a leaf. Uses the same argu‐\n",
      "ments as min_samples_split.\n",
      "\n",
      "max_leaf_nodes\n",
      "\n",
      "Maximum number of leaves.\n",
      "\n",
      "min_impurity_split\n",
      "\n",
      "Minimum impurity decrease required before a split is performed.\n",
      "\n",
      "While  it  is  useful  to  know  these  parameters  exist,  most  likely  we  will  only  be  using\n",
      "max_depth  and  min_impurity_split  because  shallower  trees  (sometimes  called\n",
      "stumps) are simpler models and thus have lower variance.\n",
      "\n",
      "14.10 Improving Performance Through Boosting\n",
      "\n",
      "Problem\n",
      "You need a model with better performance than decision trees or random forests.\n",
      "\n",
      "Solution\n",
      "Train a boosted model using AdaBoostClassifier or AdaBoostRegressor:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "14.10 Improving Performance Through Boosting \n",
      "\n",
      "| \n",
      "\n",
      "247\n",
      "\n",
      "\f",
      "target = iris.target\n",
      "\n",
      "# Create adaboost tree classifier object\n",
      "adaboost = AdaBoostClassifier(random_state=0)\n",
      "\n",
      "# Train model\n",
      "model = adaboost.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "In random forest, an ensemble (group) of randomized decision trees predicts the tar‐\n",
      "get  vector.  An  alternative,  and  often  more  powerful,  approach  is  called  boosting.  In\n",
      "one  form  of  boosting  called  AdaBoost,  we  iteratively  train  a  series  of  weak  models\n",
      "(most often a shallow decision tree, sometimes called a stump), each iteration giving\n",
      "higher priority to observations the previous model predicted incorrectly. More specif‐\n",
      "ically, in AdaBoost:\n",
      "\n",
      "1. Assign every observation, xi, an initial weight value, wi = 1\n",
      "\n",
      "n , where n is the total\n",
      "\n",
      "number of observations in the data.\n",
      "\n",
      "2. Train a “weak” model on the data.\n",
      "\n",
      "3. For each observation:\n",
      "\n",
      "a. If weak model predicts xi correctly, wi is increased.\n",
      "\n",
      "b. If weak model predicts xi incorrectly, wi is decreased.\n",
      "\n",
      "4. Train a new weak model where observations with greater wi are given greater pri‐\n",
      "\n",
      "ority.\n",
      "\n",
      "5. Repeat  steps  4  and  5  until  the  data  is  perfectly  predicted  or  a  preset  number  of\n",
      "\n",
      "weak models has been trained.\n",
      "\n",
      "The end result is an aggregated model where individual weak models focus on more\n",
      "difficult (from a prediction perspective) observations. In scikit-learn, we can imple‐\n",
      "ment AdaBoost using AdaBoostClassifier or AdaBoostRegressor. The most impor‐\n",
      "tant parameters are base_estimator, n_estimators, and learning_rate:\n",
      "\n",
      "• base_estimator is the learning algorithm to use to train the weak models. This\n",
      "will  almost  always  not  need  to  be  changed  because  by  far  the  most  common\n",
      "learner  to  use  with  AdaBoost  is  a  decision  tree—the  parameter’s  default  argu‐\n",
      "ment.\n",
      "\n",
      "• n_estimators is the number of models to iteratively train.\n",
      "• learning_rate is the contribution of each model to the weights and defaults to 1.\n",
      "Reducing the learning rate will mean the weights will be increased or decreased\n",
      "\n",
      "248 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "to a small degree, forcing the model to train slower (but sometimes resulting in\n",
      "better performance scores).\n",
      "\n",
      "• loss  is  exclusive  to  AdaBoostRegressor  and  sets  the  loss  function  to  use  when\n",
      "updating weights. This defaults to a linear loss function, but can be changed to\n",
      "square or exponential.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Explaining AdaBoost, Robert E. Schapire\n",
      "\n",
      "14.11 Evaluating Random Forests with Out-of-Bag Errors\n",
      "\n",
      "Problem\n",
      "You need to evaluate a random forest model without using cross-validation.\n",
      "\n",
      "Solution\n",
      "Calculate the model’s out-of-bag score:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create random tree classifier object\n",
      "randomforest = RandomForestClassifier(\n",
      "    random_state=0, n_estimators=1000, oob_score=True, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = randomforest.fit(features, target)\n",
      "\n",
      "# View out-of-bag-error\n",
      "randomforest.oob_score_\n",
      "\n",
      "0.95333333333333337\n",
      "\n",
      "Discussion\n",
      "In random forests, each decision tree is trained using a bootstrapped subset of obser‐\n",
      "vations. This means that for every tree there is a separate subset of observations not\n",
      "\n",
      "14.11 Evaluating Random Forests with Out-of-Bag Errors \n",
      "\n",
      "| \n",
      "\n",
      "249\n",
      "\n",
      "\f",
      "being used to train that tree. These are called out-of-bag (OOB) observations. We can\n",
      "use OOB observations as a test set to evaluate the performance of our random forest.\n",
      "\n",
      "For every observation, the learning algorithm compares the observation’s true value\n",
      "with  the  prediction  from  a  subset  of  trees  not  trained  using  that  observation.  The\n",
      "overall score is calculated and provides a single measure of a random forest’s perfor‐\n",
      "mance. OOB score estimation is an alternative to cross-validation.\n",
      "\n",
      "In scikit-learn, we can OOB scores of a random forest by setting oob_score=True in\n",
      "the random forest object (i.e., RandomForestClassifier). The score can be retrieved\n",
      "using oob_score_.\n",
      "\n",
      "250 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Trees and Forests\n",
      "\n",
      "\f",
      "CHAPTER 15\n",
      "K-Nearest Neighbors\n",
      "\n",
      "15.0 Introduction\n",
      "The K-Nearest Neighbors classifier (KNN) is one of the simplest yet most commonly\n",
      "used  classifiers  in  supervised  machine  learning.  KNN  is  often  considered  a  lazy\n",
      "learner; it doesn’t technically train a model to make predictions. Instead an observa‐\n",
      "tion is predicted to be the class of that of the largest proportion of the k nearest obser‐\n",
      "vations. For example, if an observation with an unknown class is surrounded by an\n",
      "observation of class 1, then the observation is classified as class 1. In this chapter we\n",
      "will explore how to use scikit-learn to create and use a KNN classifier.\n",
      "\n",
      "15.1 Finding an Observation’s Nearest Neighbors\n",
      "\n",
      "Problem\n",
      "You need to find an observation’s k nearest observations (neighbors).\n",
      "\n",
      "Solution\n",
      "Use scikit-learn’s NearestNeighbors:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Create standardizer\n",
      "\n",
      "251\n",
      "\n",
      "\f",
      "standardizer = StandardScaler()\n",
      "\n",
      "# Standardize features\n",
      "features_standardized = standardizer.fit_transform(features)\n",
      "\n",
      "# Two nearest neighbors\n",
      "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
      "\n",
      "# Create an observation\n",
      "new_observation = [ 1,  1,  1,  1]\n",
      "\n",
      "# Find distances and indices of the observation's nearest neighbors\n",
      "distances, indices = nearest_neighbors.kneighbors([new_observation])\n",
      "\n",
      "# View the nearest neighbors\n",
      "features_standardized[indices]\n",
      "\n",
      "array([[[ 1.03800476,  0.56925129,  1.10395287,  1.1850097 ],\n",
      "        [ 0.79566902,  0.33784833,  0.76275864,  1.05353673]]])\n",
      "\n",
      "Discussion\n",
      "In  our  solution  we  used  the  dataset  of  Iris  flowers.  We  created  an  observation,\n",
      "new_observation,  with  some  values  and  then  found  the  two  observations  that  are\n",
      "closest to our observation. indices contains the locations of the observations in our\n",
      "dataset  that  are  closest,  so  X[indices]  displays  the  values  of  those  observations.\n",
      "Intuitively,  distance  can  be  thought  of  as  a  measure  of  similarity,  so  the  two  closest\n",
      "observations are the two flowers most similar to the flower we created.\n",
      "\n",
      "How do we measure distance? scikit-learn offers a wide variety of distance metrics, d,\n",
      "including Euclidean:\n",
      "\n",
      "deuclidean = ∑i = 1\n",
      "\n",
      "n\n",
      "\n",
      "2\n",
      "\n",
      "xi − yi\n",
      "\n",
      "and Manhattan distance:\n",
      "\n",
      "n\n",
      "dmanhattan = ∑\n",
      "i = 1\n",
      "\n",
      "xi − yi\n",
      "\n",
      "By default, NearestNeighbors uses Minkowski distance:\n",
      "\n",
      "n\n",
      "dminkowski = ∑\n",
      "i = 1\n",
      "\n",
      "p\n",
      "\n",
      "xi − yi\n",
      "\n",
      "1/ p\n",
      "\n",
      "252 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: K-Nearest Neighbors\n",
      "\n",
      "\f",
      "where  xi  and  yi  are  the  two  observations  we  are  calculating  the  distance  between.\n",
      "Minkowski includes a hyperparameter, p, where p = 1 is Manhattan distance and p =\n",
      "2 is Euclidean distance, and so on. By default in scikit-learn p = 2.\n",
      "\n",
      "We can set the distance metric using the metric parameter:\n",
      "\n",
      "# Find two nearest neighbors based on euclidean distance\n",
      "nearestneighbors_euclidean = NearestNeighbors(\n",
      "n_neighbors=2, metric='euclidean').fit(features_standardized)\n",
      "\n",
      "The distance variable we created contains the actual distance measurement to each\n",
      "of the two nearest neighbors:\n",
      "\n",
      "# View distances\n",
      "distances\n",
      "\n",
      "array([[ 0.48168828,  0.73440155]])\n",
      "\n",
      "In addition, we can use kneighbors_graph to create a matrix indicating each observa‐\n",
      "tion’s nearest neighbors:\n",
      "\n",
      "# Find each observation's three nearest neighbors\n",
      "# based on euclidean distance (including itself)\n",
      "nearestneighbors_euclidean = NearestNeighbors(\n",
      "n_neighbors=3, metric=\"euclidean\").fit(features_standardized)\n",
      "\n",
      "# List of lists indicating each observation's 3 nearest neighbors\n",
      "# (including itself)\n",
      "nearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(\n",
      "    features_standardized).toarray()\n",
      "\n",
      "# Remove 1's marking an observation is a nearest neighbor to itself\n",
      "for i, x in enumerate(nearest_neighbors_with_self):\n",
      "    x[i] = 0\n",
      "\n",
      "# View first observation's two nearest neighbors\n",
      "nearest_neighbors_with_self[0]\n",
      "\n",
      "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "\n",
      "When we are finding nearest neighbors or using any learning algorithm based on dis‐\n",
      "tance,  it  is  important  to  transform  features  so  that  they  are  on  the  same  scale.  The\n",
      "\n",
      "15.1 Finding an Observation’s Nearest Neighbors \n",
      "\n",
      "| \n",
      "\n",
      "253\n",
      "\n",
      "\f",
      "reason  is  because  the  distance  metrics  treat  all  features  as  if  they  were  on  the  same\n",
      "scale, but if one feature is in millions of dollars and a second feature is in percentages,\n",
      "the distance calculated will be biased toward the former. In our solution we addressed\n",
      "this potential issue by standardizing the features using StandardScaler.\n",
      "\n",
      "15.2 Creating a K-Nearest Neighbor Classifier\n",
      "\n",
      "Problem\n",
      "Given  an  observation  of  unknown  class,  you  need  to  predict  its  class  based  on  the\n",
      "class of its neighbors.\n",
      "\n",
      "Solution\n",
      "If the dataset is not very large, use KNeighborsClassifier:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "# Create standardizer\n",
      "standardizer = StandardScaler()\n",
      "\n",
      "# Standardize features\n",
      "X_std = standardizer.fit_transform(X)\n",
      "\n",
      "# Train a KNN classifier with 5 neighbors\n",
      "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_std, y)\n",
      "\n",
      "# Create two observations\n",
      "new_observations = [[ 0.75,  0.75,  0.75,  0.75],\n",
      "                    [ 1,  1,  1,  1]]\n",
      "\n",
      "# Predict the class of two observations\n",
      "knn.predict(new_observations)\n",
      "\n",
      "array([1, 2])\n",
      "\n",
      "Discussion\n",
      "In KNN, given an observation, xu, with an unknown target class, the algorithm first\n",
      "identifies  the  k  closest  observations  (sometimes  called  xu’s  neighborhood)  based  on\n",
      "some  distance  metric  (e.g.,  Euclidean  distance),  then  these  k  observations  “vote”\n",
      "\n",
      "254 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: K-Nearest Neighbors\n",
      "\n",
      "\f",
      "based on their class, and the class that wins the vote is xu’s predicted class. More for‐\n",
      "mally, the probability xu is some class j is:\n",
      "\n",
      "1\n",
      "k ∑\n",
      "i ∈ ν\n",
      "\n",
      "I yi = j\n",
      "\n",
      "where ν is the k observation in xu’s neighborhood, yi is the class of the ith observation,\n",
      "and I is an indicator function (i.e., 1 is true, 0 otherwise). In scikit-learn we can see\n",
      "these probabilities using predict_proba:\n",
      "\n",
      "# View probability each observation is one of three classes\n",
      "knn.predict_proba(new_observations)\n",
      "\n",
      "array([[ 0. ,  0.6,  0.4],\n",
      "       [ 0. ,  0. ,  1. ]])\n",
      "\n",
      "The  class  with  the  highest  probability  becomes  the  predicted  class.  For  example,  in\n",
      "the preceding output, the first observation should be class 1 (Pr = 0.6) while the sec‐\n",
      "ond observation should be class 2 (Pr = 1), and this is just what we see:\n",
      "\n",
      "knn.predict(new_observations)\n",
      "\n",
      "array([1, 2])\n",
      "\n",
      "KNeighborsClassifier  contains  a  number  of  important  parameters  to  consider.\n",
      "First, metric sets the distance metric used (discussed more in Recipe 14.1). Second,\n",
      "n_jobs determines how many of the computer’s cores to use. Because making a pre‐\n",
      "diction requires calculating the distance from a point to every single point in the data,\n",
      "using multiple cores is highly recommended. Third, algorithm sets the method used\n",
      "to calculate the nearest neighbors. While there are real differences in the algorithms,\n",
      "by default  KNeighborsClassifier attempts to auto-select the best algorithm so you\n",
      "often don’t need to worry about this parameter. Fourth, by default KNeighborsClassi\n",
      "fier works how we described previously, with each observation in the neighborhood\n",
      "getting  one  vote;  however,  if  we  set  the  weights  parameter  to  distance,  the  closer\n",
      "observations’ votes are weighted more than observations farther away. Intuitively this\n",
      "make sense, since more similar neighbors might tell us more about an observation’s\n",
      "class than others.\n",
      "\n",
      "Finally, as discussed in Recipe 14.1, because distance calculations treat all features as if\n",
      "they are on the same scale, it is important to standardize the features prior to using a\n",
      "KNN classifier.\n",
      "\n",
      "15.2 Creating a K-Nearest Neighbor Classifier \n",
      "\n",
      "| \n",
      "\n",
      "255\n",
      "\n",
      "\f",
      "15.3 Identifying the Best Neighborhood Size\n",
      "\n",
      "Problem\n",
      "You want to select the best value for k in a k-nearest neighbors classifier.\n",
      "\n",
      "Solution\n",
      "Use model selection techniques like GridSearchCV:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create standardizer\n",
      "standardizer = StandardScaler()\n",
      "\n",
      "# Standardize features\n",
      "features_standardized = standardizer.fit_transform(features)\n",
      "\n",
      "# Create a KNN classifier\n",
      "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
      "\n",
      "# Create a pipeline\n",
      "pipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n",
      "\n",
      "# Create space of candidate values\n",
      "search_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
      "\n",
      "# Create grid search\n",
      "classifier = GridSearchCV(\n",
      "    pipe, search_space, cv=5, verbose=0).fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "The size of k has real implications in KNN classifiers. In machine learning we are try‐\n",
      "ing to find a balance between bias and variance, and in few places is that as explicit as\n",
      "the value of k. If k = n where n is the number of observations, then we have high bias\n",
      "but low variance. If k = 1, we will have low bias but high variance. The best model will\n",
      "come from finding the value of k that balances this bias-variance trade-off. In our sol‐\n",
      "ution, we used GridSearchCV to conduct five-fold cross-validation on KNN classifiers\n",
      "\n",
      "256 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: K-Nearest Neighbors\n",
      "\n",
      "\f",
      "with different values of k. When that is completed, we can see the k that produces the\n",
      "best model:\n",
      "\n",
      "# Best neighborhood size (k)\n",
      "classifier.best_estimator_.get_params()[\"knn__n_neighbors\"]\n",
      "\n",
      "6\n",
      "\n",
      "15.4 Creating a Radius-Based Nearest Neighbor Classifier\n",
      "\n",
      "Problem\n",
      "Given  an  observation  of  unknown  class,  you  need  to  predict  its  class  based  on  the\n",
      "class of all observations within a certain distance.\n",
      "\n",
      "Solution\n",
      "Use RadiusNeighborsClassifier:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.neighbors import RadiusNeighborsClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import datasets\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create standardizer\n",
      "standardizer = StandardScaler()\n",
      "\n",
      "# Standardize features\n",
      "features_standardized = standardizer.fit_transform(features)\n",
      "\n",
      "# Train a radius neighbors classifier\n",
      "rnn = RadiusNeighborsClassifier(\n",
      "    radius=.5, n_jobs=-1).fit(features_standardized, target)\n",
      "\n",
      "# Create two observations\n",
      "new_observations = [[ 1,  1,  1,  1]]\n",
      "\n",
      "# Predict the class of two observations\n",
      "rnn.predict(new_observations)\n",
      "\n",
      "array([2])\n",
      "\n",
      "Discussion\n",
      "In  KNN  classification,  an  observation’s  class  is  predicted  from  the  classes  of  its  k\n",
      "neighbors. A less common technique is classification in a radius-based (RNN) classi‐\n",
      "\n",
      "15.4 Creating a Radius-Based Nearest Neighbor Classifier \n",
      "\n",
      "| \n",
      "\n",
      "257\n",
      "\n",
      "\f",
      "fier,  where  an  observation’s  class  is  predicted  from  the  classes  of  all  observations\n",
      "within a given radius r. In scikit-learn RadiusNeighborsClassifier is very similar to\n",
      "KNeighborsClassifier, with the exception of two parameters. First, in RadiusNeigh\n",
      "borsClassifier we need to specify the radius of the fixed area used to determine if\n",
      "an  observation  is  a  neighbor  using  radius.  Unless  there  is  some  substantive  reason\n",
      "for setting  radius to some value, it is best to treat it like any other hyperparameter\n",
      "and tune it during model selection. The second useful parameter is outlier_label, \n",
      "which indicates what label to give an observation that has no observations within the\n",
      "radius—which itself can often be a useful tool for identifying outliers.\n",
      "\n",
      "258 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: K-Nearest Neighbors\n",
      "\n",
      "\f",
      "CHAPTER 16\n",
      "Logistic Regression\n",
      "\n",
      "16.0 Introduction\n",
      "Despite being called a regression, logistic regression is actually a widely used super‐\n",
      "vised classification technique. Logistic regression and its extensions, like multinomial\n",
      "logistic regression, allow us to predict the probability that an observation is of a cer‐\n",
      "tain class using a straightforward and well-understood approach. In this chapter, we\n",
      "will cover training a variety of classifiers using scikit-learn.\n",
      "\n",
      "16.1 Training a Binary Classifier\n",
      "\n",
      "Problem\n",
      "You need to train a simple classifier model.\n",
      "\n",
      "Solution\n",
      "Train a logistic regression in scikit-learn using LogisticRegression:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data with only two classes\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data[:100,:]\n",
      "target = iris.target[:100]\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "259\n",
      "\n",
      "\f",
      "# Create logistic regression object\n",
      "logistic_regression = LogisticRegression(random_state=0)\n",
      "\n",
      "# Train model\n",
      "model = logistic_regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "Despite having “regression” in its name, a logistic regression is actually a widely used\n",
      "binary classifier (i.e., the target vector can only take two values). In a logistic regres‐\n",
      "sion, a linear model (e.g., β0 + β1x) is included in a logistic (also called sigmoid) func‐\n",
      "tion, \n",
      "\n",
      "1\n",
      "\n",
      "−z , such that:\n",
      "\n",
      "1 + e\n",
      "\n",
      "P yi = 1 ∣ X =\n",
      "\n",
      "1\n",
      "− β\n",
      "\n",
      "0\n",
      "\n",
      "+ β\n",
      "\n",
      "x\n",
      "\n",
      "1\n",
      "\n",
      "1 + e\n",
      "\n",
      "where P(yi = 1 | X) is the probability of the ith observation’s target value, yi, being class\n",
      "1, X is the training data, β0 and β1 are the parameters to be learned, and e is Euler’s\n",
      "number. The effect of the logistic function is to constrain the value of the function’s\n",
      "output to between 0 and 1 so that it can be interpreted as a probability. If P(yi = 1 | X)\n",
      "is greater than 0.5, class 1 is predicted; otherwise, class 0 is predicted.\n",
      "\n",
      "In scikit-learn, we can learn a logistic regression model using LogisticRegression.\n",
      "Once it is trained, we can use the model to predict the class of new observations:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[.5, .5, .5, .5]]\n",
      "\n",
      "# Predict class\n",
      "model.predict(new_observation)\n",
      "\n",
      "array([1])\n",
      "\n",
      "In this example, our observation was predicted to be class 1. Additionally, we can see\n",
      "the probability that an observation is a member of each class:\n",
      "\n",
      "# View predicted probabilities\n",
      "model.predict_proba(new_observation)\n",
      "\n",
      "array([[ 0.18823041,  0.81176959]])\n",
      "\n",
      "Our  observation  had  an  18.8%  chance  of  being  class  0  and  81.1%  chance  of  being \n",
      "class 1.\n",
      "\n",
      "260 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Logistic Regression\n",
      "\n",
      "\f",
      "16.2 Training a Multiclass Classifier\n",
      "\n",
      "Problem\n",
      "Given more than two classes, you need to train a classifier model.\n",
      "\n",
      "Solution\n",
      "Train a logistic regression in scikit-learn with LogisticRegression using one-vs-rest\n",
      "or multinomial methods:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create one-vs-rest logistic regression object\n",
      "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
      "\n",
      "# Train model\n",
      "model = logistic_regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "On  their  own,  logistic  regressions  are  only  binary  classifiers,  meaning  they  cannot\n",
      "handle target vectors with more than two classes. However, two clever extensions to\n",
      "logistic regression do just that. First, in one-vs-rest logistic regression (OVR) a sepa‐\n",
      "rate model is trained for each class predicted whether an observation is that class or\n",
      "not (thus making it a binary classification problem). It assumes that each classifica‐\n",
      "tion problem (e.g., class 0 or not) is independent.\n",
      "\n",
      "Alternatively,  in  multinomial  logistic  regression  (MLR)  the  logistic  function  we  saw\n",
      "in Recipe 15.1 is replaced with a softmax function:\n",
      "\n",
      "P yi = k ∣ X =\n",
      "\n",
      "βkxi\n",
      "\n",
      "e\n",
      "\n",
      "β\n",
      "\n",
      "x\n",
      "\n",
      "j\n",
      "\n",
      "i\n",
      "\n",
      "K\n",
      "∑ j = 1\n",
      "\n",
      "e\n",
      "\n",
      "16.2 Training a Multiclass Classifier \n",
      "\n",
      "| \n",
      "\n",
      "261\n",
      "\n",
      "\f",
      "where P(yi = k | X) is the probability of the ith observation’s target value, yi, is class k,\n",
      "and K is the total number of classes. One practical advantage of the MLR is that its\n",
      "predicted probabilities using the predict_proba method are more reliable (i.e., better\n",
      "calibrated).\n",
      "\n",
      "When  using  LogisticRegression  we  can  select  which  of  the  two  techniques  we\n",
      "want, with OVR, ovr, being the default argument. We can switch to an MNL by set‐\n",
      "ting the argument to multinomial.\n",
      "\n",
      "16.3 Reducing Variance Through Regularization\n",
      "\n",
      "Problem\n",
      "You need to reduce the variance of your logistic regression model.\n",
      "\n",
      "Solution\n",
      "Tune the regularization strength hyperparameter, C:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LogisticRegressionCV\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create decision tree classifier object\n",
      "logistic_regression = LogisticRegressionCV(\n",
      "    penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = logistic_regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "Regularization  is  a  method  of  penalizing  complex  models  to  reduce  their  variance.\n",
      "Specifically, a penalty term is added to the loss function we are trying to minimize,\n",
      "typically the L1 and L2 penalties. In the L1 penalty:\n",
      "\n",
      "262 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Logistic Regression\n",
      "\n",
      "\f",
      "p\n",
      "α ∑\n",
      "j = 1\n",
      "\n",
      "β j\n",
      "\n",
      "where β j is the parameters of the jth of p features being learned and α is a hyperpara‐\n",
      "meter denoting the regularization strength. With the L2 penalty:\n",
      "\n",
      "p\n",
      "α ∑\n",
      "j = 1\n",
      "\n",
      "2\n",
      "β j\n",
      "\n",
      "Higher values of α increase the penalty for larger parameter values (i.e., more com‐\n",
      "plex models). scikit-learn follows the common method of using C instead of α where\n",
      "C is the inverse of the regularization strength: C = 1\n",
      "α . To reduce variance while using\n",
      "logistic regression, we can treat C as a hyperparameter to be tuned to find the value of\n",
      "C that creates the best model. In scikit-learn we can use the LogisticRegressionCV\n",
      "class to efficiently tune C. LogisticRegressionCV’s parameter, Cs, can either accept a\n",
      "range of values for C to search over (if a list of floats is supplied as an argument) or if\n",
      "supplied an integer, will generate a list of that many candidate values drawn from a\n",
      "logarithmic scale between –10,000 and 10,000.\n",
      "\n",
      "Unfortunately, LogisticRegressionCV does not allow us to search over different pen‐\n",
      "alty terms. To do this we have to use the less efficient model selection techniques dis‐\n",
      "cussed in Chapter 12.\n",
      "\n",
      "16.4 Training a Classifier on Very Large Data\n",
      "\n",
      "Problem\n",
      "You need to train a simple classifier model on a very large set of data.\n",
      "\n",
      "Solution\n",
      "Train a logistic regression in scikit-learn with LogisticRegression using the stochas‐\n",
      "tic average gradient (SAG) solver:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "16.4 Training a Classifier on Very Large Data \n",
      "\n",
      "| \n",
      "\n",
      "263\n",
      "\n",
      "\f",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create logistic regression object\n",
      "logistic_regression = LogisticRegression(random_state=0, solver=\"sag\")\n",
      "\n",
      "# Train model\n",
      "model = logistic_regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "scikit-learn’s LogisticRegression offers a number of techniques for training a logis‐\n",
      "tic  regression,  called  solvers.  Most  of  the  time  scikit-learn  will  select  the  best  solver\n",
      "automatically for us or warn us that we cannot do something with that solver. How‐\n",
      "ever, there is one particular case we should be aware of.\n",
      "\n",
      "While an exact explanation is beyond the bounds of this book (for more information\n",
      "see  Mark  Schmidt’s  slides  in  “See  Also”  on  page  264),  stochastic  average  gradient\n",
      "descent  allows  us  to  train  a  model  much  faster  than  other  solvers  when  our  data  is\n",
      "very large. However, it is also very sensitive to feature scaling, so standardizing our\n",
      "features is particularly important. We can set our learning algorithm to use this solver\n",
      "by setting solver='sag'.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Minimizing Finite Sums with the Stochastic Average Gradient Algorithm, Mark\n",
      "\n",
      "Schmidt\n",
      "\n",
      "16.5 Handling Imbalanced Classes\n",
      "\n",
      "Problem\n",
      "You need to train a simple classifier model.\n",
      "\n",
      "Solution\n",
      "Train a logistic regression in scikit-learn using LogisticRegression:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load data\n",
      "\n",
      "264 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Logistic Regression\n",
      "\n",
      "\f",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Make class highly imbalanced by removing first 40 observations\n",
      "features = features[40:,:]\n",
      "target = target[40:]\n",
      "\n",
      "# Create target vector indicating if class 0, otherwise 1\n",
      "target = np.where((target == 0), 0, 1)\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create decision tree classifier object\n",
      "logistic_regression = LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
      "\n",
      "# Train model\n",
      "model = logistic_regression.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "Like many other learning algorithms in scikit-learn, LogisticRegression comes with\n",
      "a  built-in  method  of  handling  imbalanced  classes.  If  we  have  highly  imbalanced\n",
      "classes and have not addressed it during preprocessing, we have the option of using\n",
      "the class_weight parameter to weight the classes to make certain we have a balanced\n",
      "mix of each class. Specifically, the balanced argument will automatically weigh classes\n",
      "inversely proportional to their frequency:\n",
      "\n",
      "w j =\n",
      "\n",
      "n\n",
      "kn j\n",
      "\n",
      "where wj is the weight to class j, n is the number of observations, nj is the number of\n",
      "observations in class j, and k is the total number of classes.\n",
      "\n",
      "16.5 Handling Imbalanced Classes \n",
      "\n",
      "| \n",
      "\n",
      "265\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 17\n",
      "Support Vector Machines\n",
      "\n",
      "17.0 Introduction\n",
      "To understand support vector machines, we must understand hyperplanes. Formally,\n",
      "a hyperplane is an n – 1 subspace in an n-dimensional space. While that sounds com‐\n",
      "plex,  it  actually  is  pretty  simple.  For  example,  if  we  wanted  to  divide  a  two-\n",
      "dimensional space, we’d use a one-dimensional hyperplane (i.e., a line). If we wanted\n",
      "to  divide  a  three-dimensional  space,  we’d  use  a  two-dimensional  hyperplane  (i.e.,  a\n",
      "flat piece of paper or a bed sheet). A hyperplane is simply a generalization of that con‐\n",
      "cept into n dimensions.\n",
      "\n",
      "Support vector machines classify data by finding the hyperplane that maximizes the\n",
      "margin between the classes in the training data. In a two-dimensional example with\n",
      "two classes, we can think of a hyperplane as the widest straight “band” (i.e., line with\n",
      "margins) that separates the two classes.\n",
      "\n",
      "In this chapter, we cover training support vector machines in a variety of situations\n",
      "and dive under the hood to look at how we can extend the approach to tackle com‐\n",
      "mon problems.\n",
      "\n",
      "17.1 Training a Linear Classifier\n",
      "\n",
      "Problem\n",
      "You need to train a model to classify observations.\n",
      "\n",
      "Solution\n",
      "Use a support vector classifier (SVC) to find the hyperplane that maximizes the mar‐\n",
      "gins between the classes:\n",
      "\n",
      "267\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "# Load data with only two classes and two features\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data[:100,:2]\n",
      "target = iris.target[:100]\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create support vector classifier\n",
      "svc = LinearSVC(C=1.0)\n",
      "\n",
      "# Train model\n",
      "model = svc.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "scikit-learn’s LinearSVC implements a simple SVC. To get an intuition behind what an\n",
      "SVC is doing, let us plot out the data and hyperplane. While SVCs work well in high\n",
      "dimensions, in our solution we only loaded two features and took a subset of obser‐\n",
      "vations so that the data contains only two classes. This will let us visualize the model.\n",
      "Recall  that  SVC  attempts  to  find  the  hyperplane—a  line  when  we  only  have  two\n",
      "dimensions—with  the  maximum  margin  between  the  classes.  In  the  following  code\n",
      "we plot the two classes on a two-dimensional space, then draw the hyperplane:\n",
      "\n",
      "# Load library\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Plot data points and color using their class\n",
      "color = [\"black\" if c == 0 else \"lightgrey\" for c in target]\n",
      "plt.scatter(features_standardized[:,0], features_standardized[:,1], c=color)\n",
      "\n",
      "# Create the hyperplane\n",
      "w = svc.coef_[0]\n",
      "a = -w[0] / w[1]\n",
      "xx = np.linspace(-2.5, 2.5)\n",
      "yy = a * xx - (svc.intercept_[0]) / w[1]\n",
      "\n",
      "# Plot the hyperplane\n",
      "plt.plot(xx, yy)\n",
      "plt.axis(\"off\"), plt.show();\n",
      "\n",
      "268 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "In this visualization, all observations of class 0 are black and observations of class 1\n",
      "are light gray. The hyperplane is the decision boundary deciding how new observa‐\n",
      "tions  are  classified.  Specifically,  any  observation  above  the  line  will  by  classified  as\n",
      "class 0 while any observation below the line will be classified as class 1. We can prove\n",
      "this by creating a new observation in the top-left corner of our visualization, meaning\n",
      "it should be predicted to be class 0:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[ -2,  3]]\n",
      "\n",
      "# Predict class of new observation\n",
      "svc.predict(new_observation)\n",
      "\n",
      "array([0])\n",
      "\n",
      "There are a few things to note about SVCs. First, for the sake of visualization we limi‐\n",
      "ted our example to a binary example (e.g., only two classes); however, SVCs can work\n",
      "well  with  multiple  classes.  Second,  as  our  visualization  shows,  the  hyperplane  is  by\n",
      "definition linear (i.e., not curved). This was okay in this example because the data was\n",
      "linearly separable, meaning there was a hyperplane that could perfectly separate the\n",
      "two classes. Unfortunately, in the real world this will rarely be the case.\n",
      "\n",
      "More  typically,  we  will  not  be  able  to  perfectly  separate  classes.  In  these  situations\n",
      "there is a balance between SVC maximizing the margin of the hyperplane and mini‐\n",
      "mizing the misclassification. In SVC, the latter is controlled with the hyperparameter\n",
      "\n",
      "17.1 Training a Linear Classifier \n",
      "\n",
      "| \n",
      "\n",
      "269\n",
      "\n",
      "\f",
      "C, the penalty imposed on errors. C is a parameter of the SVC learner and is the pen‐\n",
      "alty for misclassifying a data point. When C is small, the classifier is okay with mis‐\n",
      "classified  data  points  (high  bias  but  low  variance).  When  C  is  large,  the  classifier  is\n",
      "heavily penalized for misclassified data and therefore bends over backwards to avoid\n",
      "any misclassified data points (low bias but high variance).\n",
      "\n",
      "In scikit-learn, C is determined by the parameter C and defaults to C=1.0. We should\n",
      "treat C has a hyperparameter of our learning algorithm, which we tune using model\n",
      "selection techniques in Chapter 12.\n",
      "\n",
      "17.2 Handling Linearly Inseparable Classes Using Kernels\n",
      "\n",
      "Problem\n",
      "You need to train a support vector classifier, but your classes are linearly inseparable.\n",
      "\n",
      "Solution\n",
      "Train an extension of a support vector machine using kernel functions to create non‐\n",
      "linear decision boundaries:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "# Set randomization seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate two features\n",
      "features = np.random.randn(200, 2)\n",
      "\n",
      "# Use a XOR gate (you don't need to know what this is) to generate\n",
      "# linearly inseparable classes\n",
      "target_xor = np.logical_xor(features[:, 0] > 0, features[:, 1] > 0)\n",
      "target = np.where(target_xor, 0, 1)\n",
      "\n",
      "# Create a support vector machine with a radial basis function kernel\n",
      "svc = SVC(kernel=\"rbf\", random_state=0, gamma=1, C=1)\n",
      "\n",
      "# Train the classifier\n",
      "model = svc.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "A full explanation of support vector machines is outside the scope of this book. How‐\n",
      "ever,  a  short  explanation  is  likely  beneficial  for  understanding  support  vector\n",
      "\n",
      "270 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "machines and kernels. For reasons best learned elsewhere, a support vector classifier\n",
      "can be represented as:\n",
      "\n",
      "f x = β0 + ∑\n",
      "iS\n",
      "\n",
      "αiK xi, xi′\n",
      "\n",
      "where  β0  is  the  bias,  S  is  the  set  of  all  support  vector  observations,  α  are  the  model\n",
      "parameters to be learned, and (xi, xi') are pairs of two support vector observations, xi\n",
      "and xi'. Most importantly, K is a kernel function that compares the similarity between\n",
      "xi and xi'. Don’t worry if you don’t understand kernel functions. For our purposes, just\n",
      "realize that K 1) determines the type of hyperplane used to separate our classes and 2)\n",
      "we create different hyperplanes by using different kernels. For example, if we wanted\n",
      "the basic linear hyperplane like the one we created in Recipe 17.1, we can use the lin‐\n",
      "ear kernel:\n",
      "\n",
      "p\n",
      "K xi, xi′ = ∑\n",
      "j = 1\n",
      "\n",
      "xi jxi′ j\n",
      "\n",
      "where  p  is  the  number  of  features.  However,  if  we  wanted  a  nonlinear  decision\n",
      "boundary, we swap the linear kernel with a polynomial kernel:\n",
      "\n",
      "p\n",
      "K xi, xi′ = 1 + ∑\n",
      "j = 1\n",
      "\n",
      "xi jxi′ j\n",
      "\n",
      "2\n",
      "\n",
      "where d is the degree of the polynomial kernel function. Alternatively, we can use one\n",
      "of the most common kernels in support vectors machines, the radial basis function\n",
      "kernel:\n",
      "\n",
      "− γ∑\n",
      "\n",
      "p\n",
      "j = 1\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "i j\n",
      "\n",
      "i′ j\n",
      "\n",
      "2\n",
      "\n",
      "K xi, xi′ = e\n",
      "\n",
      "where  γ  is  a  hyperparameter  and  must  be  greater  than  zero.  The  main  point  of  the\n",
      "preceding explanation is that if we have linearly inseparable data we can swap out a\n",
      "linear  kernel  with  an  alternative  kernel  to  create  a  nonlinear  hyperplane  decision\n",
      "boundary.\n",
      "\n",
      "We can understand the intuition behind kernels by visualizing a simple example. This\n",
      "function,  based  on  one  by  Sebastian  Raschka,  plots  the  observations  and  decision\n",
      "boundary  hyperplane  of  a  two-dimensional  space.  You  do  not  need  to  understand\n",
      "how this function works; I have included it here so you can experiment on your own:\n",
      "\n",
      "17.2 Handling Linearly Inseparable Classes Using Kernels \n",
      "\n",
      "| \n",
      "\n",
      "271\n",
      "\n",
      "\f",
      "# Plot observations and decision boundary hyperplane\n",
      "from matplotlib.colors import ListedColormap\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def plot_decision_regions(X, y, classifier):\n",
      "    cmap = ListedColormap((\"red\", \"blue\"))\n",
      "    xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))\n",
      "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
      "    Z = Z.reshape(xx1.shape)\n",
      "    plt.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)\n",
      "\n",
      "    for idx, cl in enumerate(np.unique(y)):\n",
      "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
      "                    alpha=0.8, c=cmap(idx),\n",
      "                    marker=\"+\", label=cl)\n",
      "\n",
      "In our solution, we have data containing two features (i.e., two dimensions) and a tar‐\n",
      "get  vector  with  the  class  of  each  observation.  Importantly,  the  classes  are  assigned\n",
      "such that they are linearly inseparable. That is, there is no straight line we can draw\n",
      "that  will  divide  the  two  classes.  First,  let’s  create  a  support  vector  machine  classifier\n",
      "with a linear kernel:\n",
      "\n",
      "# Create support vector classifier with a linear kernel\n",
      "svc_linear = SVC(kernel=\"linear\", random_state=0, C=1)\n",
      "\n",
      "# Train model\n",
      "svc_linear.fit(features, target)\n",
      "\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\n",
      "Next,  since  we  have  only  two  features,  we  are  working  in  a  two-dimensional  space\n",
      "and can visualize the observations, their classes, and our model’s linear hyperplane:\n",
      "\n",
      "# Plot observations and hyperplane\n",
      "plot_decision_regions(features, target, classifier=svc_linear)\n",
      "plt.axis(\"off\"), plt.show();\n",
      "\n",
      "272 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "As we can see, our linear hyperplane did very poorly at dividing the two classes! Now,\n",
      "let’s swap out the linear kernel with a radial basis function kernel and use it to train a\n",
      "new model:\n",
      "\n",
      "# Create a support vector machine with a radial basis function kernel\n",
      "svc = SVC(kernel=\"rbf\", random_state=0, gamma=1, C=1)\n",
      "\n",
      "# Train the classifier\n",
      "model = svc.fit(features, target)\n",
      "\n",
      "And then visualize the observations and hyperplane:\n",
      "\n",
      "# Plot observations and hyperplane\n",
      "plot_decision_regions(features, target, classifier=svc)\n",
      "plt.axis(\"off\"), plt.show();\n",
      "\n",
      "17.2 Handling Linearly Inseparable Classes Using Kernels \n",
      "\n",
      "| \n",
      "\n",
      "273\n",
      "\n",
      "\f",
      "By using the radial basis function kernel we could create a decision boundary able to\n",
      "do a much better job of separating the two classes than the linear kernel. This is the\n",
      "motivation behind using kernels in support vector machines.\n",
      "\n",
      "In scikit-learn, we can select the kernel we want to use by using the kernel parame‐\n",
      "ter.  Once  we  select  a  kernel,  we  will  need  to  specify  the  appropriate  kernel  options\n",
      "such  as  the  value  of  d  (using  the  degree  parameter)  in  polynomial  kernels  and  γ\n",
      "(using the gamma parameter) in radial basis function kernels. We will also need to set\n",
      "the penalty parameter, C. When training the model, in most cases we should treat all\n",
      "of these as hyperparameters and use model selection techniques to identify the com‐\n",
      "bination of their values that produces the model with the best performance.\n",
      "\n",
      "17.3 Creating Predicted Probabilities\n",
      "\n",
      "Problem\n",
      "You need to know the predicted class probabilities for an observation.\n",
      "\n",
      "Solution\n",
      "When using scikit-learn’s SVC, set probability=True, train the model, then use pre\n",
      "dict_proba to see the calibrated probabilities:\n",
      "\n",
      "274 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create support vector classifier object\n",
      "svc = SVC(kernel=\"linear\", probability=True, random_state=0)\n",
      "\n",
      "# Train classifier\n",
      "model = svc.fit(features_standardized, target)\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[.4, .4, .4, .4]]\n",
      "\n",
      "# View predicted probabilities\n",
      "model.predict_proba(new_observation)\n",
      "\n",
      "array([[ 0.00588822,  0.96874828,  0.0253635 ]])\n",
      "\n",
      "Discussion\n",
      "Many  of  the  supervised  learning  algorithms  we  have  covered  use  probability  esti‐\n",
      "mates to predict classes. For example, in k-nearest neighbor, an observation’s k neigh‐\n",
      "bor’s  classes  were  treated  as  votes  to  create  a  probability  that  an  observation  was  of\n",
      "that  class.  Then  the  class  with  the  highest  probability  was  predicted.  SVC’s  use  of  a\n",
      "hyperplane to create decision regions does not naturally output a probability estimate\n",
      "that  an  observation  is  a  member  of  a  certain  class.  However,  we  can  in  fact  output\n",
      "calibrated class probabilities with a few caveats. In an SVC with two classes, Platt scal‐\n",
      "ing can be used, wherein first the SVC is trained, and then a separate cross-validated\n",
      "logistic regression is trained to map the SVC outputs into probabilities:\n",
      "\n",
      "P y = 1 ∣ x =\n",
      "\n",
      "1\n",
      "1 + e A * f x + B\n",
      "\n",
      "where  A  and  B  are  parameter  vectors  and  f  is  the  ith  observation’s  signed  distance\n",
      "from the hyperplane. When we have more than two classes, an extension of Platt scal‐\n",
      "ing is used.\n",
      "\n",
      "17.3 Creating Predicted Probabilities \n",
      "\n",
      "| \n",
      "\n",
      "275\n",
      "\n",
      "\f",
      "In more practical terms, creating predicted probabilities has two major issues. First,\n",
      "because  we  are  training  a  second  model  with  cross-validation,  generating  predicted\n",
      "probabilities  can  significantly  increase  the  time  it  takes  to  train  our  model.  Second,\n",
      "because the predicted probabilities are created using cross-validation, they might not\n",
      "always match the predicted classes. That is, an observation might be predicted to be\n",
      "class 1, but have a predicted probability of being class 1 of less than 0.5.\n",
      "\n",
      "In scikit-learn, the predicted probabilities must be generated when the model is being\n",
      "trained.  We  can  do  this  by  setting  SVC’s  probability  to  True.  After  the  model  is\n",
      "trained,  we  can  output  the  estimated  probabilities  for  each  class  using  pre\n",
      "dict_proba.\n",
      "\n",
      "17.4 Identifying Support Vectors\n",
      "\n",
      "Problem\n",
      "You  need  to  identify  which  observations  are  the  support  vectors  of  the  decision\n",
      "hyperplane.\n",
      "\n",
      "Solution\n",
      "Train the model, then use support_vectors_:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "#Load data with only two classes\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data[:100,:]\n",
      "target = iris.target[:100]\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create support vector classifier object\n",
      "svc = SVC(kernel=\"linear\", random_state=0)\n",
      "\n",
      "# Train classifier\n",
      "model = svc.fit(features_standardized, target)\n",
      "\n",
      "# View support vectors\n",
      "model.support_vectors_\n",
      "\n",
      "array([[-0.5810659 ,  0.43490123, -0.80621461, -0.50581312],\n",
      "       [-1.52079513, -1.67626978, -1.08374115, -0.8607697 ],\n",
      "\n",
      "276 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "       [-0.89430898, -1.46515268,  0.30389157,  0.38157832],\n",
      "       [-0.5810659 , -1.25403558,  0.09574666,  0.55905661]])\n",
      "\n",
      "Discussion\n",
      "Support  vector  machines  get  their  name  from  the  fact  that  the  hyperplane  is  being\n",
      "determined by a relatively small number of observations, called the support vectors.\n",
      "Intuitively, think of the hyperplane as being “carried” by these support vectors. These\n",
      "support vectors are therefore very important to our model. For example, if we remove\n",
      "an observation that is not a support vector from the data, the model does not change;\n",
      "however, if we remove a support vector, the hyperplane will not have the maximum\n",
      "margin.\n",
      "\n",
      "After we have trained an SVC, scikit-learn offers us a number of options for identify‐\n",
      "ing  the  support  vector.  In  our  solution,  we  used  support_vectors_  to  output  the\n",
      "actual observations’ features of the four support vectors in our model. Alternatively,\n",
      "we can view the indices of the support vectors using support_:\n",
      "\n",
      "model.support_\n",
      "\n",
      "array([23, 41, 57, 98], dtype=int32)\n",
      "\n",
      "Finally,  we  can  use  n_support_  to  find  the  number  of  support  vectors  belonging  to\n",
      "each class:\n",
      "\n",
      "model.n_support_\n",
      "\n",
      "array([2, 2], dtype=int32)\n",
      "\n",
      "17.5 Handling Imbalanced Classes\n",
      "\n",
      "Problem\n",
      "You need to train a support vector machine classifier in the presence of imbalanced\n",
      "classes.\n",
      "\n",
      "Solution\n",
      "Increase the penalty for misclassifying the smaller class using class_weight:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import numpy as np\n",
      "\n",
      "#Load data with only two classes\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data[:100,:]\n",
      "target = iris.target[:100]\n",
      "\n",
      "17.5 Handling Imbalanced Classes \n",
      "\n",
      "| \n",
      "\n",
      "277\n",
      "\n",
      "\f",
      "# Make class highly imbalanced by removing first 40 observations\n",
      "features = features[40:,:]\n",
      "target = target[40:]\n",
      "\n",
      "# Create target vector indicating if class 0, otherwise 1\n",
      "target = np.where((target == 0), 0, 1)\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Create support vector classifier\n",
      "svc = SVC(kernel=\"linear\", class_weight=\"balanced\", C=1.0, random_state=0)\n",
      "\n",
      "# Train classifier\n",
      "model = svc.fit(features_standardized, target)\n",
      "\n",
      "Discussion\n",
      "In support vector machines, C is a hyperparameter determining the penalty for mis‐\n",
      "classifying  an  observation.  One  method  for  handling  imbalanced  classes  in  support\n",
      "vector machines is to weight C by classes, so that:\n",
      "\n",
      "Ck = C * w j\n",
      "\n",
      "where  C  is  the  penalty  for  misclassification,  wj  is  a  weight  inversely  proportional  to\n",
      "class j’s frequency, and Cj is the C value for class j. The general idea is to increase the\n",
      "penalty  for  misclassifying  minority  classes  to  prevent  them  from  being  “over‐\n",
      "whelmed” by the majority class.\n",
      "\n",
      "In scikit-learn, when using SVC we can set the values for Cj automatically by setting\n",
      "class_weight='balanced'.  The  balanced  argument  automatically  weighs  classes\n",
      "such that:\n",
      "\n",
      "w j =\n",
      "\n",
      "n\n",
      "kn j\n",
      "\n",
      "where wj is the weight to class j, n is the number of observations, nj is the number of\n",
      "observations in class j, and k is the total number of classes.\n",
      "\n",
      "278 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Support Vector Machines\n",
      "\n",
      "\f",
      "CHAPTER 18\n",
      "Naive Bayes\n",
      "\n",
      "18.0 Introduction\n",
      "Bayes’  theorem  is  the  premier  method  for  understanding  the  probability  of  some\n",
      "event, P(A | B), given some new information, P(B | A), and a prior belief in the proba‐\n",
      "bility of the event, P(A):\n",
      "\n",
      "P A ∣ B =\n",
      "\n",
      "P B ∣ A P A\n",
      "P B\n",
      "\n",
      "The Bayesian method’s popularity has skyrocketed in the last decade, more and more\n",
      "rivaling  traditional  frequentist  applications  in  academia,  government,  and  business.\n",
      "In machine learning, one application of Bayes’ theorem to classification comes in the\n",
      "form of the naive Bayes classifier. Naive Bayes classifiers combine a number of desira‐\n",
      "ble qualities in practical machine learning into a single classifier. These include:\n",
      "\n",
      "1. An intuitative approach\n",
      "\n",
      "2. The ability to work with small data\n",
      "\n",
      "3. Low computation costs for training and prediction\n",
      "\n",
      "4. Often solid results in a variety of settings.\n",
      "\n",
      "Specifically, a naive Bayes classifier is based on:\n",
      "\n",
      "P y ∣ x1, ⋯, x j =\n",
      "\n",
      "P x1, ⋯x j\n",
      "\n",
      "∣ y P y\n",
      "\n",
      "P x1, ⋯, x j\n",
      "\n",
      "where:\n",
      "\n",
      "279\n",
      "\n",
      "\f",
      "• P(y | x1, ⋯, xj) is called the posterior and is the probability that an observation is\n",
      "\n",
      "class y given the observation’s values for the j features, x1, ⋯, xj.\n",
      "\n",
      "• P(x1, ...xj | y) is called likelihood and is the likelihood of an observation’s values for\n",
      "\n",
      "features, x1, ..., xj, given their class, y.\n",
      "\n",
      "• P(y) is called the prior and is our belief for the probability of class y before look‐\n",
      "\n",
      "ing at the data.\n",
      "\n",
      "• P(x1, ..., xj) is called the marginal probability.\n",
      "\n",
      "In naive Bayes, we compare an observation’s posterior values for each possible class.\n",
      "Specifically,  because  the  marginal  probability  is  constant  across  these  comparisons,\n",
      "we compare the numerators of the posterior for each class. For each observation, the\n",
      "class with the greatest posterior numerator becomes the predicted class, ŷ.\n",
      "\n",
      "There are two important things to note about naive Bayes classifiers. First, for each\n",
      "feature  in  the  data,  we  have  to  assume  the  statistical  distribution  of  the  likelihood,\n",
      "P(xj  |  y).  The  common  distributions  are  the  normal  (Gaussian),  multinomial,  and\n",
      "Bernoulli distributions. The distribution chosen is often determined by the nature of\n",
      "features  (continuous,  binary,  etc.).  Second,  naive  Bayes  gets  its  name  because  we\n",
      "assume  that  each  feature,  and  its  resulting  likelihood,  is  independent.  This  “naive”\n",
      "assumption is frequently wrong, yet in practice does little to prevent building high-\n",
      "quality classifiers.\n",
      "\n",
      "In this chapter we will cover using scikit-learn to train three types of naive Bayes clas‐\n",
      "sifiers using three different likelihood distributions.\n",
      "\n",
      "18.1 Training a Classifier for Continuous Features\n",
      "\n",
      "Problem\n",
      "You have only continuous features and you want to train a naive Bayes classifier.\n",
      "\n",
      "Solution\n",
      "Use a Gaussian naive Bayes classifier in scikit-learn:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create Gaussian Naive Bayes object\n",
      "\n",
      "280 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 18: Naive Bayes\n",
      "\n",
      "\f",
      "classifer = GaussianNB()\n",
      "\n",
      "# Train model\n",
      "model = classifer.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "The most common type of naive Bayes classifier is the Gaussian naive Bayes. In Gaus‐\n",
      "sian  naive  Bayes,  we  assume  that  the  likelihood  of  the  feature  values,  x,  given  an\n",
      "observation is of class y, follows a normal distribution:\n",
      "\n",
      "2\n",
      "\n",
      "x\n",
      "\n",
      "j\n",
      "\n",
      "− μ\n",
      "\n",
      "y\n",
      "\n",
      "2σ\n",
      "\n",
      "2\n",
      "y\n",
      "\n",
      "−\n",
      "\n",
      "e\n",
      "\n",
      "p x j\n",
      "\n",
      "∣ y =\n",
      "\n",
      "1\n",
      "2\n",
      "2πσ y\n",
      "\n",
      "2 and μy are the variance and mean values of feature xj for class y. Because of\n",
      "where σy\n",
      "the assumption of the normal distribution, Gaussian naive Bayes is best used in cases\n",
      "when all our features are continuous.\n",
      "\n",
      "In scikit-learn, we train a Gaussian naive Bayes like any other model using fit, and in\n",
      "turn can then make predictions about the class of an observation:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[ 4,  4,  4,  0.4]]\n",
      "\n",
      "# Predict class\n",
      "model.predict(new_observation)\n",
      "\n",
      "array([1])\n",
      "\n",
      "One of the interesting aspects of naive Bayes classifiers is that they allow us to assign a\n",
      "prior belief over the respected target classes. We can do this using GaussianNB’s pri\n",
      "ors parameter, which takes in a list of the probabilities assigned to each class of the\n",
      "target vector:\n",
      "\n",
      "# Create Gaussian Naive Bayes object with prior probabilities of each class\n",
      "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
      "\n",
      "# Train model\n",
      "model = classifer.fit(features, target)\n",
      "\n",
      "If we do not add any argument to the priors parameter, the prior is adjusted based\n",
      "on the data.\n",
      "\n",
      "Finally, note that the raw predicted probabilities from Gaussian naive Bayes (output‐\n",
      "ted using predict_proba) are not calibrated. That is, they should not be believed. If\n",
      "\n",
      "18.1 Training a Classifier for Continuous Features \n",
      "\n",
      "| \n",
      "\n",
      "281\n",
      "\n",
      "\f",
      "we want to create useful predicted probabilities, we will need to calibrate them using\n",
      "an isotonic regression or a related method.\n",
      "\n",
      "See Also\n",
      "\n",
      "• How the Naive Bayes Classifier Works in Machine Learning, Dataaspirant\n",
      "\n",
      "18.2 Training a Classifier for Discrete and Count Features\n",
      "\n",
      "Problem\n",
      "Given discrete or count data, you need to train a naive Bayes classifier.\n",
      "\n",
      "Solution\n",
      "Use a multinomial naive Bayes classifier:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Create text\n",
      "text_data = np.array(['I love Brazil. Brazil!',\n",
      "                      'Brazil is best',\n",
      "                      'Germany beats both'])\n",
      "\n",
      "# Create bag of words\n",
      "count = CountVectorizer()\n",
      "bag_of_words = count.fit_transform(text_data)\n",
      "\n",
      "# Create feature matrix\n",
      "features = bag_of_words.toarray()\n",
      "\n",
      "# Create target vector\n",
      "target = np.array([0,0,1])\n",
      "\n",
      "# Create multinomial naive Bayes object with prior probabilities of each class\n",
      "classifer = MultinomialNB(class_prior=[0.25, 0.5])\n",
      "\n",
      "# Train model\n",
      "model = classifer.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "Multinomial naive Bayes works similarly to Gaussian naive Bayes, but the features are\n",
      "assumed to be multinomially distributed. In practice, this means that this classifier is\n",
      "commonly used when we have discrete data (e.g., movie ratings ranging from 1 to 5).\n",
      "\n",
      "282 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 18: Naive Bayes\n",
      "\n",
      "\f",
      "One of the most common uses of multinomial naive Bayes is text classification using\n",
      "bags of words or tf-idf approaches (see Recipes 6.8 and 6.9).\n",
      "\n",
      "In our solution, we created a toy text dataset of three observations, and converted the\n",
      "text  strings  into  a  bag-of-words  feature  matrix  and  an  accompanying  target  vector.\n",
      "We then used MultinomialNB to train a model while defining the prior probabilities\n",
      "for the two classes (pro-brazil and pro-germany).\n",
      "\n",
      "MultinomialNB  works  similarly  to  GaussianNB;  models  are  trained  using  fit,  and\n",
      "observations can be predicted using predict:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[0, 0, 0, 1, 0, 1, 0]]\n",
      "\n",
      "# Predict new observation's class\n",
      "model.predict(new_observation)\n",
      "\n",
      "array([0])\n",
      "\n",
      "If class_prior is not specified, prior probabilities are learned using the data. How‐\n",
      "ever,  if  we  want  a  uniform  distribution  to  be  used  as  the  prior,  we  can  set\n",
      "fit_prior=False.\n",
      "\n",
      "Finally, MultinomialNB contains an additive smoothing hyperparameter, alpha, that\n",
      "should  be  tuned.  The  default  value  is  1.0,  with  0.0  meaning  no  smoothing  takes \n",
      "place.\n",
      "\n",
      "18.3 Training a Naive Bayes Classifier for Binary Features\n",
      "\n",
      "Problem\n",
      "You have binary feature data and need to train a naive Bayes classifier.\n",
      "\n",
      "Solution\n",
      "Use a Bernoulli naive Bayes classifier:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "\n",
      "# Create three binary features\n",
      "features = np.random.randint(2, size=(100, 3))\n",
      "\n",
      "# Create a binary target vector\n",
      "target = np.random.randint(2, size=(100, 1)).ravel()\n",
      "\n",
      "# Create Bernoulli Naive Bayes object with prior probabilities of each class\n",
      "classifer = BernoulliNB(class_prior=[0.25, 0.5])\n",
      "\n",
      "18.3 Training a Naive Bayes Classifier for Binary Features \n",
      "\n",
      "| \n",
      "\n",
      "283\n",
      "\n",
      "\f",
      "# Train model\n",
      "model = classifer.fit(features, target)\n",
      "\n",
      "Discussion\n",
      "The Bernoulli naive Bayes classifier assumes that all our features are binary such that\n",
      "they  take  only  two  values  (e.g.,  a  nominal  categorical  feature  that  has  been  one-hot\n",
      "encoded).  Like  its  multinomial  cousin,  Bernoulli  naive  Bayes  is  often  used  in  text\n",
      "classification, when our feature matrix is simply the presence or absence of a word in\n",
      "a document. Furthermore, like MultinomialNB, BernoulliNB has an additive smooth‐\n",
      "ing  hyperparameter,  alpha,  we  will  want  to  tune  using  model  selection  techniques.\n",
      "Finally,  if  we  want  to  use  priors  we  can  use  the  class_prior  parameter  with  a  list\n",
      "containing the prior probabilities for each class. If we want to specify a uniform prior,\n",
      "we can set fit_prior=False:\n",
      "\n",
      "model_uniform_prior = BernoulliNB(class_prior=None, fit_prior=True)\n",
      "\n",
      "18.4 Calibrating Predicted Probabilities\n",
      "\n",
      "Problem\n",
      "You want to calibrate the predicted probabilities from naive Bayes classifiers so they\n",
      "are interpretable.\n",
      "\n",
      "Solution\n",
      "Use CalibratedClassifierCV:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.calibration import CalibratedClassifierCV\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create Gaussian Naive Bayes object\n",
      "classifer = GaussianNB()\n",
      "\n",
      "# Create calibrated cross-validation with sigmoid calibration\n",
      "classifer_sigmoid = CalibratedClassifierCV(classifer, cv=2, method='sigmoid')\n",
      "\n",
      "# Calibrate probabilities\n",
      "classifer_sigmoid.fit(features, target)\n",
      "\n",
      "# Create new observation\n",
      "\n",
      "284 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 18: Naive Bayes\n",
      "\n",
      "\f",
      "new_observation = [[ 2.6,  2.6,  2.6,  0.4]]\n",
      "\n",
      "# View calibrated probabilities\n",
      "classifer_sigmoid.predict_proba(new_observation)\n",
      "\n",
      "array([[ 0.31859969,  0.63663466,  0.04476565]])\n",
      "\n",
      "Discussion\n",
      "Class  probabilities  are  a  common  and  useful  part  of  machine  learning  models.  In\n",
      "scikit-learn,  most  learning  algortihms  allow  us  to  see  the  predicted  probabilities  of\n",
      "class membership using predict_proba. This can be extremely useful if, for instance,\n",
      "we want to only predict a certain class if the model predicts the probability that it is\n",
      "that class is over 90%. However, some models, including naive Bayes classifiers, out‐\n",
      "put probabilities that are not based on the real world. That is, predict_proba might\n",
      "predict an observation has a 0.70 chance of being a certain class, when the reality is\n",
      "that it is 0.10 or 0.99. Specifically in naive Bayes, while the ranking of predicted prob‐\n",
      "abilities for the different target classes is valid, the raw predicted probabilities tend to\n",
      "take on extreme values close to 0 and 1.\n",
      "\n",
      "To obtain meaningful predicted probabilities we need conduct what is called calibra‐\n",
      "tion.  In  scikit-learn  we  can  use  the  CalibratedClassifierCV  class  to  create  well-\n",
      "calibrated predicted probabilities using k-fold cross-validation. In CalibratedClassi\n",
      "fierCV the training sets are used to train the model and the test set is used to calibrate\n",
      "the predicted probabilities. The returned predicted probabilities are the average of the\n",
      "k-folds.\n",
      "\n",
      "Using our solution we can see the difference between raw and well-calibrated predic‐\n",
      "ted probabilities. In our solution, we created a Gaussian naive Bayes classifier. If we\n",
      "train that classifier and then predict the class probabilities for a new observation, we\n",
      "can see very extreme probability estimates:\n",
      "\n",
      "# Train a Gaussian naive Bayes then predict class probabilities\n",
      "classifer.fit(features, target).predict_proba(new_observation)\n",
      "\n",
      "array([[  2.58229098e-04,   9.99741447e-01,   3.23523643e-07]])\n",
      "\n",
      "However, if after we calibrate the predicted probabilities (which we did in our solu‐\n",
      "tion), we get very different results:\n",
      "\n",
      "# View calibrated probabilities\n",
      "classifer_sigmoid.predict_proba(new_observation)\n",
      "\n",
      "array([[ 0.31859969,  0.63663466,  0.04476565]])\n",
      "\n",
      "CalibratedClassifierCV offers two calibration methods—Platt’s sigmoid model and\n",
      "isotonic  regression—defined  by  the  method  paramenter.  While  we  don’t  have  the\n",
      "space to go into the specifics, because isotonic regression is nonparametric it tends to\n",
      "overfit when sample sizes are very small (e.g., 100 observations). In our solution we\n",
      "\n",
      "18.4 Calibrating Predicted Probabilities \n",
      "\n",
      "| \n",
      "\n",
      "285\n",
      "\n",
      "\f",
      "used  the  Iris  dataset  with  150  observations  and  therefore  used  the  Platt’s  sigmoid\n",
      "model.\n",
      "\n",
      "286 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 18: Naive Bayes\n",
      "\n",
      "\f",
      "CHAPTER 19\n",
      "Clustering\n",
      "\n",
      "19.0 Introduction\n",
      "In much of this book we have looked at supervised machine learning—where we have\n",
      "access to both the features and the target. This is, unfortunately, not always the case.\n",
      "Frequently,  we  run  into  situations  where  we  only  know  the  features.  For  example,\n",
      "imagine we have records of sales from a grocery store and we want to break up sales\n",
      "by whether or not the shopper is a member of a discount club. This would be impos‐\n",
      "sible  using  supervised  learning  because  we  don’t  have  a  target  to  train  and  evaluate\n",
      "our models. However, there is another option: unsupervised learning. If the behavior\n",
      "of discount club members and nonmembers in the grocery store is actually disparate,\n",
      "then  the  average  difference  in  behavior  between  two  members  will  be  smaller  than\n",
      "the average difference in behavior between a member and nonmember shopper. Put\n",
      "another way, there will be two clusters of observations.\n",
      "\n",
      "The goal of clustering algorithms is to identify those latent groupings of observations,\n",
      "which if done well, allow us to predict the class of observations even without a target\n",
      "vector.  There  are  many  clustering  algorithms  and  they  have  a  wide  variety  of\n",
      "approaches to identifying the clusters in data. In this chapter, we will cover a selection\n",
      "of clustering algorithms using scikit-learn and how to use them in practice.\n",
      "\n",
      "19.1 Clustering Using K-Means\n",
      "\n",
      "Problem\n",
      "You want to group observations into k groups.\n",
      "\n",
      "287\n",
      "\n",
      "\f",
      "Solution\n",
      "Use k-means clustering:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "# Create k-mean object\n",
      "cluster = KMeans(n_clusters=3, random_state=0, n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = cluster.fit(features_std)\n",
      "\n",
      "Discussion\n",
      "k-means  clustering  is  one  of  the  most  common  clustering  techniques.  In  k-means\n",
      "clustering,  the  algorithm  attempts  to  group  observations  into  k  groups,  with  each\n",
      "group  having  roughly  equal  variance.  The  number  of  groups,  k,  is  specified  by  the\n",
      "user as a hyperparameter. Specifically, in k-means:\n",
      "\n",
      "1. k cluster “center” points are created at random locations.\n",
      "\n",
      "2. For each observation:\n",
      "\n",
      "a. The distance between each observation and the k center points is calculated.\n",
      "\n",
      "b. The observation is assigned to the cluster of the nearest center point.\n",
      "\n",
      "3. The  center  points  are  moved  to  the  means  (i.e.,  centers)  of  their  respective\n",
      "\n",
      "clusters.\n",
      "\n",
      "4. Steps 2 and 3 are repeated until no observation changes in cluster membership.\n",
      "\n",
      "At this point the algorithm is considered converged and stops.\n",
      "\n",
      "It is important to note three things about k-means. First, k-means clustering assumes\n",
      "the clusters are convex shaped (e.g., a circle, a sphere). Second, all features are equally\n",
      "scaled. In our solution, we standardized the features to meet this assumption. Third,\n",
      "the groups are balanced (i.e., have roughly the same number of observations). If we\n",
      "suspect  that  we  cannot  meet  these  assumptions,  we  might  try  other  clustering\n",
      "approaches.\n",
      "\n",
      "288 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Clustering\n",
      "\n",
      "\f",
      "In  scikit-learn,  k-means  clustering  is  implemented  in  the  KMeans  class.  The  most\n",
      "important parameter is n_clusters, which sets the number of clusters k. In some sit‐\n",
      "uations, the nature of the data will determine the value for k (e.g., data on a school’s\n",
      "students  will  have  one  cluster  per  grade),  but  often  we  don’t  know  the  number  of\n",
      "clusters.  In  these  cases,  we  will  want  to  select  k  based  on  using  some  criteria.  For\n",
      "example, silhouette coefficients (see Recipe 11.9) measure the similarity within clus‐\n",
      "ters  compared  with  the  similarity  between  clusters.  Furthermore,  because  k-means\n",
      "clustering  is  computationally  expensive,  we  might  want  to  take  advantage  of  all  the\n",
      "cores on our computer. We can do this by setting n_jobs=-1.\n",
      "\n",
      "In our solution, we cheated a little and used the Iris flower data, in which we know\n",
      "there are three classes. Therefore, we set k = 3. We can use labels_ to see the predic‐\n",
      "ted classes of each observation:\n",
      "\n",
      "# View predict class\n",
      "model.labels_\n",
      "\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2,\n",
      "       0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
      "       2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)\n",
      "\n",
      "If we compare this to the observation’s true class we can see that despite the difference\n",
      "in class labels (i.e., 1, 2, and 3), k-means did reasonably well:\n",
      "\n",
      "# View true class\n",
      "iris.target\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "However,  as  you  might  imagine,  the  performance  of  k-means  drops  considerably,\n",
      "even critically, if we select the wrong number of clusters.\n",
      "\n",
      "Finally, as with other scikit-learns we can use the trained cluster to predict the value\n",
      "of new observations:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[0.8, 0.8, 0.8, 0.8]]\n",
      "\n",
      "# Predict observation's cluster\n",
      "model.predict(new_observation)\n",
      "\n",
      "array([0], dtype=int32)\n",
      "\n",
      "19.1 Clustering Using K-Means \n",
      "\n",
      "| \n",
      "\n",
      "289\n",
      "\n",
      "\f",
      "The observation is predicted to belong to the cluster whose center point is closest. We\n",
      "can even use cluster_centers_ to see those center points:\n",
      "\n",
      "# View cluster centers\n",
      "model.cluster_centers_\n",
      "\n",
      "array([[ 1.13597027,  0.09659843,  0.996271  ,  1.01717187],\n",
      "       [-1.01457897,  0.84230679, -1.30487835, -1.25512862],\n",
      "       [-0.05021989, -0.88029181,  0.34753171,  0.28206327]])\n",
      "\n",
      "See Also\n",
      "\n",
      "• Introduction to K-means Clustering, DataScience.com\n",
      "\n",
      "19.2 Speeding Up K-Means Clustering\n",
      "\n",
      "Problem\n",
      "You want to group observations into k groups, but k-means takes too long.\n",
      "\n",
      "Solution\n",
      "Use mini-batch k-means:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "# Create k-mean object\n",
      "cluster = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=100)\n",
      "\n",
      "# Train model\n",
      "model = cluster.fit(features_std)\n",
      "\n",
      "Discussion\n",
      "Mini-batch  k-means  works  similarly  to  the  k-means  algorithm  discussed  in  Recipe\n",
      "19.1.  Without  going  into  too  much  detail,  the  difference  is  that  in  mini-batch\n",
      "k-means the most computationally costly step is conducted on only a random sample\n",
      "\n",
      "290 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Clustering\n",
      "\n",
      "\f",
      "of observations as opposed to all observations. This approach can significantly reduce\n",
      "the time required for the algorithm to find convergence (i.e., fit the data) with only a\n",
      "small cost in quality.\n",
      "\n",
      "MiniBatchKMeans  works  similarly  to  KMeans,  with  one  significant  difference:  the\n",
      "batch_size parameter. batch_size controls the number of randomly selected obser‐\n",
      "vations  in  each  batch.  The  larger  the  size  of  the  batch,  the  more  computationally\n",
      "costly the training process.\n",
      "\n",
      "19.3 Clustering Using Meanshift\n",
      "\n",
      "Problem\n",
      "You  want  to  group  observations  without  assuming  the  number  of  clusters  or  their\n",
      "shape.\n",
      "\n",
      "Solution\n",
      "Use meanshift clustering:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cluster import MeanShift\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "# Create meanshift object\n",
      "cluster = MeanShift(n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = cluster.fit(features_std)\n",
      "\n",
      "Discussion\n",
      "One  of  the  disadvantages  of  k-means  clustering  we  discussed  previously  is  that  we\n",
      "needed  to  set  the  number  of  clusters,  k,  prior  to  training,  and  the  method  made\n",
      "assumptions about the shape of the clusters. One clustering algorithm without these\n",
      "limitations is meanshift.\n",
      "\n",
      "Meanshift is a simple concept, but somewhat difficult to explain. Therefore, an anal‐\n",
      "ogy  might  be  the  best  approach.  Imagine  a  very  foggy  football  field  (i.e.,  a  two-\n",
      "\n",
      "19.3 Clustering Using Meanshift \n",
      "\n",
      "| \n",
      "\n",
      "291\n",
      "\n",
      "\f",
      "dimensional  feature  space)  with  100  people  standing  on  it  (i.e.,  our  observations).\n",
      "Because it is foggy, a person can only see a short distance. Every minute each person\n",
      "looks  around  and  takes  a  step  in  the  direction  of  the  most  people  they  can  see.  As\n",
      "time goes on, people start to group up as they repeatedly take steps toward larger and\n",
      "larger  crowds.  The  end  result  is  clusters  of  people  around  the  field.  People  are\n",
      "assigned to the clusters in which they end up.\n",
      "\n",
      "scikit-learn’s  actual  implementation  of  meanshift,  MeanShift,  is  more  complex  but\n",
      "follows the same basic logic. MeanShift has two important parameters we should be\n",
      "aware of. First, bandwidth sets the radius of the area (i.e., kernel) an observation uses\n",
      "to determine the direction to shift. In our analogy, bandwidth was how far a person\n",
      "could see through the fog. We can set this parameter manually, but by default a rea‐\n",
      "sonable bandwidth is estimated automatically (with a significant increase in computa‐\n",
      "tional cost). Second, sometimes in meanshift there are no other observations within\n",
      "an  observation’s  kernel.  That  is,  a  person  on  our  football  field  cannot  see  a  single\n",
      "other  person.  By  default,  MeanShift  assigns  all  these  “orphan”  observations  to  the\n",
      "kernel of the nearest observation. However, if we want to leave out these orphans, we\n",
      "can set cluster_all=False wherein orphan observations are given the label of -1.\n",
      "\n",
      "See Also\n",
      "\n",
      "• The mean shift clustering algorithm, EFAVDB\n",
      "\n",
      "19.4 Clustering Using DBSCAN\n",
      "\n",
      "Problem\n",
      "You want to group observations into clusters of high density.\n",
      "\n",
      "Solution\n",
      "Use DBSCAN clustering:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cluster import DBSCAN\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "292 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Clustering\n",
      "\n",
      "\f",
      "# Create meanshift object\n",
      "cluster = DBSCAN(n_jobs=-1)\n",
      "\n",
      "# Train model\n",
      "model = cluster.fit(features_std)\n",
      "\n",
      "Discussion\n",
      "DBSCAN is motivated by the idea that clusters will be areas where many observations\n",
      "are densely packed together and makes no assumptions of cluster shape. Specifically,\n",
      "in DBSCAN:\n",
      "\n",
      "1. A random observation, xi, is chosen.\n",
      "\n",
      "2. If  xi  has  a  minimum  number  of  close  neighbors,  we  consider  it  to  be  part  of  a\n",
      "\n",
      "cluster.\n",
      "\n",
      "3. Step  2  is  repeated  recursively  for  all  of  xi’s  neighbors,  then  neighbor’s  neighbor,\n",
      "\n",
      "and so on. These are the cluster’s core observations.\n",
      "\n",
      "4. Once step 3 runs out of nearby observations, a new random point is chosen (i.e.,\n",
      "\n",
      "restarting step 1).\n",
      "\n",
      "Once  this  is  complete,  we  have  a  set  of  core  observations  for  a  number  of  clusters.\n",
      "Finally, any observation close to a cluster but not a core sample is considered part of a\n",
      "cluster, while any observation not close to the cluster is labeled an outlier.\n",
      "\n",
      "DBSCAN has three main parameters to set:\n",
      "\n",
      "eps\n",
      "\n",
      "The maximum distance from an observation for another observation to be con‐\n",
      "sidered its neighbor.\n",
      "\n",
      "min_samples\n",
      "\n",
      "The minimum number of observations less than eps distance from an observa‐\n",
      "tion for it to be considered a core observation.\n",
      "\n",
      "metric\n",
      "\n",
      "The  distance  metric  used  by  eps—for  example,  minkowski  or  euclidean  (note\n",
      "that if Minkowski distance is used, the parameter p can be used to set the power\n",
      "of the Minkowski metric).\n",
      "\n",
      "If we look at the clusters in our training data we can see two clusters have been identi‐\n",
      "fied, 0 and 1, while outlier observations are labeled -1:\n",
      "\n",
      "# Show cluster membership\n",
      "model.labels_\n",
      "\n",
      "19.4 Clustering Using DBSCAN \n",
      "\n",
      "| \n",
      "\n",
      "293\n",
      "\n",
      "\f",
      "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,\n",
      "        0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
      "        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
      "        1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,\n",
      "       -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "       -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
      "        1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,\n",
      "       -1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,\n",
      "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1])\n",
      "\n",
      "See Also\n",
      "\n",
      "• DBSCAN, Wikipedia\n",
      "\n",
      "19.5 Clustering Using Hierarchical Merging\n",
      "\n",
      "Problem\n",
      "You want to group observations using a hierarchy of clusters.\n",
      "\n",
      "Solution\n",
      "Use agglomerative clustering:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "\n",
      "# Standardize features\n",
      "scaler = StandardScaler()\n",
      "features_std = scaler.fit_transform(features)\n",
      "\n",
      "# Create meanshift object\n",
      "cluster = AgglomerativeClustering(n_clusters=3)\n",
      "\n",
      "# Train model\n",
      "model = cluster.fit(features_std)\n",
      "\n",
      "Discussion\n",
      "Agglomerative clustering is a powerful, flexible hierarchical clustering algorithm. In\n",
      "agglomerative  clustering,  all  observations  start  as  their  own  clusters.  Next,  clusters\n",
      "meeting some criteria are merged together. This process is repeated, growing clusters\n",
      "\n",
      "294 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Clustering\n",
      "\n",
      "\f",
      "until some end point is reached. In scikit-learn, AgglomerativeClustering uses the\n",
      "linkage parameter to determine the merging strategy to minimize the following:\n",
      "\n",
      "1. Variance of merged clusters (ward)\n",
      "2. Average distance between observations from pairs of clusters (average)\n",
      "3. Maximum distance between observations from pairs of clusters (complete)\n",
      "\n",
      "Two other parameters are useful to know. First, the affinity parameter determines \n",
      "the  distance  metric  used  for  linkage  (minkowski,  euclidean,  etc.).  Second,  n_clus\n",
      "ters sets the number of clusters the clustering algorithm will attempt to find. That is,\n",
      "clusters are successively merged until there are only n_clusters remaining.\n",
      "\n",
      "As with other clustering algorithms we have covered, we can use labels_ to see the\n",
      "cluster in which every observation is assigned:\n",
      "\n",
      "# Show cluster membership\n",
      "model.labels_\n",
      "\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2,\n",
      "       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0,\n",
      "       2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "19.5 Clustering Using Hierarchical Merging \n",
      "\n",
      "| \n",
      "\n",
      "295\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 20\n",
      "Neural Networks\n",
      "\n",
      "20.0 Introduction\n",
      "At the heart of neural networks is the unit (also called a node or neuron). A unit takes\n",
      "in  one  or  more  inputs,  multiplies  each  input  by  a  parameter  (also  called  a  weight),\n",
      "sums  the  weighted  input’s  values  along  with  some  bias  value  (typically  1),  and  then\n",
      "feeds  the  value  into  an  activation  function.  This  output  is  then  sent  forward  to  the\n",
      "other neurals deeper in the neural network (if they exist).\n",
      "\n",
      "Feedforward  neural  networks—also  called  multilayer  perceptron—are  the  simplest\n",
      "artificial  neural  network  used  in  any  real-world  setting.  Neural  networks  can  be\n",
      "visualized as a series of connected layers that form a network connecting an observa‐\n",
      "tion’s  feature  values  at  one  end,  and  the  target  value  (e.g.,  observation’s  class)  at  the\n",
      "other  end.  The  name  feedforward  comes  from  the  fact  that  an  observation’s  feature\n",
      "values are fed “forward” through the network, with each layer successively transform‐\n",
      "ing the feature values with the goal that the output at the end is the same as the tar‐\n",
      "get’s value.\n",
      "\n",
      "Specifically, feedforward neural networks contain three types of layers of units. At the\n",
      "start of the neural network is an input layer where each unit contains an observation’s\n",
      "value for a single feature. For example, if an observation has 100 features, the input\n",
      "layer has 100 nodes. At the end of the neural network is the output layer, which trans‐\n",
      "forms  the  output  of  the  hidden  layers  into  values  useful  for  the  task  at  hand.  For\n",
      "example, if our goal was binary classification, we could use an output layer with a sin‐\n",
      "gle unit that uses a sigmoid function to scale its own output to between 0 and 1, rep‐\n",
      "resenting  a  predicted  class  probability.  Between  the  input  and  output  layers  are  the\n",
      "so-called “hidden” layers (which aren’t hidden at all). These hidden layers successively\n",
      "transform the feature values from the input layer to something that, once processed\n",
      "by  the  output  layer,  resembles  the  target  class.  Neural  networks  with  many  hidden\n",
      "\n",
      "297\n",
      "\n",
      "\f",
      "layers  (e.g.,  10,  100,  1,000)  are  considered  “deep”  networks  and  their  application  is\n",
      "called deep learning.\n",
      "\n",
      "Neural networks are typically created with all parameters initialized as small random\n",
      "values from a Gaussian or normal uniform. Once an observation (or more often a set\n",
      "number  of  observations  called  a  batch)  is  fed  through  the  network,  the  outputted\n",
      "value  is  compared  with  the  observation’s  true  value  using  a  loss  function.  This  is\n",
      "called forward propagation. Next an algorithm goes “backward” through the network\n",
      "identifying how much each parameter contributed to the error between the predicted\n",
      "and  true  values,  a  process  called  backpropagation.  At  each  parameter,  the  optimiza‐\n",
      "tion algorithm determines how much each weight should be adjusted to improve the\n",
      "output.\n",
      "\n",
      "Neural networks learn by repeating this process of forward propagation and backpro‐\n",
      "pagation  for  every  observation  in  the  training  data  multiple  times  (each  time  all\n",
      "observations have been sent through the network is called an epoch and training typi‐\n",
      "cally consists of multiple epochs), iteratively updating the values of the parameters.\n",
      "\n",
      "In this chapter, we will use the popular Python library Keras to build, train, and eval‐\n",
      "uate  a  variety  of  neural  networks.  Keras  is  a  high-level  library,  using  other  libraries\n",
      "like TensorFlow and Theano as its “engine.” For us the advantage of Keras is that we\n",
      "can focus on network design and training, leaving the specifics of the tensor opera‐\n",
      "tions to the other libraries.\n",
      "\n",
      "Neural  networks  created  using  Keras  code  can  be  trained  using  both  CPUs  (i.e.,  on\n",
      "your  laptop)  and  GPUs  (i.e.,  on  a  specialized  deep  learning  computer).  In  the  real\n",
      "world with real data, it is highly advisable to train neural networks using GPUs; how‐\n",
      "ever, for the sake of learning, all the neural networks in this book are small and sim‐\n",
      "ple  enough  to  be  trained  on  your  laptop  in  only  a  few  minutes.  Just  be  aware  that\n",
      "when we have larger networks and more training data, training using CPUs is signifi‐\n",
      "cantly slower than training using GPUs.\n",
      "\n",
      "20.1 Preprocessing Data for Neural Networks\n",
      "\n",
      "Problem\n",
      "You want to preprocess data for use in a neural network.\n",
      "\n",
      "Solution\n",
      "Standardize each feature using scikit-learn’s StandardScaler:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn import preprocessing\n",
      "import numpy as np\n",
      "\n",
      "298 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "# Create feature\n",
      "features = np.array([[-100.1, 3240.1],\n",
      "                     [-200.2, -234.1],\n",
      "                     [5000.5, 150.1],\n",
      "                     [6000.6, -125.1],\n",
      "                     [9000.9, -673.1]])\n",
      "\n",
      "# Create scaler\n",
      "scaler = preprocessing.StandardScaler()\n",
      "\n",
      "# Transform the feature\n",
      "features_standardized = scaler.fit_transform(features)\n",
      "\n",
      "# Show feature\n",
      "features_standardized\n",
      "\n",
      "array([[-1.12541308,  1.96429418],\n",
      "       [-1.15329466, -0.50068741],\n",
      "       [ 0.29529406, -0.22809346],\n",
      "       [ 0.57385917, -0.42335076],\n",
      "       [ 1.40955451, -0.81216255]])\n",
      "\n",
      "Discussion\n",
      "While this recipe is very similar to Recipe 4.3, it is worth repeating because of how\n",
      "important it is for neural networks. Typically, a neural network’s parameters are ini‐\n",
      "tialized (i.e., created) as small random numbers. Neural networks often behave poorly\n",
      "when  the  feature  values  are  much  larger  than  parameter  values.  Furthermore,  since\n",
      "an observation’s feature values are combined as they pass through individual units, it\n",
      "is important that all features have the same scale.\n",
      "\n",
      "For  these  reasons,  it  is  best  practice  (although  not  always  necessary;  for  example,\n",
      "when we have all binary features) to standardize each feature such that the feature’s\n",
      "values have the mean of 0 and the standard deviation of 1. This can be easily accom‐\n",
      "plished with scikit-learn’s StandardScaler.\n",
      "\n",
      "You can see the effect of the standardization by checking the mean and standard devi‐\n",
      "ation of our first features:\n",
      "\n",
      "# Print mean and standard deviation\n",
      "print(\"Mean:\", round(features_standardized[:,0].mean()))\n",
      "print('\"Standard deviation:\", features_standardized[:,0].std())\n",
      "\n",
      "Mean: 0.0\n",
      "Standard deviation: 1.0\n",
      "\n",
      "20.1 Preprocessing Data for Neural Networks \n",
      "\n",
      "| \n",
      "\n",
      "299\n",
      "\n",
      "\f",
      "20.2 Designing a Neural Network\n",
      "\n",
      "Problem\n",
      "You want to design a neural network.\n",
      "\n",
      "Solution\n",
      "Use Keras’ Sequential model:\n",
      "\n",
      "# Load libraries\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "Neural networks consist of layers of units. However, there is incredible variety in the\n",
      "types  of  layers  and  how  they  are  combined  to  form  the  network’s  architecture.  At\n",
      "present, while there are commonly used architecture patterns (which we will cover in\n",
      "this chapter), the truth is that selecting the right architecture is mostly an art and the\n",
      "topic of much research.\n",
      "\n",
      "To construct a feedforward neural network in Keras, we need to make a number of\n",
      "choices  about  both  the  network  architecture  and  training  process.  Remember  that\n",
      "each unit in the hidden layers:\n",
      "\n",
      "1. Receives a number of inputs.\n",
      "\n",
      "2. Weights each input by a parameter value.\n",
      "\n",
      "3. Sums together all weighted inputs along with some bias (typically 1).\n",
      "\n",
      "300 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "4. Most often then applies some function (called an activation function).\n",
      "\n",
      "5. Sends the output on to units in the next layer.\n",
      "\n",
      "First,  for  each  layer  in  the  hidden  and  output  layers  we  must  define  the  number  of\n",
      "units to include in the layer and the activation function. Overall, the more units we\n",
      "have  in  a  layer,  the  more  our  network  is  able  to  learn  complex  patterns.  However,\n",
      "more units might make our network overfit the training data in a way detrimental to\n",
      "the performance on the test data.\n",
      "\n",
      "For hidden layers, a popular activation function is the rectified linear unit (ReLU):\n",
      "\n",
      "f z = max 0, z\n",
      "\n",
      "where z is the sum of the weighted inputs and bias. As we can see, if z is greater than\n",
      "0,  the  activation  function  returns  z;  otherwise,  the  function  returns  0.  This  simple\n",
      "activation  function  has  a  number  of  desirable  properties  (a  discussion  of  which  is\n",
      "beyond the scope of this book) and this has made it a popular choice in neural net‐\n",
      "works. We should be aware, however, that many dozens of activation functions exist.\n",
      "\n",
      "Second, we need to define the number of hidden layers to use in the network. More\n",
      "layers  allow  the  network  to  learn  more  complex  relationships,  but  with  a  computa‐\n",
      "tional cost.\n",
      "\n",
      "Third, we have to define the structure of the activation function (if any) of the output\n",
      "layer. The nature of the output function is often determined by the goal of the net‐\n",
      "work. Here are some common output layer patterns:\n",
      "\n",
      "Binary classification\n",
      "\n",
      "One unit with a sigmoid activation function.\n",
      "\n",
      "Multiclass classification\n",
      "\n",
      "k units (where k is the number of target classes) and a softmax activation func‐\n",
      "tion.\n",
      "\n",
      "Regression\n",
      "\n",
      "One unit with no activation function.\n",
      "\n",
      "Fourth, we need to define a loss function (the function that measures how well a pre‐\n",
      "dicted value matches the true value); this is again often determined by the problem\n",
      "type:\n",
      "\n",
      "Binary classification\n",
      "\n",
      "Binary cross-entropy.\n",
      "\n",
      "Multiclass classification\n",
      "\n",
      "Categorical cross-entropy.\n",
      "\n",
      "20.2 Designing a Neural Network \n",
      "\n",
      "| \n",
      "\n",
      "301\n",
      "\n",
      "\f",
      "Regression\n",
      "\n",
      "Mean square error.\n",
      "\n",
      "Fifth, we have to define an optimizer, which intuitively can be thought of as our strat‐\n",
      "egy “walking around” the loss function to find the parameter values that produce the\n",
      "lowest  error.  Common  choices  for  optimizers  are  stochastic  gradient  descent,  sto‐\n",
      "chastic gradient descent with momentum, root mean square propagation, and adap‐\n",
      "tive moment estimation (more information on these optimizers in “See Also” on page\n",
      "302).\n",
      "\n",
      "Sixth, we can select one or more metrics to use to evaluate the performance, such as\n",
      "accuracy.\n",
      "\n",
      "Keras  offers  two  ways  for  creating  neural  networks.  Keras’  sequential  model  creates\n",
      "neural networks by stacking together layers. An alternative method for creating neu‐\n",
      "ral networks is called the functional API, but that is more for researchers rather than\n",
      "practitioners.\n",
      "\n",
      "In  our  solution,  we  created  a  two-layer  neural  network  (when  counting  layers  we\n",
      "don’t include the input layer because it does not have any parameters to learn) using\n",
      "Keras’ sequential model. Each layer is “dense” (also called fully connected), meaning\n",
      "that  all  the  units  in  the  previous  layer  are  connected  to  all  the  neurals  in  the  next\n",
      "layer. In the first hidden layer we set units=16, meaning that layer contains 16 units\n",
      "with ReLU activation functions: activation='relu'. In Keras, the first hidden layer\n",
      "of any network has to include an input_shape parameter, which is the shape of fea‐\n",
      "ture data. For example, (10,) tells the first layer to expect each observation to have 10\n",
      "feature  values.  Our  second  layer  is  the  same  as  the  first,  without  the  need  for  the\n",
      "input_shape parameter. This network is designed for binary classification so the out‐\n",
      "put layer contains only one unit with a sigmoid activation function, which constrains\n",
      "the output to between 0 and 1 (representing the probability an observation is class 1).\n",
      "\n",
      "Finally, before we can train our model, we need to tell Keras how we want our net‐\n",
      "work  to  learn.  We  do  this  using  the  compile  method,  with  our  optimization  algo‐\n",
      "rithm  (RMSProp),  loss  function  (binary_crossentropy),  and  one  or  more  perfor‐\n",
      "mance metrics.\n",
      "\n",
      "See Also\n",
      "\n",
      "• Losses, Keras\n",
      "\n",
      "• Loss functions for classification, Wikipedia\n",
      "\n",
      "• On Loss Functions for Deep Neural Networks in Classification, Katarzyna Jano‐\n",
      "\n",
      "cha, Wojciech Marian Czarnecki\n",
      "\n",
      "302 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "20.3 Training a Binary Classifier\n",
      "\n",
      "Problem\n",
      "You want to train a binary classifier neural network.\n",
      "\n",
      "Solution\n",
      "Use  Keras  to  construct  a  feedforward  neural  network  and  train  it  using  the  fit\n",
      "method:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
      "    number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "20.3 Training a Binary Classifier \n",
      "\n",
      "| \n",
      "\n",
      "303\n",
      "\n",
      "\f",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=1, # Print description after each epoch\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================]\n",
      "- 1s - loss: 0.4215 - acc: 0.8105 - val_loss: 0.3386 - val_acc: 0.8555\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================]\n",
      "- 1s - loss: 0.3242 - acc: 0.8645 - val_loss: 0.3258 - val_acc: 0.8633\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================]\n",
      "- 1s - loss: 0.3121 - acc: 0.8699 - val_loss: 0.3265 - val_acc: 0.8599\n",
      "\n",
      "Discussion\n",
      "In Recipe 20.2, we discussed how to construct a neural network using Keras’ sequen‐\n",
      "tial model. In this recipe we train that neural network using real data. Specifically, we\n",
      "use 50,000 movie reviews (25,000 as training data, 25,000 held out for testing), cate‐\n",
      "gorized as positive or negative. We convert the text of the reviews into 5,000 binary\n",
      "features indicating the presence of one of the 1,000 most frequent words. Put more\n",
      "simply, our neural networks will use 25,000 observations, each with 1,000 features, to\n",
      "predict if a movie review is positive or negative.\n",
      "\n",
      "The neural network we are using is the same as the one in Recipe 20.2 (see there for a\n",
      "detailed explanation). The only addition is that in that recipe we only created the neu‐\n",
      "ral network, we didn’t train it.\n",
      "\n",
      "In Keras, we train our neural network using the fit method. There are six significant\n",
      "parameters  to  define.  The  first  two  parameters  are  the  features  and  target  vector  of\n",
      "the training data. We can view the shape of feature matrix using shape:\n",
      "\n",
      "# View shape of feature matrix\n",
      "features_train.shape\n",
      "\n",
      "(25000, 1000)\n",
      "\n",
      "The epochs parameter defines how many epochs to use when training the data. ver\n",
      "bose  determines  how  much  information  is  outputted  during  the  training  process,\n",
      "with  0  being  no  output,  1  outputting  a  progress  bar,  and  2  one  log  line  per  epoch.\n",
      "batch_size sets the number of observations to propagate through the network before\n",
      "updating the parameters.\n",
      "\n",
      "304 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "Finally, we held out a test set of data to use to evaluate the model. These test features\n",
      "and test target vector can be arguments of validation_data, which will use them for\n",
      "evaluation. Alternatively, we could have used validation_split to define what frac‐\n",
      "tion of the training data we want to hold out for evaluation.\n",
      "\n",
      "In scikit-learn the fit method returned a trained model, but in Keras the fit method\n",
      "returns a History object containing the loss values and performance metrics at each \n",
      "epoch.\n",
      "\n",
      "20.4 Training a Multiclass Classifier\n",
      "\n",
      "Problem\n",
      "You want to train a multiclass classifier neural network.\n",
      "\n",
      "Solution\n",
      "Use Keras to construct a feedforward neural network with an output layer with soft‐\n",
      "max activation functions:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import reuters\n",
      "from keras.utils.np_utils import to_categorical\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 5000\n",
      "\n",
      "# Load feature and target data\n",
      "data = reuters.load_data(num_words=number_of_features)\n",
      "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
      "\n",
      "# Convert feature data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# One-hot encode target vector to create a target matrix\n",
      "target_train = to_categorical(target_vector_train)\n",
      "target_test = to_categorical(target_vector_test)\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "20.4 Training a Multiclass Classifier \n",
      "\n",
      "| \n",
      "\n",
      "305\n",
      "\n",
      "\f",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=100,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a softmax activation function\n",
      "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target\n",
      "                      epochs=3, # Three epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "In this solution we created a similar neural network to the binary classifier from the\n",
      "last  recipe,  but  with  some  notable  changes.  First,  our  data  is  11,228  Reuters  news‐\n",
      "wires. Each newswire is categorized into 46 topics. We prepared our feature data by\n",
      "converting the newswires into 5,000 binary features (denoting the presence of a cer‐\n",
      "tain word in the newswires). We prepared the target data by one-hot encoding it so\n",
      "that  we  obtain  a  target  matrix  denoting  which  of  the  46  classes  an  observation\n",
      "belongs to:\n",
      "\n",
      "# View target matrix\n",
      "target_train\n",
      "\n",
      "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "\n",
      "Second,  we  increased  the  number  of  units  in  each  of  the  hidden  layers  to  help  the\n",
      "neural network represent the more complex relationship between the 46 classes.\n",
      "\n",
      "306 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "Third, since this is a multiclass classification problem, we used an output layer with\n",
      "46 units (one per class) containing a softmax activation function. The softmax activa‐\n",
      "tion function will return an array of 46 values summing to 1. These 46 values repre‐\n",
      "sent an observation’s probability of being a member of each of the 46 classes.\n",
      "\n",
      "Fourth,  we  used  a  loss  function  suited  to  multiclass  classification,  the  categorical\n",
      "cross-entropy loss function, categorical_crossentropy.\n",
      "\n",
      "20.5 Training a Regressor\n",
      "\n",
      "Problem\n",
      "You want to train a neural network for regression.\n",
      "\n",
      "Solution\n",
      "Use Keras to construct a feedforward neural network with a single output unit and no\n",
      "activation function:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from sklearn.datasets import make_regression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import preprocessing\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_regression(n_samples = 10000,\n",
      "                                   n_features = 3,\n",
      "                                   n_informative = 3,\n",
      "                                   n_targets = 1,\n",
      "                                   noise = 0.0,\n",
      "                                   random_state = 0)\n",
      "\n",
      "# Divide our data into training and test sets\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "features, target, test_size=0.33, random_state=0)\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=32,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(features_train.shape[1],)))\n",
      "\n",
      "20.5 Training a Regressor \n",
      "\n",
      "| \n",
      "\n",
      "307\n",
      "\n",
      "\f",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with no activation function\n",
      "network.add(layers.Dense(units=1))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"mse\", # Mean squared error\n",
      "                optimizer=\"RMSprop\", # Optimization algorithm\n",
      "                metrics=[\"mse\"]) # Mean squared error\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=10, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "It  is  completely  possible  to  create  a  neural  network  to  predict  continuous  values\n",
      "instead of class probabilities. In the case of our binary classifier (Recipe 20.3) we used\n",
      "an  output  layer  with  a  single  unit  and  a  sigmoid  activation  function  to  produce  a\n",
      "probability that an observation was class 1. Importantly, the sigmoid activation func‐\n",
      "tion constrained the outputted value to between 0 and 1. If we remove that constraint\n",
      "by having no activation function, we allow the output to be a continuous value.\n",
      "\n",
      "Furthermore, because we are training a regression, we should use an appropriate loss\n",
      "function and evaluation metric, in our case the mean square error:\n",
      "\n",
      "MSE =\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "n ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "yi − yi\n",
      "\n",
      "where n is the number of observations; yi is the true value of the target we are trying\n",
      "to predict, y, for observation i; and ŷi is the model’s predicted value for yi.\n",
      "\n",
      "Finally, because we are using simulated data using scikit-learn, make_regression, we\n",
      "didn’t have to standardize the features. It should be noted, however, that in almost all\n",
      "real-world cases standardization would be necessary.\n",
      "\n",
      "308 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "20.6 Making Predictions\n",
      "\n",
      "Problem\n",
      "You want to use a neural network to make predictions.\n",
      "\n",
      "Solution\n",
      "Use  Keras  to  construct  a  feedforward  neural  network,  then  make  predictions  using\n",
      "predict:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 10000\n",
      "\n",
      "# Load data and target vector from IMDB movie data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert IMDB data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "20.6 Making Predictions \n",
      "\n",
      "| \n",
      "\n",
      "309\n",
      "\n",
      "\f",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "# Predict classes of test set\n",
      "predicted_target = network.predict(features_test)\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "Making predictions is easy in Keras. Once we have trained our neural network we can\n",
      "use the predict method, which takes as an argument a set of features and returns the\n",
      "predicted output for each observation. In our solution our neural network is set up\n",
      "for  binary  classification  so  the  predicted  output  is  the  probability  of  being  class  1.\n",
      "Observations with predicted values very close to 1 are highly likely to be class 1, while\n",
      "observations with predicted values very close to 0 are highly likely to be class 0. For\n",
      "example, this is the predicted probability that the first observation in our test feature\n",
      "matrix is class 1:\n",
      "\n",
      "# View the probability the first observation is class 1\n",
      "predicted_target[0]\n",
      "\n",
      "array([ 0.83937484], dtype=float32)\n",
      "\n",
      "20.7 Visualize Training History\n",
      "\n",
      "Problem\n",
      "You want to find the “sweet spot” in a neural network’s loss and/or accuracy score.\n",
      "\n",
      "Solution\n",
      "Use Matplotlib to visualize the loss of the test and training set over each epoch:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "310 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "# Set the number of features we want\n",
      "number_of_features = 10000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target\n",
      "                      epochs=15, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=1000, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "# Get training and test loss histories\n",
      "training_loss = history.history[\"loss\"]\n",
      "test_loss = history.history[\"val_loss\"]\n",
      "\n",
      "# Create count of the number of epochs\n",
      "epoch_count = range(1, len(training_loss) + 1)\n",
      "\n",
      "# Visualize loss history\n",
      "plt.plot(epoch_count, training_loss, \"r--\")\n",
      "plt.plot(epoch_count, test_loss, \"b-\")\n",
      "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
      "plt.xlabel(\"Epoch\")\n",
      "\n",
      "20.7 Visualize Training History \n",
      "\n",
      "| \n",
      "\n",
      "311\n",
      "\n",
      "\f",
      "plt.ylabel(\"Loss\")\n",
      "plt.show();\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Alternatively, we can use the same approach to visualize the training and test accu‐\n",
      "racy over each epoch:\n",
      "\n",
      "# Get training and test accuracy histories\n",
      "training_accuracy = history.history[\"acc\"]\n",
      "test_accuracy = history.history[\"val_acc\"]\n",
      "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
      "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
      "\n",
      "# Visualize accuracy history\n",
      "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
      "plt.xlabel(\"Epoch\")\n",
      "plt.ylabel(\"Accuracy Score\")\n",
      "plt.show();\n",
      "\n",
      "312 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "Discussion\n",
      "When our neural network is new, it will have a poor performance. As the neural net‐\n",
      "work learns on the training data, the model’s error on both the training and test set\n",
      "will tend to increase. However, at a certain point the neural network starts “memoriz‐\n",
      "ing”  the  training  data,  and  overfits.  When  this  starts  happening,  the  training  error\n",
      "will decrease while the test error will start increasing. Therefore, in many cases there\n",
      "is a “sweet spot” where the test error (which is the error we mainly care about) is at its\n",
      "lowest  point.  This  effect  can  be  plainly  seen  in  the  solution  where  we  visualize  the\n",
      "training and test loss at each epoch. Note that the test error is lowest around epoch\n",
      "five,  after  which  the  training  loss  continues  to  increase  while  the  test  loss  starts\n",
      "increasing. At this point onward, the model is overfitting.\n",
      "\n",
      "20.8 Reducing Overfitting with Weight Regularization\n",
      "\n",
      "Problem\n",
      "You want to reduce overfitting.\n",
      "\n",
      "Solution\n",
      "Try penalizing the parameters of the network, also called weight regularization:\n",
      "\n",
      "20.8 Reducing Overfitting with Weight Regularization \n",
      "\n",
      "| \n",
      "\n",
      "313\n",
      "\n",
      "\f",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras import regularizers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         kernel_regularizer=regularizers.l2(0.01),\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         kernel_regularizer=regularizers.l2(0.01),\n",
      "                         activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "314 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "One  strategy  to  combat  overfitting  neural  networks  is  by  penalizing  the  parameters\n",
      "(i.e., weights) of the neural network such that they are driven to be small values—cre‐\n",
      "ating a simpler model less prone to overfit. This method is called weight regulariza‐\n",
      "tion or weight decay. More specifically, in weight regularization a penalty is added to\n",
      "the loss function, such as the L2 norm.\n",
      "\n",
      "In  Keras,  we  can  add  a  weight  regularization  by  including  using  kernel_regular\n",
      "izer=regularizers.l2(0.01)  in  a  layer’s  parameters.  In  this  example,  0.01  deter‐\n",
      "mines how much we penalize higher parameter values.\n",
      "\n",
      "20.9 Reducing Overfitting with Early Stopping\n",
      "\n",
      "Problem\n",
      "You want to reduce overfitting.\n",
      "\n",
      "Solution\n",
      "Try stopping training when the test loss stops decreasing, a strategy called early stop‐\n",
      "ping:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "\n",
      "20.9 Reducing Overfitting with Early Stopping \n",
      "\n",
      "| \n",
      "\n",
      "315\n",
      "\n",
      "\f",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Set callback functions to early stop training and save the best model so far\n",
      "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
      "             ModelCheckpoint(filepath=\"best_model.h5\",\n",
      "                             monitor=\"val_loss\",\n",
      "                             save_best_only=True)]\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=20, # Number of epochs\n",
      "                      callbacks=callbacks, # Early stopping\n",
      "                      verbose=0, # Print description after each epoch\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "As we discussed in Recipe 20.7, typically in the first training epochs both the training\n",
      "and test errors will decrease, but at some point the network will start “memorizing”\n",
      "the  training  data,  causing  the  training  error  to  continue  to  decrease  even  while  the\n",
      "test  error  starts  increasing.  Because  of  this  phenomenon,  one  of  the  most  common\n",
      "and very effective methods to counter overfitting is to monitor the training process\n",
      "and  stop  training  when  the  test  error  starts  to  increase.  This  strategy  is  called  early\n",
      "stopping.\n",
      "\n",
      "In Keras, we can implement early stopping as a callback function. Callbacks are func‐\n",
      "tions that can be applied at certain stages of the training process, such as at the end of\n",
      "each  epoch.  Specifically,  in  our  solution,  we  included  EarlyStopping(moni\n",
      "tor='val_loss', patience=2) to define that we wanted to monitor the test (valida‐\n",
      "tion)  loss  at  each  epoch  and  after  the  test  loss  has  not  improved  after  two  epochs,\n",
      "\n",
      "316 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "training  is  interrupted.  However,  since  we  set  patience=2,  we  won’t  get  the  best\n",
      "model, but the model two epochs after the best model. Therefore, optionally, we can\n",
      "include  a  second  operation,  ModelCheckpoint,  which  saves  the  model  to  a  file  after\n",
      "every checkpoint (which can be useful in case a multiday training session is interrup‐\n",
      "ted  for  some  reason).  It  would  be  helpful  for  us,  if  we  set  save_best_only=True,\n",
      "because then ModelCheckpoint will only save the best model.\n",
      "\n",
      "20.10 Reducing Overfitting with Dropout\n",
      "\n",
      "Problem\n",
      "You want to reduce overfitting.\n",
      "\n",
      "Solution\n",
      "Introduce noise into your network’s architecture using dropout:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add a dropout layer for input layer\n",
      "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add a dropout layer for previous hidden layer\n",
      "\n",
      "20.10 Reducing Overfitting with Dropout \n",
      "\n",
      "| \n",
      "\n",
      "317\n",
      "\n",
      "\f",
      "network.add(layers.Dropout(0.5))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add a dropout layer for previous hidden layer\n",
      "network.add(layers.Dropout(0.5))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "Dropout  is  a  popular  and  powerful  method  for  regularizing  neural  networks.  In\n",
      "dropout,  every  time  a  batch  of  observations  is  created  for  training,  a  proportion  of\n",
      "the  units  in  one  or  more  layers  is  multiplied  by  zero  (i.e.,  dropped).  In  this  setting,\n",
      "every  batch  is  trained  on  the  same  network  (e.g.,  the  same  parameters),  but  each\n",
      "batch is confronted by a slightly different version of that network’s architecture.\n",
      "\n",
      "Dropout  is  effective  because  by  constantly  and  randomly  dropping  units  in  each\n",
      "batch, it forces units to learn parameter values able to perform under a wide variety\n",
      "of network architectures. That is, they learn to be robust to disruptions (i.e., noise) in\n",
      "the  other  hidden  units,  and  this  prevents  the  network  from  simply  memorizing  the\n",
      "training data.\n",
      "\n",
      "It is possible to add dropout to both the hidden and input layers. When an input layer\n",
      "is dropped, its feature value is not introduced into the network for that batch. A com‐\n",
      "mon choice for the portion of units to drop is 0.2 for input units and 0.5 for hidden\n",
      "units.\n",
      "\n",
      "In  Keras,  we  can  implement  dropout  by  adding  Dropout  layers  into  our  network\n",
      "architecture. Each Dropout layer will drop a user-defined hyperparameter of units in\n",
      "the previous layer every batch. Remember in Keras the input layer is assumed to be\n",
      "the first layer and not added using add. Therefore, if we want to add dropout to the\n",
      "\n",
      "318 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "input layer, the first layer we add in our network architecture is a dropout layer. This\n",
      "layer  contains  both  the  proportion  of  the  input  layer’s  units  to  drop  0.2  and\n",
      "input_shape defining the shape of the observation data. Next, we add a dropout layer\n",
      "with 0.5 after each of the hidden layers.\n",
      "\n",
      "20.11 Saving Model Training Progress\n",
      "\n",
      "Problem\n",
      "Given a neural network that will take a long time to train, you want to save your pro‐\n",
      "gress in case the training process is interrupted.\n",
      "\n",
      "Solution\n",
      "Use the callback function ModelCheckpoint to save the model after every epoch:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
      "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "20.11 Saving Model Training Progress \n",
      "\n",
      "| \n",
      "\n",
      "319\n",
      "\n",
      "\f",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Set callback functions to early stop training and save the best model so far\n",
      "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      callbacks=checkpoint, # Checkpoint\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "In Recipe 20.8 we used the callback function  ModelCheckpoint in conjunction with\n",
      "EarlyStopping  to  end  monitoring  and  end  training  when  the  test  error  stopped\n",
      "improving. However, there is another, more mundane reason for using  ModelCheck\n",
      "point. In the real world, it is common for neural networks to train for hours or even\n",
      "days.  During  that  time  a  lot  can  go  wrong:  computers  can  lose  power,  servers  can\n",
      "crash, or inconsiderate graduate students can close your laptop.\n",
      "\n",
      "ModelCheckpoint  alleviates  this  problem  by  saving  the  model  after  every  epoch.\n",
      "Specifically, after every epoch ModelCheckpoint saves a model to the location speci‐\n",
      "fied by the filepath parameter. If we include only a filename (e.g., models.hdf5) that\n",
      "file will be overridden with the latest model every epoch. If we only wanted to save\n",
      "the  best  model  according  to  the  performance  of  some  loss  function,  we  can  set\n",
      "save_best_only=True  and  monitor='val_loss'  to  not  override  a  file  if  the  model\n",
      "has a worse test loss than the previous model. Alternatively, we can save every epoch’s\n",
      "model as its own file by including the epoch number and test loss score into the file‐\n",
      "name  itself.  For  example,  if  we  set  filepath  to  model_{epoch:02d}_{val_loss:.\n",
      "2f}.hdf5, the name of the file containing the model saved after the 11th epoch with a\n",
      "test loss value of 0.33 would be model_10_0.35.hdf5 (notice that the epoch number is\n",
      "0-indexed).\n",
      "\n",
      "320 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "20.12 k-Fold Cross-Validating Neural Networks\n",
      "\n",
      "Problem\n",
      "You want to evaluate a neural network using k-fold cross-validation.\n",
      "\n",
      "Solution\n",
      "Often  k-fold  cross-validating  neural  networks  is  neither  necessary  nor  advisable.\n",
      "However, if it is appropriate: use Keras’ scikit-learn wrapper to allow Keras’ sequential\n",
      "models to use the scikit-learn API:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Number of features\n",
      "number_of_features = 100\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_classification(n_samples = 10000,\n",
      "                                       n_features = number_of_features,\n",
      "                                       n_informative = 3,\n",
      "                                       n_redundant = 0,\n",
      "                                       n_classes = 2,\n",
      "                                       weights = [.5, .5],\n",
      "                                       random_state = 0)\n",
      "\n",
      "# Create function returning a compiled network\n",
      "def create_network():\n",
      "\n",
      "    # Start neural network\n",
      "    network = models.Sequential()\n",
      "\n",
      "    # Add fully connected layer with a ReLU activation function\n",
      "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
      "        number_of_features,)))\n",
      "\n",
      "    # Add fully connected layer with a ReLU activation function\n",
      "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "    # Add fully connected layer with a sigmoid activation function\n",
      "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "20.12 k-Fold Cross-Validating Neural Networks \n",
      "\n",
      "| \n",
      "\n",
      "321\n",
      "\n",
      "\f",
      "    # Compile neural network\n",
      "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                    optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "    # Return compiled network\n",
      "    return network\n",
      "\n",
      "# Wrap Keras model so it can be used by scikit-learn\n",
      "neural_network = KerasClassifier(build_fn=create_network,\n",
      "                                 epochs=10,\n",
      "                                 batch_size=100,\n",
      "                                 verbose=0)\n",
      "\n",
      "# Evaluate neural network using three-fold cross-validation\n",
      "cross_val_score(neural_network, features, target, cv=3)\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "array([ 0.90461907,  0.77437743,  0.87068707])\n",
      "\n",
      "Discussion\n",
      "Theoretically,  there  is  no  reason  we  cannot  use  cross-validation  to  evaluate  neural\n",
      "networks. However, neural networks are often used on very large data and can take\n",
      "hours or even days to train. For this reason, if the training time is long, adding the\n",
      "computational  expense  of  k-fold  cross-validation  is  unadvisable.  For  example,  a\n",
      "model normally taking one day to train would take 10 days to evaluate using 10-fold\n",
      "cross-validation. If we have large data, it is often appropriate to simply evaluate the\n",
      "neural network on some test set.\n",
      "\n",
      "If we have smaller data, k-fold cross-validation can be useful to maximize our ability\n",
      "to  evaluate  the  neural  network’s  performance.  This  is  possible  in  Keras  because  we\n",
      "can “wrap” any neural network such that it can use the evaluation features available in\n",
      "scikit-learn,  including  k-fold  cross-validation.  To  accomplish  this,  we  first  have  to\n",
      "create a function that returns a compiled neural network. Next we use KerasClassi\n",
      "fier  (if  we  have  a  classifier;  if  we  have  a  regressor  we  can  use  KerasRegressor)  to\n",
      "wrap  the  model  so  it  can  be  used  by  scikit-learn.  After  this,  we  can  use  our  neural\n",
      "network like any other scikit-learn learning algorithm (e.g., random forests, logistic\n",
      "regression).  In  our  solution,  we  used  cross_val_score  to  run  a  three-fold  cross-\n",
      "validation on our neural network.\n",
      "\n",
      "20.13 Tuning Neural Networks\n",
      "\n",
      "Problem\n",
      "You want to automatically select the best hyperparameters for your neural network.\n",
      "\n",
      "322 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "Solution\n",
      "Combine  a  Keras  neural  network  with  scikit-learn’s  model  selection  tools  like  Grid\n",
      "SearchCV:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Number of features\n",
      "number_of_features = 100\n",
      "\n",
      "# Generate features matrix and target vector\n",
      "features, target = make_classification(n_samples = 10000,\n",
      "                                       n_features = number_of_features,\n",
      "                                       n_informative = 3,\n",
      "                                       n_redundant = 0,\n",
      "                                       n_classes = 2,\n",
      "                                       weights = [.5, .5],\n",
      "                                       random_state = 0)\n",
      "\n",
      "# Create function returning a compiled network\n",
      "def create_network(optimizer=\"rmsprop\"):\n",
      "\n",
      "    # Start neural network\n",
      "    network = models.Sequential()\n",
      "\n",
      "    # Add fully connected layer with a ReLU activation function\n",
      "    network.add(layers.Dense(units=16,\n",
      "                             activation=\"relu\",\n",
      "                             input_shape=(number_of_features,)))\n",
      "\n",
      "    # Add fully connected layer with a ReLU activation function\n",
      "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "    # Add fully connected layer with a sigmoid activation function\n",
      "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "    # Compile neural network\n",
      "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                    optimizer=optimizer, # Optimizer\n",
      "                    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "    # Return compiled network\n",
      "    return network\n",
      "\n",
      "20.13 Tuning Neural Networks \n",
      "\n",
      "| \n",
      "\n",
      "323\n",
      "\n",
      "\f",
      "# Wrap Keras model so it can be used by scikit-learn\n",
      "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
      "\n",
      "# Create hyperparameter space\n",
      "epochs = [5, 10]\n",
      "batches = [5, 10, 100]\n",
      "optimizers = [\"rmsprop\", \"adam\"]\n",
      "\n",
      "# Create hyperparameter options\n",
      "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
      "\n",
      "# Create grid search\n",
      "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
      "\n",
      "# Fit grid search\n",
      "grid_result = grid.fit(features, target)\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "In Recipes 12.1 and 12.2, we covered using scikit-learn’s model selection techniques\n",
      "to  identify  the  best  hyperparameters  of  a  scikit-learn  model.  In  Recipe  20.12  we\n",
      "learned that we can wrap our neural network so it can use the scikit-learn API. In this\n",
      "recipe  we  combine  these  two  techniques  to  identify  the  best  hyperparameters  of  a\n",
      "neural network.\n",
      "\n",
      "The  hyperparameters  of  a  model  are  important  and  should  be  selected  with  care.\n",
      "However,  before  we  get  it  into  our  heads  that  model  selection  strategies  like  grid\n",
      "search are a good idea, we must realize that if our model would normally have taken\n",
      "12 hours or a day to train, this grid search process could take a week or more. There‐\n",
      "fore, automatic hyperparameter tuning of neural networks is not the silver bullet, but\n",
      "it is a useful tool to have in certain circumstances.\n",
      "\n",
      "In our solution we conducted a cross-validated grid search over a number of options\n",
      "for  the  optimization  algorithm,  number  of  epochs,  and  batch  size.  Even  this  toy\n",
      "example took a few minutes to run, but once it is done we can use best_params_ to \n",
      "view the hyperparameters of the neural network with the best results:\n",
      "\n",
      "# View hyperparameters of best neural network\n",
      "grid_result.best_params_\n",
      "\n",
      "{'batch_size': 10, 'epochs': 5, 'optimizer': 'adam'}\n",
      "\n",
      "324 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "20.14 Visualizing Neural Networks\n",
      "\n",
      "Problem\n",
      "You want to quickly visualize a neural network’s architecture.\n",
      "\n",
      "Solution\n",
      "Use Keras’ model_to_dot or plot_model:\n",
      "\n",
      "# Load libraries\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from IPython.display import SVG\n",
      "from keras.utils.vis_utils import model_to_dot\n",
      "from keras.utils import plot_model\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Visualize network architecture\n",
      "SVG(model_to_dot(network, show_shapes=True).create(prog=\"dot\", format=\"svg\"))\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "20.14 Visualizing Neural Networks \n",
      "\n",
      "| \n",
      "\n",
      "325\n",
      "\n",
      "\f",
      "Alternatively, if we want to save the visualization as a file, we can use plot_model:\n",
      "\n",
      "# Save the visualization as a file\n",
      "plot_model(network, show_shapes=True, to_file=\"network.png\")\n",
      "\n",
      "Discussion\n",
      "Keras  provides  utility  functions  to  quickly  visualize  neural  networks.  If  we  wish  to\n",
      "display  a  neural  network  in  a  Jupyter  Notebook,  we  can  use  model_to_dot.  The\n",
      "show_shapes parameter shows the shape of the inputs and outputs and can help with\n",
      "debugging. For a simpler model, we can set show_shapes=True:\n",
      "\n",
      "# Visualize network architecture\n",
      "SVG(model_to_dot(network, show_shapes=False).create(prog=\"dot\", format=\"svg\"))\n",
      "\n",
      "326 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "20.15 Classifying Images\n",
      "\n",
      "Problem\n",
      "You want to classify images using a convolutional neural network.\n",
      "\n",
      "Solution\n",
      "Use Keras to create a neural network with at least one convolutional layer:\n",
      "\n",
      "import numpy as np\n",
      "from keras.datasets import mnist\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Dropout, Flatten\n",
      "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
      "from keras.utils import np_utils\n",
      "from keras import backend as K\n",
      "\n",
      "# Set that the color channel value will be first\n",
      "K.set_image_data_format(\"channels_first\")\n",
      "\n",
      "# Set seed\n",
      "np.random.seed(0)\n",
      "\n",
      "20.15 Classifying Images \n",
      "\n",
      "| \n",
      "\n",
      "327\n",
      "\n",
      "\f",
      "# Set image information\n",
      "channels = 1\n",
      "height = 28\n",
      "width = 28\n",
      "\n",
      "# Load data and target from MNIST data\n",
      "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
      "\n",
      "# Reshape training image data into features\n",
      "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
      "\n",
      "# Reshape test image data into features\n",
      "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
      "\n",
      "# Rescale pixel intensity to between 0 and 1\n",
      "features_train = data_train / 255\n",
      "features_test = data_test / 255\n",
      "\n",
      "# One-hot encode target\n",
      "target_train = np_utils.to_categorical(target_train)\n",
      "target_test = np_utils.to_categorical(target_test)\n",
      "number_of_classes = target_test.shape[1]\n",
      "\n",
      "# Start neural network\n",
      "network = Sequential()\n",
      "\n",
      "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
      "network.add(Conv2D(filters=64,\n",
      "                   kernel_size=(5, 5),\n",
      "                   input_shape=(channels, width, height),\n",
      "                   activation='relu'))\n",
      "\n",
      "# Add max pooling layer with a 2x2 window\n",
      "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "# Add dropout layer\n",
      "network.add(Dropout(0.5))\n",
      "\n",
      "# Add layer to flatten input\n",
      "network.add(Flatten())\n",
      "\n",
      "# # Add fully connected layer of 128 units with a ReLU activation function\n",
      "network.add(Dense(128, activation=\"relu\"))\n",
      "\n",
      "# Add dropout layer\n",
      "network.add(Dropout(0.5))\n",
      "\n",
      "# Add fully connected layer with a softmax activation function\n",
      "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
      "\n",
      "# Compile neural network\n",
      "\n",
      "328 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "network.fit(features_train, # Features\n",
      "            target_train, # Target\n",
      "            epochs=2, # Number of epochs\n",
      "            verbose=0, # Don't print description after each epoch\n",
      "            batch_size=1000, # Number of observations per batch\n",
      "            validation_data=(features_test, target_test)) # Data for evaluation\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "<keras.callbacks.History at 0x133f37e80>\n",
      "\n",
      "Discussion\n",
      "Convolutional neural networks (also called ConvNets) are a popular type of network\n",
      "that has proven very effective at computer vision (e.g., recognizing cats, dogs, planes,\n",
      "and even hot dogs). It is completely possible to use feedforward neural networks on\n",
      "images, where each pixel is a feature. However, when doing so we run into two major\n",
      "problems.  First,  feedforward  neural  networks  do  not  take  into  account  the  spatial\n",
      "structure of the pixels. For example, in a 10 × 10 pixel image we might convert it into\n",
      "a  vector  of  100  pixel  features,  and  in  this  case  feedforward  would  consider  the  first\n",
      "feature  (e.g.,  pixel  value)  to  have  the  same  relationship  with  the  10th  feature  as  the\n",
      "11th feature. However, in reality the 10th feature represents a pixel on the far side of\n",
      "the image as the first feature, while the 11th feature represents the pixel immediately\n",
      "below the first pixel. Second, and relatedly, feedforward neural networks learn global\n",
      "relationships  in  the  features  instead  of  local  patterns.  In  more  practical  terms,  this\n",
      "means that feedforward neural networks are not able to detect an object regardless of\n",
      "where it appears in an image. For example, imagine we are training a neural network\n",
      "to  recognize  faces,  and  these  faces  might  appear  anywhere  in  the  image  from  the\n",
      "upper right to the middle to the lower left.\n",
      "\n",
      "The  power  of  convolutional  neural  networks  is  their  ability  handle  both  of  these\n",
      "issues (and others). A complete explanation of convolutional neural networks is well\n",
      "beyond the scope of this book, but a brief explanation will be useful. The data of an\n",
      "individual  image  contains  two  or  three  dimensions:  height,  width,  and  depth.  The\n",
      "first two should be obvious, but the last deserves explanation. The depth is the color\n",
      "of a pixel. In grayscale images the depth is only one (the intensity of the pixel) and\n",
      "therefore the image is a matrix. However, in color images a pixel’s color is represented\n",
      "by  multiple  values.  For  example,  in  an  RGB  image  a  pixel’s  color  is  represented  by\n",
      "three values representing red, green, and blue. Therefore, the data for an image can\n",
      "be imagined to be a three-dimensional tensor: width × height × depth (called feature\n",
      "maps).  In  convolutional  neural  networks,  a  convolution  (don’t  worry  if  you  don’t\n",
      "\n",
      "20.15 Classifying Images \n",
      "\n",
      "| \n",
      "\n",
      "329\n",
      "\n",
      "\f",
      "know  what  that  means)  can  be  imagined  to  slide  a  window  over  the  pixels  of  an\n",
      "image, looking at both the individual pixel and also its neighbors. It then transforms\n",
      "the raw image data into a new three-dimensional tensor where the first two dimen‐\n",
      "sions  are  approximately  width  and  height,  while  the  third  dimension  (which  con‐\n",
      "tained  the  color  values)  now  represents  the  patterns—called  filters—(for  example,  a\n",
      "sharp corner or sweeping gradient) to which that pixel “belongs.”\n",
      "\n",
      "The second important concept for our purposes is that of pooling layers. Pooling lay‐\n",
      "ers move a window over our data (though usually they only look at every nth pixel,\n",
      "called  striding)  downsizing  our  data  by  summarizing  the  window  in  some  way.  The\n",
      "most common method is max pooling where the maximum value in each window is\n",
      "sent to the next layer. One reason for max pooling is merely practical; the convolu‐\n",
      "tional  process  creates  many  more  parameters  to  learn,  which  can  get  ungainly  very\n",
      "quickly. Second, more intuitively, max pooling can be thought of as “zooming out” of\n",
      "an image.\n",
      "\n",
      "An example might be useful here. Imagine we have an image containing a dog’s face.\n",
      "The first convolutional layer might find patterns like shape edges. Then we use a max\n",
      "pool layer to “zoom out,” and a second convolutional layer to find patterns like dog\n",
      "ears. Finally we use another max pooling layer to zoom out again and a final convolu‐\n",
      "tional layer to find patterns like dogs’ faces.\n",
      "\n",
      "Finally,  fully  connected  layers  are  often  used  at  the  end  of  the  network  to  do  the\n",
      "actual classification.\n",
      "\n",
      "While our solution might look like a lot of lines of code, it is actually very similar to\n",
      "our binary classifier from earlier in this chapter. In this solution we used the famous\n",
      "MNIST  dataset,  which  is  a  de  facto  benchmark  dataset  in  machine  learning.  The\n",
      "MNIST dataset contains 70,000 small images (28 × 28) of handwritten digits from 0\n",
      "to 9. This dataset is labeled so that we know the actual digit (i.e., class) for each small\n",
      "image. The standard training-test split is to use 60,000 images for training and 10,000\n",
      "for testing.\n",
      "\n",
      "We reorganized the data into the format expected by a convolutional network. Specif‐\n",
      "ically, we used reshape to convert the observation data such that it is the shape Keras\n",
      "expects.  Next,  we  rescaled  the  values  to  be  between  0  and  1,  since  training  perfor‐\n",
      "mance  can  suffer  if  an  observation’s  values  are  much  greater  than  the  network’s\n",
      "parameters (which are initialized as small numbers). Finally, we one-hot encoded the\n",
      "target  data  so  that  every  observation’s  target  has  10  classes,  representing  the  digits\n",
      "0–9.\n",
      "\n",
      "With the image data process we can build our convolutional network. First, we add a\n",
      "convolutional  layer  and  specify  the  number  of  filters  and  other  characteristics.  The\n",
      "size of the window is a hyperparameter; however, 3 × 3 is standard practice for most\n",
      "images, while larger windows are often used in larger images. Second, we add a max\n",
      "\n",
      "330 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "pooling  layer,  summarizing  the  nearby  pixels.  Third,  we  add  a  dropout  layer  to\n",
      "reduce the chances of overfitting. Fourth, we add a flatten layer to convert the convo‐\n",
      "lutionary inputs into a format able to be used by a fully connected layer. Finally, we\n",
      "add  the  fully  connected  layers  and  an  output  layer  to  do  the  actual  classification.\n",
      "Notice  that  because  this  is  a  multiclass  classification  problem,  we  use  the  softmax\n",
      "activation function in the output layer.\n",
      "\n",
      "It should be noted that this is a pretty simple convolutional neural network. It is com‐\n",
      "mon to see a much deeper network with many more convolutional and max pooling\n",
      "layers stacked together.\n",
      "\n",
      "20.16 Improving Performance with Image Augmentation\n",
      "\n",
      "Problem\n",
      "You want to improve the performance of your convolutional neural network.\n",
      "\n",
      "Solution\n",
      "For  better  results,  preprocess  the  images  and  augment  the  data  beforehand  using\n",
      "ImageDataGenerator:\n",
      "\n",
      "# Load library\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "\n",
      "# Create image augmentation\n",
      "augmentation = ImageDataGenerator(featurewise_center=True, # Apply ZCA whitening\n",
      "                                  zoom_range=0.3, # Randomly zoom in on images\n",
      "                                  width_shift_range=0.2, # Randomly shift images\n",
      "                                  horizontal_flip=True, # Randomly flip images\n",
      "                                  rotation_range=90) # Randomly rotate\n",
      "\n",
      "# Process all images from the directory 'raw/images'\n",
      "augment_images = augmentation.flow_from_directory(\"raw/images\", # Image folder\n",
      "                                                  batch_size=32, # Batch size\n",
      "                                                  class_mode=\"binary\", # Classes\n",
      "                                                  save_to_dir=\"processed/images\")\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Found 12665 images belonging to 2 classes.\n",
      "\n",
      "Discussion\n",
      "First an apology—this solution’s code will not immediately run for you because you\n",
      "do not have the required folders of images. However, since the most common situa‐\n",
      "\n",
      "20.16 Improving Performance with Image Augmentation \n",
      "\n",
      "| \n",
      "\n",
      "331\n",
      "\n",
      "\f",
      "tion  is  having  a  directory  of  images,  I  wanted  to  include  it.  The  technique  should\n",
      "easily translate to your own images.\n",
      "\n",
      "One way to improve the performance of a convolutional neural network is to prepro‐\n",
      "cess  the  images.  We  discussed  a  number  of  techniques  in  Chapter  8;  however,  it  is\n",
      "worth  noting  that  Keras’  ImageDataGenerator  contains  a  number  of  basic  prepro‐\n",
      "cessing techniques. For example, in our solution we used featurewise_center=True\n",
      "to standardize the pixels across the entire data.\n",
      "\n",
      "A second technique to improve performance is to add noise. An interesting feature of\n",
      "neural  networks  is  that,  counterintuitively,  their  performance  often  improves  when\n",
      "noise is added to the data. The reason is that the additional noise can make the neural\n",
      "networks more robust in the face of real-world noise and prevents them from overfit‐\n",
      "ting the data.\n",
      "\n",
      "When  training  convolutional  neural  networks  for  images,  we  can  add  noise  to  our\n",
      "observations  by  randomly  transforming  images  in  various  ways,  such  as  flipping\n",
      "images  or  zooming  in  on  images.  Even  small  changes  can  significantly  improve\n",
      "model  performance.  We  can  use  the  same  ImageDataGenerator  class  to  conduct\n",
      "these  transformations.  The  Keras  documentation  (referenced  in  “See  Also”  on  page\n",
      "333) specifies the full list of available transformations; however, our example contains\n",
      "a sample of them are, including random zooming, shifting, flipping, and rotation.\n",
      "\n",
      "It is important to note that the output of flow_from_directory is a Python generator\n",
      "object. This is because in most cases we will want to process the images on-demand\n",
      "as  they  are  sent  to  the  neural  network  for  training.  If  we  wanted  to  process  all  the\n",
      "images prior to training, we could simply iterate over the generator.\n",
      "\n",
      "Finally,  since  augment_images  is  a  generator,  when  training  our  neural  network  we\n",
      "will have to use fit_generator instead of fit. For example:\n",
      "\n",
      "# Train neural network\n",
      "network.fit_generator(augment_images,\n",
      "                      #Number of times to call the generator for each epoch\n",
      "                      steps_per_epoch=2000,\n",
      "                      # Number of epochs\n",
      "                      epochs=5,\n",
      "                      # Test data generator\n",
      "                      validation_data=augment_images_test,\n",
      "                      # Number of items to call the generator\n",
      "                      # for each test epoch\n",
      "                      validation_steps=800)\n",
      "\n",
      "Note all the raw images used in this recipe are available on GitHub.\n",
      "\n",
      "332 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "See Also\n",
      "\n",
      "• Image Preprocessing, Keras\n",
      "\n",
      "20.17 Classifying Text\n",
      "\n",
      "Problem\n",
      "You want to classify text data.\n",
      "\n",
      "Solution\n",
      "Use a long short-term memory recurrent neural network:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing import sequence\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Use padding or truncation to make each observation have 400 features\n",
      "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
      "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add an embedding layer\n",
      "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
      "\n",
      "# Add a long short-term memory layer with 128 units\n",
      "network.add(layers.LSTM(units=128))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "\n",
      "20.17 Classifying Text \n",
      "\n",
      "| \n",
      "\n",
      "333\n",
      "\n",
      "\f",
      "                optimizer=\"Adam\", # Adam optimization\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(features_train, # Features\n",
      "                      target_train, # Target\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # Do not print description after each epoch\n",
      "                      batch_size=1000, # Number of observations per batch\n",
      "                      validation_data=(features_test, target_test)) # Test data\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Discussion\n",
      "Oftentimes we have text data that we want to classify. While it is possible to use a type\n",
      "of convolutional network, we are going to focus on a more popular option: the recur‐\n",
      "rent neural network. The key feature of recurrent neural networks is that information\n",
      "loops  back  in  the  network.  This  gives  recurrent  neural  networks  a  type  of  memory\n",
      "they can use to better understand sequential data. A popular type of recurrent neural\n",
      "network is the long short-term memory (LSTM) network, which allows for informa‐\n",
      "tion to loop backward in the network. For a more detailed explanation, see the addi‐\n",
      "tional resources.\n",
      "\n",
      "In  this  solution,  we  have  our  movie  review  data  from  Recipe  20.3  and  we  want  to\n",
      "train an LSTM network to predict if these reviews are positive or negative. Before we\n",
      "can train our network, a little data processing is needed. Our text data comes in the\n",
      "form of a list of integers:\n",
      "\n",
      "# View first observation\n",
      "print(data_train[0])\n",
      "\n",
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25,\n",
      "    100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2,\n",
      "    336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2,\n",
      "    19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2,\n",
      "    4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4,\n",
      "    2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135,\n",
      "    48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107,\n",
      "    117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46,\n",
      "    7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2,\n",
      "    18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224,\n",
      "    92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32,\n",
      "    15, 16, 2, 19, 178, 32]\n",
      "\n",
      "Each  integer  in  this  list  corresponds  to  some  word.  However,  because  each  review\n",
      "does not contain the same number of words, each observation is not the same length.\n",
      "Therefore, before we can input this data into our neural network, we need to make all\n",
      "the observations the same length. We can do this using pad_sequences. pad_sequen\n",
      "\n",
      "334 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 20: Neural Networks\n",
      "\n",
      "\f",
      "ces pads each observation’s data so that they are all the same size. We can see this if\n",
      "we look at our first observation after it is processed by pad_sequences:\n",
      "\n",
      "# View first observation\n",
      "print(features_test[0])\n",
      "\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1  89  27   2   2  17 199 132   5   2  16   2  24   8\n",
      " 760   4   2   7   4  22   2   2  16   2  17   2   7   2   2   9   4   2\n",
      "   8  14 991  13 877  38  19  27 239  13 100 235  61 483   2   4   7   4\n",
      "  20 131   2  72   8  14 251  27   2   7 308  16 735   2  17  29 144  28\n",
      "  77   2  18  12]\n",
      "\n",
      "Next,  we  use  one  of  the  most  promising  techniques  in  natural  language  processing:\n",
      "word embeddings. In our binary classifier from Recipe 20.3, we one-hot encoded the\n",
      "observations  and  used  that  as  inputs  to  the  neural  network.  However,  this  time  we\n",
      "will represent each word as a vector in a multidimensional space, and allow the dis‐\n",
      "tance between two vectors to represent the similarity between words. In Keras we can\n",
      "do this by adding an Embedding layer. For each value sent to the Embedding layer, it\n",
      "will  output  a  vector  representing  that  word.  The  following  layer  is  our  LSTM  layer\n",
      "with 128 units, which allows for information from earlier inputs to be used in futures,\n",
      "directly addressing the sequential nature of the data. Finally, because this is a binary\n",
      "classification problem (each review is positive or negative), we add a fully connected\n",
      "output layer with one unit and a sigmoid activation function.\n",
      "\n",
      "It is worth noting that LSTMs are a very broad topic and the focus of much research.\n",
      "This recipe, while hopefully useful, is far from the last word on the implementation of \n",
      "LSTMs.\n",
      "\n",
      "20.17 Classifying Text \n",
      "\n",
      "| \n",
      "\n",
      "335\n",
      "\n",
      "\f",
      "\f",
      "CHAPTER 21\n",
      "Saving and Loading Trained Models\n",
      "\n",
      "21.0 Introduction\n",
      "In the last 20 chapters and around 200 recipes, we have covered how to take raw data\n",
      "and use machine learning to create well-performing predictive models. However, for\n",
      "all  our  work  to  be  worthwhile  we  eventually  need  to  do  something  with  our  model,\n",
      "such as integrating it with an existing software application. To accomplish this goal,\n",
      "we need to be able to both save our models after training and load them when they\n",
      "are needed by an application. That is the focus of our final chapter together.\n",
      "\n",
      "21.1 Saving and Loading a scikit-learn Model\n",
      "\n",
      "Problem\n",
      "You have a trained scikit-learn model and want to save it and load it elsewhere.\n",
      "\n",
      "Solution\n",
      "Save the model as a pickle file:\n",
      "\n",
      "# Load libraries\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import datasets\n",
      "from sklearn.externals import joblib\n",
      "\n",
      "# Load data\n",
      "iris = datasets.load_iris()\n",
      "features = iris.data\n",
      "target = iris.target\n",
      "\n",
      "# Create decision tree classifer object\n",
      "classifer = RandomForestClassifier()\n",
      "\n",
      "337\n",
      "\n",
      "\f",
      "# Train model\n",
      "model = classifer.fit(features, target)\n",
      "\n",
      "# Save model as pickle file\n",
      "joblib.dump(model, \"model.pkl\")\n",
      "\n",
      "['model.pkl']\n",
      "\n",
      "Once the model is saved we can use scikit-learn in our destination application (e.g.,\n",
      "web application) to load the model:\n",
      "\n",
      "# Load model from file\n",
      "classifer = joblib.load(\"model.pkl\")\n",
      "\n",
      "And use it make predictions:\n",
      "\n",
      "# Create new observation\n",
      "new_observation = [[ 5.2,  3.2,  1.1,  0.1]]\n",
      "\n",
      "# Predict observation's class\n",
      "classifer.predict(new_observation)\n",
      "\n",
      "array([0])\n",
      "\n",
      "Discussion\n",
      "The first step in using a model in production is to save that model as a file that can be\n",
      "loaded  by  another  application  or  workflow.  We  can  accomplish  this  by  saving  the\n",
      "model as a pickle file, a Python-specific data format. Specifically, to save the model we\n",
      "use joblib, which is a library extending pickle for cases when we have large NumPy\n",
      "arrays—a common occurrence for trained models in scikit-learn.\n",
      "\n",
      "When saving scikit-learn models, be aware that saved models might not be compati‐\n",
      "ble between versions of scikit-learn; therefore, it can be helpful to include the version\n",
      "of scikit-learn used in the model in the filename:\n",
      "\n",
      "# Import library\n",
      "import sklearn\n",
      "\n",
      "# Get scikit-learn version\n",
      "scikit_version = joblib.__version__\n",
      "\n",
      "# Save model as pickle file\n",
      "joblib.dump(model, \"model_{version}.pkl\".format(version=scikit_version))\n",
      "\n",
      "['model_0.11.pkl']\n",
      "\n",
      "338 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 21: Saving and Loading Trained Models\n",
      "\n",
      "\f",
      "21.2 Saving and Loading a Keras Model\n",
      "\n",
      "Problem\n",
      "You have a trained Keras model and want to save it and load it elsewhere.\n",
      "\n",
      "Solution\n",
      "Save the model as HDF5:\n",
      "\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras.models import load_model\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "\n",
      "# Set the number of features we want\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(train_data, train_target), (test_data, test_target) = imdb.load_data(\n",
      "    num_words=number_of_features)\n",
      "\n",
      "# Convert movie review data to a one-hot encoded feature matrix\n",
      "tokenizer = Tokenizer(num_words=number_of_features)\n",
      "train_features = tokenizer.sequences_to_matrix(train_data, mode=\"binary\")\n",
      "test_features = tokenizer.sequences_to_matrix(test_data, mode=\"binary\")\n",
      "\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add fully connected layer with a ReLU activation function\n",
      "network.add(layers.Dense(units=16,\n",
      "                         activation=\"relu\",\n",
      "                         input_shape=(number_of_features,)))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
      "\n",
      "# Compile neural network\n",
      "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
      "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
      "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
      "\n",
      "# Train neural network\n",
      "history = network.fit(train_features, # Features\n",
      "\n",
      "21.2 Saving and Loading a Keras Model \n",
      "\n",
      "| \n",
      "\n",
      "339\n",
      "\n",
      "\f",
      "                      train_target, # Target vector\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # No output\n",
      "                      batch_size=100, # Number of observations per batch\n",
      "                      validation_data=(test_features, test_target)) # Test data\n",
      "\n",
      "# Save neural network\n",
      "network.save(\"model.h5\")\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "We can then load the model either in another application or for additional training:\n",
      "\n",
      "# Load neural network\n",
      "network = load_model(\"model.h5\")\n",
      "\n",
      "Discussion\n",
      "Unlike  scikit-learn,  Keras  does  not  recommend  you  save  models  using  pickle.\n",
      "Instead,  models  are  saved  as  an  HDF5  file.  The  HDF5  file  contains  everything  you\n",
      "need  to  not  only  load  the  model  to  make  predictions  (i.e.,  architecture  and  trained\n",
      "parameters), but also to restart training (i.e., loss and optimizer settings and the cur‐\n",
      "rent state).\n",
      "\n",
      "340 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 21: Saving and Loading Trained Models\n",
      "\n",
      "\f",
      "Index\n",
      "\n",
      "A\n",
      "accuracy, 186-188\n",
      "AdaBoostClassifier, 247-249\n",
      "AdaBoostRegressor, 247-249\n",
      "adaptiveThreshold, 137-140\n",
      "affinity, 295\n",
      "Agglomerative clustering, 294-295\n",
      "algorithm, 255\n",
      "algorithms for faster model selection, 219-220\n",
      "ANOVA F-value statistic, 174, 176\n",
      "append, 56\n",
      "apply, 54, 55\n",
      "area under the ROC curve (AUCROC), 192\n",
      "arrays\n",
      "\n",
      "descriptive statistics about, 8\n",
      "max and min methods, 7\n",
      "one-dimensional, 1\n",
      "reshaping, 9-10\n",
      "slicing, 126-128\n",
      "two-dimensional, 2\n",
      "augment_images, 332\n",
      "average, 8, 42\n",
      "axis, 8, 55, 56\n",
      "\n",
      "B\n",
      "back-filling, 120\n",
      "backpropagation, 298\n",
      "Bag of Words model, 104-106\n",
      "balanced, 245, 265\n",
      "bandwidth, 292\n",
      "baseline classification model, 184-186\n",
      "baseline regression model, 183-184\n",
      "base_estimator, 248\n",
      "batch_size, 291, 304\n",
      "\n",
      "Bayes' theorem, 279-286\n",
      "\n",
      "(see also naive Bayes classifiers)\n",
      "\n",
      "Beautiful Soup, 97\n",
      "Bernoulli naive Bayes, 283-284\n",
      "best_estimator_, 215\n",
      "best_params, 324\n",
      "BigramTagger, 103\n",
      "Binarizer, 73\n",
      "binarizing images, 137-140\n",
      "binary classifier thresholds, 189-192\n",
      "binary classifiers, 259-260, 301, 303-305\n",
      "binary feature variance thresholding, 171\n",
      "bins, 74\n",
      "blurring images, 128-130\n",
      "boolean conditions, 112\n",
      "bootstrap, 240, 241\n",
      "Brown Corpus text tagger, 103\n",
      "\n",
      "C\n",
      "C, 210\n",
      "calibrating predicted probabilities, 284-286\n",
      "callbacks, 316, 319\n",
      "Canny edge detector, 144-146\n",
      "categorical data, 81-94\n",
      "\n",
      "dictionary encoding, 86-88\n",
      "imbalanced classes, 90-94\n",
      "imputing missing values, 88-90\n",
      "nominal features encoding, 82-84\n",
      "nominal versus ordinal, 81\n",
      "ordinal features encoding, 84-86\n",
      "\n",
      "chi-squared statistics, 174-176\n",
      "class separability maximization, 162-164\n",
      "classes, 102\n",
      "classification_report, 203\n",
      "\n",
      "341\n",
      "\n",
      "\f",
      "classifier prediction evaluation, 186-188\n",
      "classifier__, 215\n",
      "cleaning and parsing HTML, 97\n",
      "cleaning text, 95-96\n",
      "clustering, 74-76, 287-295\n",
      "\n",
      "Agglomerative, 294-295\n",
      "cluster_centers_, 290\n",
      "DBSCAN, 292-293\n",
      "evaluating clustering models, 198-199\n",
      "k-means, 75, 288\n",
      "meanshift, 291\n",
      "mini-batch k-means, 290\n",
      "\n",
      "color isolation, 135-137\n",
      "compile, 302\n",
      "concat, 55\n",
      "confusion matrix, 194-196\n",
      "contamination, 70\n",
      "contrast in images, 133-135\n",
      "convolutional neural networks (ConvNets),\n",
      "\n",
      "327-332\n",
      "\n",
      "corner detection, 146-150\n",
      "correlation matrix, 172\n",
      "count, 42\n",
      "CountVectorizer, 104-106\n",
      "criterion, 236\n",
      "cropping images, 126-128\n",
      "cross-validation\n",
      "\n",
      "cross_val_score, 221\n",
      "\n",
      "cross-validation (CV), 178, 179, 193\n",
      "\n",
      "cross_val_score, 182, 186-188, 322\n",
      "GridSearchCV, 210-211\n",
      "k-fold cross-validation, 321-322\n",
      "nested, 220-222\n",
      "out-of-bag (OOB) observations as alterna‐\n",
      "\n",
      "tive to, 250\n",
      "\n",
      "CSR matrices, 4\n",
      "CSV files, 27\n",
      "custom evaluation metric, 199-201\n",
      "cv, 182\n",
      "\n",
      "D\n",
      "data\n",
      "\n",
      "categorical (see categorical data)\n",
      "defined, xiii\n",
      "missing values, 76-79\n",
      "\n",
      "data loading, 23-31\n",
      "CSV files, 27\n",
      "Excel file, 28\n",
      "JSON files, 29\n",
      "\n",
      "342 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "sample datasets, 23-24\n",
      "simulated data, 25-27\n",
      "from SQL database, 30\n",
      "\n",
      "data wrangling, 33-60\n",
      "\n",
      "(see also DataFrames)\n",
      "\n",
      "DataFrames, 33-60\n",
      "\n",
      "applying function over all elements, 54\n",
      "applying function to groups, 55\n",
      "concatenating, 55-57\n",
      "conditional statements, 38\n",
      "creating, 34\n",
      "deleting columns, 46-47\n",
      "deleting rows, 47\n",
      "describing data in, 35-37\n",
      "descriptive statistics and, 42-43\n",
      "dropping duplicate rows, 48-49\n",
      "grouping rows, 50-53\n",
      "index values, 38\n",
      "looping over a column, 53\n",
      "merging, 57-60\n",
      "missing values selection, 44-46\n",
      "navigating, 37-38\n",
      "renaming columns, 41-42\n",
      "replacing values, 39-41\n",
      "unique values in, 43\n",
      "\n",
      "dates and times (datetimes) (see time series\n",
      "\n",
      "data)\n",
      "\n",
      "DBSCAN clustering, 292-293\n",
      "decision trees\n",
      "\n",
      "controlling tree size, 246-247\n",
      "DecisionTreeClassifier, 233-234\n",
      "DecisionTreeRegressor, 235-236\n",
      "model visualization, 236-238\n",
      "\n",
      "deep learning, 298\n",
      "describe, 36, 36\n",
      "determinants, 12\n",
      "diagonal, 13\n",
      "dictionaries of features, 86-88\n",
      "dictionary of candidate learning algorithms,\n",
      "\n",
      "214-215\n",
      "\n",
      "DictVectorize, 86-88\n",
      "dimensionality reduction, 157-167\n",
      "\n",
      "(see also feature extraction; feature selec‐\n",
      "\n",
      "tion)\n",
      "discreditization, 73\n",
      "distance, 255\n",
      "distance metrics, 253\n",
      "document frequency, 107\n",
      "DOT format, 236-238\n",
      "\n",
      "\f",
      "dot products, 16\n",
      "downsampling, 92, 94\n",
      "drop/drop_duplicates, 46, 48-49\n",
      "dropout, 317\n",
      "DummyClassifier, 184-186\n",
      "dummying, 84\n",
      "DummyRegressor, 183-184\n",
      "\n",
      "E\n",
      "early stopping, 315-317\n",
      "edge detection, 144-146\n",
      "Eigenvalues/Eigenvectors, 15\n",
      "elements, applying operations to, 6\n",
      "epochs, 304\n",
      "eps, 293\n",
      "Euclidean distance, 253\n",
      "evaluating models (see model evaluation)\n",
      "Excel files, 28\n",
      "explained_variance_ratio_, 163\n",
      "\n",
      "F\n",
      "false positive rate (FPR), 191\n",
      "feature creation, 150-152\n",
      "feature extraction, 157-167\n",
      "\n",
      "linear discriminant analysis (LDA), 162-164\n",
      "linearly inseparable data, 160-162\n",
      "non-negative matrix factorization (NMF),\n",
      "\n",
      "165-166\n",
      "\n",
      "principal component analysis (PCA),\n",
      "\n",
      "158-162\n",
      "\n",
      "Truncated Singular Value Decomposition\n",
      "\n",
      "(TSVD), 166-167\n",
      "\n",
      "feature reduction, 231-232\n",
      "feature selection, 157, 169-178\n",
      "\n",
      "binary feature variance thresholding, 171\n",
      "highly correlated features, 172-174\n",
      "irrelevant features removal, 174-176\n",
      "methods for, 169\n",
      "recursive feature elimination (RFE),\n",
      "\n",
      "176-178\n",
      "\n",
      "variance thresholding (VT), 170-172\n",
      "\n",
      "features_pca__n_components, 217\n",
      "FeatureUnion, 216\n",
      "feature_importances_, 241-243\n",
      "feedforward neural networks, 297, 300\n",
      "filepath, 320\n",
      "filter2D, 131\n",
      "fit, xiii, 62\n",
      "fit_generator, 332\n",
      "\n",
      "fit_transform, 62\n",
      "flatten, 11\n",
      "flow_from_directory, 332\n",
      "for loops, 54\n",
      "forests (see random forests; tree-based models)\n",
      "forward propagation, 298\n",
      "forward-filling, 120\n",
      "FunctionTransformer, 68, 69\n",
      "\n",
      "G\n",
      "Gaussian naive Bayes, 280-282\n",
      "get_feature_names, 87\n",
      "goodFeaturesToTrack, 148\n",
      "GrabCut, 140-144\n",
      "gridsearch.fit, 221\n",
      "GridSearchCV, 210-211, 216, 221, 256-257,\n",
      "\n",
      "322-324\n",
      "\n",
      "groupby, 50-51, 55\n",
      "\n",
      "H\n",
      "Harris corner detector, 146-150\n",
      "HDF5 model, 339-340\n",
      "head, 35, 36\n",
      "hierarchical merging, 294-295\n",
      "highly correlated features, 172-174\n",
      "histogram equalization, 133-135\n",
      "histograms, 153-156\n",
      "hold-out, 180\n",
      "HTML, parsing and cleaning, 97\n",
      "hyperparameters, xiii\n",
      "selecting, 322-324\n",
      "tuning/optimizating (see model selection)\n",
      "value effects, 205-207\n",
      "hyperplanes, 267, 276-277\n",
      "\n",
      "I\n",
      "iloc, 37, 38\n",
      "image augmentation, 331\n",
      "image classification, 121-156\n",
      "\n",
      "background removal, 140-144\n",
      "binarizing, 137-140\n",
      "blurring, 128-130\n",
      "color histograms as features, 153-156\n",
      "contrast, 133-135\n",
      "corner detection, 146-150\n",
      "cropping, 126-128\n",
      "edge detection, 144-146\n",
      "feature creation, 150-152\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "343\n",
      "\n",
      "\f",
      "isolating colors, 135-137\n",
      "loading, 122-124\n",
      "mean color as a feature, 152\n",
      "resizing, 125\n",
      "saving, 124\n",
      "sharpening, 131-132\n",
      "ImageDataGenerator, 332\n",
      "imbalanced classes, 245, 277\n",
      "imputation, 78, 88-90\n",
      "Imputer, 78\n",
      "imwrite, 124\n",
      "index slicing, 112\n",
      "interaction features, 66\n",
      "interactive effects, 225-227\n",
      "interpolation, 119\n",
      "IQR, 70\n",
      "irrelevant classification features removal,\n",
      "\n",
      "174-176\n",
      "\n",
      "isnull, 44\n",
      "\n",
      "J\n",
      "JSON files, 29\n",
      "\n",
      "K\n",
      "k-fold cross-validation (KFCV), 181-182,\n",
      "\n",
      "321-322\n",
      "\n",
      "k-means clustering, 75, 288\n",
      "k-nearest neighbors (KNN), 78-79, 89-90,\n",
      "\n",
      "251-258\n",
      "creating a KNN classifier, 254-255\n",
      "finding best k value, 256-257\n",
      "locating, 251-254\n",
      "radius-based (RNN) classifier, 257-258\n",
      "\n",
      "Keras, 298\n",
      "\n",
      "convolutional neural networks with,\n",
      "\n",
      "327-331\n",
      "\n",
      "Dropout, 317-319\n",
      "EarlyStopping, 315-317\n",
      "fit method of training, 304\n",
      "GridSearchCV, 322-324\n",
      "input_shape, 302\n",
      "model saving and loading, 339-340\n",
      "ModelCheckpoint, 319-320\n",
      "model_to_dot, 325-326\n",
      "plot_model, 325-326\n",
      "predict, 309-310\n",
      "scikit-learn wrapper, 321-322\n",
      "Sequential, 300-302\n",
      "softmax activation, 305-307\n",
      "\n",
      "344 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "training for regression, 307-308\n",
      "weight regularization, 313-315\n",
      "\n",
      "KerasClassifier, 322\n",
      "kernelPCA, 160-162\n",
      "kernels, 128-130, 131\n",
      "\n",
      "for nonlinear decision boundaries, 270-274\n",
      "for non-linear dimensionality reduction,\n",
      "\n",
      "160-162\n",
      "\n",
      "KNeighborsClassifier, 254-255\n",
      "kneighbors_graph, 253\n",
      "kurtosis, 43\n",
      "\n",
      "L\n",
      "L1/L2 norm, 66\n",
      "LabelBinarizer, 82\n",
      "lagged time features, 116\n",
      "lasso regression, 229-232\n",
      "learning algorithms, xiii\n",
      "learning curves, 201-203\n",
      "learning_rate, 248\n",
      "limit_direction, 120\n",
      "linear discriminant analysis, 162-164\n",
      "linear regression, 223-232\n",
      "\n",
      "feature reduction, 231-232\n",
      "fitting a line, 223-225\n",
      "interactive effects, 225-227\n",
      "nonlinear relationships in, 227-229\n",
      "variance reduction with regularization,\n",
      "\n",
      "229-231\n",
      "\n",
      "LinearDiscriminantAnalysis, 164\n",
      "LinearSVC, 267-270\n",
      "linkage, 295\n",
      "loading images, 122-124\n",
      "loc, 37, 38\n",
      "logistic regression, 259-265\n",
      "\n",
      "binary classifiers, 259-260\n",
      "and large data sets, 263\n",
      "multinomial (MLR), 261-262\n",
      "variance reduction with regularization,\n",
      "\n",
      "262-263\n",
      "\n",
      "LogisticRegressionCV, 219-220, 263\n",
      "long short-term memory (LSTM) recurrent\n",
      "\n",
      "neural network, 333-335\n",
      "\n",
      "loops, 54\n",
      "loss/loss functions, xiii, 298, 301-302\n",
      "\n",
      "M\n",
      "make_blobs, 26-27\n",
      "make_circles, 160-162\n",
      "\n",
      "\f",
      "make_classification, 25-27\n",
      "make_regression, 25-26\n",
      "make_scorer, 200-201\n",
      "Manhattan distance, 253\n",
      "Matplotlib, 122, 310-313\n",
      "matrices\n",
      "\n",
      "adding/subtracting, 17\n",
      "calculating trace, 14\n",
      "compressed sparse row (CSR), 4\n",
      "confusion matrix, 194-196\n",
      "creating, 2-4\n",
      "describing, 6\n",
      "determinants, 12\n",
      "diagonal elements, 13\n",
      "factorization, 165-166\n",
      "finding Eigenvalues/Eigenvectors, 15\n",
      "flattening, 12\n",
      "inverting, 19\n",
      "multiplying, 18\n",
      "rank, 12\n",
      "selecting elements in, 4\n",
      "\n",
      "m1sei, 5\n",
      "sparse, 3, 105\n",
      "max pooling, 330\n",
      "maximum/minimum values, 7, 42\n",
      "max_depth, 247\n",
      "max_features, 239, 241\n",
      "max_output_value, 139\n",
      "mean, 42\n",
      "mean color as feature, 152\n",
      "mean squared error (MSE), 196-198\n",
      "meanshift clustering, 291\n",
      "median, 43\n",
      "merge, 57-60\n",
      "metric, 253, 255, 293\n",
      "mini-batch k-means clustering, 290\n",
      "Minkowski distance, 253\n",
      "MinMaxScaler, 61-62\n",
      "min_impurity_split, 247\n",
      "min_samples, 293\n",
      "Missing At Random (MAR), 77\n",
      "Missing Completely At Random (MCAR), 77\n",
      "missing data, 76-79\n",
      "missing data in time series, 118-120\n",
      "Missing Not At Random (MNAR), 77\n",
      "mode, 43\n",
      "model evaluation, 179-207\n",
      "\n",
      "baseline classification model, 184-186\n",
      "baseline regression model, 183-184\n",
      "\n",
      "binary classifier prediction evaluation,\n",
      "\n",
      "186-188\n",
      "\n",
      "binary classifier thresholds, 189-192\n",
      "classification report, 203\n",
      "classifier performance visualization,\n",
      "\n",
      "194-196\n",
      "\n",
      "clustering models, 198-199\n",
      "cross-validation (CV), 179\n",
      "custom evaluation metric, 199-201\n",
      "hyperparameter value effects, 205-207\n",
      "multiclass classifier predictions, 192-194\n",
      "regression models, 196-198\n",
      "training set size visualization, 201-203\n",
      "\n",
      "model selection, 209-222\n",
      "\n",
      "algorithm-specific methods for speed,\n",
      "\n",
      "219-220\n",
      "\n",
      "exhaustive search, 210-211\n",
      "from multiple learning algorithms, 214-215\n",
      "parallelization for speed, 217-218\n",
      "post-selection performance evaluation,\n",
      "\n",
      "220-222\n",
      "\n",
      "preprocessing during, 215-217\n",
      "randomized search, 212-213\n",
      "\n",
      "model.pkl, 338, 338\n",
      "ModelCheckpoint, 317, 319-320\n",
      "models, xiii\n",
      "model_to_dot, 325-326\n",
      "moving time windows, 117-118\n",
      "multiclass classifier predictions, 192-194\n",
      "multiclass classifiers, 301, 305-307\n",
      "multinomial logistic regression (MLR), 261-262\n",
      "multinomial naive Bayes, 282-283\n",
      "\n",
      "N\n",
      "n-grams, 105\n",
      "naive Bayes classifiers, 279-286\n",
      "\n",
      "Bernoulli, 283-284\n",
      "calibrating predicted probabilities, 284-286\n",
      "Gaussian, 280-282\n",
      "multinomial, 282-283\n",
      "\n",
      "Natural Language Toolkit (NLTK), 99\n",
      "\n",
      "PorterStemmer, 100\n",
      "pre-trained parts of speech tagger, 101-103\n",
      "stopwords, 99\n",
      "word_tokenize, 98\n",
      "\n",
      "neg_mean_squared_error, 197\n",
      "nested cross-validation (CV), 220-222\n",
      "neural networks, 297-335\n",
      "\n",
      "binary classification, 303-305\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "345\n",
      "\n",
      "\f",
      "convolutional, 327-332\n",
      "deep, 298\n",
      "designing, 300\n",
      "dropout, 317\n",
      "early stopping, 315-317\n",
      "feedforward, 297, 300\n",
      "hyperparameter selection, 322-324\n",
      "image augmentation, 331\n",
      "image classification, 327-331\n",
      "k-fold cross-validation (KFCV), 321-322\n",
      "making predictions, 309-310\n",
      "multiclass classifiers, 305-307\n",
      "overfitting, reducing, 313-319\n",
      "preprocessing data for, 298-299\n",
      "recurrent, 333-335\n",
      "regression training, 307-308\n",
      "saving model training process, 319-320\n",
      "text data classification, 333-335\n",
      "training history visualization, 310-313\n",
      "visualizing, 325-326\n",
      "weight regularization, 313-315\n",
      "\n",
      "nominal categorical data, 81\n",
      "non-negative matrix factorization (NMF),\n",
      "\n",
      "165-166\n",
      "\n",
      "nonlinear decision boundaries, 270-274\n",
      "Normalizer, 64-66\n",
      "normalizing observations, 64-66\n",
      "notnull, 44\n",
      "numerical data\n",
      "\n",
      "clustering observations, 74-76\n",
      "discreditization, 73\n",
      "imputing missing values, 78-79\n",
      "observations with missing values, 76-77\n",
      "observations, normalizing, 64-66\n",
      "outliers, detecting, 69-71\n",
      "outliers, handling, 71-72\n",
      "polynomial and interaction features, 66-68\n",
      "rescaling, 61-62\n",
      "standardizing, 63-64\n",
      "transforming features, 68\n",
      "\n",
      "NumPy\n",
      "\n",
      "add/subtract, 17\n",
      "creating matrices in, 2-4\n",
      "creating vectors in, 1\n",
      "for deleting missing values, 76\n",
      "describing matrices in, 6\n",
      "det, 12\n",
      "diagonal, 13\n",
      "dot, 16, 18\n",
      "\n",
      "346 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "flatten, 11, 150\n",
      "inv, 19\n",
      "linalg.eig, 15\n",
      "matrix_rank, 12\n",
      "max and min, 7\n",
      "mean, var, and std, 8\n",
      "NaN, 45, 72\n",
      "offset, 14\n",
      "random, 20\n",
      "reshape, 9-10, 12\n",
      "selecting elements in, 4-5\n",
      "trace, 14\n",
      "transpose, 10\n",
      "vectorize function, 6\n",
      "\n",
      "n_clusters, 289, 295\n",
      "n_components, 159, 164, 166, 167\n",
      "n_estimators, 240, 241, 248\n",
      "n_iter, 213\n",
      "n_jobs, 218, 255\n",
      "n_jobs=-1, 182\n",
      "n_support_, 277\n",
      "\n",
      "O\n",
      "observations, xiii\n",
      "\n",
      "clustering, 74-76\n",
      "deleting those with missing values, 76-77\n",
      "\n",
      "offset, 14\n",
      "one-hot encoding, 82-84\n",
      "one-vs-rest logistic regression (OVR), 261-262\n",
      "Open Source Computer Vision Library\n",
      "\n",
      "(OpenCV), 121\n",
      "Canny edge detector, 144-146\n",
      "cornerHarris, 146-150\n",
      "equalizeHist, 133-135\n",
      "imread, 122\n",
      "imwrite, 124\n",
      "optimizers, 302\n",
      "ordinal categorical data, 81, 84-86\n",
      "out-of-bag (OOB) observations, 249\n",
      "outliers\n",
      "\n",
      "detecting, 69-71\n",
      "handling, 71\n",
      "outlier_label, 258\n",
      "overfitting, 313-319\n",
      "\n",
      "P\n",
      "pad_sequences, 334\n",
      "pandas\n",
      "\n",
      "apply, 68, 69\n",
      "\n",
      "\f",
      "create_engine, 31\n",
      "DataFrame object (see DataFrames)\n",
      "descriptive statistics, 42-43\n",
      "for deleting missing values, 76\n",
      "json_normalize, 30\n",
      "read_csv, 28\n",
      "read_excel, 28\n",
      "read_json, 29\n",
      "read_sql_query, 30, 31\n",
      "rolling, 117\n",
      "Series.dt, 113, 115\n",
      "shift, 116\n",
      "TimeDelta, 114\n",
      "to_datetime, 109\n",
      "transformation in, 68\n",
      "tz_localize, 111\n",
      "weekday_name, 115\n",
      "parallelization, 217-218\n",
      "parameters, xiii\n",
      "parsing and cleaning HTML, 97\n",
      "Penn Treebank tags, 102\n",
      "performance, xiii\n",
      "performance boosting, 247-249\n",
      "performance evaluation, 220-222\n",
      "pickle model, 337-338\n",
      "Platt scaling, 275\n",
      "plot_model, 325-326\n",
      "polynomial regression, 227-229\n",
      "PolynomialFeatures, 66-68, 226-229\n",
      "pooling layers, 330\n",
      "PorterStemmer, 100\n",
      "pos_tag, 103\n",
      "precision, 187\n",
      "predicted probabilities, 274-276, 284-286\n",
      "predictions, 309-310\n",
      "preprocess, 217\n",
      "preprocessing steps in model selection, 215-217\n",
      "principal component analysis (PCA), 158-162\n",
      "punctuation, removing, 98\n",
      "\n",
      "R\n",
      "Radius-based (RNN) classifier, 257-258\n",
      "random forests, 233\n",
      "\n",
      "(see also tree-based models)\n",
      "comparing feature importance, 241-243\n",
      "out-of-bag (OOB) observations, 249\n",
      "RandomForestClassifier, 91, 203, 207,\n",
      "\n",
      "238-240\n",
      "\n",
      "RandomForestRegressor, 240-241\n",
      "\n",
      "selecting important features, 243-244\n",
      "\n",
      "random variables, 20\n",
      "RandomizedSearchCV, 212-213\n",
      "rank, 12\n",
      "recall, 187\n",
      "Receiving Operating Characteristic (ROC)\n",
      "\n",
      "curve, 189-192\n",
      "\n",
      "rectified linear unit (RELU), 301\n",
      "recurrent neural network, 333-335\n",
      "recursive feature elimination (RFE), 176-178\n",
      "regression function, 301\n",
      "regression model evaluation, 196-198\n",
      "regression training, 307-308\n",
      "regularization, 229-231, 262-263, 318\n",
      "regularization penalty, 210\n",
      "rename, 41\n",
      "resample, 51-53\n",
      "rescaling, 61-62\n",
      "reshape, 9-10, 12\n",
      "resize, 125\n",
      "resizing images, 125\n",
      "RFECV, 176-178\n",
      "RGB versus GBR, 124\n",
      "ridge regression, 229-231\n",
      "RobustScaler, 64\n",
      "rolling time windows, 117-118\n",
      "\n",
      "S\n",
      "saving images, 124\n",
      "score, 184\n",
      "scoring, 182\n",
      "search, exhaustive, 210-211\n",
      "SelectFromModel, 243-244\n",
      "Series.dt, 113\n",
      "shape, 36, 304\n",
      "sharpening images, 131-132\n",
      "Shi-Tomasi corner detector, 148\n",
      "show_shapes, 326\n",
      "shrinkage penalty (see regularization)\n",
      "silhouette coefficient, 199\n",
      "silhouette_score, 199, 199\n",
      "skewness, 43\n",
      "slicing arrays, 126-128\n",
      "softmax activation, 305-307\n",
      "sparse data feature reduction, 166-167\n",
      "sparse matrices, 105\n",
      "SQL queries, 30\n",
      "sqrt, 240\n",
      "standard deviation, 8, 43, 63\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "347\n",
      "\n",
      "\f",
      "standard error of the mean, 43\n",
      "standardization, 63\n",
      "standardizer, 182\n",
      "StandardScaler, 63, 298-299\n",
      "stochastic average gradient (SAG) solver, 263\n",
      "stopwords, 99\n",
      "strategy, 184, 185\n",
      "stratified, 185\n",
      "stratified k-fold, 181\n",
      "strings, converting to dates, 109\n",
      "sum, 42\n",
      "support vector classifiers (SVCs), 267-270\n",
      "support vector machines, 267-278\n",
      "\n",
      "identifying support vectors, 276-277\n",
      "imbalanced classes, 277\n",
      "kernel functions for nonlinear decision\n",
      "\n",
      "boundaries, 270-274\n",
      "\n",
      "predicted probabilities for, 274-276\n",
      "support vector classifiers (SVCs) and,\n",
      "\n",
      "267-270\n",
      "support_vectors_, 277\n",
      "svd_solver=\"randomized\", 159\n",
      "\n",
      "T\n",
      "tail, 36\n",
      "term frequency, 107\n",
      "term frequency-inverse document frequency\n",
      "\n",
      "(tf-idf), 106\n",
      "\n",
      "text handling, 95-108\n",
      "\n",
      "cleaning text, 95-96\n",
      "encoding with Bag of Words model,\n",
      "\n",
      "104-106\n",
      "\n",
      "parsing and cleaning HTML, 97\n",
      "removing punctuation, 98\n",
      "stemming words, 100\n",
      "stop words removal, 99\n",
      "tagging parts of speech, 101-103\n",
      "weighting word importance, 106-108\n",
      "word tokenization, 98\n",
      "\n",
      "TfidfVectorizer, 106\n",
      "thresholding, 138\n",
      "time series data, 109-120\n",
      "\n",
      "calculating difference between dates, 114\n",
      "converting strings to dates, 109\n",
      "encoding days of week, 115\n",
      "lagged features, 116\n",
      "missing data, 118-120\n",
      "multiple date feature creation, 113, 114\n",
      "rolling time windows, 117-118\n",
      "\n",
      "348 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "selecting dates and times, 112\n",
      "time zones, 111-112\n",
      "\n",
      "TimeDelta, 114\n",
      "tokenization, 98\n",
      "toy datasets, 24\n",
      "to_datetime, 109\n",
      "trace, 14\n",
      "train, xiii\n",
      "training set size effects, 201-203\n",
      "transform, 62\n",
      "transforming features, 68\n",
      "translate, 98\n",
      "transposing, 10\n",
      "tree-based models, 233-250\n",
      "\n",
      "boosting, 247-249\n",
      "controlling tree size, 246-247\n",
      "decision tree classifier, 233-234\n",
      "decision tree regressor, 235-236\n",
      "imbalanced classes, 245\n",
      "out-of-bag (OOB) observations, 249\n",
      "random forest classifier, 238-240\n",
      "random forest feature importance, 241-243\n",
      "random forest feature selection, 243-244\n",
      "random forest regressor, 240-241\n",
      "visualizing, 236-238\n",
      "\n",
      "TrigramTagger, 103\n",
      "true positive rate (TPR), 191\n",
      "Truncated Singular Value Decomposition\n",
      "\n",
      "(TSVD), 166-167\n",
      "\n",
      "tz_localize, 111\n",
      "\n",
      "U\n",
      "uniform, 186\n",
      "UnigramTagger, 103\n",
      "unique, 43\n",
      "upsampling, 92\n",
      "\n",
      "V\n",
      "validation, 180\n",
      "validation curve, 205-207\n",
      "validation_curve, 207\n",
      "validation_data, 305\n",
      "validation_split, 305\n",
      "value_counts, 43\n",
      "variance, 8, 43\n",
      "variance reduction, 262-263\n",
      "variance thresholding, 170-172\n",
      "vectorize, 6\n",
      "vectors\n",
      "\n",
      "\f",
      "calculating dot products, 16\n",
      "creating, 1\n",
      "selecting elements in, 4-5\n",
      "\n",
      "verbose, 211, 304\n",
      "visualization of classifier performance, 194-196\n",
      "\n",
      "weight regularization, 313-315\n",
      "weights, 255\n",
      "whiten=True, 159\n",
      "word embeddings, 335\n",
      "word tokenization, 98\n",
      "\n",
      "W\n",
      "weekday_name, 115\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "349\n",
      "\n",
      "\f",
      "About the Author\n",
      "\n",
      "Chris  Albon  is  a  data  scientist  and  political  scientist  with  a  decade  of  experience\n",
      "applying statistical learning, artificial intelligence, and software engineering to politi‐\n",
      "cal,  social,  and  humanitarian  efforts—from  election  monitoring  to  disaster  relief.\n",
      "Currently, Chris is the Chief Data Scientist at BRCK, a Kenyan startup building a rug‐\n",
      "ged network for frontier market internet users.\n",
      "\n",
      "Colophon\n",
      "\n",
      "The  animal  on  the  cover  of  Machine  Learning  with  Python  Cookbook  is  the  Narina\n",
      "trogon (Apaloderma narina), which is named for the mistress of French ornithologist\n",
      "François Levaillant, who derived the name from a Khoikhoi word for “flower”, as his\n",
      "mistress’s  name  was  difficult  to  pronounce.  The  Narina  trogon  is  largely  found  in\n",
      "Africa, inhabiting both low and highlands, and tropical and temperate climates, usu‐\n",
      "ally nesting in the hollows of trees. Its diverse range of habitats makes it a species of\n",
      "least conservation concern.\n",
      "\n",
      "The Narina trogon eats mostly insects and small invertebrates as well as small rodents\n",
      "and reptiles. Males, which are more brightly colored, give off a grating, low, repeated\n",
      "hoot to defend territory and attract mates. Both sexes have green upper plumage and\n",
      "metallic  blue-green  tail  feathers.  Female  faces  and  chest  plumages  are  brown,  while\n",
      "males  have  bright  red  undersides.  Immature  birds  have  similar  coloring  to  females\n",
      "with distinct white tips to their inner wings.\n",
      "\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com.\n",
      "\n",
      "The cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\n",
      "writer  and  Guardian  Sans.  The  text  font  is  Adobe  Minion  Pro;  the  heading  font  is\n",
      "Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "\n",
      "\f",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa26cef",
   "metadata": {},
   "source": [
    "# Extract image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ebeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF\n",
    "#!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e695cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "import PIL.Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63e596ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\E-Book\\\\ML.pdf\")\n",
    "counter = 1\n",
    "\n",
    "for i in range(len(pdf)):\n",
    "    page = pdf[i]\n",
    "    images = page.get_images()\n",
    "    for image in images:\n",
    "        base_img = pdf.extract_image(image[0])\n",
    "        image_data = base_img[\"image\"]\n",
    "        img = PIL.Image.open(io.BytesIO(image_data))\n",
    "        extension = base_img['ext']\n",
    "        img.save(open(f'image{counter}.{extension}', \"wb\"))\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abeb7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99696b89",
   "metadata": {},
   "source": [
    "# extract table from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcad34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tabula-py) (1.4.4)\n",
      "Requirement already satisfied: distro in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tabula-py) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tabula-py) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cf87fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "# https://nbviewer.org/github/chezou/tabula-py/blob/master/examples/tabula_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tables = tabula.read_pdf(\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\E-Book\\\\ML.pdf\", pages='all')\n",
    "\n",
    "if not tables.empty:\n",
    "    df = tables[0]\n",
    "else:\n",
    "    print(\"No tables found in the pdf.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26e75311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/tabula-py/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105fd70e",
   "metadata": {},
   "source": [
    "# convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33041011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e41b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88ef246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your sentence here : \n",
      ">உங்கள் வீட்டில் உள்ள தங்கத்தை வருமானம் ஈட்ட வைக்க இது ஒரு முறை. அதாவது, உங்கள் வீட்டிலுள்ள தங்கத்தை, அரசிடம் ஒப்படைத்து, அதற்கு ஈடான தொகைக்கு வருடா வருடம் வட்டி வருமானமும், தங்கத்தின் மதிப்பு அதிகரிக்கும் பயனையும் ஒரு சேர அனுபவிக்கலாம்.  Read more at: https://tamil.oneindia.com/news/chennai/do-you-know-what-is-gold-monetization-566649.html\n"
     ]
    }
   ],
   "source": [
    "my_sentence = input(\"Enter your sentence here : \\n>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9159bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = gTTS(my_sentence, lang='hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8d4c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.save(\"tamil_hindi.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc253d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32dcb099",
   "metadata": {},
   "source": [
    "# Case Study with ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92a20110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21c079f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv('sentiment_train', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e05fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>Brokeback Mountain was boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>So Brokeback Mountain was really depressing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>As I sit here, watching the MTV Movie Awards, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok brokeback mountain is such a horrible movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh, and Brokeback Mountain was a terrible movie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0             1            The Da Vinci Code book is just awesome.\n",
       "1             1  this was the first clive cussler i've ever rea...\n",
       "2             1                   i liked the Da Vinci Code a lot.\n",
       "3             1                   i liked the Da Vinci Code a lot.\n",
       "4             1  I liked the Da Vinci Code but it ultimatly did...\n",
       "...         ...                                                ...\n",
       "6913          0                     Brokeback Mountain was boring.\n",
       "6914          0       So Brokeback Mountain was really depressing.\n",
       "6915          0  As I sit here, watching the MTV Movie Awards, ...\n",
       "6916          0    Ok brokeback mountain is such a horrible movie.\n",
       "6917          0   Oh, and Brokeback Mountain was a terrible movie.\n",
       "\n",
       "[6918 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d4b1af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>I loved the Da Vinci Code, but now I want some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>i thought da vinci code was great, same with k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code is actually a good movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>I thought the Da Vinci Code was a pretty good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code is one of the most beautiful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code is an * amazing * book, do n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>then I turn on the light and the radio and enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code was REALLY good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>i love da vinci code....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>i loved da vinci code..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>TO NIGHT:: THE DA VINCI CODE AND A BEAUTIFUL M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>THE DA VINCI CODE is AN AWESOME BOOK....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Thing is, I enjoyed The Da Vinci Code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>very da vinci code slash amazing race.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "0           1            The Da Vinci Code book is just awesome.\n",
       "1           1  this was the first clive cussler i've ever rea...\n",
       "2           1                   i liked the Da Vinci Code a lot.\n",
       "3           1                   i liked the Da Vinci Code a lot.\n",
       "4           1  I liked the Da Vinci Code but it ultimatly did...\n",
       "5           1  that's not even an exaggeration ) and at midni...\n",
       "6           1  I loved the Da Vinci Code, but now I want some...\n",
       "7           1  i thought da vinci code was great, same with k...\n",
       "8           1      The Da Vinci Code is actually a good movie...\n",
       "9           1  I thought the Da Vinci Code was a pretty good ...\n",
       "10          1  The Da Vinci Code is one of the most beautiful...\n",
       "11          1  The Da Vinci Code is an * amazing * book, do n...\n",
       "12          1  then I turn on the light and the radio and enj...\n",
       "13          1                 The Da Vinci Code was REALLY good.\n",
       "14          1                           i love da vinci code....\n",
       "15          1                            i loved da vinci code..\n",
       "16          1  TO NIGHT:: THE DA VINCI CODE AND A BEAUTIFUL M...\n",
       "17          1           THE DA VINCI CODE is AN AWESOME BOOK....\n",
       "18          1             Thing is, I enjoyed The Da Vinci Code.\n",
       "19          1             very da vinci code slash amazing race."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[train_ds.sentiment==1][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a547f580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>da vinci code was a terrible movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>Then again, the Da Vinci code is super shitty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>The Da Vinci Code comes out tomorrow, which su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>i thought the da vinci code movie was really b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>0</td>\n",
       "      <td>God, Yahoo Games has this truly-awful looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>0</td>\n",
       "      <td>Da Vinci Code does suck.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>0</td>\n",
       "      <td>And better...-We all know Da Vinci code is bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>0</td>\n",
       "      <td>Last time, Da Vinci code is also a bit disappo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>0</td>\n",
       "      <td>And better...-We all know Da Vinci code is bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>0</td>\n",
       "      <td>And better..-We all know Da Vinci code is bogu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3953</th>\n",
       "      <td>0</td>\n",
       "      <td>And better..-We all know Da Vinci code is bogu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>0</td>\n",
       "      <td>If Jesus is fabricated a la the Da Vinci Code ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>0</td>\n",
       "      <td>I think this bolsters my arguments that both E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>0</td>\n",
       "      <td>And better..-We all know Da Vinci code is bogu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>0</td>\n",
       "      <td>I really hate The Da Vinci Code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>0</td>\n",
       "      <td>The more you say The Da Vinci Code is evil, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>0</td>\n",
       "      <td>da vinci code sucked too long and too slow.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>0</td>\n",
       "      <td>Da Vinci Code sucked, as expected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>0</td>\n",
       "      <td>not sure if i already threw this at you but I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>0</td>\n",
       "      <td>This is why I hate the Da Vinci Code: because ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "3943          0                da vinci code was a terrible movie.\n",
       "3944          0  Then again, the Da Vinci code is super shitty ...\n",
       "3945          0  The Da Vinci Code comes out tomorrow, which su...\n",
       "3946          0  i thought the da vinci code movie was really b...\n",
       "3947          0  God, Yahoo Games has this truly-awful looking ...\n",
       "3948          0                           Da Vinci Code does suck.\n",
       "3949          0  And better...-We all know Da Vinci code is bog...\n",
       "3950          0  Last time, Da Vinci code is also a bit disappo...\n",
       "3951          0  And better...-We all know Da Vinci code is bog...\n",
       "3952          0  And better..-We all know Da Vinci code is bogu...\n",
       "3953          0  And better..-We all know Da Vinci code is bogu...\n",
       "3954          0  If Jesus is fabricated a la the Da Vinci Code ...\n",
       "3955          0  I think this bolsters my arguments that both E...\n",
       "3956          0  And better..-We all know Da Vinci code is bogu...\n",
       "3957          0                   I really hate The Da Vinci Code.\n",
       "3958          0  The more you say The Da Vinci Code is evil, th...\n",
       "3959          0    da vinci code sucked too long and too slow.....\n",
       "3960          0                 Da Vinci Code sucked, as expected.\n",
       "3961          0  not sure if i already threw this at you but I ...\n",
       "3962          0  This is why I hate the Da Vinci Code: because ..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[train_ds.sentiment==0][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb52796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6918 entries, 0 to 6917\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  6918 non-null   int64 \n",
      " 1   text       6918 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 108.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b1d129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.countplot(x='sentiment', data=train_ds)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(p.get_height(), (p.get_x() + 0.1, p.get_height()+50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "46e1ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.00375831165076\n",
      "56.99624168834924\n"
     ]
    }
   ],
   "source": [
    "review_volumn = train_ds['sentiment'].value_counts()\n",
    "print((review_volumn[0]/train_ds.shape[0])*100)\n",
    "print((review_volumn[1]/train_ds.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08be54",
   "metadata": {},
   "source": [
    "# Count vector model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "100879da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "feature_vector = count_vectorizer.fit(train_ds.text)\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf7be8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of features : 2132\n"
     ]
    }
   ],
   "source": [
    "features = feature_vector.get_feature_names()\n",
    "print(\"Total no of features :\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fba26818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depth',\n",
       " 'comparsions',\n",
       " 'ootp',\n",
       " 'impossible',\n",
       " 'crusade',\n",
       " 'whatever',\n",
       " 'attraction',\n",
       " 'lunch',\n",
       " 'congrats',\n",
       " 'runs',\n",
       " 'free',\n",
       " 'biased',\n",
       " 'used',\n",
       " 'arenas',\n",
       " 'consumed',\n",
       " 'writes',\n",
       " 'despised',\n",
       " 'wicked',\n",
       " 'thats',\n",
       " 'lynn',\n",
       " 'yes',\n",
       " 'these',\n",
       " 'same',\n",
       " 'explosions',\n",
       " 'inappropriate',\n",
       " 'creature',\n",
       " '1984',\n",
       " 'dudeee',\n",
       " 'spite',\n",
       " 'girl',\n",
       " 'need',\n",
       " 'conversation',\n",
       " 'move',\n",
       " 'fan',\n",
       " 'fade',\n",
       " 'hips',\n",
       " 'according',\n",
       " 'unfortunate',\n",
       " 'loathed',\n",
       " 'symantec',\n",
       " 'soooooooo',\n",
       " 'tent',\n",
       " 'them',\n",
       " 'most',\n",
       " 'oceans',\n",
       " 'anyhow',\n",
       " 'die',\n",
       " 'few',\n",
       " 'watch',\n",
       " 'tea']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "random.sample(features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef41f10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2122</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 2132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  2122  \\\n",
       "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "6913     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "6914     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "6915     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "6916     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "6917     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "      2123  2124  2125  2126  2127  2128  2129  2130  2131  \n",
       "0        0     0     0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "6913     0     0     0     0     0     0     0     0     0  \n",
       "6914     0     0     0     0     0     0     0     0     0  \n",
       "6915     0     0     0     0     0     0     0     0     0  \n",
       "6916     0     0     0     0     0     0     0     0     0  \n",
       "6917     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[6918 rows x 2132 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_features = count_vectorizer.transform(train_ds.text)\n",
    "train_ds_df = pd.DataFrame(train_ds_features.toarray())\n",
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3bf38366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6918, 2132)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "297fe0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>007</th>\n",
       "      <th>10</th>\n",
       "      <th>10pm</th>\n",
       "      <th>12</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>1984</th>\n",
       "      <th>1st</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yip</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yuh</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>µª</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 2132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  007  10  10pm  12  16  17  1984  1st  200  ...  yip  you  young  \\\n",
       "0      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "1      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "2      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "3      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "4      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "...   ..  ...  ..   ...  ..  ..  ..   ...  ...  ...  ...  ...  ...    ...   \n",
       "6913   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6914   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6915   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6916   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6917   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "\n",
       "      younger  your  yuck  yuh  zach  zen  µª  \n",
       "0           0     0     0    0     0    0   0  \n",
       "1           0     0     0    0     0    0   0  \n",
       "2           0     0     0    0     0    0   0  \n",
       "3           0     0     0    0     0    0   0  \n",
       "4           0     0     0    0     0    0   0  \n",
       "...       ...   ...   ...  ...   ...  ...  ..  \n",
       "6913        0     0     0    0     0    0   0  \n",
       "6914        0     0     0    0     0    0   0  \n",
       "6915        0     0     0    0     0    0   0  \n",
       "6916        0     0     0    0     0    0   0  \n",
       "6917        0     0     0    0     0    0   0  \n",
       "\n",
       "[6918 rows x 2132 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df.columns = features\n",
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "225b8cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                     text\n",
       "0          1  The Da Vinci Code book is just awesome."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e72a8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quaintly</th>\n",
       "      <th>queens</th>\n",
       "      <th>queer</th>\n",
       "      <th>question</th>\n",
       "      <th>questions</th>\n",
       "      <th>quick</th>\n",
       "      <th>quip</th>\n",
       "      <th>quirky</th>\n",
       "      <th>quite</th>\n",
       "      <th>quiz</th>\n",
       "      <th>...</th>\n",
       "      <th>refusing</th>\n",
       "      <th>regardless</th>\n",
       "      <th>rehearsal</th>\n",
       "      <th>related</th>\n",
       "      <th>relaxed</th>\n",
       "      <th>release</th>\n",
       "      <th>relic</th>\n",
       "      <th>religion</th>\n",
       "      <th>religious</th>\n",
       "      <th>reminded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   quaintly  queens  queer  question  questions  quick  quip  quirky  quite  \\\n",
       "0         0       0      0         0          0      0     0       0      0   \n",
       "\n",
       "   quiz  ...  refusing  regardless  rehearsal  related  relaxed  release  \\\n",
       "0     0  ...         0           0          0        0        0        0   \n",
       "\n",
       "   relic  religion  religious  reminded  \n",
       "0      0         0          0         0  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df.iloc[0:1,1500:1550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e1bf46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  4, ...,  1, 80,  1], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_count  = np.sum(train_ds_features.toarray(), axis=0)\n",
    "features_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e4edfe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10pm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>yuck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>yuh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>zach</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>zen</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>µª</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features  counts\n",
       "0          00       1\n",
       "1         007       1\n",
       "2          10       4\n",
       "3        10pm       1\n",
       "4          12       1\n",
       "...       ...     ...\n",
       "2127     yuck       1\n",
       "2128      yuh       1\n",
       "2129     zach       1\n",
       "2130      zen      80\n",
       "2131       µª       1\n",
       "\n",
       "[2132 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count_df = pd.DataFrame(dict(features=features, counts=features_count))\n",
    "feature_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2eebe2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Density of the words')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(feature_count_df.counts, bins=100, range=(0,2132))\n",
    "plt.xlabel(\"Frequency of words\")\n",
    "plt.ylabel(\"Density of the words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a413f0e",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0f2f1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>about</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acceptable</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aching</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>yes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>yet</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>you</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>your</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>zen</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       features  counts\n",
       "0            10       4\n",
       "1         about     129\n",
       "2    absolutely     101\n",
       "3    acceptable      81\n",
       "4        aching      80\n",
       "..          ...     ...\n",
       "495         yes       5\n",
       "496         yet       8\n",
       "497         you     331\n",
       "498        your       9\n",
       "499         zen      80\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "feature_vector = count_vectorizer.fit(train_ds.text)\n",
    "features = feature_vector.get_feature_names()\n",
    "train_ds_features = count_vectorizer.transform(train_ds.text)\n",
    "features_count  = np.sum(train_ds_features.toarray(), axis=0)\n",
    "feature_count = pd.DataFrame(dict(features=features, counts=features_count))\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc84ffc",
   "metadata": {},
   "source": [
    "# removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "224a4785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "len(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f648950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1878a2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>about</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acceptable</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aching</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>days</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>decided</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>deep</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>demons</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>depressing</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      features  counts\n",
       "0           10       4\n",
       "1        about     129\n",
       "2   absolutely     101\n",
       "3   acceptable      81\n",
       "4       aching      80\n",
       "..         ...     ...\n",
       "95        days       5\n",
       "96     decided       4\n",
       "97        deep      80\n",
       "98      demons       6\n",
       "99  depressing     188\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2da38dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely awesome</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absolutely love</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acceptable</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aching</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>wotshisface needs</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>yeah</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>yeah got</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>zen</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>zen da</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  counts\n",
       "0            absolutely     101\n",
       "1    absolutely awesome      81\n",
       "2       absolutely love      10\n",
       "3            acceptable      81\n",
       "4                aching      80\n",
       "..                  ...     ...\n",
       "495   wotshisface needs      80\n",
       "496                yeah     171\n",
       "497            yeah got      80\n",
       "498                 zen      80\n",
       "499              zen da      80\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=my_stop_words,ngram_range=(1, 2), max_features=500)\n",
    "feature_vector = count_vectorizer.fit(train_ds.text)\n",
    "features = feature_vector.get_feature_names()\n",
    "train_ds_features = count_vectorizer.transform(train_ds.text)\n",
    "features_count  = np.sum(train_ds_features.toarray(), axis=0)\n",
    "feature_count = pd.DataFrame(dict(features=features, counts=features_count))\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "263f0332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>da</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>da vinci</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>vinci</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>mountain</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>brokeback mountain</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>love</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>mission</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>mission impossible</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>impossible</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>like</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>movie</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>sucks</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>sucked</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>hate</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>love brokeback</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>vinci awesome</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>vinci sucked</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>really</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>vinci sucks</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>stupid</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>just</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>suck</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>know</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>love da</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>loved</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>want</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>right</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>think</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>loved brokeback</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>depressing</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>awesome movie</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>mountain awesome</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>people</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>horrible</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>reading</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>series</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>terrible</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>fucking</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>yeah</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>story</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>stupid brokeback</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>oh</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>mountain terrible</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>left</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>terrible movie</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>guy</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>ok</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>start</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  counts\n",
       "71                   da    2001\n",
       "72             da vinci    2001\n",
       "460               vinci    2001\n",
       "290            mountain    2000\n",
       "43            brokeback    2000\n",
       "44   brokeback mountain    1999\n",
       "254                love    1624\n",
       "14              awesome    1127\n",
       "286             mission    1094\n",
       "287  mission impossible    1093\n",
       "188          impossible    1093\n",
       "235                like     974\n",
       "306               movie     783\n",
       "417               sucks     602\n",
       "411              sucked     600\n",
       "154                hate     578\n",
       "256      love brokeback     475\n",
       "462       vinci awesome     474\n",
       "468        vinci sucked     454\n",
       "353              really     374\n",
       "469         vinci sucks     367\n",
       "406              stupid     365\n",
       "208                just     287\n",
       "410                suck     276\n",
       "221                know     276\n",
       "257             love da     273\n",
       "265               loved     256\n",
       "474                want     254\n",
       "364               right     249\n",
       "436               think     202\n",
       "266     loved brokeback     192\n",
       "81           depressing     188\n",
       "17        awesome movie     186\n",
       "291    mountain awesome     185\n",
       "331              people     183\n",
       "182            horrible     181\n",
       "347             reading     181\n",
       "384              series     177\n",
       "428            terrible     174\n",
       "120             fucking     174\n",
       "496                yeah     171\n",
       "404               story     171\n",
       "409    stupid brokeback     170\n",
       "318                  oh     170\n",
       "304   mountain terrible     167\n",
       "230                left     166\n",
       "429      terrible movie     164\n",
       "148                 guy     163\n",
       "320                  ok     163\n",
       "400               start     161"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count.sort_values('counts', ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "724d240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(['harry','potter','movies','code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f42e0d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "644f5562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely awesome</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absolutely love</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acceptable</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aching</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>wotshisface needs</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>yeah</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>yeah got</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>zen</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>zen da</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  counts\n",
       "0            absolutely     101\n",
       "1    absolutely awesome      81\n",
       "2       absolutely love      10\n",
       "3            acceptable      81\n",
       "4                aching      80\n",
       "..                  ...     ...\n",
       "495   wotshisface needs      80\n",
       "496                yeah     171\n",
       "497            yeah got      80\n",
       "498                 zen      80\n",
       "499              zen da      80\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=my_stop_words,ngram_range=(1, 2), max_features=500)\n",
    "feature_vector = count_vectorizer.fit(train_ds.text)\n",
    "features = feature_vector.get_feature_names()\n",
    "train_ds_features = count_vectorizer.transform(train_ds.text)\n",
    "features_count  = np.sum(train_ds_features.toarray(), axis=0)\n",
    "feature_count = pd.DataFrame(dict(features=features, counts=features_count))\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8678250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6918x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 60146 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e0bdfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely awesome</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>aching</th>\n",
       "      <th>aching cock</th>\n",
       "      <th>acne</th>\n",
       "      <th>acne love</th>\n",
       "      <th>action</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>whimpering noises</th>\n",
       "      <th>worth</th>\n",
       "      <th>worth know</th>\n",
       "      <th>wotshisface</th>\n",
       "      <th>wotshisface needs</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yeah got</th>\n",
       "      <th>zen</th>\n",
       "      <th>zen da</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolutely  absolutely awesome  absolutely love  acceptable  aching  \\\n",
       "0              0                   0                0           0       0   \n",
       "1              0                   0                0           0       0   \n",
       "2              0                   0                0           0       0   \n",
       "3              0                   0                0           0       0   \n",
       "4              0                   0                0           0       0   \n",
       "...          ...                 ...              ...         ...     ...   \n",
       "6913           0                   0                0           0       0   \n",
       "6914           0                   0                0           0       0   \n",
       "6915           0                   0                0           0       0   \n",
       "6916           0                   0                0           0       0   \n",
       "6917           0                   0                0           0       0   \n",
       "\n",
       "      aching cock  acne  acne love  action  actually  ...  whimpering noises  \\\n",
       "0               0     0          0       0         0  ...                  0   \n",
       "1               0     0          0       0         0  ...                  0   \n",
       "2               0     0          0       0         0  ...                  0   \n",
       "3               0     0          0       0         0  ...                  0   \n",
       "4               0     0          0       0         0  ...                  0   \n",
       "...           ...   ...        ...     ...       ...  ...                ...   \n",
       "6913            0     0          0       0         0  ...                  0   \n",
       "6914            0     0          0       0         0  ...                  0   \n",
       "6915            0     0          0       0         0  ...                  0   \n",
       "6916            0     0          0       0         0  ...                  0   \n",
       "6917            0     0          0       0         0  ...                  0   \n",
       "\n",
       "      worth  worth know  wotshisface  wotshisface needs  yeah  yeah got  zen  \\\n",
       "0         0           0            0                  0     0         0    0   \n",
       "1         0           0            0                  0     0         0    0   \n",
       "2         0           0            0                  0     0         0    0   \n",
       "3         0           0            0                  0     0         0    0   \n",
       "4         0           0            0                  0     0         0    0   \n",
       "...     ...         ...          ...                ...   ...       ...  ...   \n",
       "6913      0           0            0                  0     0         0    0   \n",
       "6914      0           0            0                  0     0         0    0   \n",
       "6915      0           0            0                  0     0         0    0   \n",
       "6916      0           0            0                  0     0         0    0   \n",
       "6917      0           0            0                  0     0         0    0   \n",
       "\n",
       "      zen da  sentiment  \n",
       "0          0          1  \n",
       "1          0          1  \n",
       "2          0          1  \n",
       "3          0          1  \n",
       "4          0          1  \n",
       "...      ...        ...  \n",
       "6913       0          0  \n",
       "6914       0          0  \n",
       "6915       0          0  \n",
       "6916       0          0  \n",
       "6917       0          0  \n",
       "\n",
       "[6918 rows x 501 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
    "train_ds_df.columns = features\n",
    "train_ds_df['sentiment'] = train_ds.sentiment\n",
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "489337b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_ds_df.iloc[:,0:-1]\n",
    "y = train_ds_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "29a7bd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely awesome</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>aching</th>\n",
       "      <th>aching cock</th>\n",
       "      <th>acne</th>\n",
       "      <th>acne love</th>\n",
       "      <th>action</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>whimpering</th>\n",
       "      <th>whimpering noises</th>\n",
       "      <th>worth</th>\n",
       "      <th>worth know</th>\n",
       "      <th>wotshisface</th>\n",
       "      <th>wotshisface needs</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yeah got</th>\n",
       "      <th>zen</th>\n",
       "      <th>zen da</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolutely  absolutely awesome  absolutely love  acceptable  aching  \\\n",
       "0           0                   0                0           0       0   \n",
       "1           0                   0                0           0       0   \n",
       "2           0                   0                0           0       0   \n",
       "3           0                   0                0           0       0   \n",
       "4           0                   0                0           0       0   \n",
       "\n",
       "   aching cock  acne  acne love  action  actually  ...  whimpering  \\\n",
       "0            0     0          0       0         0  ...           0   \n",
       "1            0     0          0       0         0  ...           0   \n",
       "2            0     0          0       0         0  ...           0   \n",
       "3            0     0          0       0         0  ...           0   \n",
       "4            0     0          0       0         0  ...           0   \n",
       "\n",
       "   whimpering noises  worth  worth know  wotshisface  wotshisface needs  yeah  \\\n",
       "0                  0      0           0            0                  0     0   \n",
       "1                  0      0           0            0                  0     0   \n",
       "2                  0      0           0            0                  0     0   \n",
       "3                  0      0           0            0                  0     0   \n",
       "4                  0      0           0            0                  0     0   \n",
       "\n",
       "   yeah got  zen  zen da  \n",
       "0         0    0       0  \n",
       "1         0    0       0  \n",
       "2         0    0       0  \n",
       "3         0    0       0  \n",
       "4         0    0       0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "042f87ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b3227c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_ds_features, train_ds.sentiment, \n",
    "                                                    test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ddfad625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b7dff3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5188, 500) (1730, 500) (5188,) (1730,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c34e8",
   "metadata": {},
   "source": [
    "# Building Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae620b3",
   "metadata": {},
   "source": [
    "## Naive Bayes Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "553c8b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4cca7136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519    1\n",
       "4413    0\n",
       "5491    0\n",
       "2185    1\n",
       "1739    1\n",
       "       ..\n",
       "599     1\n",
       "5695    0\n",
       "1361    1\n",
       "1547    1\n",
       "4959    0\n",
       "Name: sentiment, Length: 5188, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91d486d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5188x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "794ff77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5188, 500)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train.toarray()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34f1c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_nb = nb_clf.predict(x_train.toarray())\n",
    "y_pred_test_nb = nb_clf.predict(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4b5fab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7501bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9824595219737856\n",
      "****************************************************************************************************\n",
      "Test Accuracy : 0.9838150289017341\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy :\", accuracy_score(y_train, y_pred_train_nb))\n",
    "print(\"**********\"*10)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_test_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "# DecisionTree\n",
    "# RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3bf7a",
   "metadata": {},
   "source": [
    "# how to create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c8066891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a21a5738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b9ba0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_ds['text'],train_ds['sentiment'], \n",
    "                                                    test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "38043704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('BOW', CountVectorizer()), ('RF', RandomForestClassifier())])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = Pipeline([('BOW', CountVectorizer()), ('RF',RandomForestClassifier())])\n",
    "models.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "652b5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = models.predict(x_train)\n",
    "y_pred_test = models.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9751f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "****************************************************************************************************\n",
      "Test Accuracy : 0.9947976878612717\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy :\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"**********\"*10)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dc41b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = [\"This movie was awesome and I love it\"]\n",
    "msg2 = [\"One of the worst movie i watched ever. Please don't waste your money and time.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "55f4086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(models.predict(msg1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "53ec9c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(models.predict(msg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbaae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b569da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d84854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3eda29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
