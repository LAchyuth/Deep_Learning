{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5706c141",
   "metadata": {},
   "source": [
    "# ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882978f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62fb06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'the')\n",
      "('ngramize', 'the', 'given')\n",
      "('the', 'given', 'sentences')\n"
     ]
    }
   ],
   "source": [
    "user_input = 'I want to ngramize the given sentences'\n",
    "n =3\n",
    "\n",
    "sixgrams = ngrams(user_input.split(),n)\n",
    "\n",
    "for ngrams in sixgrams:\n",
    "    print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82876ee1",
   "metadata": {},
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd95c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\achyu\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a4c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea03e122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I want to ngramize the given sentences\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = TextBlob('I want to ngramize the given sentences')\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05a8556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I']),\n",
       " WordList(['want']),\n",
       " WordList(['to']),\n",
       " WordList(['ngramize']),\n",
       " WordList(['the']),\n",
       " WordList(['given']),\n",
       " WordList(['sentences'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f837c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want']),\n",
       " WordList(['want', 'to']),\n",
       " WordList(['to', 'ngramize']),\n",
       " WordList(['ngramize', 'the']),\n",
       " WordList(['the', 'given']),\n",
       " WordList(['given', 'sentences'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3e4138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want', 'to']),\n",
       " WordList(['want', 'to', 'ngramize']),\n",
       " WordList(['to', 'ngramize', 'the']),\n",
       " WordList(['ngramize', 'the', 'given']),\n",
       " WordList(['the', 'given', 'sentences'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction gives the better result compared to TextBlob, so always FE is advised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645bb2e",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba108da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_lang =TextBlob(\"王志安曾长期在中国中央电视台供职，担任记者和主持人。2019年，王志安被中国全网封杀，并于次年开始定居日本。图为2015年11月4日，王志安在中国北京国贸大酒店出席凤凰财经峰会（资料照片）现居日本的前中国中央电视台调查记者王志安，因在台湾一场网络脱口秀节目中发表涉及残障人士的言论，引发争议。台湾内政部移民署周三（1月24日）以王志安“从事与许可目的不符的活动”为由，宣布废止其入境许可，并禁止其在未来五年内前往台湾。55岁的王志安在本月前往台湾，为其在YouTube上开设的时政自媒体节目报道总统大选，并参加了台湾知名网络脱口秀《贺珑夜夜秀》的录影。在节目中，王志安针对台湾大选发表看法，批评台湾造势晚会的舞台像演唱会现场，将残障人士作为煽情工具。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b679be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"王志安曾长期在中国中央电视台供职，担任记者和主持人。2019年，王志安被中国全网封杀，并于次年开始定居日本。图为2015年11月4日，王志安在中国北京国贸大酒店出席凤凰财经峰会（资料照片）现居日本的前中国中央电视台调查记者王志安，因在台湾一场网络脱口秀节目中发表涉及残障人士的言论，引发争议。台湾内政部移民署周三（1月24日）以王志安“从事与许可目的不符的活动”为由，宣布废止其入境许可，并禁止其在未来五年内前往台湾。55岁的王志安在本月前往台湾，为其在YouTube上开设的时政自媒体节目报道总统大选，并参加了台湾知名网络脱口秀《贺珑夜夜秀》的录影。在节目中，王志安针对台湾大选发表看法，批评台湾造势晚会的舞台像演唱会现场，将残障人士作为煽情工具。\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ebed491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Wang Zhi'an has been working on China CCTV for a long time as journalists and hosts. In 2019, Wang Zhi'an was blocked by China ’s entire network and began to settle in Japan the following year. The picture shows that on November 4, 2015, Wang Zhian attended the Phoenix Finance Summit (data photo) at the Beijing International Trade Hotel, China. The remarks trigger controversy. The Immigration Department of the Taiwan Ministry of the Interior announced on the grounds of Wednesday (January 24) on the grounds that Wang Zhi'an announced the abolition of its entry permission and banned it from going to Taiwan within the next five years. Wang Zhi'an, 55, went to Taiwan this month to report the presidential election for the current political self -media program opened on YouTube and participated in the video of the well -known online talk show \"He Long Ye Show\". In the show, Wang Zhi'an expressed his views on the Taiwan election and criticized the stage of the Taiwan party like a concert scene, using disabled people as sensational tools.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang.translate(from_lang = 'zh-CN', to = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cf56a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"వాంగ్ జియాన్ చైనా సిసిటివిలో చాలా కాలంగా జర్నలిస్టులు మరియు ఆతిథ్యమిస్తారు. 2019 లో, వాంగ్ జియాన్ చైనా యొక్క మొత్తం నెట్‌వర్క్ చేత నిరోధించబడింది మరియు మరుసటి సంవత్సరం జపాన్‌లో స్థిరపడటం ప్రారంభించింది. చైనాలోని బీజింగ్ ఇంటర్నేషనల్ ట్రేడ్ హోటల్‌లో వాంగ్ జియాన్ ఫీనిక్స్ ఫైనాన్స్ సమ్మిట్ (డేటా ఫోటో) కు వాంగ్ జియాన్ హాజరయ్యారని ఈ చిత్రం చూపిస్తుంది. వ్యాఖ్యలు వివాదాన్ని ప్రేరేపించాయి. తైవాన్ ఇంటీరియర్ మంత్రిత్వ శాఖ యొక్క ఇమ్మిగ్రేషన్ విభాగం బుధవారం (జనవరి 24) మైదానంలో ప్రకటించింది, వాంగ్ జియాన్ తన ప్రవేశ అనుమతిని రద్దు చేస్తున్నట్లు ప్రకటించింది మరియు రాబోయే ఐదేళ్ళలో తైవాన్‌కు వెళ్ళకుండా నిషేధించింది. వాంగ్ జియాన్, 55, ప్రస్తుత రాజకీయ స్వీయ -మీడియా ప్రోగ్రాం కోసం అధ్యక్ష ఎన్నికలను యూట్యూబ్‌లో ప్రారంభించి, ప్రసిద్ధ ఆన్‌లైన్ టాక్ షో \"హి లాంగ్ యే షో\" యొక్క వీడియోలో పాల్గొనడానికి ఈ నెలలో తైవాన్‌కు వెళ్లారు. ప్రదర్శనలో, వాంగ్ జియాన్ తైవాన్ ఎన్నికలపై తన అభిప్రాయాలను వ్యక్తం చేశాడు మరియు తైవాన్ పార్టీ వేదికను కచేరీ దృశ్యంలా విమర్శించాడు, వికలాంగులను సంచలనాత్మక సాధనంగా ఉపయోగించాడు.\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang.translate(from_lang = 'zh-CN', to = 'te')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f3934",
   "metadata": {},
   "source": [
    "# spell_check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c20f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am goinng to sey that  u have a gud nowledge\n",
      "*****************\n",
      "I am going to see that  u have a god knowledge\n"
     ]
    }
   ],
   "source": [
    "Sentence= TextBlob(\"I am goinng to sey that  u have a gud nowledge\")\n",
    "print(Sentence)\n",
    "print(\"*****************\")\n",
    "print(Sentence.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da82ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://textblob.readthedocs.io/en/dev/   --about textblob and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245fd8e",
   "metadata": {},
   "source": [
    "# PDF-Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14eba63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\achyu\\anaconda3\\lib\\site-packages (20221105)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8b71db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "722ea09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = extract_text(\"D:\\\\Learnbay\\\\Books & Reference Material\\\\Books\\\\2019BurkovTheHundred-pageMachineLearning.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7139488d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TheHundred-PageMachineLearningBookAndriy Burkov\\x0c“Allmodelsarewrong,butsomeareuseful.”—GeorgeBoxThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovTheHundred-PageMachineLearningBook-Draft\\x0cPreface\\n\\nLet’s start by telling the truth: machines don’t learn. What a typical “learning machine”\\ndoes, is ﬁnding a mathematical formula, which, when applied to a collection of inputs (called\\n“training data”), produces the desired outputs. This mathematical formula also generates the\\ncorrect outputs for most other inputs (distinct from the training data) on the condition that\\nthose inputs come from the same or a similar statistical distribution as the one the training\\ndata was drawn from.\\n\\nWhy isn’t that learning? Because if you slightly distort the inputs, the output is very likely\\nto become completely wrong. It’s not how learning in animals works. If you learned to play\\na video game by looking straight at the screen, you would still be a good player if someone\\nrotates the screen slightly. A machine learning algorithm, if it was trained by “looking”\\nstraight at the screen, unless it was also trained to recognize rotation, will fail to play the\\ngame on a rotated screen.\\n\\nSo why the name “machine learning” then? The reason, as is often the case, is marketing:\\nArthur Samuel, an American pioneer in the ﬁeld of computer gaming and artiﬁcial intelligence,\\ncoined the term in 1959 while at IBM. Similarly to how in the 2010s IBM tried to market\\nthe term “cognitive computing” to stand out from competition, in the 1960s, IBM used the\\nnew cool term “machine learning” to attract both clients and talented employees.\\n\\nAs you can see, just like artiﬁcial intelligence is not intelligence, machine learning is not\\nlearning. However, machine learning is a universally recognized term that usually refers\\nto the science and engineering of building machines capable of doing various useful things\\nwithout being explicitly programmed to do so. So, the word “learning” in the term is used\\nby analogy with the learning in animals rather than literally.\\n\\nWho This Book is For\\n\\nThis book contains only those parts of the vast body of material on machine learning developed\\nsince the 1960s that have proven to have a signiﬁcant practical value. A beginner in machine\\nlearning will ﬁnd in this book just enough details to get a comfortable level of understanding\\nof the ﬁeld and start asking the right questions.\\n\\nPractitioners with experience can use this book as a collection of directions for further\\nself-improvement. The book also comes in handy when brainstorming at the beginning of a\\nproject, when you try to answer the question whether a given technical or business problem\\nis “machine-learnable” and, if yes, which techniques you should try to solve it.\\n\\nHow to Use This Book\\n\\nIf you are about to start learning machine learning, you should read this book from the\\nbeginning to the end. (It’s just a hundred pages, not a big deal.) If you are interested\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cin a speciﬁc topic covered in the book and want to know more, most sections have a QR\\ncode. By scanning one of those QR codes with your phone, you will get a link to a page on\\nthe book’s companion wiki theMLbook.com with additional materials: recommended reads,\\nvideos, Q&As, code snippets, tutorials, and other bonuses.\\n\\nThe book’s wiki is continuously updated with contributions from the book’s author himself\\nas well as volunteers from all over the world. So this book, like a good wine, keeps getting\\nbetter after you buy it.\\n\\nScan the QR code below with your phone to get to the book’s wiki:\\n\\nSome sections don’t have a QR code, but they still most likely have a wiki page. You can\\nﬁnd it by submitting the section’s title to the wiki’s search engine.\\n\\nShould You Buy This Book?\\n\\nThis book is distributed on the “read ﬁrst, buy later” principle. I ﬁrmly believe that paying\\nfor the content before consuming it is buying a pig in a poke. You can see and try a car in a\\ndealership before you buy it. You can try on a shirt or a dress in a department store. You\\nhave to be able to read a book before paying for it.\\n\\nThe read ﬁrst, buy later principle implies that you can freely download the book, read it and\\nshare it with your friends and colleagues. If you liked the book, only then you have to buy it.\\n\\nNow you are all set. Enjoy your reading!\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c1 Introduction\\n\\n1.1 What is Machine Learning\\n\\nMachine learning is a subﬁeld of computer science that is concerned with building algorithms\\nwhich, to be useful, rely on a collection of examples of some phenomenon. These examples\\ncan come from nature, be handcrafted by humans or generated by another algorithm.\\n\\nMachine learning can also be deﬁned as the process of solving a practical problem by 1)\\ngathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\\nThat statistical model is assumed to be used somehow to solve the practical problem.\\n\\nTo save keystrokes, I use the terms “learning” and “machine learning” interchangeably.\\n\\n1.2 Types of Learning\\n\\nLearning can be supervised, semi-supervised, unsupervised and reinforcement.\\n\\n1.2.1 Supervised Learning\\n\\n{\\n\\n(xi, yi)\\n}\\n\\nN\\nIn supervised learning1, the dataset is the collection of labeled examples\\ni=1.\\nEach element xi among N is called a feature vector. A feature vector is a vector in which\\neach dimension j = 1, . . . , D contains a value that describes the example somehow. That\\nvalue is called a feature and is denoted as x(j). For instance, if each example x in our\\ncollection represents a person, then the ﬁrst feature, x(1), could contain height in cm, the\\nsecond feature, x(2), could contain weight in kg, x(3) could contain gender, and so on. For all\\nexamples in the dataset, the feature at position j in the feature vector always contains the\\nsame kind of information. It means that if x(2)\\ncontains weight in kg in some example xi,\\nthen x(2)\\nk will also contain weight in kg in every example xk, k = 1, . . . , N . The label yi can\\nbe either an element belonging to a ﬁnite set of classes\\n, or a real number, or a\\nmore complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated,\\nin this book yi is either one of a ﬁnite set of classes or a real number. You can see a class as\\na category to which an example belongs. For instance, if your examples are email messages\\nand your problem is spam detection, then you have two classes\\n\\n1, 2, . . . , C\\n{\\n\\nspam, not_spam\\n\\n}\\n\\ni\\n\\n{\\n\\n.\\n}\\n\\nThe goal of a supervised learning algorithm is to use the dataset to produce a model\\nthat takes a feature vector x as input and outputs information that allows deducing the label\\nfor this feature vector. For instance, the model created using the dataset of people could\\ntake as input a feature vector describing a person and output a probability that the person\\nhas cancer.\\n\\n1In this book, if a term is in bold, that means that this term can be found in the index at the end of the\\n\\nbook.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0c1.2.2 Unsupervised Learning\\n\\nN\\nIn unsupervised learning, the dataset is a collection of unlabeled examples\\ni=1.\\nAgain, x is a feature vector, and the goal of an unsupervised learning algorithm is\\nto create a model that takes a feature vector x as input and either transforms it into\\nanother vector or into a value that can be used to solve a practical problem. For example,\\nin clustering, the model returns the id of the cluster for each feature vector in the dataset.\\nIn dimensionality reduction, the output of the model is a feature vector that has fewer\\nfeatures than the input x; in outlier detection, the output is a real number that indicates\\nhow x is di\\x00erent from a “typical” example in the dataset.\\n\\nxi}\\n{\\n\\n1.2.3 Semi-Supervised Learning\\n\\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples.\\nUsually, the quantity of unlabeled examples is much higher than the number of labeled\\nexamples. The goal of a semi-supervised learning algorithm is the same as the goal of\\nthe supervised learning algorithm. The hope here is that using many unlabeled examples can\\nhelp the learning algorithm to ﬁnd (we might say “produce” or “compute”) a better model2.\\n\\n1.2.4 Reinforcement Learning\\n\\nReinforcement learning is a subﬁeld of machine learning where the machine “lives” in an\\nenvironment and is capable of perceiving the state of that environment as a vector of\\nfeatures. The machine can execute actions in every state. Di\\x00erent actions bring di\\x00erent\\nrewards and could also move the machine to another state of the environment. The goal\\nof a reinforcement learning algorithm is to learn a policy. A policy is a function f (similar\\nto the model in supervised learning) that takes the feature vector of a state as input and\\noutputs an optimal action to execute in that state. The action is optimal if it maximizes the\\nexpected average reward.\\n\\nReinforcement learning solves a particular kind of problems where\\ndecision making is sequential, and the goal is long-term, such as game\\nplaying, robotics, resource management, or logistics. In this book, I\\nput emphasis on one-shot decision making where input examples are\\nindependent of one another and the predictions made in the past. I\\nleave reinforcement learning out of the scope of this book.\\n\\n2It could look counter-intuitive that learning could beneﬁt from adding more unlabeled examples. It seems\\nlike we add more uncertainty to the problem. However, when you add unlabeled examples, you add more\\ninformation about your problem: a larger sample reﬂects better the probability distribution the data we\\nlabeled came from. Theoretically, a learning algorithm should be able to leverage this additional information.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0c1.3 How Supervised Learning Works\\n\\nIn this section, I brieﬂy explain how supervised learning works so that you have the picture\\nof the whole process before we go into detail. I decided to use supervised learning as an\\nexample because it’s the type of machine learning most frequently used in practice.\\n\\nThe supervised learning process starts with gathering the data. The data for supervised\\nlearning is a collection of pairs (input, output). Input could be anything, for example, email\\nmessages, pictures, or sensor measurements. Outputs are usually real numbers, or labels (e.g.\\n“spam”, “not_spam”, “cat”, “dog”, “mouse”, etc). In some cases, outputs are vectors (e.g.,\\nfour coordinates of the rectangle around a person on the picture), sequences (e.g. [“adjective”,\\n“adjective”, “noun”] for the input “big beautiful car”), or have some other structure.\\n\\nLet’s say the problem that you want to solve using supervised learning is spam detection.\\nYou gather the data, for example, 10,000 email messages, each with a label either “spam” or\\n“not_spam” (you could add those labels manually or pay someone to do that for us). Now,\\nyou have to convert each email message into a feature vector.\\n\\nThe data analyst decides, based on their experience, how to convert a real-world entity, such\\nas an email message, into a feature vector. One common way to convert a text into a feature\\nvector, called bag of words, is to take a dictionary of English words (let’s say it contains\\n20,000 alphabetically sorted words) and stipulate that in our feature vector:\\n\\n• the ﬁrst feature is equal to 1 if the email message contains the word “a”; otherwise,\\n\\nthis feature is 0;\\n\\n• the second feature is equal to 1 if the email message contains the word “aaron”; otherwise,\\n\\nthis feature equals 0;\\n\\n• . . .\\n• the feature at position 20,000 is equal to 1 if the email message contains the word\\n\\n“zulu”; otherwise, this feature is equal to 0.\\n\\nYou repeat the above procedure for every email message in our collection, which gives\\nus 10,000 feature vectors (each vector having the dimensionality of 20,000) and a label\\n(“spam”/“not_spam”).\\n\\nNow you have a machine-readable input data, but the output labels are still in the form of\\nhuman-readable text. Some learning algorithms require transforming labels into numbers.\\nFor example, some algorithms require numbers like 0 (to represent the label “not_spam”)\\nand 1 (to represent the label “spam”). The algorithm I use to illustrate supervised learning is\\ncalled Support Vector Machine (SVM). This algorithm requires that the positive label (in\\nour case it’s “spam”) has the numeric value of +1 (one), and the negative label (“not_spam”)\\nhas the value of\\n\\n1 (minus one).\\n\\n≠\\n\\nAt this point, you have a dataset and a learning algorithm, so you are ready to apply\\nthe learning algorithm to the dataset to get the model.\\n\\nSVM sees every feature vector as a point in a high-dimensional space (in our case, space\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cis 20,000-dimensional). The algorithm puts all feature vectors on an imaginary 20,000-\\ndimensional plot and draws an imaginary 20,000-dimensional line (a hyperplane) that separates\\nexamples with positive labels from examples with negative labels. In machine learning, the\\nboundary separating the examples of di\\x00erent classes is called the decision boundary.\\n\\nThe equation of the hyperplane is given by two parameters, a real-valued vector w of the\\nsame dimensionality as our input feature vector x, and a real number b like this:\\n\\n≠\\nwhere the expression wx means w(1)x(1) + w(2)x(2) + . . . + w(D)x(D), and D is the number\\nof dimensions of the feature vector x.\\n\\nwx\\n\\nb = 0,\\n\\n(If some equations aren’t clear to you right now, in Chapter 2 we revisit the math and\\nstatistical concepts necessary to understand them. For the moment, try to get an intuition of\\nwhat’s happening here. It all becomes more clear after you read the next chapter.)\\n\\nNow, the predicted label for some input feature vector x is given like this:\\n\\ny = sign(wx\\n\\nb),\\n\\n≠\\n\\nwhere sign is a mathematical operator that takes any value as input and returns +1 if the\\ninput is a positive number or\\n\\n1 if the input is a negative number.\\n\\n≠\\n\\nThe goal of the learning algorithm — SVM in this case — is to leverage the dataset and ﬁnd\\nthe optimal values wú and bú for parameters w and b. Once the learning algorithm identiﬁes\\nthese optimal values, the model f (x) is then deﬁned as:\\n\\nf (x) = sign(wúx\\n\\nbú)\\n\\n≠\\n\\nTherefore, to predict whether an email message is spam or not spam using an SVM model,\\nyou have to take a text of the message, convert it into a feature vector, then multiply this\\nvector by wú, subtract bú and take the sign of the result. This will give us the prediction (+1\\nmeans “spam”,\\n\\n1 means “not_spam”).\\n\\n≠\\n\\nNow, how does the machine ﬁnd wú and bú? It solves an optimization problem. Machines\\nare good at optimizing functions under constraints.\\n\\nSo what are the constraints we want to satisfy here? First of all, we want the model to predict\\nthe labels of our 10,000 examples correctly. Remember that each example i = 1, . . . , 10000 is\\ngiven by a pair (xi, yi), where xi is the feature vector of example i and yi is its label that\\ntakes values either\\n• wxi ≠\\n• wxi ≠\\n\\n1 or +1. So the constraints are naturally:\\n\\n1 if yi = +1, and\\n\\nØ\\nÆ ≠\\n\\n1 if yi =\\n\\n1\\n≠\\n\\nb\\nb\\n\\n≠\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cw x\\xa0 — \\xa0b\\xa0=\\xa00\\n\\n2\\xa0||\\n||\\xa0\\n\\nw\\n\\nw x\\xa0 — \\xa0b\\xa0=\\xa0 — 1\\n\\nw x\\xa0 — \\xa0b\\xa0=\\xa01\\n\\nx(2)\\n\\nb\\xa0||\\n||\\xa0\\n\\nw\\n\\nx(1)\\n\\nFigure 1: An example of an SVM model for two-dimensional feature vectors.\\n\\nWe would also prefer that the hyperplane separates positive examples from negative ones with\\nthe largest margin. The margin is the distance between the closest examples of two classes,\\nas deﬁned by the decision boundary. A large margin contributes to a better generalization,\\nthat is how well the model will classify new examples in the future. To achieve that, we need\\n\\nto minimize the Euclidean norm of w denoted by\\n\\nw\\nÎ\\n\\nÎ\\n\\nand given by\\n\\nSo, the optimization problem that we want the machine to solve looks like this:\\n\\nD\\nj=1(w(j))2.\\n\\nÒq\\n\\nMinimize\\nis just a compact way to write the above two constraints.\\n\\nsubject to yi(wxi ≠\\n\\n1 for i = 1, . . . , N . The expression yi(wxi ≠\\n\\nb)\\n\\nw\\n\\nØ\\n\\nÎ\\n\\nÎ\\n\\nb)\\n\\n1\\n\\nØ\\n\\nThe solution of this optimization problem, given by wú and bú, is called the statistical\\nmodel, or, simply, the model. The process of building the model is called training.\\n\\nFor two-dimensional feature vectors, the problem and the solution can be visualized as shown\\nin ﬁg. 1. The blue and orange circles represent, respectively, positive and negative examples,\\nand the line given by wx\\n\\nb = 0 is the decision boundary.\\n\\n≠\\n\\nWhy, by minimizing the norm of w, do we ﬁnd the highest margin between the two classes?\\n1 deﬁne two parallel hyperplanes,\\nGeometrically, the equations wx\\nas you see in ﬁg. 1. The distance between these hyperplanes is given by 2\\n, so the smaller\\nw\\nÎ\\n\\nb = 1 and wx\\n\\nb =\\n\\n≠\\n\\n≠\\n\\n≠\\n\\nÎ\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cthe norm\\n\\nw\\nÎ\\n\\nÎ\\n\\n, the larger the distance between these two hyperplanes.\\n\\nThat’s how Support Vector Machines work. This particular version of the algorithm builds\\nthe so-called linear model. It’s called linear because the decision boundary is a straight line\\n(or a plane, or a hyperplane). SVM can also incorporate kernels that can make the decision\\nboundary arbitrarily non-linear. In some cases, it could be impossible to perfectly separate\\nthe two groups of points because of noise in the data, errors of labeling, or outliers (examples\\nvery di\\x00erent from a “typical” example in the dataset). Another version of SVM can also\\nincorporate a penalty hyperparameter for misclassiﬁcation of training examples of speciﬁc\\nclasses. We study the SVM algorithm in more detail in Chapter 3.\\n\\nAt this point, you should retain the following: any classiﬁcation learning algorithm that\\nbuilds a model implicitly or explicitly creates a decision boundary. The decision boundary\\ncan be straight, or curved, or it can have a complex form, or it can be a superposition of\\nsome geometrical ﬁgures. The form of the decision boundary determines the accuracy of\\nthe model (that is the ratio of examples whose labels are predicted correctly). The form of\\nthe decision boundary, the way it is algorithmically or mathematically computed based on\\nthe training data, di\\x00erentiates one learning algorithm from another.\\n\\nIn practice, there are two other essential di\\x00erentiators of learning algorithms to consider:\\nspeed of model building and prediction processing time. In many practical cases, you would\\nprefer a learning algorithm that builds a less accurate model fast. Additionally, you might\\nprefer a less accurate model that is much quicker at making predictions.\\n\\n1.4 Why the Model Works on New Data\\n\\nWhy is a machine-learned model capable of predicting correctly the labels of new, previously\\nunseen examples? To understand that, look at the plot in ﬁg. 1. If two classes are separable\\nfrom one another by a decision boundary, then, obviously, examples that belong to each class\\nare located in two di\\x00erent subspaces which the decision boundary creates.\\n\\nIf the examples used for training were selected randomly, independently of one another, and\\nfollowing the same procedure, then, statistically, it is more likely that the new negative\\nexample will be located on the plot somewhere not too far from other negative examples.\\nThe same concerns the new positive example: it will likely come from the surroundings of\\nother positive examples. In such a case, our decision boundary will still, with high probability,\\nseparate well new positive and negative examples from one another. For other, less likely\\nsituations, our model will make errors, but because such situations are less likely, the number\\nof errors will likely be smaller than the number of correct predictions.\\n\\nIntuitively, the larger is the set of training examples, the more unlikely that the new examples\\nwill be dissimilar to (and lie on the plot far from) the examples used for training. To minimize\\nthe probability of making errors on new examples, the SVM algorithm, by looking for the\\nlargest margin, explicitly tries to draw the decision boundary in such a way that it lies as far\\nas possible from examples of both classes.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cThe reader interested in knowing more about the learnability and un-\\nderstanding the close relationship between the model error, the size of\\nthe training set, the form of the mathematical equation that deﬁnes\\nthe model, and the time it takes to build the model is encouraged to\\nread about the PAC learning. The PAC (for “probably approximately\\ncorrect”) learning theory helps to analyze whether and under what\\nconditions a learning algorithm will probably output an approximately\\ncorrect classiﬁer.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c2 Notation and Deﬁnitions\\n\\n2.1 Notation\\n\\nLet’s start by revisiting the mathematical notation we all learned at school, but some likely\\nforgot right after the prom.\\n\\n2.1.1 Scalars, Vectors, and Sets\\n\\nA scalar is a simple numerical value, like 15 or\\n≠\\nvalues are denoted by an italic letter, like x or a.\\n\\n3.25. Variables or constants that take scalar\\n\\nFigure 1: Three vectors visualized as directions and as points.\\n\\nA vector is an ordered list of scalar values, called attributes. We denote a vector as a bold\\ncharacter, for example, x or w. Vectors can be visualized as arrows that point to some\\ndirections as well as points in a multi-dimensional space. Illustrations of three two-dimensional\\n2, 5], and c = [1, 0] is given in ﬁg. 1. We denote an attribute of a\\nvectors, a = [2, 3], b = [\\nvector as an italic value with an index, like this: w(j) or x(j). The index j denotes a speciﬁc\\ndimension of the vector, the position of an attribute in the list. For instance, in the vector a\\nshown in red in ﬁg. 1, a(1) = 2 and a(2) = 3.\\n\\n≠\\n\\nThe notation x(j) should not be confused with the power operator, like this x2 (squared) or\\nx3 (cubed). If we want to apply a power operator, say square, to an indexed attribute of a\\nvector, we write like this: (x(j))2.\\nA variable can have two or more indices, like this: x(j)\\nneural networks, we denote as x(j)\\n\\nor like this x(k)\\nl,u the input feature j of unit u in layer l.\\n\\ni,j . For example, in\\n\\ni\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cS\\n\\nIn this case, it is denoted using accolades, for example,\\n\\nA set is an unordered collection of unique elements. We denote a set as a calligraphic\\n. A set of numbers can be ﬁnite (include a ﬁxed amount\\ncapital character, for example,\\nof values).\\nor\\nx1, x2, x3, x4, . . . , xn}\\n. A set can be inﬁnite and include all values in some interval. If a set\\n{\\nincludes all values between a and b, including a and b, it is denoted using brackets as [a, b].\\nIf the set doesn’t include the values a and b, such a set is denoted using parentheses like this:\\n(a, b). For example, the set [0, 1] includes such values as 0, 0.0001, 0.25, 0.784, 0.9995, and\\n1.0. A special set denoted R includes all numbers from minus inﬁnity to plus inﬁnity.\\n\\n1, 3, 18, 23, 235\\n{\\n\\n}\\n\\n, we write x\\n\\nWhen an element x belongs to a set\\nan intersection of two sets\\n1, 3, 5, 8\\n{\\nWe can obtain a new set\\nS3 ΩS 1 ﬁS 2. For example\\n\\ngives the new set\\n\\nS1 and\\n\\n1, 8, 4\\n}\\n\\n1, 8\\n{\\n}\\nS3 as a union of two sets\\n1, 3, 5, 8\\n{\\n\\n1, 8, 4\\n}\\n\\n}ﬁ{\\n\\n}ﬂ{\\n\\nS\\n\\n.\\n\\nS2. In this case, we write\\n\\nœS\\n\\n. We can obtain a new set\\n\\nS3 as\\nS3 ΩS 1 ﬂS 2. For example\\n\\nS2.\\nS1 and\\ngives the new set\\n\\nIn this case, we write\\n1, 3, 4, 5, 8\\n.\\n}\\n{\\n\\n2.1.2 Capital Sigma Notation\\n\\nThe summation over a collection X =\\nx = [x(1), x(2), . . . , x(m\\n\\nx1, x2, . . . , xn\\n{\\n≠\\n1), x(m)] is denoted like this:\\n\\n≠\\n\\n1, xn}\\n\\nor over the attributes of a vector\\n\\nn\\n\\ni=1\\nÿ\\n\\nxi\\n\\ndef\\n= x1 + x2 + . . . + xn\\n\\n1 + xn, or else:\\n\\n≠\\n\\nm\\n\\nj=1\\nÿ\\n\\nThe notation\\n\\ndef\\n= means “is deﬁned as”.\\n\\n2.1.3 Capital Pi Notation\\n\\nx(j) def\\n\\n= x(1) + x(2) + . . . + x(m\\n\\n≠\\n\\n1) + x(m).\\n\\nA notation analogous to capital sigma is the capital pi notation. It denotes a product of\\nelements in a collection or attributes of a vector:\\n\\nn\\n\\ni=1\\nŸ\\n\\nxi\\n\\ndef\\n\\n= x1 ·\\n\\nx2 ·\\n\\n. . .\\n\\nxn\\n\\n·\\n\\n1 ·\\n≠\\n\\nxn,\\n\\nwhere a\\nalso means a multiplied by b.\\n\\n·\\n\\nb means a multiplied by b. Where possible, we omit\\n\\nto simplify the notation, so ab\\n\\n·\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0c2.1.4 Operations on Sets\\n\\nA derived set creation operator looks like this:\\nthat we create a new set\\ngreater than 3.\\n\\nS\\n\\nx2\\nÕ by putting into it x squared such that that x is in\\n\\n. This notation means\\n, and x is\\n\\n, x > 3\\n\\nΩ{\\n\\nœS\\n\\nS\\n\\nx\\n\\n}\\n\\n|\\n\\nÕ\\n\\nS\\n\\nThe cardinality operator\\n\\n|S|\\n\\nreturns the number of elements in set\\n\\n.\\n\\nS\\n\\n2.1.5 Operations on Vectors\\n\\nThe sum of two vectors x + z is deﬁned as the vector [x(1) + z(1), x(2) + z(2), . . . , x(m) + z(m)].\\n\\nThe di\\x00erence of two vectors x\\nz(m)].\\n\\n≠\\n\\nz is deﬁned as the vector [x(1)\\n\\nz(1), x(2)\\n\\n≠\\n\\n≠\\n\\nz(2), . . . , x(m)\\n\\n≠\\n\\nA vector multiplied by a scalar is a vector. For example xc def\\n\\n= [cx(1), cx(2), . . . , cx(m)].\\n\\nA dot-product of two vectors is a scalar. For example, wx\\nthe dot-product is denoted as w\\nOtherwise, the dot-product is undeﬁned.\\n\\nm\\ni=1 w(i)x(i). In some books,\\nx. The two vectors must be of the same dimensionality.\\nq\\n\\n·\\n\\ndef\\n=\\n\\nThe multiplication of a matrix W by a vector x gives another vector as a result. Let our\\nmatrix be,\\n\\nW =\\n\\nw(1,1) w(1,2) w(1,3)\\nw(2,1) w(2,2) w(2,3)\\n5\\n\\n6\\n\\n.\\n\\nWhen vectors participate in operations on matrices, a vector is by default represented as a\\nmatrix with one column. When the vector is on the right of the matrix, it remains a column\\nvector. We can only multiply a matrix by vector if the vector has the same number of rows\\ndef\\n= [x(1), x(2), x(3)]. Then Wx\\nas the number of columns in the matrix. Let our vector be x\\nis a two-dimensional vector deﬁned as,\\n\\nWx =\\n\\nw(1,1) w(1,2) w(1,3)\\nw(2,1) w(2,2) w(2,3)\\n5\\n\\n6\\n\\nx(1)\\nx(2)\\nx(3)T\\nS\\nV\\nU\\n\\ndef\\n=\\n\\n=\\n\\nw(1,1)x(1) + w(1,2)x(2) + w(1,3)x(3)\\nw(2,1)x(1) + w(2,2)x(2) + w(2,3)x(3)\\n5\\nw(1)x\\nw(2)x\\n5\\n\\n6\\n\\n6\\n\\nIf our matrix had, say, ﬁve rows, the result of the above product would be a ﬁve-dimensional\\nvector.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cWhen the vector is on the left side of the matrix in the multiplication, then it has to be\\ntransposed before we multiply it by the matrix. The transpose of the vector x denoted as x€\\nmakes a row vector out of a column vector. Let’s say,\\n\\nthen,\\n\\nx =\\n\\nx(1)\\nx(2)\\n5\\n\\n6\\n\\n,\\n\\nx€\\n\\ndef\\n=\\n\\nx(1), x(2)\\n\\n.\\n\\nË\\n\\nÈ\\n\\nThe multiplication of the vector x by the matrix W is given by x€W,\\n\\nx€W =\\n\\nx(1), x(2)\\n\\nw(1,1) w(1,2) w(1,3)\\nw(2,1) w(2,2) w(2,3)\\n\\nË\\n\\ndef\\n=\\n\\n6\\nw(1,1)x(1) + w(2,1)x(2), w(1,2)x(1) + w(2,2)x(2), w(1,3)x(1) + w(2,3)x(2)\\n\\nÈ 5\\n\\n#\\n\\n$\\n\\nAs you can see, we can only multiply a vector by a matrix if the vector has the same number\\nof dimensions as the number of rows in the matrix.\\n\\n2.1.6 Functions\\n\\nA function is a relation that associates each element x of a set\\n, the domain of the function,\\nto a single element y of another set\\n, the codomain of the function. A function usually has a\\nname. If the function is called f , this relation is denoted y = f (x) (read f of x), the element\\nx is the argument or input of the function, and y is the value of the function or the output.\\nThe symbol that is used for representing the input is the variable of the function (we often\\nsay that f is a function of the variable x).\\n\\nX\\n\\nY\\n\\nf (c) for every x in some open\\nWe say that f (x) has a local minimum at x = c if f (x)\\ninterval around x = c. An interval is a set of real numbers with the property that any number\\nthat lies between two numbers in the set is also included in the set. An open interval does\\nnot include its endpoints and is denoted using parentheses. For example, (0, 1) means greater\\nthan 0 and less than 1. The minimal value among all the local minima is called the global\\nminimum. See illustration in ﬁg. 2.\\n\\nØ\\n\\nA vector function, denoted as y = f (x) is a function that returns a vector y. It can have a\\nvector or a scalar argument.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cFigure 2: A local and a global minima of a function.\\n\\n2.1.7 Max and Arg Max\\n\\nGiven a set of values\\n\\n=\\n\\nA\\n\\na1, a2, . . . , an}\\n{\\n\\n, the operator,\\n\\nf (a)\\n\\nmax\\na\\nœA\\n\\nreturns the highest value f (a) for all elements in the set\\n\\n. On the other hand, the operator,\\n\\nA\\n\\narg max\\n\\nf (a)\\n\\na\\n\\nœA\\n\\nreturns the element of the set\\n\\nthat maximizes f (a).\\n\\nA\\n\\nSometimes, when the set is implicit or inﬁnite, we can write maxa f (a) or arg max\\n\\na\\n\\nf (a).\\n\\nOperators min and arg min operate in a similar manner.\\n\\n2.1.8 Assignment Operator\\n\\nThe expression a\\nWe say that the variable a gets assigned a new value. Similarly, a\\ntwo-dimensional vector a gets the value [a1, a2].\\n\\nf (x) means that the variable a gets the new value: the result of f (x).\\n[a1, a2] means that the\\n\\nΩ\\n\\nΩ\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\nglobal  minimumlocal  minimum6420–2 –4 –6 f(x)\\x0c2.1.9 Derivative and Gradient\\n\\nA derivative f Õ of a function f is a function or a value that describes how fast f grows (or\\n3, then the function grows (or\\ndecreases). If the derivative is a constant value, like 5 or\\ndecreases) constantly at any point x of its domain. If the derivative f Õ is a function, then the\\nfunction f can grow at a di\\x00erent pace in di\\x00erent regions of its domain. If the derivative f Õ\\nis positive at some point x, then the function f grows at this point. If the derivative of f is\\nnegative at some x, then the function decreases at this point. The derivative of zero at x\\nmeans that the function’s slope at x is horizontal.\\n\\n≠\\n\\nThe process of ﬁnding a derivative is called di\\x00erentiation.\\n\\nDerivatives for basic functions are known. For example if f (x) = x2, then f Õ(x) = 2x; if\\nf (x) = 2x then f Õ(x) = 2; if f (x) = 2 then f Õ(x) = 0 (the derivative of any function f (x) = c,\\nwhere c is a constant value, is zero).\\n\\nIf the function we want to di\\x00erentiate is not basic, we can ﬁnd its derivative using the\\nchain rule. For example if F (x) = f (g(x)), where f and g are some functions, then F Õ(x) =\\nf Õ(g(x))gÕ(x). For example if F (x) = (5x + 1)2 then g(x) = 5x + 1 and f (g(x)) = (g(x))2.\\nBy applying the chain rule, we ﬁnd F Õ(x) = 2(5x + 1)gÕ(x) = 2(5x + 1)5 = 50x + 10.\\n\\nGradient is the generalization of derivative for functions that take several inputs (or one\\ninput in the form of a vector or some other complex structure). A gradient of a function\\nis a vector of partial derivatives. You can look at ﬁnding a partial derivative of a function\\nas the process of ﬁnding the derivative by focusing on one of the function’s inputs and by\\nconsidering all other inputs as constant values.\\n\\nFor example, if our function is deﬁned as f ([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial\\nderivative of function f with respect to x(1), denoted as\\n\\nˆf\\nˆx(1) , is given by,\\n\\nˆf\\nˆx(1) = a + 0 + 0 = a,\\n\\nwhere a is the derivative of the function ax(1); the two zeroes are respectively derivatives of\\nbx(2) and c, because x(2) is considered constant when we compute the derivative with respect\\nto x(1), and the derivative of any constant is zero.\\n\\nSimilarly, the partial derivative of function f with respect to x(2),\\n\\nˆf\\nˆx(2) , is given by,\\n\\nˆf\\nˆx(2) = 0 + b + 0 = b.\\n\\nThe gradient of function f , denoted as\\n\\nˆx(2) ].\\nThe chain rule works with partial derivatives too, as I illustrate in Chapter 4.\\n\\nf is given by the vector [ ˆf\\n\\nˆx(1) , ˆf\\n\\nÒ\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0c(a)\\n\\n(b)\\n\\nFigure 3: A probability mass function and a probability density function.\\n\\n2.2 Random Variable\\n\\nA random variable, usually written as an italic capital letter, like X, is a variable whose\\npossible values are numerical outcomes of a random phenomenon. There are two types of\\nrandom variables: discrete and continuous.\\n\\nA discrete random variable takes on only a countable number of distinct values such as red,\\nyellow, blue or 1, 2, 3, . . ..\\n\\nThe probability distribution of a discrete random variable is described by a list of probabilities\\nassociated with each of its possible values. This list of probabilities is called probability mass\\nfunction (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) =\\n0.25. Each probability in a probability mass function is a value greater than or equal to 0.\\nThe sum of probabilities equals 1 (ﬁg. 3a).\\n\\nA continuous random variable takes an inﬁnite number of possible values in some interval.\\nExamples include height, weight, and time. Because the number of values of a continuous\\nrandom variable X is inﬁnite, the probability Pr(X = c) for any c is 0. Therefore, instead\\nof the list of probabilities, the probability distribution of a continuous random variable (a\\ncontinuous probability distribution) is described by a probability density function (pdf). The\\npdf is a function whose codomain is nonnegative and the area under the curve is equal to 1\\n(ﬁg. 3b).\\n\\nLet a discrete random variable X have k possible values\\ndenoted as E[X] is given by,\\n\\nxi}\\n\\n{\\n\\nk\\ni=1. The expectation of X\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cE[X]\\n\\ndef\\n=\\n\\nk\\n\\ni=1\\nÿ\\n\\nxi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) +\\n\\n+ xk Pr(X = xk),\\n\\n(1)\\n\\n· · ·\\n\\nwhere Pr(X = xi) is the probability that X has the value xi according to the pmf. The\\nexpectation of a random variable is also called the mean, average or expected value and is\\nfrequently denoted with the letter µ. The expectation is one of the most important statistics\\nof a random variable. Another important statistic is the standard deviation. For a discrete\\nrandom variable, the standard deviation usually denoted as ‡ is given by:\\n\\n‡ def\\n=\\n\\nE[(X\\n\\n≠\\n\\nµ)2] =\\n\\nPr(X = x1)(x1 ≠\\n\\nµ)2 + Pr(X = x2)(x2 ≠\\n\\nµ)2 +\\n\\n+ Pr(X = xk)(xk ≠\\n\\n· · ·\\n\\nµ)2,\\n\\n\\uf8ff\\n\\nwhere µ = E[X].\\n\\n\\uf8ff\\n\\nThe expectation of a continuous random variable X is given by,\\n\\nwhere fX is the pdf of the variable X and\\n\\n⁄R\\n\\nR\\n\\nis the integral of function xfX .\\n\\nE[X]\\n\\ndef\\n=\\n\\nxfX (x) dx,\\n\\n(2)\\n\\nIntegral is an equivalent of the summation over all values of the function when the function\\nhas a continuous domain. It equals the area under the curve of the function. The property of\\nthe pdf that the area under its curve is 1 mathematically means that\\n\\nfX (x) dx = 1.\\n\\ns\\n\\nR\\n\\nMost of the time we don’t know fX , but we can observe some values of X. In machine\\nlearning, we call these values examples, and the collection of these examples is called a\\nsample or a dataset.\\n\\ns\\n\\n2.3 Unbiased Estimators\\n\\nN\\nBecause fX is usually unknown, but we have a sample SX =\\ni=1, we often content\\nourselves not with the true values of statistics of the probability distribution, such as\\nexpectation, but with their unbiased estimators.\\nWe say that ˆ◊(SX ) is an unbiased estimator of some statistic ◊ calculated using a sample SX\\ndrawn from an unknown probability distribution if ˆ◊(SX ) has the following property:\\n\\nxi}\\n{\\n\\nE\\n\\nˆ◊(SX )\\nÈ\\n\\nË\\n\\n= ◊,\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cwhere ˆ◊ is a sample statistic, obtained using a sample SX and not the real statistic ◊ that\\ncan be obtained only knowing X; the expectation is taken over all possible samples drawn\\nfrom X. Intuitively, this means that if you can have an unlimited number of such samples\\nas SX , and you compute some unbiased estimator, such as ˆµ, using each sample, then the\\naverage of all these ˆµ equals the real statistic µ that you would get computed on X.\\n\\nIt can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or\\nN\\neq. 2) is given by 1\\ni=1 xi (called in statistics the sample mean).\\nN\\n\\nq\\n2.4 Bayes’ Rule\\n\\nY = y) is the probability of the random variable X to\\nThe conditional probability Pr(X = x\\n|\\nhave a speciﬁc value x given that another random variable Y has a speciﬁc value of y. The\\nBayes’ Rule (also known as the Bayes’ Theorem) stipulates that:\\n\\nY = y) =\\nPr(X = x\\n|\\n\\nPr(Y = y\\n\\nX = x) Pr(X = x)\\n|\\nPr(Y = y)\\n\\n.\\n\\n2.5 Parameter Estimation\\n\\nBayes’ Rule comes in handy when we have a model of X’s distribution, and this model f◊ is a\\nfunction that has some parameters in the form of a vector ◊. An example of such a function\\ncould be the Gaussian function that has two parameters, µ and ‡, and is deﬁned as:\\n\\nf◊(x) =\\n\\n1\\nÔ2ﬁ‡2\\n\\ne≠\\n\\n(x\\n\\nµ)2\\n\\n≠\\n2‡2\\n\\n,\\n\\nwhere ◊ def\\n\\n= [µ, ‡].\\n\\nThis function has all the properties of a pdf. Therefore, we can use it as a model of an\\nunknown distribution of X. We can update the values of parameters in the vector ◊ from the\\ndata using the Bayes’ Rule:\\n\\nPr(◊ = ˆ◊\\nX = x)\\n|\\n\\nΩ\\n\\n◊ = ˆ◊) Pr(◊ = ˆ◊)\\nPr(X = x\\n|\\n\\nPr(X = x)\\n\\n=\\n\\n◊ = ˆ◊)\\nwhere Pr(X = x\\n|\\n\\ndef\\n= fˆ◊.\\n\\n◊ = ˆ◊) Pr(◊ = ˆ◊)\\nPr(X = x\\n|\\n◊ = ˜◊)\\n˜◊ Pr(X = x\\n|\\n\\n.\\n\\n(3)\\n\\nq\\n\\nIf we have a sample\\nPr(◊ = ˆ◊) by applying Bayes’ Rule iteratively, one example x\\nPr(◊ = ˆ◊) can be guessed such that\\ndi\\x00erent ˆ◊ is called the prior.\\n\\nof X and the set of possible values for ◊ is ﬁnite, we can easily estimate\\nat a time. The initial value\\nˆ◊ Pr(◊ = ˆ◊) = 1. This guess of the probabilities for\\n\\nœS\\n\\nS\\n\\nq\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0cX = x1) for all possible values ˆ◊. Then, before updating\\nFirst, we compute Pr(◊ = ˆ◊\\n|\\nPr(◊ = ˆ◊\\nX = x) once again, this time for x = x2 œS\\nusing eq. 3, we replace the prior\\n|\\nPr(◊ = ˆ◊\\nPr(◊ = ˆ◊) in eq. 3 by the new estimate Pr(◊ = ˆ◊)\\nX = x).\\n|\\n\\nΩ\\n\\n1\\nN\\n\\nœS\\n\\nx\\n\\nThe best value of the parameters ◊ú given one example is obtained using the principle of\\nmaximum-likelihood:\\n\\nq\\n\\n◊ú = arg max\\n\\n◊\\n\\nN\\n\\ni=1\\nŸ\\n\\nPr(◊ = ˆ◊\\nX = xi).\\n|\\n\\n(4)\\n\\nIf the set of possible values for ◊ isn’t ﬁnite, then we need to optimize eq. 4 directly using a\\nnumerical optimization routine, such as gradient descent, which we consider in Chapter 4.\\nUsually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because\\nthe logarithm of a product becomes the sum of logarithms and it’s easier for the machine to\\nwork with the sum than with a product1.\\n\\n2.6 Classiﬁcation vs. Regression\\n\\nClassiﬁcation is a problem of automatically assigning a label to an unlabeled example.\\nSpam detection is a famous example of classiﬁcation.\\n\\nIn machine learning, the classiﬁcation problem is solved by a classiﬁcation learning algorithm\\nthat takes a collection of labeled examples as inputs and produces a model that can take\\nan unlabeled example as input and either directly output a label or output a number that\\ncan be used by the data analyst to deduce the label easily. An example of such a number is\\na probability.\\n\\nIn a classiﬁcation problem, a label is a member of a ﬁnite set of classes. If the size of\\nthe set of classes is two (“sick”/“healthy”, “spam”/“not_spam”), we talk about binary\\nclassiﬁcation (also called binomial in some books).\\n\\nMulticlass classiﬁcation (also called multinomial) is a classiﬁcation problem with three\\nor more classes2.\\n\\nWhile some learning algorithms naturally allow for more than two classes, others are by nature\\nbinary classiﬁcation algorithms. There are strategies allowing to turn a binary classiﬁcation\\nlearning algorithm into a multiclass one. I talk about one of them in Chapter 7.\\n\\nRegression is a problem of predicting a real-valued label (often called a target) given an\\nunlabeled example. Estimating house price valuation based on house features, such as area,\\nthe number of bedrooms, location and so on is a famous example of regression.\\n\\n1Multiplication of many numbers can give either a very small result or a very large one. It often results in\\n\\nthe problem of numerical overﬂow when the machine cannot store such extreme numbers in memory.\\n\\n2There’s still one label per example though.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0cThe regression problem is solved by a regression learning algorithm that takes a collection\\nof labeled examples as inputs and produces a model that can take an unlabeled example as\\ninput and output a target.\\n\\n2.7 Model-Based vs. Instance-Based Learning\\n\\nMost supervised learning algorithms are model-based. We have already seen one such\\nalgorithm: SVM. Model-based learning algorithms use the training data to create a model\\nthat has parameters learned from the training data. In SVM, the two parameters we saw\\nwere wú and bú. After the model was built, the training data can be discarded.\\n\\nInstance-based learning algorithms use the whole dataset as the model. One instance-based\\nalgorithm frequently used in practice is k-Nearest Neighbors (kNN). In classiﬁcation, to\\npredict a label for an input example the kNN algorithm looks at the close neighborhood of\\nthe input example in the space of feature vectors and outputs the label that it saw the most\\noften in this close neighborhood.\\n\\n2.8 Shallow vs. Deep Learning\\n\\nA shallow learning algorithm learns the parameters of the model directly from the features\\nof the training examples. Most supervised learning algorithms are shallow. The notorious\\nexceptions are neural network learning algorithms, speciﬁcally those that build neural\\nnetworks with more than one layer between input and output. Such neural networks are\\ncalled deep neural networks. In deep neural network learning (or, simply, deep learning),\\ncontrary to shallow learning, most model parameters are learned not directly from the features\\nof the training examples, but from the outputs of the preceding layers.\\n\\nDon’t worry if you don’t understand what that means right now. We look at neural networks\\nmore closely in Chapter 6.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c3 Fundamental Algorithms\\n\\nIn this chapter, I describe ﬁve algorithms which are not just the most known but also either\\nvery e\\x00ective on their own or are used as building blocks for the most e\\x00ective learning\\nalgorithms out there.\\n\\n3.1 Linear Regression\\n\\nLinear regression is a popular regression learning algorithm that learns a model which is a\\nlinear combination of features of the input example.\\n\\n3.1.1 Problem Statement\\n\\nN\\ni=1, where N is the size of the collection,\\n(xi, yi)\\nWe have a collection of labeled examples\\n}\\n{\\nxi is the D-dimensional feature vector of example i = 1, . . . , N , yi is a real-valued1 target\\nand every feature x(j)\\n\\n, j = 1, . . . , D, is also a real number.\\n\\ni\\n\\nWe want to build a model fw,b(x) as a linear combination of features of example x:\\n\\nfw,b(x) = wx + b,\\n\\n(1)\\n\\nwhere w is a D-dimensional vector of parameters and b is a real number. The notation fw,b\\nmeans that the model f is parametrized by two values: w and b.\\n\\nfw,b(x). Two\\nWe will use the model to predict the unknown y for a given x like this: y\\nmodels parametrized by two di\\x00erent pairs (w, b) will likely produce two di\\x00erent predictions\\nwhen applied to the same example. We want to ﬁnd the optimal values (wú, bú). Obviously,\\nthe optimal values of parameters deﬁne the model that makes the most accurate predictions.\\n\\nΩ\\n\\nYou could have noticed that the form of our linear model in eq. 1 is very similar to the form\\nof the SVM model. The only di\\x00erence is the missing sign operator. The two models are\\nindeed similar. However, the hyperplane in the SVM plays the role of the decision boundary:\\nit’s used to separate two groups of examples from one another. As such, it has to be as far\\nfrom each group as possible.\\n\\nOn the other hand, the hyperplane in linear regression is chosen to be as close to all training\\nexamples as possible.\\n\\nYou can see why this latter requirement is essential by looking at the illustration in ﬁg. 1. It\\ndisplays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We\\ncan use this line to predict the value of the target ynew for a new unlabeled input example\\nxnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\\x00erence\\n\\n1To say that yi is real-valued, we write yi œ\\nof numbers from minus inﬁnity to plus inﬁnity.\\n\\nR, where R denotes the set of all real numbers, an inﬁnite set\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cFigure 1: Linear Regression for one-dimensional examples.\\n\\nwith the one-dimensional case is that the regression model is not a line but a plane (for two\\ndimensions) or a hyperplane (for D > 2).\\n\\nNow you see why it’s essential to have the requirement that the regression hyperplane lies as\\nclose to the training examples as possible: if the blue line in ﬁg. 1 was far from the blue dots,\\nthe prediction ynew would have fewer chances to be correct.\\n\\n3.1.2 Solution\\n\\nTo get this latter requirement satisﬁed, the optimization procedure which we use to ﬁnd the\\noptimal values for wú and bú tries to minimize the following expression:\\n\\n1\\nN\\n\\nÿi=1...N\\n\\n(fw,b(xi)\\n\\nyi)2.\\n\\n≠\\n\\n(2)\\n\\nIn mathematics, the expression we minimize or maximize is called an objective function, or,\\nyi)2 in the above objective is called the loss\\nsimply, an objective. The expression (f (xi)\\n≠\\nfunction. It’s a measure of penalty for misclassiﬁcation of example i. This particular choice\\nof the loss function is called squared error loss. All model-based learning algorithms have\\na loss function and what we do to ﬁnd the best model is we try to minimize the objective\\nknown as the cost function. In linear regression, the cost function is given by the average\\nloss, also called the empirical risk. The average loss, or empirical risk, for a model, is the\\naverage of all penalties obtained by applying the model to the training data.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cWhy is the loss in linear regression a quadratic function? Why couldn’t we get the absolute\\nvalue of the di\\x00erence between the true target yi and the predicted value f (xi) and use that\\nas a penalty? We could. Moreover, we also could use a cube instead of a square.\\n\\nNow you probably start realizing how many seemingly arbitrary decisions are made when we\\ndesign a machine learning algorithm: we decided to use the linear combination of features to\\npredict the target. However, we could use a square or some other polynomial to combine the\\nvalues of features. We could also use some other loss function that makes sense: the absolute\\ndi\\x00erence between f (xi) and yi makes sense, the cube of the di\\x00erence too; the binary loss\\n(1 when f (xi) and yi are di\\x00erent and 0 when they are the same) also makes sense, right?\\n\\nIf we made di\\x00erent decisions about the form of the model, the form of the loss function,\\nand about the choice of the algorithm that minimizes the average loss to ﬁnd the best values\\nof parameters, we would end up inventing a di\\x00erent machine learning algorithm. Sounds\\neasy, doesn’t it? However, do not rush to invent a new learning algorithm. The fact that it’s\\ndi\\x00erent doesn’t mean that it will work better in practice.\\n\\nPeople invent new learning algorithms for one of the two main reasons:\\n\\n1. The new algorithm solves a speciﬁc practical problem better than the existing algorithms.\\n2. The new algorithm has better theoretical guarantees on the quality of the model it\\n\\nproduces.\\n\\nOne practical justiﬁcation of the choice of the linear form for the model is that it’s simple.\\nWhy use a complex model when you can use a simple one? Another consideration is that\\nlinear models rarely overﬁt. Overﬁtting is the property of a model such that the model\\npredicts very well labels of the examples used during training but frequently makes errors\\nwhen applied to examples that weren’t seen by the learning algorithm during training.\\n\\nAn example of overﬁtting in regression is shown in ﬁg. 2. The data used to build the red\\nregression line is the same as in ﬁg. 1. The di\\x00erence is that this time, this is the polynomial\\nregression with a polynomial of degree 10. The regression line predicts almost perfectly the\\ntargets almost all training examples, but will likely make signiﬁcant errors on new data, as\\nyou can see in ﬁg. 1 for xnew. We talk more about overﬁtting and how to avoid it Chapter 5.\\n\\nNow you know why linear regression can be useful:\\nit doesn’t overﬁt much. But what\\nabout the squared loss? Why did we decide that it should be squared? In 1705, the French\\nmathematician Adrien-Marie Legendre, who ﬁrst published the sum of squares method for\\ngauging the quality of the model stated that squaring the error before summing is convenient.\\nWhy did he say that? The absolute value is not convenient, because it doesn’t have a\\ncontinuous derivative, which makes the function not smooth. Functions that are not smooth\\ncreate unnecessary di\\x00culties when employing linear algebra to ﬁnd closed form solutions\\nto optimization problems. Closed form solutions to ﬁnding an optimum of a function are\\nsimple algebraic expressions and are often preferable to using complex numerical optimization\\nmethods, such as gradient descent (used, among others, to train neural networks).\\n\\nIntuitively, squared penalties are also advantageous because they exaggerate the di\\x00erence\\nbetween the true target and the predicted one according to the value of this di\\x00erence. We\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cFigure 2: Overﬁtting.\\n\\nmight also use the powers 3 or 4, but their derivatives are more complicated to work with.\\n\\nFinally, why do we care about the derivative of the average loss? Remember from algebra\\nthat if we can calculate the gradient of the function in eq. 2, we can then set this gradient to\\nzero2 and ﬁnd the solution to a system of equations that gives us the optimal values wú and\\nbú. You can spend several minutes and check it yourself.\\n\\n3.2 Logistic Regression\\n\\nThe ﬁrst thing to say is that logistic regression is not a regression, but a classiﬁcation learning\\nalgorithm. The name comes from statistics and is due to the fact that the mathematical\\nformulation of logistic regression is similar to that of linear regression.\\n\\nI explain logistic regression on the case of binary classiﬁcation. However, it can naturally be\\nextended to multiclass classiﬁcation.\\n\\n3.2.1 Problem Statement\\n\\nIn logistic regression, we still want to model yi as a linear function of xi, however, with a\\nbinary yi this is not straightforward. The linear combination of features such as wxi + b is a\\nfunction that spans from minus inﬁnity to plus inﬁnity, while yi has only two possible values.\\n\\n2To ﬁnd the minimum or the maximum of a function, we set the gradient to zero because the value of the\\n\\ngradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\nyxnewnew\\x0cFigure 3: Standard logistic function.\\n\\nAt the time where the absence of computers required scientists to perform manual calculations,\\nthey were eager to ﬁnd a linear classiﬁcation model. They ﬁgured out that if we deﬁne a\\nnegative label as 0 and the positive label as 1, we would just need to ﬁnd a simple continuous\\nfunction whose codomain is (0, 1). In such a case, if the value returned by the model for\\ninput x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled\\nas positive. One function that has such a property is the standard logistic function (also\\nknown as the sigmoid function):\\n\\nf (x) =\\n\\n1\\n1 + e≠\\n\\nx ,\\n\\nwhere e is the base of the natural logarithm (also called Euler’s number; ex is also known as\\nthe exp(x) function in Excel and many programming languages). Its graph is depicted in ﬁg.\\n3.\\n\\nBy looking at the graph of the standard logistic function, we can see how well it ﬁts our\\nclassiﬁcation purpose: if we optimize the values of x and b appropriately, we could interpret\\nthe output of f (x) as the probability of yi being positive. For example, if it’s higher than or\\nequal to the threshold 0.5 we would say that the class of x is positive; otherwise, it’s negative.\\nIn practice, the choice of the threshold could be di\\x00erent depending on the problem. We\\nreturn to this discussion in Chapter 5 when we talk about model performance assessment.\\n\\nSo our logistic regression model looks like this:\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cfw,b(x)\\n\\ndef\\n=\\n\\n1\\n(wx+b)\\n1 + e≠\\n\\n.\\n\\n(3)\\n\\nYou can see the familiar term wx + b from linear regression. Now, how do we ﬁnd the best\\nvalues wú and bú for our model? In linear regression, we minimized the empirical risk which\\nwas deﬁned as the average squared error loss, also known as the mean squared error or\\nMSE.\\n\\n3.2.2 Solution\\n\\nIn logistic regression, instead of using a squared loss and trying to minimize the empirical\\nrisk, we maximize the likelihood of our training set according to the model. In statistics, the\\nlikelihood function deﬁnes how likely the observation (an example) is according to our model.\\n\\nFor instance, assume that we have a labeled example (xi, yi) in our training data. Assume\\nalso that we have found (guessed) some speciﬁc values ˆw and ˆb of our parameters. If we now\\napply our model f ˆw,ˆb to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is\\nthe positive class, the likelihood of yi being the positive class, according to our model, is\\ngiven by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is\\ngiven by 1\\n\\np.\\n\\n≠\\n\\nThe optimization criterion in logistic regression is called maximum likelihood. Instead of\\nminimizing the average loss, like in linear regression, we now maximize the likelihood of the\\ntraining data according to our model:\\n\\nLw,b\\n\\ndef\\n=\\n\\nfw,b(xi)yi(1\\n\\nfw,b(xi))(1\\n\\nyi).\\n\\n≠\\n\\n≠\\n\\n(4)\\n\\nŸi=1...N\\nfw,b(x))(1\\n\\nThe expression fw,b(x)yi(1\\nway of saying: “fw,b(x) when yi = 1 and (1\\n(1\\n1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason.\\n\\nyi) may look scary but it’s just a fancy mathematical\\nfw,b(x)) otherwise”. Indeed, if yi = 1, then\\nyi) = 0 and we know that anything power 0 equals\\n\\n≠\\nyi) equals 1 because (1\\n\\nfw,b(x))(1\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\nin the objective function instead\\nYou may have noticed that we used the product operator\\nof the sum operator\\nwhich was used in linear regression. It’s because the likelihood of\\nr\\nobserving N labels for N examples is the product of likelihoods of each observation (assuming\\nthat all observations are independent of one another, which is the case). You can draw\\na parallel with the multiplication of probabilities of outcomes in a series of independent\\nexperiments in the probability theory.\\n\\nq\\n\\nBecause of the exp function used in the model, in practice, it’s more convenient to maximize\\nthe log-likelihood instead of likelihood. The log-likelihood is deﬁned like follows:\\n\\nLogLw,b\\n\\ndef\\n= ln(L(w,b(x)) =\\n\\nN\\n\\ni=1\\nÿ\\n\\nyi ln fw,b(x) + (1\\n\\nyi) ln (1\\n\\n≠\\n\\n≠\\n\\nfw,b(x)).\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cBecause ln is a strictly increasing function, maximizing this function is the same as maximizing\\nits argument, and the solution to this new optimization problem is the same as the solution\\nto the original problem.\\n\\nContrary to linear regression, there’s no closed form solution to the above optimization\\nproblem. A typical numerical optimization procedure used in such cases is gradient descent.\\nI talk about it in the next chapter.\\n\\n3.3 Decision Tree Learning\\n\\nA decision tree is an acyclic graph that can be used to make decisions. In each branching\\nnode of the graph, a speciﬁc feature j of the feature vector is examined. If the value of the\\nfeature is below a speciﬁc threshold, then the left branch is followed; otherwise, the right\\nbranch is followed. As the leaf node is reached, the decision is made about the class to which\\nthe example belongs.\\n\\nAs the title of the section suggests, a decision tree can be learned from data.\\n\\n3.3.1 Problem Statement\\n\\n. We\\nLike previously, we have a collection of labeled examples; labels belong to the set\\n}\\nwant to build a decision tree that would allow us to predict the class of an example given a\\nfeature vector.\\n\\n0, 1\\n{\\n\\n3.3.2 Solution\\n\\nThere are various formulations of the decision tree learning algorithm. In this book, we\\nconsider just one, called ID3.\\n\\nThe optimization criterion, in this case, is the average log-likelihood:\\n\\nN\\n\\nyi ln fID3(xi) + (1\\n\\n1\\nN\\n\\ni=1\\nÿ\\nwhere fID3 is a decision tree.\\n\\nyi) ln (1\\n\\n≠\\n\\n≠\\n\\nfID3(xi)),\\n\\n(5)\\n\\nBy now, it looks very similar to logistic regression. However, contrary to the logistic regression\\nlearning algorithm which builds a parametric model fwú,bú by ﬁnding an optimal solution\\nto the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a\\nnon-parametric model fID3(x)\\n\\ndef\\n= Pr(y = 1\\nx).\\n|\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cx\\n\\nx\\n\\nYes\\n\\nNo\\n\\nx(3)\\xa0<\\xa018.3?\\n\\nS={(x1,\\xa0y1),\\xa0(x2,\\xa0y2),\\xa0(x3,\\xa0y3),\\n(x4,\\xa0y4),\\xa0(x5,\\xa0y5),\\xa0(x6,\\xa0y6),\\n(x7,\\xa0y7),\\xa0(x8,\\xa0y8),\\xa0(x9,\\xa0y9),\\n(x10,\\xa0y10),\\xa0(x11,\\xa0y11),\\xa0(x12,\\xa0y12)}\\n\\nS\\xad\\xa0=\\xa0{(x1,\\xa0y1),\\xa0(x2,\\xa0y2),\\n(x4,\\xa0y4),\\xa0(x6,\\xa0y6),\\xa0(x7,\\xa0y7),\\n(x8,\\xa0y8),\\xa0(x9,\\xa0y9)}\\xa0\\n\\nS+\\xa0=\\xa0{(x3,\\xa0y3),\\xa0(x5,\\xa0y5),\\xa0(x10,\\xa0y10),\\n(x11,\\xa0y11),\\xa0(x12,\\xa0y12)}\\xa0\\n\\nPr(y\\xa0=\\xa01|x)\\xa0=\\xa0(y1+y2+y3+y4+y5\\xa0\\n+y6+y7+y8+y9+y10+y11+y12)/12\\n\\nPr(y\\xa0=\\xa01|x)\\xa0=\\xa0(y1+y2+y4\\xa0\\n+y6+y7+y8+y9)/7\\n\\nPr(y\\xa0=\\xa01|x)\\xa0=\\n(y3+y5+y10+y11+y12)/5\\n\\nPr(y\\xa0=\\xa01|x)\\n\\n(a)\\n\\nPr(y\\xa0=\\xa01|x)\\n\\nPr(y\\xa0=\\xa01|x)\\n\\n(b)\\n\\ncontains 12 labeled\\nFigure 4: An illustration of a decision tree building algorithm. The set\\nexamples. (a) In the beginning, the decision tree only contains the start node; it makes the\\nsame prediction for any input. (b) The decision tree after the ﬁrst split; it tests whether\\nfeature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the\\ntwo leaf nodes.\\n\\nS\\n\\nThe ID3 learning algorithm works as follows. Let\\nbeginning, the decision tree only has a start node that contains all examples:\\nStart with a constant model f S\\n\\ndenote a set of labeled examples. In the\\nN\\ni=1.\\n\\n(xi, yi)\\n}\\n\\ndef\\n=\\n\\nS\\n\\nS\\n\\n{\\n\\nID3:\\n\\n1\\n\\nf SID3 =\\n\\ny.\\n\\n|S| ÿ(x,y)\\nœS\\n\\n(6)\\n\\nThe prediction given by the above model, f S\\ncorresponding decision tree is shown in ﬁg 4a.\\n\\nID3(x), would be the same for any input x. The\\n\\ndef\\n=\\n\\nThen we search through all features j = 1, . . . , D and all thresholds t, and split the set S\\n, x(j) < t\\ninto two subsets:\\n.\\n}\\n}\\nThe two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs\\n(j, t) how good the split with pieces\\nS+ is. Finally, we pick the best such values (j, t),\\n(or\\nS+ and\\nsplit\\nquit if no split produces a model that’s su\\x00ciently better than the current one). A decision\\n\\n, form two new leaf nodes, and continue recursively on\\n\\nS+ and\\n\\nS+ =\\n\\nS, x(j)\\n\\n(x, y)\\n\\n(x, y)\\n\\n(x, y)\\n\\n(x, y)\\n\\ninto\\n\\nand\\n\\nand\\n\\nœS\\n\\nS≠\\n\\nS≠\\n\\nS≠\\n\\nS≠\\n\\nØ\\n\\nœ\\n\\nS\\n\\n{\\n\\n{\\n\\nt\\n\\n|\\n\\n|\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0ctree after one split is illustrated in ﬁg 4b.\\n\\nNow you should wonder what do the words “evaluate how good the split is” mean. In ID3, the\\ngoodness of a split is estimated by using the criterion called entropy. Entropy is a measure of\\nuncertainty about a random variable. It reaches its maximum when all values of the random\\nvariables are equiprobable. Entropy reaches its minimum when the random variable can have\\nonly one value. The entropy of a set of examples\\n\\nis given by:\\n\\nS\\n\\nH(\\n\\nS\\n\\n) =\\n\\nf SID3 ln f SID3 ≠\\n≠\\n\\n(1\\n\\n≠\\n\\nf SID3) ln(1\\n\\nf SID3).\\n\\n≠\\n\\nWhen we split a set of examples by a certain feature j and a threshold t, the entropy of a\\nsplit, H(\\n\\nS+), is simply a weighted sum of two entropies:\\n\\nS≠\\n\\n,\\n\\nH(\\n\\nS≠\\n\\n,\\n\\nS+) = |S≠|\\n|S|\\n\\nH(\\n\\nS≠\\n\\n) + |S+|\\n|S|\\n\\nH(\\n\\nS+).\\n\\n(7)\\n\\nSo, in ID3, at each step, at each leaf node, we ﬁnd a split that minimizes the entropy given\\nby eq. 7 or we stop at this leaf node.\\n\\nThe algorithm stops at a leaf node in any of the below situations:\\n\\n• All examples in the leaf node are classiﬁed correctly by the one-piece model (eq. 6).\\n• We cannot ﬁnd an attribute to split upon.\\n• The split reduces the entropy less than some ‘ (the value for which has to be found\\n\\nexperimentally3).\\n\\n• The tree reaches some maximum depth d (also has to be found experimentally).\\n\\nBecause in ID3, the decision to split the dataset on each iteration is local (doesn’t depend\\non future splits), the algorithm doesn’t guarantee an optimal solution. The model can be\\nimproved by using techniques like backtracking during the search for the optimal decision\\ntree at the cost of possibly taking longer to build a model.\\n\\nThe entropy-based split criterion intuitively makes sense: entropy\\nreaches its minimum of 0 when all examples in\\nhave the same label;\\non the other hand, the entropy is at its maximum of 1 when exactly\\none-half of examples in\\nis labeled with 1, making such a leaf useless\\nfor classiﬁcation. The only remaining question is how this algorithm\\napproximately maximizes the average log-likelihood criterion. I leave it\\nfor further reading.\\n\\nS\\n\\nS\\n\\n3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0c3.4 Support Vector Machine\\n\\nWe already considered SVM in the introduction, so this section only ﬁlls a couple of blanks.\\nTwo critical questions need to be answered:\\n\\n1. What if there’s noise in the data and no hyperplane can perfectly separate positive\\n\\nexamples from negative ones?\\n\\n2. What if the data cannot be separated using a plane, but could be separated by a\\n\\nhigher-order polynomial?\\n\\nFigure 5: Linearly non-separable cases. Left: the presence of noise. Right:\\nnonlinearity.\\n\\ninherent\\n\\nYou can see both situations depicted in ﬁg 5. In the left case, the data could be separated by\\na straight line if not for the noise (outliers or examples with wrong labels). In the right case,\\nthe decision boundary is a circle and not a straight line.\\n\\nRemember that in SVM, we want to satisfy the following constraints:\\n\\na) wxi ≠\\nb) wxi ≠\\n\\nb\\nb\\n\\nØ\\nÆ ≠\\n\\n1 if yi = +1, and\\n\\n1 if yi =\\n\\n1\\n≠\\nw\\nÎ\\n\\nso that the hyperplane was equally distant from the closest\\nWe also want to minimize\\n2, and the use of\\nexamples of each class. Minimizing\\n||\\nthis term makes it possible to perform quadratic programming optimization later on. The\\noptimization problem for SVM, therefore, looks like this:\\n\\nis equivalent to minimizing 1\\n\\nw\\nÎ\\n\\n2 ||\\n\\nw\\n\\nÎ\\n\\nÎ\\n\\nmin\\n\\n1\\n2 ||\\n\\nw\\n\\n2, such that yi(xiw\\n||\\n\\n≠\\n\\nb)\\n\\n≠\\n\\n1\\n\\nØ\\n\\n0, i = 1, . . . , N.\\n\\n(8)\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0c3.4.1 Dealing with Noise\\n\\nTo extend SVM to cases in which the data is not linearly separable, we introduce the hinge\\nloss function: max (0, 1\\n\\nb)).\\n\\nyi(wxi ≠\\n\\n≠\\n\\nThe hinge loss function is zero if the constraints a) and b) are satisﬁed, in other words, if wxi\\nlies on the correct side of the decision boundary. For data on the wrong side of the decision\\nboundary, the function’s value is proportional to the distance from the decision boundary.\\n\\nWe then wish to minimize the following cost function,\\n\\nC\\n\\nw\\nÎ\\n\\n2 +\\nÎ\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nmax (0, 1\\n\\nyi(wxi ≠\\n\\n≠\\n\\nb)) ,\\n\\nwhere the hyperparameter C determines the tradeo\\x00 between increasing the size of the\\ndecision boundary and ensuring that each xi lies on the correct side of the decision boundary.\\nThe value of C is usually chosen experimentally, just like ID3’s hyperparameters ‘ and d.\\nSVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is\\nreferred to as a hard-margin SVM.\\n\\nAs you can see, for su\\x00ciently high values of C, the second term in the cost function will\\nbecome negligible, so the SVM algorithm will try to ﬁnd the highest margin by completely\\nignoring misclassiﬁcation. As we decrease the value of C, making classiﬁcation errors is\\nbecoming more costly, so the SVM algorithm will try to make fewer mistakes by sacriﬁcing\\nthe margin size. As we have already discussed, a larger margin is better for generalization.\\nTherefore, C regulates the tradeo\\x00 between classifying the training data well (minimizing\\nempirical risk) and classifying future examples well (generalization).\\n\\n3.4.2 Dealing with Inherent Non-Linearity\\n\\nSVM can be adapted to work with datasets that cannot be separated by a hyperplane in\\nits original space. However, if we manage to transform the original space into a space of\\nhigher dimensionality, we could hope that the examples will become linearly separable in this\\ntransformed space. In SVMs, using a function to implicitly transform the original space into\\na higher dimensional space during the cost function optimization is called the kernel trick.\\n\\nThe e\\x00ect of applying the kernel trick is illustrated in ﬁg. 6. As you can see, it’s possible\\nto transform a two-dimensional non-linearly-separable data into a linearly-separable three-\\ndimensional data using a speciﬁc mapping „ : x\\n„(x), where „(x) is a vector of higher\\ndimensionality than x. For the example of 2D data in ﬁg. 5 (right), the mapping „ for\\nexample x = [q, p] that projects this example into a 3D space (ﬁg. 6) would look like this\\ndef\\n= (q2, Ô2qp, p2), where q2 means q squared. You see now that the data becomes\\n„([q, p])\\nlinearly separable in the transformed space.\\n\\n‘æ\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0cFigure 6: The data from ﬁg. 5 (right) becomes linearly separable after a transformation into\\na three-dimensional space.\\n\\nHowever, we don’t know a priori which mapping „ would work for our data. If we ﬁrst\\ntransform all our input examples using some mapping into very high dimensional vectors and\\nthen apply SVM to this data, and we try all possible mapping functions, the computation\\ncould become very ine\\x00cient, and we would never solve our classiﬁcation problem.\\n\\nFortunately, scientists ﬁgured out how to use kernel functions (or, simply, kernels) to\\ne\\x00ciently work in higher-dimensional spaces without doing this transformation explicitly. To\\nunderstand how kernels work, we have to see ﬁrst how the optimization algorithm for SVM\\nﬁnds the optimal values for w and b.\\n\\nThe method traditionally used to solve the optimization problem in eq. 8 is the method of\\nLagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to\\nsolve an equivalent problem formulated like this:\\n\\nmax\\n–1...–N\\n\\n–i ≠\\n\\n1\\n2\\n\\nN\\n\\ni=1\\nÿ\\n\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nÿk=1\\n\\nyi–i(xixk)yk–k subject to\\n\\nN\\n\\ni=1\\nÿ\\n\\n–iyi = 0 and –i Ø\\n\\n0, i = 1, . . . , N,\\n\\nwhere –i are called Lagrange multipliers. When formulated like this, the optimization\\nproblem becomes a convex quadratic optimization problem, e\\x00ciently solvable by quadratic\\nprogramming algorithms.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n14\\n\\n\\x0cNow, you could have noticed that in the above formulation, there is a term xixk, and this is\\nthe only place where the feature vectors are used. If we want to transform our vector space\\ninto higher dimensional space, we need to transform xi into „(xi) and xj into „(xj) and\\nthen multiply „(xi) and „(xj). It would be very costly to do so.\\n\\nOn the other hand, we are only interested in the result of the dot-product xixk, which, as\\nwe know, is a real number. We don’t care how this number was obtained as long as it’s\\ncorrect. By using the kernel trick, we can get rid of a costly transformation of original\\nfeature vectors into higher-dimensional vectors and avoid computing their dot-product. We\\nreplace that by a simple operation on the original feature vectors that gives the same\\nresult. For example, instead of transforming (q1, p1) into (q2\\n1) and (q2, p2) into\\n2, Ô2q2p2, p2\\n(q2\\n2)\\nto obtain (q2\\n2) we could ﬁnd the dot-product between (q1, p1) and (q2, p2)\\n1p2\\nto get (q1q2 + p1p2) and then square it to get exactly the same result (q2\\n2).\\n\\n2) and then computing the dot-product of (q2\\n2 + 2q1q2p1p2 + p2\\n\\n2, Ô2q2p2, p2\\n1q2\\n\\n2 + 2q1q2p1p2 + p2\\n\\n1, Ô2q1p1, p2\\n\\n1, Ô2q1p1, p2\\n\\n1) and (q2\\n\\n1q2\\n\\n1p2\\n\\nThat was an example of the kernel trick, and we used the quadratic kernel k(xi, xk)\\nMultiple kernel functions exist, the most widely used of which is the RBF kernel:\\n\\ndef\\n= (xixk)2.\\n\\nk(x, xÕ) = exp\\n\\nx\\n\\nÎ\\n\\n2\\nÎ\\n\\nxÕ\\n≠\\n2‡2\\n\\n,\\n\\n4\\n\\n≠\\n\\n3\\n\\nwhere\\nEuclidean distance is given by the following equation:\\n\\n2 is the squared Euclidean distance between two feature vectors. The\\nÎ\\n\\nx\\nÎ\\n\\nxÕ\\n\\n≠\\n\\nd(xi, xk)\\n\\ndef\\n=\\n\\nx(1)\\ni ≠\\n\\nx(1)\\nk\\n\\n2\\n\\n+\\n\\n2\\n\\n1\\n\\nx(2)\\ni ≠\\n\\nx(2)\\nk\\n\\n2\\n\\n+\\n\\n+\\n\\n· · ·\\n\\n2\\n\\n1\\n\\nÚ1\\n\\nx(N )\\ni ≠\\n\\nx(N )\\nk\\n\\n2\\n\\n=\\n\\n2\\n\\nD\\n\\nˆ\\nı\\nı\\nÙ\\n\\nj=1 1\\nÿ\\n\\nx(j)\\ni ≠\\n\\nx(j)\\nk\\n\\n2\\n\\n.\\n\\n2\\n\\nIt can be shown that the feature space of the RBF (for “radial basis function”) kernel has\\nan inﬁnite number of dimensions. By varying the hyperparameter ‡, the data analyst can\\nchoose between getting a smooth or curvy decision boundary in the original space.\\n\\n3.5 k-Nearest Neighbors\\n\\nk-Nearest Neighbors (kNN) is a non-parametric learning algorithm. Contrary to other\\nlearning algorithms that allow discarding the training data after the model is built, kNN\\nkeeps all training examples in memory. Once a new, previously unseen example x comes in,\\nthe kNN algorithm ﬁnds k training examples closest to x and returns the majority label (in\\ncase of classiﬁcation) or the average label (in case of regression).\\n\\nThe closeness of two points is given by a distance function. For example, Euclidean distance\\nseen above is frequently used in practice. Another popular choice of the distance function is\\nthe negative cosine similarity. Cosine similarity deﬁned as,\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n15\\n\\n\\x0cs(xi, xk)\\n\\ndef\\n= cos(\\\\(xi, xk)) =\\n\\nD\\n\\nj=1 x(j)\\n\\ni x(j)\\n\\nk\\n\\nD\\nj=1\\n\\nq\\nx(j)\\ni\\n\\nÚ\\n\\nq\\n\\n1\\n\\n2\\n\\nÚ\\n\\n2\\n\\nq\\n\\nD\\nj=1\\n\\nx(j)\\nk\\n\\n1\\n\\n2\\n\\n,\\n\\n2\\n\\nis a measure of similarity of the directions of two vectors. If the angle between two vectors\\nis 0 degrees, then two vectors point to the same direction, and cosine similarity is equal to\\n1. If the vectors are orthogonal, the cosine similarity is 0. For vectors pointing in opposite\\ndirections, the cosine similarity is\\n1. If we want to use cosine similarity as a distance metric,\\nwe need to multiply it by\\n1. Other popular distance metrics include Chebychev distance,\\nMahalanobis distance, and Hamming distance. The choice of the distance metric, as well as\\nthe value for k, are the choices the analyst makes before running the algorithm. So these\\nare hyperparameters. The distance metric could also be learned from data (as opposed to\\nguessing it). We talk about that in Chapter 10.\\n\\n≠\\n\\n≠\\n\\nNow you know how the model building algorithm works and how the prediction is made. A\\nreasonable question is what is the cost function here? Surprisingly, this question has not\\nbeen well studied in the literature, despite the algorithm’s popularity since the earlier 1960s.\\nThe only attempt to analyze the cost function of kNN I’m aware of was undertaken by Li\\nand Yang in 20034. Below, I outline their considerations.\\n\\nFor simplicity, let’s make our derivation under the assumptions of binary classiﬁcation\\n) with cosine similarity and normalized feature vectors5. Under these assumptions,\\n(y\\nkNN does a locally linear classiﬁcation with the vector of coe\\x00cients,\\n\\n0, 1\\n}\\n\\nœ{\\n\\nwx =\\n\\nyÕxÕ,\\n\\n(9)\\n\\nÿ(xÕ,yÕ)\\n\\nœRk(x)\\nRk(x) is the set of k nearest neighbors to the input example x. The above equation\\nwhere\\nsays that we take the sum of all nearest neighbor feature vectors to some input vector x\\nby ignoring those that have label 0. The classiﬁcation decision is obtained by deﬁning a\\nthreshold on the dot-product wxx which, in the case of normalized feature vectors, is equal\\nto the cosine similarity between wx and x.\\n\\nNow, deﬁning the cost function like this:\\n\\nL =\\n\\n≠\\n\\nÿ(xÕ,yÕ)\\nœ\\n\\nRk(x)\\n\\nyÕxÕwx +\\n\\n1\\n2 ||\\n\\nw\\n\\n2\\n||\\n\\nand setting the ﬁrst order derivative of the right-hand side to zero yields the formula for the\\ncoe\\x00cient vector in eq. 9.\\n\\n4F. Li and Y. Yang, “A loss function analysis for classiﬁcation methods in text categorization,” in ICML\\n\\n2003, pp. 472–479, 2003.\\n\\n5We discuss normalization later; for the moment assume that all features of feature vectors were squeezed\\n\\ninto the range [0, 1].\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n16\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c4 Anatomy of a Learning Algorithm\\n\\n4.1 Building Blocks of a Learning Algorithm\\n\\nYou may have noticed by reading the previous chapter that each learning algorithm we saw\\nconsisted of three parts:\\n\\n1) a loss function;\\n2) an optimization criterion based on the loss function (a cost function, for example); and\\n3) an optimization routine that leverages training data to ﬁnd a solution to the optimization\\n\\ncriterion.\\n\\nThese are the building blocks of any learning algorithm. You saw in the previous chapter\\nthat some algorithms were designed to explicitly optimize a speciﬁc criterion (both linear and\\nlogistic regressions, SVM). Some others, including decision tree learning and kNN, optimize\\nthe criterion implicitly. Decision tree learning and kNN are among the oldest machine\\nlearning algorithms and were invented experimentally based on intuition, without a speciﬁc\\nglobal optimization criterion in mind, and (like it often happens in scientiﬁc history) the\\noptimization criteria were developed later to explain why those algorithms work.\\n\\nBy reading modern literature on machine learning, you often encounter references to gradient\\ndescent or stochastic gradient descent. These are two most frequently used optimization\\nalgorithms used in cases where the optimization criterion is di\\x00erentiable.\\n\\nGradient descent is an iterative optimization algorithm for ﬁnding the minimum of a function.\\nTo ﬁnd a local minimum of a function using gradient descent, one starts at some random\\npoint and takes steps proportional to the negative of the gradient (or approximate gradient)\\nof the function at the current point.\\n\\nGradient descent can be used to ﬁnd optimal parameters for linear and logistic regression,\\nSVM and also neural networks which we consider later. For many models, such as logistic\\nregression or SVM, the optimization criterion is convex. Convex functions have only one\\nminimum, which is global. Optimization criteria for neural networks are not convex, but in\\npractice even ﬁnding a local minimum su\\x00ces.\\n\\nLet’s see how gradient descent works.\\n\\n4.2 Gradient Descent\\n\\nIn this section, I demonstrate how gradient descent ﬁnds the solution to a linear regression\\nproblem1. I illustrate my description with Python source code as well as with plots that\\nshow how the solution improves after some iterations of the gradient descent algorithm.\\n\\n1As you know, linear regression has a closed form solution. That means that gradient descent is not\\nneeded to solve this speciﬁc type of problem. However, for illustration purposes, linear regression is a perfect\\nproblem to explain gradient descent.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cI use a dataset with only one feature. However, the optimization criterion will have two\\nparameters: w and b. The extension to multi-dimensional training data is straightforward:\\nyou have variables w(1), w(2), and b for two-dimensional data, w(1), w(2), w(3), and b for\\nthree-dimensional data and so on.\\n\\nFigure 1: The original data. The Y-axis corresponds to the sales in units (the quantity we\\nwant to predict), the X-axis corresponds to our feature: the spendings on radio ads in M$.\\n\\nTo give a practical example, I use the real dataset with the following columns: the Spendings\\nof various companies on radio advertising each year and their annual Sales in terms of units\\nsold. We want to build a regression model that we can use to predict units sold based on\\nhow much a company spends on radio advertising. Each row in the dataset represents one\\nspeciﬁc company:\\n\\nCompany\\n\\nSpendings, M$\\n\\nSales, Units\\n\\n1\\n2\\n3\\n4\\n..\\n\\n37.8\\n39.3\\n45.9\\n41.3\\n..\\n\\n22.1\\n10.4\\n9.3\\n18.5\\n..\\n\\nWe have data for 200 companies, so we have 200 training examples. Fig. 1 shows all examples\\non a 2D plot.\\n\\nRemember that the linear regression model looks like this: f (x) = wx + b. We don’t know\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cwhat the optimal values for w and b are and we want to learn them from data. To do that,\\nwe look for such values for w and b that minimize the mean squared error:\\n\\nl =\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\n(yi ≠\\n\\n(wxi + b))2.\\n\\nGradient descent starts with calculating the partial derivative for every parameter:\\n\\nˆl\\nˆw\\n\\nˆl\\nˆb\\n\\n=\\n\\n=\\n\\n1\\nN\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\nN\\n\\ni=1\\nÿ\\n\\n2xi(yi ≠\\n≠\\n\\n(wxi + b));\\n\\n2(yi ≠\\n≠\\n\\n(wxi + b)).\\n\\n(1)\\n\\n(wx + b))2 with respect to w we applied the\\nTo ﬁnd the partial derivative of the term (yi ≠\\nchain rule. Here, we have the chain f = f2(f1) where f1 = yi ≠\\n1 . To ﬁnd\\na partial derivative of f with respect to w we have to ﬁrst ﬁnd the partial derivative of f with\\n(wx + b)) (from calculus, we know that the derivative\\nrespect to f2 which is equal to 2(yi ≠\\nˆx x2 = 2x) and then we have to multiply it by the partial derivative of yi ≠\\n(wx + b) with\\nN\\nx. So overall ˆl\\nˆw = 1\\nrespect to w which is equal to\\n(wxi + b)). In a similar\\n2xi(yi ≠\\ni=1 ≠\\nN\\nway, the partial derivative of l with respect to b, ˆl\\nˆb , was calculated.\\n\\n(wx + b) and f2 = f 2\\n\\n≠\\n\\nˆf\\n\\nq\\n\\nWe initialize2 w0 = 0 and b0 = 0 and then iterate through our training examples, each\\nexample having the form of (xi, yi) = (Spendingsi, Salesi). For each training example, we\\nupdate w and b using our partial derivatives. The learning rate – controls the size of an\\nupdate:\\n\\nwi Ω\\nbi Ω\\n\\n2xi(yi ≠\\n(wi\\n2(yi ≠\\n\\n1xi + bi\\n\\n(wi\\n≠\\nN\\n1xi + bi\\n\\n– ≠\\n\\n– ≠\\n\\n1))\\n\\n≠\\n\\n;\\n\\n≠\\nN\\n\\n1))\\n\\n≠\\n\\n,\\n\\n(2)\\n\\nwhere wi and bi denote the values of w and b after using the example (xi, yi) for the update.\\n\\nOne pass through all training examples is called an epoch. Typically, we need multiple\\nepochs until we start seeing that the values for w and b don’t change much; then we stop.\\n\\n2In complex models, such as neural networks, which have thousands of parameters, the initialization of\\nparameters may signiﬁcantly a\\x00ect the solution found using gradient descent. There are di\\x00erent initialization\\nmethods (at random, with all zeroes, with small values around zero, and others) and it is an important choice\\nthe data analyst has to make.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cIt’s hard to imagine a machine learning engineer who doesn’t use Python. So, if you waited\\nfor the right moment to start learning Python, this is that moment. Below we show how to\\nprogram gradient descent in Python.\\n\\nThe function that updates the parameters w and b during one epoch is shown below:\\n\\ndef update_w_and_b(spendings, sales, w, b, alpha):\\n\\ndl_dw = 0.0\\ndl_db = 0.0\\nN = len(spendings)\\n\\nfor i in range(N):\\n\\ndl_dw += -2*spendings[i]*(sales[i] - (w*spendings[i] + b))\\ndl_db += -2*(sales[i] - (w*spendings[i] + b))\\n\\n# update w and b\\nw = w - (1/float(N))*dl_dw*alpha\\nb = b - (1/float(N))*dl_db*alpha\\n\\nreturn w, b\\n\\nThe function that loops over multiple epochs is shown below:\\n\\ndef train(spendings, sales, w, b, alpha, epochs):\\n\\nfor e in range(epochs):\\n\\nw, b = update_w_and_b(spendings, sales, w, b, alpha)\\n\\n# log the progress\\nif e % 400 == 0:\\n\\nprint(\"epoch:\", e, \"loss: \", avg_loss(spendings, sales, w, b))\\n\\nreturn w, b\\n\\nThe function avg_loss in the above code snippet is a function that computes the mean\\nsquared error. It is deﬁned as:\\n\\ndef avg_loss(spendings, sales, w, b):\\n\\nN = len(spendings)\\ntotal_error = 0.0\\nfor i in range(N):\\n\\ntotal_error += (sales[i] - (w*spendings[i] + b))**2\\n\\nreturn total_error / float(N)\\n\\nIf we run the train function for – = 0.001, w = 0.0, b = 0.0, and 15000 epochs, we will see\\nthe following output (shown partially):\\n\\nepoch: 0 loss: 92.32078294903626\\nepoch: 400 loss: 33.79131790081576\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cepoch: 800 loss: 27.9918542960729\\nepoch: 1200 loss: 24.33481690722147\\nepoch: 1600 loss: 22.028754937538633\\n...\\nepoch: 2800 loss: 19.07940244306619\\n\\nEpoch 0\\n\\nEpoch 400\\n\\nEpoch 800\\n\\nEpoch 1200\\n\\nEpoch 1600\\n\\nEpoch 3000\\n\\nFigure 2: The evolution of the regression line through gradient descent epochs.\\n\\nYou can see that the average loss decreases as the train function loops through epochs. Fig.\\n2 shows the evolution of the regression line through epochs.\\n\\nFinally, once we have found the optimal values of parameters w and b, the only missing piece\\nis a function that makes predictions:\\n\\ndef predict(x, w, b):\\nreturn w*x + b\\n\\nTry to execute the following code:\\n\\nw, b = train(x, y, 0.0, 0.0, 0.001, 15000)\\nx_new = 23.0\\ny_new = predict(x_new, w, b)\\nprint(y_new)\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cThe output is 13.97.\\n\\nThe gradient descent algorithm is sensitive to the choice of the step –. It is also slow for large\\ndatasets. Fortunately, several signiﬁcant improvements to this algorithm have been proposed.\\n\\nStochastic gradient descent (SGD) is a version of the algorithm that speeds up the\\ncomputation by approximating the gradient using smaller batches (subsets) of the training\\ndata. SGD itself has various “upgrades”. Adagrad is a version of SGD that scales – for\\neach parameter according to the history of gradients. As a result, – is reduced for very large\\ngradients and vice-versa. Momentum is a method that helps accelerate SGD by orienting\\nthe gradient descent in the relevant direction and reducing oscillations. In neural network\\ntraining, variants of SGD such as RMSprop and Adam, are most frequently used.\\n\\nNotice that gradient descent and its variants are not machine learning algorithms. They are\\nsolvers of minimization problems in which the function to minimize has a gradient in most\\npoints of its domain.\\n\\n4.3 How Machine Learning Engineers Work\\n\\nUnless you are a research scientist or work for a huge corporation with a large R&D budget,\\nyou usually don’t implement machine learning algorithms yourself. You don’t implement\\ngradient descent or some other solver either. You use libraries, most of which are open\\nsource. A library is a collection of algorithms and supporting tools implemented with stability\\nand e\\x00ciency in mind. The most frequently used in practice open-source machine learning\\nlibrary is scikit-learn. It’s written in Python and C. Here’s how you do linear regression in\\nscikit-learn:\\n\\ndef train(x, y):\\n\\nfrom sklearn.linear_model import LinearRegression\\nmodel = LinearRegression().fit(x,y)\\nreturn model\\n\\nmodel = train(x,y)\\n\\nx_new = 23.0\\ny_new = model.predict(x_new)\\nprint(y_new)\\n\\nThe output will, again, be 13.97. Easy, right? You can replace LinearRegression with some\\nother type of regression learning algorithm without modifying anything else. It just works.\\nThe same can be said about classiﬁcation. You can easily replace LogisticRegression algorithm\\nwith SVC algorithm (this is scikit-learn’s name for the Support Vector Machine algorithm),\\nDecisionTreeClassiﬁer, NearestNeighbors or many other classiﬁcation learning algorithms\\nimplemented in scikit-learn.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0c4.4 Learning Algorithms’ Particularities\\n\\nHere we outline some practical particularities that can di\\x00erentiate one learning algorithm\\nfrom another. You already know that di\\x00erent learning algorithms can have di\\x00erent\\nhyperparameters (C in SVM, ‘ and d in ID3). Solvers such as gradient descent can also have\\nhyperparameters, like – for example.\\n\\nSome algorithms, like decision tree learning, can accept categorical features. For example, if\\nyou have a feature “color” that can take values “red”, “yellow”, or “green”, you can keep\\nthis feature as is. SVM, logistic and linear regression, as well as kNN (with cosine similarity\\nor Euclidean distance metrics), expect numerical values for all features. All algorithms\\nimplemented in scikit-learn expect numerical features. I show in the next chapter how to\\nconvert categorical features into numerical ones.\\n\\nSome algorithms, like SVM, allow the data analyst to provide weightings for each class.\\nThese weightings inﬂuence how the decision boundary is drawn. If the weight of some class\\nis high, the learning algorithm tries to not make errors in predicting training examples of\\nthis class (typically, for the cost of making an error elsewhere). That could be important if\\ninstances of some class are in the minority in your training data, but you would like to avoid\\nmisclassifying examples of that class as much as possible.\\n\\nSome classiﬁcation models, like SVM and kNN, given a feature vector only output the class.\\nOthers, like logistic regression or decision trees, can also return the score between 0 and 1\\nwhich can be interpreted as either how conﬁdent the model is about the prediction or as the\\nprobability that the input example belongs to a certain class3.\\n\\nSome classiﬁcation algorithms (like decision tree learning, logistic regression, or SVM) build the\\nmodel using the whole dataset at once. If you have got additional labeled examples, you have\\nto rebuild the model from scratch. Other algorithms (such as Naïve Bayes, multilayer percep-\\ntron, SGDClassiﬁer/SGDRegressor, PassiveAggressiveClassiﬁer/PassiveAggressiveRegressor\\nin scikit-learn) can be trained iteratively, one batch at a time. Once new training examples\\nare available, you can update the model using only the new data.\\n\\nFinally, some algorithms, like decision tree learning, SVM, and kNN can be used for both clas-\\nsiﬁcation and regression, while others can only solve one type of problem: either classiﬁcation\\nor regression, but not both.\\n\\nUsually, each library provides the documentation that explains what kind of problem each\\nalgorithm solves, what input values are allowed and what kind of output the model produces.\\nThe documentation also provides information on hyperparameters.\\n\\n3If it’s really necessary, the score for SVM and kNN predictions could be synthetically created using some\\n\\nsimple techniques.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cAndriy Burkov\\'s\\n\\n\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c5 Basic Practice\\n\\nUntil now, I only mentioned in passing some problems a data analyst can encounter when\\nworking on a machine learning problem: feature engineering, overﬁtting, and hyperparameter\\ntuning. In this chapter, we talk about these and other challenges that have to be addressed\\nbefore you can type model = LogisticRegresion().ﬁt(x,y) in scikit-learn.\\n\\n5.1 Feature Engineering\\n\\nWhen a product manager tells you “We need to be able to predict whether a particular\\ncustomer will stay with us. Here are the logs of customers’ interactions with our product for\\nﬁve years.” you cannot just grab the data, load it into a library and get a prediction. You\\nneed to build a dataset ﬁrst.\\n\\nRemember from the ﬁrst chapter that the dataset is the collection of labeled examples\\nN\\ni=1. Each element xi among N is called a feature vector. A feature vector is a\\n(xi, yi)\\n}\\n{\\nvector in which each dimension j = 1, . . . , D contains a value that describes the example\\nsomehow. That value is called a feature and is denoted as x(j).\\n\\nThe problem of transforming raw data into a dataset is called feature engineering. For\\nmost practical problems, feature engineering is a labor-intensive process that demands from\\nthe data analyst a lot of creativity and, preferably, domain knowledge.\\n\\nFor example, to transform the logs of user interaction with a computer system, one could\\ncreate features that contain information about the user and various statistics extracted from\\nthe logs. For each user, one feature would contain the price of the subscription; other features\\nwould contain the frequency of connections per day, week and year. Another feature would\\ncontain the average session duration in seconds or the average response time for one request,\\nand so on. Everything measurable can be used as a feature. The role of the data analyst is to\\ncreate informative features: those would allow the learning algorithm to build a model that\\npredicts well labels of the data used for training. Highly informative features are also called\\nfeatures with high predictive power. For example, the average duration of a user’s session\\nhas high predictive power for the problem of predicting whether the user will keep using the\\napplication in the future.\\n\\nWe say that a model has a low bias when it predicts well the training data. That is, the\\nmodel makes few mistakes when we try to predict labels of the examples used to build the\\nmodel.\\n\\n5.1.1 One-Hot Encoding\\n\\nSome learning algorithms only work with numerical feature vectors. When some feature in\\nyour dataset is categorical, like “colors” or “days of the week,” you can transform such a\\ncategorical feature into several binary ones.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cIf your example has a categorical feature “colors” and this feature has three possible values:\\n“red,” “yellow,” “green,” you can transform this feature into a vector of three numerical\\nvalues:\\n\\nred = [1, 0, 0]\\nyellow = [0, 1, 0]\\ngreen = [0, 0, 1]\\n\\n(1)\\n\\nBy doing so, you increase the dimensionality of your feature vectors. You should not transform\\nred into 1, yellow into 2, and green into 3 to avoid increasing the dimensionality because that\\nwould imply that there’s an order among the values in this category and this speciﬁc order is\\nimportant for the decision making. If the order of a feature’s values is not important, using\\nordered numbers as values is likely to confuse the learning algorithm,1 because the algorithm\\nwill try to ﬁnd a regularity where there’s no one, which may potentially lead to overﬁtting.\\n\\n5.1.2 Binning\\n\\nAn opposite situation, occurring less frequently in practice, is when you have a numerical\\nfeature but you want to convert it into a categorical one. Binning (also called bucketing)\\nis the process of converting a continuous feature into multiple binary features called bins or\\nbuckets, typically based on value range. For example, instead of representing age as a single\\nreal-valued feature, the analyst could chop ranges of age into discrete bins: all ages between\\n0 and 5 years-old could be put into one bin, 6 to 10 years-old could be in the second bin, 11\\nto 15 years-old could be in the third bin, and so on.\\n\\nFor example, suppose in our feature j = 18 represents age. By applying binning, we replace\\nthis feature with the corresponding bins. Let the three new bins, “age_bin1”, “age_bin2”\\nand “age_bin3” be added with indexes j = 123, j = 124 and j = 125 respectively. Now if\\nx(18)\\ni = 7 for some example xi, then we set feature x(124)\\ni = 13, then we set\\nfeature x(125)\\n\\nto 1; if x(18)\\n\\nto 1, and so on.\\n\\ni\\n\\ni\\n\\nIn some cases, a carefully designed binning can help the learning algorithm to learn using\\nfewer examples. It happens because we give a “hint” to the learning algorithm that if the\\nvalue of a feature falls within a speciﬁc range, the exact value of the feature doesn’t matter.\\n\\n1When the ordering of values of some categorical variable matters, we can replace those values by numbers\\nby keeping only one variable. For example, if our variable represents the quality of an article, and the\\nvalues are\\n, then we could replace those categories by numbers, for example,\\n1, 2, 3, 4\\n.\\n\\npoor, decent, good, excellent\\n\\n{\\n\\n}\\n\\n{\\n\\n}\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0c5.1.3 Normalization\\n\\nNormalization is the process of converting an actual range of values which a numerical\\nfeature can take, into a standard range of values, typically in the interval [\\n\\n1, 1] or [0, 1].\\n\\n≠\\n\\nFor example, suppose the natural range of a particular feature is 350 to 1450. By subtracting\\n350 from every value of the feature, and dividing the result by 1100, one can normalize those\\nvalues into the range [0, 1].\\n\\nMore generally, the normalization formula looks like this:\\n\\n¯x(j) =\\n\\nx(j)\\n≠\\nmax(j)\\n\\nmin(j)\\n\\n,\\n\\nmin(j)\\n\\n≠\\n\\nwhere min(j) and max(j) are, respectively, the minimum and the maximum value of the\\nfeature j in the dataset.\\n\\nWhy do we normalize? Normalizing the data is not a strict requirement. However, in practice,\\nit can lead to an increased speed of learning. Remember the gradient descent example from\\nthe previous chapter. Imagine you have a two-dimensional feature vector. When you update\\nthe parameters of w(1) and w(2), you use partial derivatives of the average squared error with\\nrespect to w(1) and w(2). If x(1) is in the range [0, 1000] and x(2) the range [0, 0.0001], then\\nthe derivative with respect to a larger feature will dominate the update.\\n\\nAdditionally, it’s useful to ensure that our inputs are roughly in the same relatively small\\nrange to avoid problems which computers have when working with very small or very big\\nnumbers (known as numerical overﬂow).\\n\\n5.1.4 Standardization\\n\\nStandardization (or z-score normalization) is the procedure during which the feature\\nvalues are rescaled so that they have the properties of a standard normal distribution with\\nµ = 0 and ‡ = 1, where µ is the mean (the average value of the feature, averaged over all\\nexamples in the dataset) and ‡ is the standard deviation from the mean.\\n\\nStandard scores (or z-scores) of features are calculated as follows:\\n\\nˆx(j) =\\n\\nx(j)\\n\\nµ(j)\\n\\n≠\\n‡(j)\\n\\n.\\n\\nYou may ask when you should use normalization and when standardization. There’s no\\ndeﬁnitive answer to this question. Usually, if your dataset is not too big and you have time,\\nyou can try both and see which one performs better for your task.\\n\\nIf you don’t have time to run multiple experiments, as a rule of thumb:\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0c• unsupervised learning algorithms, in practice, more often beneﬁt from standardization\\n\\nthan from normalization;\\n\\n• standardization is also preferred for a feature if the values this feature takes are\\n\\ndistributed close to a normal distribution (so-called bell curve);\\n\\n• again, standardization is preferred for a feature if it can sometimes have extremely high\\nor low values (outliers); this is because normalization will “squeeze” the normal values\\ninto a very small range;\\n\\n• in all other cases, normalization is preferable.\\n\\nModern implementations of the learning algorithms, which you can ﬁnd in popular libraries,\\nare robust to features lying in di\\x00erent ranges. Feature rescaling is usually beneﬁcial to most\\nlearning algorithms, but in many cases, the model will still be good when trained from the\\noriginal features.\\n\\n5.1.5 Dealing with Missing Features\\n\\nIn some cases, the data comes to the analyst in the form of a dataset with features already\\ndeﬁned. In some examples, values of some features can be missing. That often happens when\\nthe dataset was handcrafted, and the person working on it forgot to ﬁll some values or didn’t\\nget them measured at all.\\n\\nThe typical approaches of dealing with missing values for a feature include:\\n\\n• Removing the examples with missing features from the dataset. That can be done if\\n\\nyour dataset is big enough so you can sacriﬁce some training examples.\\n\\n• Using a learning algorithm that can deal with missing feature values (depends on the\\n\\nlibrary and a speciﬁc implementation of the algorithm).\\n\\n• Using a data imputation technique.\\n\\n5.1.6 Data Imputation Techniques\\n\\nOne technique consists in replacing the missing value of a feature by an average value of this\\nfeature in the dataset:\\n\\nˆx(j) =\\n\\nx(j).\\n\\n1\\nN\\n\\n≠\\n\\nAnother technique is to replace the missing value by the same value outside the normal range\\nof values. For example, if the normal range is [0, 1], then you can set the missing value equal\\n1. The idea is that the learning algorithm will learn what is it better to do when the\\nto 2 or\\nfeature has a value signiﬁcantly di\\x00erent from other values. Alternatively, you can replace the\\nmissing value by a value in the middle of the range. For example, if the range for a feature is\\n1, 1], you can set the missing value to be equal to 0. Here, the idea is that if we use the\\n[\\n≠\\nvalue in the middle of the range to replace missing features, such value will not signiﬁcantly\\na\\x00ect the prediction.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cA more advanced technique is to use the missing value as the target variable for a regression\\nproblem. You can use all remaining features [x(1)\\n] to form\\na feature vector ˆxi, set ˆyi = x(j), where j is the feature with a missing value. Now we can\\nbuild a regression model to predict ˆy from the feature vectors ˆx. Of course, to build training\\nexamples (ˆx, ˆy), you only use those examples from the original dataset, in which the value of\\nfeature j is present.\\n\\n, . . . , x(D)\\n\\n, . . . , x(j\\ni\\n\\n, x(j+1)\\ni\\n\\n, x(2)\\ni\\n\\n1)\\n\\n≠\\n\\ni\\n\\ni\\n\\nFinally, if you have a signiﬁcantly large dataset and just a few features with missing values,\\nyou can increase the dimensionality of your feature vectors by adding a binary indicator\\nfeature for each feature with missing values. Let’s say feature j = 12 in your D-dimensional\\ndataset has missing values. For each feature vector x, you then add the feature j = D + 1\\nwhich is equal to 1 if the value of feature 12 is present in x and 0 otherwise. The missing\\nfeature value then can be replaced by 0 or any number of your choice.\\n\\nAt prediction time, if your example is not complete, you should use the same data imputation\\ntechnique to ﬁll the missing features as the technique you used to complete the training data.\\n\\nBefore you start working on the learning problem, you cannot tell which data imputation\\ntechnique will work the best. Try several techniques, build several models and select the one\\nthat works the best.\\n\\n5.2 Learning Algorithm Selection\\n\\nChoosing a machine learning algorithm can be a di\\x00cult task. If you have much time, you\\ncan try all of them. However, usually the time you have to solve a problem is limited. You\\ncan ask yourself several questions before starting to work on the problem. Depending on\\nyour answers, you can shortlist some algorithms and try them on your data.\\n\\n• Explainability\\n\\nDoes your model have to be explainable to a non-technical audience? Most very accurate\\nlearning algorithms are so-called “black boxes.” They learn models that make very few errors,\\nbut why a model made a speciﬁc prediction could be very hard to understand and even\\nharder to explain. Examples of such models are neural networks or ensemble models.\\n\\nOn the other hand, kNN, linear regression, or decision tree learning algorithms produce\\nmodels that are not always the most accurate, however, the way they make their prediction\\nis very straightforward.\\n\\n• In-memory vs. out-of-memory\\n\\nCan your dataset be fully loaded into the RAM of your server or personal computer? If\\nyes, then you can choose from a wide variety of algorithms. Otherwise, you would prefer\\nincremental learning algorithms that can improve the model by adding more data\\ngradually.\\n\\n• Number of features and examples\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cHow many training examples do you have in your dataset? How many features does each\\nexample have? Some algorithms, including neural networks and gradient boosting (we\\nconsider both later), can handle a huge number of examples and millions of features. Others,\\nlike SVM, can be very modest in their capacity.\\n\\n• Categorical vs. numerical features\\n\\nIs your data composed of categorical only, or numerical only features, or a mix of both?\\nDepending on your answer, some algorithms cannot handle your dataset directly, and you\\nwould need to convert your categorical features into numerical ones by using some techniques\\nlike one-hot encoding.\\n\\n• Nonlinearity of the data\\n\\nIs your data linearly separable or can it be modeled using a linear model? If yes, SVM with\\nthe linear kernel, logistic regression or linear regression can be a good choice. Otherwise,\\ndeep neural networks or ensemble algorithms, discussed in Chapters 6 and 7, might work\\nbetter for your data.\\n\\n• Training speed\\n\\nHow much time is a learning algorithm allowed to use to build a model? Neural networks\\nare known to be slow to train. Simple algorithms like logistic and linear regression as well\\nas decision tree learning are much faster. Some specialized libraries contain very e\\x00cient\\nimplementations of some algorithms; you may prefer to do research online to ﬁnd such\\nlibraries. Some algorithms, such as random forests, beneﬁt from the availability of multiple\\nCPU cores, so their model building time can be signiﬁcantly reduced on a machine with\\ndozens of CPU cores.\\n\\n• Prediction speed\\n\\nHow fast does the model have to be when generating predictions? Will your model be used in\\nproduction where very high throughput is required? Some algorithms, like SVMs, linear and\\nlogistic regression, or some types of neural networks, are extremely fast at the prediction time.\\nSome others, like kNN, ensemble algorithms, and very deep or recurrent neural networks,\\ncan be slower2.\\n\\nIf you don’t want to guess the best algorithm for your data, a popular way to choose one is\\nby testing it on the validation set. We talk about that below.\\n\\nAlternatively, if you use scikit-learn, you could try their algorithm selection diagram shown\\nin ﬁg. 1.\\n\\n2The prediction speeds of kNN and ensemble methods implemented in the modern libraries are still very\\n\\nfast. Don’t be afraid of using these algorithms in your practice.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0c.\\n\\nn\\nr\\na\\ne\\nl\\n-\\nt\\ni\\nk\\ni\\nc\\ns\\n\\nr\\no\\nf\\n\\nm\\na\\nr\\ng\\na\\ni\\nd\\n\\nn\\no\\ni\\nt\\nc\\ne\\nl\\ne\\ns\\n\\nm\\nh\\nt\\ni\\nr\\no\\ng\\nl\\na\\n\\ni\\n\\ng\\nn\\nn\\nr\\na\\ne\\nl\\n\\ni\\n\\ne\\nn\\nh\\nc\\na\\nM\\n\\n:\\n1\\n\\ne\\nr\\nu\\ng\\ni\\nF\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0c5.3 Three Sets\\n\\nUntil now, I used the expressions “dataset” and “training set” interchangeably. However, in\\npractice data analysts work with three sets of labeled examples:\\n\\n1) training set,\\n2) validation set, and\\n3) test set.\\n\\nOnce you have got your annotated dataset, the ﬁrst thing you do is you shu\\x00e the examples\\nand split the dataset into three subsets: training, validation, and test. The training set is\\nusually the biggest one, and you use it to build the model. The validation and test sets are\\nroughly the same sizes, much smaller than the size of the training set. The learning algorithm\\ncannot use examples from these two subsets to build a model. That is why those two sets are\\noften called hold-out sets.\\n\\nThere’s no optimal proportion to split the dataset into these three subsets. In the past, the\\nrule of thumb was to use 70% of the dataset for training, 15% for validation and 15% for\\ntesting. However, in the age of big data, datasets often have millions of examples. In such\\ncases, it could be reasonable to keep 95% for training and 2.5%/2.5% for validation/testing.\\n\\nYou may wonder, what is the reason to have three sets and not one. The answer is simple:\\nwhen we build a model, what we do not want is for the model to only do well at predicting\\nlabels of examples the learning algorithms has already seen. A trivial algorithm that simply\\nmemorizes all training examples and then uses the memory to “predict” their labels will make\\nno mistakes when asked to predict the labels of the training examples, but such an algorithm\\nwould be useless in practice. What we really want is that our model predicts well examples\\nthat the learning algorithm didn’t see. So we want good performance on a hold-out set.\\n\\nWhy do we need two hold-out sets and not one? We use the validation set to 1) choose the\\nlearning algorithm and 2) ﬁnd the best values of hyperparameters. We use the test set to\\nassess the model before delivering it to the client or putting it in production.\\n\\n5.4 Underﬁtting and Overﬁtting\\n\\nI mentioned above the notion of bias. I said that a model has a low bias if it predicts well\\nthe labels of the training data. If the model makes many mistakes on the training data,\\nwe say that the model has a high bias or that the model underﬁts. So, underﬁtting is the\\ninability of the model to predict well the labels of the data it was trained on. There could be\\nseveral reasons for underﬁtting, the most important of which are:\\n\\n• your model is too simple for the data (for example a linear model can often underﬁt);\\n• the features you engineered are not informative enough.\\n\\nThe ﬁrst reason is easy to illustrate in the case of one-dimensional regression: the dataset can\\nresemble a curved line, but our model is a straight line. The second reason can be illustrated\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cUnderﬁtting\\n\\nGood ﬁt\\n\\nOverﬁtting\\n\\nFigure 2: Examples of underﬁtting (linear model), good ﬁt (quadratic model), and overﬁtting\\n(polynomial of degree 15).\\n\\nlike this: let’s say you want to predict whether a patient has cancer, and the features you\\nhave are height, blood pressure, and heart rate. These three features are clearly not good\\npredictors for cancer so our model will not be able to learn a meaningful relationship between\\nthese features and the label.\\n\\nThe solution to the problem of underﬁtting is to try a more complex model or to engineer\\nfeatures with higher predictive power.\\n\\nOverﬁtting is another problem a model can exhibit. The model that overﬁts predicts very\\nwell the training data but poorly the data from at least one of the two hold-out sets. I already\\ngave an illustration of overﬁtting in Chapter 3. Several reasons can lead to overﬁtting, the\\nmost important of which are:\\n\\n• your model is too complex for the data (for example a very tall decision tree or a very\\n\\ndeep or wide neural network often overﬁt);\\n\\n• you have too many features but a small number of training examples.\\n\\nIn the literature, you can ﬁnd another name for the problem of overﬁtting: the problem of\\nhigh variance. This term comes from statistics. The variance is an error of the model due to\\nits sensitivity to small ﬂuctuations in the training set. It means that if your training data\\nwas sampled di\\x00erently, the learning would result in a signiﬁcantly di\\x00erent model. Which\\nis why the model that overﬁts performs poorly on the test data: test and training data are\\nsampled from the dataset independently of one another.\\n\\nSeveral solutions to the problem of overﬁtting are possible:\\n\\n1. Try a simpler model (linear instead of polynomial regression, or SVM with a linear\\n\\nkernel instead of RBF, a neural network with fewer layers/units).\\n\\n2. Reduce the dimensionality of examples in the dataset (for example, by using one of the\\n\\ndimensionality reduction techniques discussed in Chapter 9).\\n\\n3. Add more training data, if possible.\\n4. Regularize the model.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0cFigure 2 illustrates a one-dimensional dataset for which a regression model underﬁts, ﬁts well\\nand overﬁts the data.\\n\\nRegularization is the most widely used approach to prevent overﬁtting.\\n\\n5.5 Regularization\\n\\nEven the simplest model, such as linear, can overﬁt the data. That usually happens when the\\ndata is high-dimensional, but the number of training examples is relatively low. In fact, when\\nfeature vectors are very high-dimensional, the linear learning algorithm can build a model\\nthat assigns non-zero values to most dimensions w(j) in the parameter vector w, trying to\\nﬁnd very complex relationships between all available features to predict labels of training\\nexamples perfectly.\\n\\nSuch a complex model will most likely predict poorly the labels of the hold-out examples.\\nThis is because by trying to perfectly predict labels of all training examples, the model will\\nalso learn the idiosyncrasies of the training set: the noise in the values of features of the\\ntraining examples, the sampling imperfection due to the small dataset size, and other artifacts\\nextrinsic to the decision problem in hand but present in the training set.\\n\\nRegularization is an umbrella-term that encompasses methods that force the learning\\nalgorithm to build a less complex model. In practice, that often leads to slightly higher\\nbias but signiﬁcantly reduces the variance. This problem is known in the literature as the\\nbias-variance tradeo\\x00 .\\n\\nThe two most widely used types of regularization are called L1 regularization and L2\\nregularization. The idea is quite simple. To create a regularized model, we modify the\\nobjective function by adding a penalizing term whose value is higher when the model is more\\ncomplex.\\n\\nFor simplicity, I illustrate regularization using the example of linear regression. The same\\nprinciple can be applied to a wide variety of models.\\n\\nRecall the linear regression objective we want to minimize:\\n\\nmin\\nw,b\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\n(fw,b(xi)\\n\\nyi)2.\\n\\n≠\\n\\nAn L1-regularized objective looks like this:\\n\\nmin\\nw,b\\n\\nC\\n\\nw\\n|\\n\\n|\\n\\n+\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\n(fw,b(xi)\\n\\nyi)2,\\n\\n≠\\n\\n(2)\\n\\n(3)\\n\\nand C is a hyperparameter that controls the importance of regular-\\nwhere\\nization. If we set C to zero, the model becomes a standard non-regularized linear regression\\n\\nw\\n|\\n\\n|\\n\\n|\\n\\ndef\\n=\\n\\nD\\nj=1 |\\n\\nw(j)\\n\\nq\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0cmodel. On the other hand, if we set to C to a high value, the learning algorithm will try to\\nset most w(j) to a very small value or zero to minimize the objective, the model will become\\nvery simple which can lead to underﬁtting. Your role as the data analyst is to ﬁnd such\\na value of the hyperparameter C that doesn’t increase the bias too much but reduces the\\noverﬁtting to a level reasonable for the problem in hand. In the next section, I will show how\\nto do that.\\n\\nAn L2-regularized objective will look like this:\\n\\nmin\\nw,b\\n\\nC\\n\\nw\\nÎ\\n\\n2 +\\nÎ\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\n(fw,b(xi)\\n\\n≠\\n\\nyi)2, where\\n\\nw\\n\\n2 def\\n=\\nÎ\\n\\nÎ\\n\\n(w(j))2.\\n\\n(4)\\n\\nD\\n\\nj=1\\nÿ\\n\\nIn practice, L1 regularization produces a sparse model, a model that has most of its\\nparameters (in case of linear models, most of w(j)) equal to zero (provided the hyperparameter\\nC is large enough). So L1 makes feature selection by deciding which features are essential\\nfor prediction and which are not. That can be useful in case you want to increase model\\nexplainability. However, if your only goal is to maximize the performance of the model on\\nthe hold-out data, then L2 usually gives better results. L2 also has the advantage of being\\ndi\\x00erentiable, so gradient descent can be used for optimizing the objective function.\\n\\nL1 and L2 regularization methods are also combined in what is called elastic net regular-\\nization with L1 and L2 regularizations being special cases. You can ﬁnd in the literature\\nthe name ridge regularization for L2 and lasso for L1.\\n\\nIn addition to being widely used with linear models, L1 and L2 regularization are also\\nfrequently used with neural networks and many other types of models, which directly\\nminimize an objective function.\\n\\nNeural networks also beneﬁt from two other regularization techniques: dropout and batch-\\nnormalization. There are also non-mathematical methods that have a regularization e\\x00ect:\\ndata augmentation and early stopping. We talk about these techniques in Chapter 8.\\n\\n5.6 Model Performance Assessment\\n\\nOnce you have a model which our learning algorithm has built using the training set, how\\ncan you say how good the model is? You use the test set to assess the model.\\n\\nThe test set contains the examples that the learning algorithm has never seen before, so if\\nour model performs well on predicting the labels of the examples from the test set, we say\\nthat our model generalizes well or, simply, that it’s good.\\n\\nTo be more rigorous, machine learning specialists use various formal metrics and tools to\\nassess the model performance. For regression, the assessment of the model is quite simple. A\\nwell-ﬁtting regression model results in predicted values close to the observed data values. The\\nmean model, which always predicts the average of the labels in the training data, generally\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0cwould be used if there were no informative features. The ﬁt of a regression model being\\nassessed should, therefore, be better than the ﬁt of the mean model. If this is the case, then\\nthe next step is to compare the performances of the model on the training and the test data.\\n\\nTo do that, we compute the mean squared error3 (MSE) for the training, and, separately,\\nfor the test data. If the MSE of the model on the test data is substantially higher than\\nthe MSE obtained on the training data, this is a sign of overﬁtting. Regularization or a\\nbetter hyperparameter tuning could solve the problem. The meaning of “substantially higher”\\ndepends on the problem in hand and has to be decided by the data analyst jointly with the\\ndecision maker/product owner who ordered the model.\\n\\nFor classiﬁcation, things are a little bit more complicated. The most widely used metrics and\\ntools to assess the classiﬁcation model are:\\n\\n• confusion matrix,\\n• accuracy,\\n• cost-sensitive accuracy,\\n• precision/recall, and\\n• area under the ROC curve.\\n\\nTo simplify the illustration, I use a binary classiﬁcation problem. Where necessary, I show\\nhow to extend the approach to the multiclass case.\\n\\n5.6.1 Confusion Matrix\\n\\nThe confusion matrix is a table that summarizes how successful the classiﬁcation model\\nis at predicting examples belonging to various classes. One axis of the confusion matrix\\nis the label that the model predicted, and the other axis is the actual label. In a binary\\nclassiﬁcation problem, there are two classes. Let’s say, the model predicts two classes: “spam”\\nand “not_spam”:\\n\\nspam (predicted)\\n\\nnot spam (predicted)\\n\\nspam (actual)\\nnot spam (actual)\\n\\n23 (TP)\\n12 (FP)\\n\\n1 (FN)\\n556 (TN)\\n\\nThe above confusion matrix shows that of the 24 examples that actually were spam, the\\nmodel correctly classiﬁed 23 as spam. In this case, we say that we have 23 true positives\\nor TP = 23. The model incorrectly classiﬁed 1 example as not spam. In this case, we have 1\\nfalse negative, or FN = 1. Similarly, of 568 examples that actually were not spam, 556 were\\ncorrectly classiﬁed (556 true negatives or TN = 556), and 12 were incorrectly classiﬁed (12\\nfalse positives, FP = 12).\\n\\nThe confusion matrix for multiclass classiﬁcation has as many rows and columns as there are\\n\\n3Or any other type of loss function you used to build your optimization problem.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n14\\n\\n\\x0cdi\\x00erent classes. It can help you to determine mistake patterns. For example, a confusion\\nmatrix could reveal that a model trained to recognize di\\x00erent species of animals tends to\\nmistakenly predict “cat” instead of “panther,” or “mouse” instead of “rat.” In this case, you\\ncan decide to add more labeled examples of these species to help the learning algorithm to\\n“see” the di\\x00erence between them. Alternatively, you can decide to add additional features\\nthe learning algorithm can use to build a model that would better distinguish between these\\nspecies.\\n\\nConfusion matrices can be used to calculate two important performance metrics: precision\\nand recall.\\n\\n5.6.2 Precision/Recall\\n\\nThe two most frequently used metrics to assess the model are precision and recall. Precision\\nis the ratio of correct positive predictions to the overall number of positive predictions:\\n\\nPrecision =\\n\\nTP\\nTP + FP\\n\\n.\\n\\nRecall is the ratio of correct positive predictions to the overall number of positive examples\\nin the test set:\\n\\nRecall =\\n\\nTP\\nTP + FN\\n\\n.\\n\\nTo understand the meaning and importance of precision and recall for the model assessment it\\nis often useful to think about the prediction problem as the problem of research of documents\\nin the database using a query. The precision is the proportion of relevant documents in the\\nlist of all returned documents. The recall is the ratio of the relevant documents returned\\nby the search engine to the total number of the relevant documents that could have been\\nreturned.\\n\\nIn the case of the spam detection problem, we want to have high precision (we want to avoid\\nmaking mistakes by detecting that a legitimate message is spam) and we are ready to tolerate\\nlower recall (we tolerate some spam messages in our inbox).\\n\\nAlmost always, in practice, we have to choose between a high precision or a high recall. It’s\\nusually impossible to have both. We can achieve either of the two by various means:\\n\\n• by assigning a higher weighting to messages with spam (the SVM algorithm accepts\\n\\nweightings of classes as input);\\n\\n• by tuning hyperparameters such that the precision or recall on the validation set are\\n\\nmaximized;\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n15\\n\\n\\x0c• by varying the decision threshold for algorithms that return probabilities of classes;\\nfor instance, if we use logistic regression or decision tree, to increase precision (at the\\ncost of a lower recall), we can decide that the prediction will be positive only if the\\nprobability returned by the model is higher than 0.9.\\n\\nEven if precision and recall are deﬁned for the binary classiﬁcation case, you can always use\\nit to assess a multiclass classiﬁcation model. To do that, ﬁrst select a class for which you\\nwant to assess these metrics. Then you consider all examples of the selected class as positives\\nand all examples of the remaining classes as negatives.\\n\\n5.6.3 Accuracy\\n\\nAccuracy is given by the number of correctly classiﬁed examples divided by the total number\\nof classiﬁed examples. In terms of the confusion matrix, it is given by:\\n\\nAccuracy =\\n\\nTP + TN\\nTP + TN + FP + FN\\n\\n.\\n\\n(5)\\n\\nAccuracy is a useful metric when errors in predicting all classes are equally important. In\\ncase of the spam/not spam, this may not be the case. For example, you would tolerate false\\npositives less than false negatives. A false positive in spam detection is the situation in which\\nyour friend sends you an email, but the model labels it as spam and doesn’t show you. On\\nthe other hand, the false negative is less of a problem: if your model doesn’t detect a small\\npercentage of spam messages, it’s not a big deal.\\n\\n5.6.4 Cost-Sensitive Accuracy\\n\\nFor dealing with the situation in which di\\x00erent classes have di\\x00erent importance, a useful\\nmetric is cost-sensitive accuracy. To compute a cost-sensitive accuracy, you ﬁrst assign a\\ncost (a positive number) to both types of mistakes: FP and FN. You then compute the counts\\nTP, TN, FP, FN as usual and multiply the counts for FP and FN by the corresponding cost\\nbefore calculating the accuracy using eq. 5.\\n\\n5.6.5 Area under the ROC Curve (AUC)\\n\\nThe ROC curve (ROC stands for “receiver operating characteristic,” the term comes from\\nradar engineering) is a commonly used method to assess the performance of classiﬁcation\\nmodels. ROC curves use a combination the true positive rate (the proportion of positive\\nexamples predicted correctly, deﬁned exactly as recall) and false positive rate (the proportion\\nof negative examples predicted incorrectly) to build up a summary picture of the classiﬁcation\\nperformance.\\n\\nThe true positive rate and the false positive rate are respectively deﬁned as,\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n16\\n\\n\\x0cTPR =\\n\\nTP\\n(TP + FN)\\n\\nand FPR =\\n\\nFP\\n(FP + TN)\\n\\n.\\n\\nROC curves can only be used to assess classiﬁers that return some conﬁdence score (or a\\nprobability) of prediction. For example, logistic regression, neural networks, and decision\\ntrees (and ensemble models based on decision trees) can be assessed using ROC curves.\\n\\nTo draw a ROC curve, we ﬁrst discretize the range of the conﬁdence score. If this range for\\nour model is [0, 1], then we can discretize it like this: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1].\\nThen, we use each discrete value as the prediction threshold and predict the labels of examples\\nin our dataset using our model and this threshold. For example, if we want to compute TPR\\nand FPR for the threshold equal to 0.7, we apply the model to each example, get the score,\\nand, if the score if higher than or equal to 0.7, we predict the positive class; otherwise, we\\npredict the negative class.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n17\\n\\n\\x0c1\\n\\ne\\nt\\na\\nr\\n\\ne\\nv\\ni\\nt\\ni\\ns\\no\\np\\ne\\nu\\nr\\nT\\n\\n0\\n\\n1\\n\\ne\\nt\\na\\nr\\n\\ne\\nv\\ni\\nt\\ni\\ns\\no\\np\\ne\\nu\\nr\\nT\\n\\nAUC\\xa0=\\xa00.4\\n\\nFalse\\xa0positive\\xa0rate\\n\\n1\\n\\n1\\n\\ne\\nt\\na\\nr\\n\\ne\\nv\\ni\\nt\\ni\\ns\\no\\np\\ne\\nu\\nr\\nT\\n\\n0\\n\\n1\\n\\ne\\nt\\na\\nr\\n\\ne\\nv\\ni\\nt\\ni\\ns\\no\\np\\ne\\nu\\nr\\nT\\n\\nAUC\\xa0=\\xa00.5\\n\\nFalse\\xa0positive\\xa0rate\\n\\n1\\n\\nAUC\\xa0=\\xa00.6\\n\\nAUC\\xa0=\\xa00.85\\n\\n0\\n\\nFalse\\xa0positive\\xa0rate\\n\\n1\\n\\n0\\n\\nFalse\\xa0positive\\xa0rate\\n\\n1\\n\\nFigure 3: Area under the ROC curve.\\n\\nIt’s easy to see that if the threshold is 0, all our\\nLook at the illustration in Figure 3.\\npredictions will be positive, so both TPR and FPR will be 1 (the upper right corner). On\\nthe other hand, if the threshold is 1, then no positive prediction will be made, both TPR\\nand FPR will be 0 which corresponds to the lower left corner.\\n\\nThe higher the area under the ROC curve (AUC), the better the classiﬁer. A classiﬁer\\nwith an AUC higher than 0.5 is better than a random classiﬁer. If AUC is lower than 0.5,\\nthen something is wrong with your model. A perfect classiﬁer would have an AUC of 1.\\nUsually, if our model behaves well, we obtain a good classiﬁer by selecting the value of the\\nthreshold that gives TPR close to 1 while keeping FPR near 0.\\n\\nROC curves are widely used because they are relatively simple to understand, they capture\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n18\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\x0cmore than one aspect of the classiﬁcation (by taking both false positives and false negatives\\ninto account) and allow visually and with low e\\x00ort comparing the performance of di\\x00erent\\nmodels.\\n\\n5.7 Hyperparameter Tuning\\n\\nWhen I presented learning algorithms, I mentioned that you as a data analyst have to select\\ngood values for the algorithm’s hyperparameters, such as ‘ and d for ID3, C for SVM, or –\\nfor gradient descent. But what does that exactly mean? Which value is the best and how to\\nﬁnd it? In this section, I answer these essential questions.\\n\\nAs you already know, hyperparameters aren’t optimized by the learning algorithm itself. The\\ndata analyst has to “tune” hyperparameters by experimentally ﬁnding the best combination\\nof values, one per hyperparameter.\\n\\nOne typical way to do that, when you have enough data to have a decent validation set (in\\nwhich each class is represented by at least a couple of dozen examples) and the number of\\nhyperparameters and their range is not too large is to use grid search.\\n\\nGrid search is the most simple hyperparameter tuning strategy. Let’s say you train an SVM\\nand you have two hyperparameters to tune: the penalty parameter C (a positive real number)\\nand the kernel (either “linear” or “rbf”).\\n\\nIf it’s the ﬁrst time you are working with this dataset, you don’t know what is the possible\\nrange of values for C. The most common trick is to use a logarithmic scale. For example, for\\nC you can try the following values: [0.001, 0.01, 0.1, 1.0, 10, 100, 1000]. In this case you have\\n14 combinations of hyperparameters to try: [(0.001, “linear”), (0.01, “linear”), (0.1, “linear”),\\n(1.0, “linear”), (10, “linear”), (100, “linear”), (1000, “linear”), (0.001, “rbf”), (0.01, “rbf”),\\n(0.1, “rbf”), (1.0, “rbf”), (10, “rbf”), (100, “rbf”), (1000, “rbf”)].\\n\\nYou use the training set and train 14 models, one for each combination of hyperparameters.\\nThen you assess the performance of each model on the validation data using one of the\\nmetrics we discussed in the previous section (or some other metric that matters to you).\\nFinally, you keep the model that performs the best according to the metric.\\n\\nOnce you have found the best pair of hyperparameters, you can try to explore the values\\nclose to the best ones in some region around them. Sometimes, this can result in an even\\nbetter model.\\n\\nFinally, you assess the selected model using the test set.\\n\\nAs you could notice, trying all combinations of hyperparameters, especially if there are\\nmore than a couple of them, could be time-consuming, especially for large datasets. There\\nare more e\\x00cient techniques, such as random search and Bayesian hyperparameter\\noptimization.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n19\\n\\n\\x0cRandom search di\\x00ers from grid search in that you no longer provide a discrete set of\\nvalues to explore for each hyperparameter; instead, you provide a statistical distribution for\\neach hyperparameter from which values are randomly sampled and set the total number of\\ncombinations you want to try.\\n\\nBayesian techniques di\\x00er from random or grid search in that they use past evaluation results\\nto choose the next values to evaluate. The idea is to limit expensive optimization of the\\nobjective function by choosing the next hyperparameter values based on those that have done\\nwell in the past.\\n\\nThere are also gradient-based techniques, evolutionary optimiza-\\ntion techniques, and other algorithmic hyperparameter tuning tech-\\nniques. Most modern machine learning libraries implement one or more\\nsuch techniques. There are also hyperparameter tuning libraries that can\\nhelp you to tune hyperparameters of virtually any learning algorithm,\\nincluding ones you programmed yourself.\\n\\n5.7.1 Cross-Validation\\n\\nWhen you don’t have a decent validation set to tune your hyperparameters on, the common\\ntechnique that can help you is called cross-validation. When you have few training examples,\\nit could be prohibitive to have both validation and test set. You would prefer to use more\\ndata to train the model. In such a case, you only split your data into a training and a test\\nset. Then you use cross-validation to on the training set to simulate a validation set.\\n\\nCross-validation works like follows. First, you ﬁx the values of the hyperparameters you want\\nto evaluate. Then you split your training set into several subsets of the same size. Each\\nsubset is called a fold. Typically, ﬁve-fold cross-validation is used in practice. With ﬁve-fold\\n. Each\\ncross-validation, you randomly split your training data into ﬁve folds:\\nFk, k = 1, . . . , 5 contains 20% of your training data. Then you train ﬁve models as follows.\\nTo train the ﬁrst model, f1, you use all examples from folds F2, F3, F4, and F5 as the training\\nset and the examples from F1 as the validation set. To train the second model, f2, you\\nuse the examples from folds F1, F 3, F4, and F5 to train and the examples from F2 as the\\nvalidation set. You continue building models iteratively like this and compute the value of\\nthe metric of interest on each validation set, from F1 to F5. Then you average the ﬁve values\\nof the metric to get the ﬁnal value.\\n\\nF1, F2, . . . , F5}\\n{\\n\\nYou can use grid search with cross-validation to ﬁnd the best values of hyperparameters for\\nyour model. Once you have found these values, you use the entire training set to build the\\nmodel with these best values of hyperparameters you have found via cross-validation. Finally,\\nyou assess the model using the test set.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n20\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c6 Neural Networks and Deep Learning\\n\\nFirst of all, you already know what a neural network is, and you already know how to build\\nsuch a model. Yes, it’s logistic regression! As a matter of fact, the logistic regression model,\\nor rather its generalization for multiclass classiﬁcation, called the softmax regression model,\\nis a standard unit in a neural network.\\n\\n6.1 Neural Networks\\n\\nIf you understood linear regression, logistic regression, and gradient descent, understanding\\nneural networks would not be a problem.\\n\\nA neural network (NN), just like a regression or an SVM model, is a mathematical function:\\n\\ny = fN N (x).\\n\\nThe function fN N has a particular form: it’s a nested function. You have probably already\\nheard of neural network layers. So, for a 3-layer neural network that returns a scalar, fN N\\nlooks like this:\\n\\ny = fN N (x) = f3(f2(f1(x))).\\n\\nIn the above equation, f2, f3 are vector functions of the following form:\\n\\nfl(z)\\n\\ndef\\n= gl(Wlz + bl),\\n\\n(1)\\n\\nwhere l is called the layer index and can span from 1 to any number of layers. The function\\ngl is called an activation function. It is a ﬁxed, usually nonlinear function chosen by the\\ndata analyst before the learning is started. The parameters Wl (a matrix) and bl (a vector)\\nfor each layer are learned using the familiar gradient descent by optimizing, depending on the\\ntask, a particular cost function (such as MSE). Compare eq. 1 with the equation for logistic\\nregression, where you replace gl by the sigmoid function, and you will not see any di\\x00erence.\\nThe function f1 is a scalar function for the regression task, but can also be a vector function\\ndepending on your problem.\\n\\nYou may probably wonder why a matrix Wl is used and not a vector wl. The reason is\\nthat gl is a vector function. Each row wl,u (u for unit) of the matrix Wl is a vector of\\nthe same dimensionality as z. Let al,u = wl,uz + bl,u. The output of fl(z) is a vector\\n[gl(al,1), gl(al,2), . . . , gl(al,sizel )], where gl is some scalar function1, and sizel is the number of\\nunits in layer l. To make it more concrete, let’s consider one architecture of neural networks\\ncalled multilayer perceptron and often referred to as a vanilla neural network.\\n\\n1A scalar function outputs a scalar, that is a simple number and not a vector.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0c6.1.1 Multilayer Perceptron Example\\n\\nWe have a closer look at one particular conﬁguration of neural networks called feed-forward\\nneural networks (FFNN), and more speciﬁcally the architecture called a multilayer\\nperceptron (MLP). As an illustration, we consider an MLP with three layers. Our network\\ntakes a two-dimensional feature vector as input and outputs a number. This FFNN can be a\\nregression or a classiﬁcation model, depending on the activation function used in the third,\\noutput layer.\\n\\nOur MLP is depicted in ﬁg. 1. The neural network is represented graphically as a connected\\ncombination of units logically organized into one or more layers. Each unit is represented by\\neither a circle or a rectangle. The inbound arrow represents an input of a unit and indicates\\nwhere this input came from. The outbound arrow indicates the output of a unit.\\n\\nThe output of each unit is the result of the mathematical operation written inside the circle\\nor a rectangle. Circle units don’t do anything with the input; they just send their input\\ndirectly to the output.\\n\\nThe following happens in each rectangle unit. Firstly, all inputs of the unit are joined together\\nto form an input vector. Then the unit applies a linear transformation to the input vector,\\nexactly like linear regression model does with its input feature vector. Finally, the unit\\napplies an activation function g to the result of the linear transformation and obtains the\\noutput value, a real number. In a vanilla FFNN, the output value of a unit of some layer\\nbecomes an input value of each of the units of the subsequent layer.\\n\\nIn ﬁg. 1, the activation function gl has one index: l, the index of the layer the unit belongs to.\\nUsually, all units of a layer use the same activation function, but it’s not strictly necessary.\\nEach layer can have a di\\x00erent number of units. Each unit has its own parameters wl,u\\nand bl,u, where u is the index of the unit, and l is the index of the layer. The vector yl\\nin each unit is deﬁned as [y(1)\\n≠\\n[x(1), . . . , x(D)].\\n\\n1\\n≠\\n1]. The vector x in the ﬁrst layer is deﬁned as\\n\\n1, y(4)\\n\\n1, y(2)\\n\\n1, y(3)\\n\\n≠\\n\\n≠\\n\\n≠\\n\\nl\\n\\nl\\n\\nl\\n\\nl\\n\\nAs you can see in ﬁg. 1, in multilayer perceptron all outputs of one layer are connected to\\neach input of the succeeding layer. This architecture is called fully-connected. A neural\\nnetwork can contain fully-connected layers. Those are the layers whose units receive as\\ninputs the outputs of each of the units of the previous layer.\\n\\n6.1.2 Feed-Forward Neural Network Architecture\\n\\nIf we want to solve a regression or a classiﬁcation problem discussed in previous chapters, the\\nlast (the rightmost) layer of a neural network usually contains only one unit. If the activation\\nfunction glast of the last unit is linear, then the neural network is a regression model. If the\\nglast is a logistic function, the neural network is a binary classiﬁcation model.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0c)\\n3\\nf\\n(\\n3\\n\\nr\\ne\\ny\\na\\nl\\n\\n)\\n2\\nf\\n(\\n2\\n\\nr\\ne\\ny\\na\\nl\\n\\n)\\n1\\nf\\n(\\n1\\n\\nr\\ne\\ny\\na\\nl\\n\\ny\\n\\n)\\n1\\n\\n,\\n\\n3\\nb\\n+\\n2\\ny\\n1\\n3\\n\\n,\\n\\nw\\n\\n(\\n3\\ng\\n←\\ny\\n\\n)\\n1\\n(\\n\\n2\\ny\\n\\n)\\n2\\n(\\n\\n2\\ny\\n\\n)\\n3\\n(\\n\\n2\\ny\\n\\n)\\n4\\n(\\n\\n2\\ny\\n\\n)\\n2\\n,\\n2\\nb\\n+\\n1\\ny\\n2\\n,\\n2\\n\\nw\\n\\n(\\n2\\ng\\n←\\n\\n)\\n3\\n,\\n2\\nb\\n+\\n1\\ny\\n3\\n,\\n2\\n\\nw\\n\\n(\\n2\\ng\\n←\\n\\n)\\n4\\n,\\n2\\nb\\n+\\n1\\ny\\n4\\n,\\n2\\n\\nw\\n\\n(\\n2\\ng\\n←\\n\\n)\\n1\\n,\\n2\\nb\\n+\\n1\\ny\\n1\\n,\\n2\\n\\nw\\n\\n(\\n2\\ng\\n←\\n\\n)\\n1\\n(\\n\\n2\\ny\\n\\n)\\n2\\n(\\n\\n2\\ny\\n\\n)\\n3\\n(\\n\\n2\\ny\\n\\n)\\n4\\n(\\n\\n2\\ny\\n\\n)\\n1\\n(\\n\\n1\\ny\\n\\n)\\n4\\n(\\n\\n1\\ny\\n\\n)\\n1\\n,\\n1\\nb\\n+\\nx\\n1\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n)\\n2\\n,\\n1\\nb\\n+\\nx\\n2\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n)\\n3\\n,\\n1\\nb\\n+\\nx\\n3\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n)\\n4\\n,\\n1\\nb\\n+\\nx\\n4\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n)\\n1\\n(\\n\\n1\\ny\\n\\n)\\n2\\n(\\n\\n1\\ny\\n\\n)\\n3\\n(\\n\\n1\\ny\\n\\n)\\n4\\n(\\n\\n1\\ny\\n\\n)\\n1\\n(\\nx\\n\\n)\\n2\\n(\\nx\\n\\n)\\n1\\n(\\nx\\n\\n)\\n2\\n(\\nx\\n\\ne\\nn\\no\\n\\nh\\nt\\ni\\nw\\nr\\ne\\ny\\na\\nl\\n\\nt\\nu\\np\\nt\\nu\\no\\n\\ne\\nn\\no\\n\\nd\\nn\\na\\n\\ns\\nt\\ni\\nn\\nu\\n\\nr\\nu\\no\\nf\\n\\nh\\nt\\ni\\nw\\ns\\nr\\ne\\ny\\na\\nl\\n\\no\\nw\\nt\\n\\n,\\nt\\nu\\np\\nn\\ni\\n\\nl\\na\\nn\\no\\ni\\ns\\nn\\ne\\nm\\ni\\nd\\n-\\no\\nw\\nt\\n\\nh\\nt\\ni\\nw\\nn\\no\\nr\\nt\\np\\ne\\nc\\nr\\ne\\np\\n\\nr\\ne\\ny\\na\\nl\\ni\\nt\\nl\\nu\\nm\\nA\\n\\n:\\n1\\n\\ne\\nr\\nu\\ng\\ni\\nF\\n\\n.\\nt\\ni\\nn\\nu\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\x0cThe data analyst is free to choose any mathematical function as gl,u, assuming it’s di\\x00eren-\\ntiable2. The latter property is essential for gradient descent, which is used to ﬁnd the values\\nof the parameters wl,u and bl,u for all l and u. The primary purpose of having nonlinear\\ncomponents in the function fN N is to allow the neural network to approximate nonlinear\\nfunctions. Without nonlinearities, fN N would be linear, no matter how many layers it has.\\nThe reason is that Wlz + bl is a linear function and a linear function of a linear function is\\nalso a linear function.\\n\\nPopular choices of activation functions are the logistic function, already known to you, as well\\nas TanH and ReLU. The former is the hyperbolic tangent function, similar to the logistic\\nfunction but ranging from\\n1 to 1 (without reaching them). The latter is the rectiﬁed linear\\nunit function, which equals to zero when its input z is negative and to z otherwise:\\n\\n≠\\n\\ntanh(z) =\\n\\nz\\nez\\ne≠\\nz ,\\n≠\\nez + e≠\\n\\nrelu(z) =\\n\\n0\\nz\\nI\\n\\nif z < 0\\notherwise\\n\\n.\\n\\nAs I said above, Wl in the expression Wlz + bl, is a matrix, while bl is a vector. That looks\\ndi\\x00erent from linear regression’s wz + b. In matrix Wl, each row u corresponds to a vector of\\nparameters wl,u. The dimensionality of the vector wl,u equals to the number of units in the\\ndef\\nlayer l\\n= [wl,1z, wl,2z, . . . , wl,sizel z]. Then\\nthe sum al + bl gives a sizel-dimensional vector cl. Finally, the function gl(cl) produces the\\nvector yl\\n\\n1. The operation Wlz results in a vector al\\n\\n] as output.\\n\\n, . . . , y(sizel)\\nl\\n\\ndef\\n= [y(1)\\nl\\n\\n, y(2)\\nl\\n\\n≠\\n\\n6.2 Deep Learning\\n\\nDeep learning refers to training neural networks with more than two non-output layers. In the\\npast, it became more di\\x00cult to train such networks as the number of layers grew. The two\\nbiggest challenges were referred to as the problems of exploding gradient and vanishing\\ngradient as gradient descent was used to train the network parameters.\\n\\nWhile the problem of exploding gradient was easier to deal with by applying simple techniques\\nlike gradient clipping and L1 or L2 regularization, the problem of vanishing gradient\\nremained intractable for decades.\\n\\nWhat is vanishing gradient and why does it arise? To update the values of parameters in\\nneural networks the algorithm called backpropagation is typically used. Backpropagation\\nis an e\\x00cient algorithm for computing gradients on neural networks using the chain rule. In\\nChapter 4, we have already seen how the chain rule is used to calculate partial derivatives of\\n\\n2The function has to be di\\x00erentiable across its whole domain or in the majority of the points of its\\n\\ndomain. For example, ReLU is not di\\x00erentiable at 0.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0ca complex function. During gradient descent, the neural network’s parameters receive an\\nupdate proportional to the partial derivative of the cost function with respect to the current\\nparameter in each iteration of training. The problem is that in some cases, the gradient will\\nbe vanishingly small, e\\x00ectively preventing some parameters from changing their value. In\\nthe worst case, this may completely stop the neural network from further training.\\n\\nTraditional activation functions, such as the hyperbolic tangent function I mentioned above,\\nhave gradients in the range (0, 1), and backpropagation computes gradients by the chain rule.\\nThat has the e\\x00ect of multiplying n of these small numbers to compute gradients of the earlier\\n(leftmost) layers in an n-layer network, meaning that the gradient decreases exponentially\\nwith n. That results in the e\\x00ect that the earlier layers train very slowly, if at all.\\n\\nHowever, the modern implementations of neural network learning algorithms allow you to\\ne\\x00ectively train very deep neural networks (up to hundreds of layers). The ReLU activation\\nfunction su\\x00ers much less from the problem of vanishing gradient. Also, long short-term\\nmemory (LSTM) networks, which we consider below, as well as such techniques as skip\\nconnections used in residual neural networks allow you to train even deeper neural\\nnetworks, with thousands of layers.\\n\\nTherefore, today, since the problems of vanishing and exploding gradient are mostly solved\\n(or their e\\x00ect diminished) to a great extent, the term “deep learning” refers to training\\nneural networks using the modern algorithmic and mathematical toolkit independently of\\nhow deep the neural network is. In practice, many business problems can be solved with\\nneural networks having 2-3 layers between the input and output layers. The layers that are\\nneither input nor output are often called hidden layers.\\n\\n6.2.1 Convolutional Neural Network\\n\\nYou may have noticed that the number of parameters an MLP can have grows very fast as you\\nmake your network bigger. More speciﬁcally, as you add one layer, you add sizel(sizel\\n1 + 1)\\nparameters (our matrix Wl plus the vector bl). That means that if you add another 1000-unit\\nlayer to an existing neural network, then you add more than 1 million additional parameters\\nto your model. Optimizing such big models is a very computationally intensive problem.\\n\\n≠\\n\\nWhen our training examples are images, the input is very high-dimensional. If you want\\nto learn to classify images using an MLP, the optimization problem is likely to become\\nintractable.\\n\\nA convolutional neural network (CNN) is a special kind of FFNN that signiﬁcantly\\nreduces the number of parameters in a deep neural network with many units without losing\\ntoo much in the quality of the model. CNNs have found applications in image and text\\nprocessing where they beat many previously established benchmarks.\\n\\nBecause CNNs were invented with image processing in mind, I explain them on the image\\nclassiﬁcation example.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cYou may have noticed that in images, pixels that are close to one another usually represent\\nthe same type of information: sky, water, leaves, fur, bricks, and so on. The exception from\\nthe rule are the edges: the parts of an image where two di\\x00erent objects “touch” one another.\\n\\nSo, if we can train the neural network to recognize regions of the same information as well\\nas the edges, then this knowledge would allow the neural network to predict the object\\nrepresented in the image. For example, if the neural network detected multiple skin regions\\nand edges that look like parts of an oval with skin-like tone on the inside and bluish tone on\\nthe outside, then it is very likely that there’s a face on the sky background. If our goal is to\\ndetect people on pictures, the neural network will most likely succeed in predicting a person\\nin this picture.\\n\\nHaving in mind that the most important information in the image is local, we can split the\\nimage into square patches using a moving window approach3. We can then train multiple\\nsmaller regression models at once, each small regression model receiving a square patch as\\ninput. The goal of each small regression model is to learn to detect a speciﬁc kind of pattern\\nin the input patch. For example, one small regression model will learn to detect the sky;\\nanother one will detect the grass, the third one will detect edges of a building, and so on.\\n\\nIn CNNs, a small regression model looks like the one in ﬁg. 1, but it only has the layer 1 and\\ndoesn’t have layers 2 and 3. To detect some pattern, a small regression model has to learn\\nthe parameters of a matrix F (for “ﬁlter”) of size p\\np, where p is the size of a patch. Let’s\\nassume, for simplicity, that the input image is back and white, with 1 representing black and\\n0 representing white pixels. Assume also that our patches are 3 by 3 pixels (p = 3). Some\\npatch could then look like the following matrix P (for “patch”):\\n\\n◊\\n\\nP =\\n\\nS\\n\\n0\\n1\\n0\\n\\n1\\n1\\n1\\n\\n0\\n1\\n0\\n\\n.\\n\\nT\\n\\nU\\n\\nV\\n\\nThe above patch represents a pattern that looks like a cross. The small regression model that\\nwill detect such patterns (and only them) would need to learn a 3 by 3 parameter matrix F\\nwhere parameters at positions corresponding to the 1s in the input patch would be positive\\nnumbers, while the parameters in positions corresponding to 0s would be close to zero. If\\nwe calculate the dot-product between matrices P and F and then sum all values from the\\nresulting vector, the value we obtain is higher the more similar F is to P . For instance,\\nassume that F looks like this:\\n\\nF =\\n\\nS\\n\\nU\\n\\n2\\n4\\n3\\n\\n3\\n1\\n0\\n\\n.\\n\\nT\\n\\nV\\n\\n1, 3\\n\\n0\\n2\\n0\\n\\n·\\n\\nThen,\\n\\nP\\n\\nF = [0\\n\\n0 + 2\\n\\n1 + 0\\n\\n0, 2\\n\\n1 + 4\\n\\n1 + 3\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n0 + 1\\n\\n1 + 0\\n\\n1] = [2, 9, 1].\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n3Consider this as if you looked at a dollar bill in a microscope. To see the whole bill you have to gradually\\nmove your bill from left to right and from top to bottom. At each moment in time, you see only a part of the\\nbill of ﬁxed dimensions. This approach is called moving window.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cThen the sum of all elements of the above vector is 2 + 9 + 1 = 12. This operation — the dot\\nproduct between a patch and a ﬁlter and then summing the values — is called convolution.\\n\\nIf our input patch P had a di\\x00erent patten, for example, that of a letter T,\\n\\n1\\n0\\n0\\n\\n1\\n1\\n1\\n\\n1\\n0\\n0\\n\\n,\\n\\nT\\n\\nV\\n\\nP =\\n\\nS\\n\\nU\\n\\nthen the convolution would give a lower result: 0 + 9 + 0 = 9. So, you can see the more\\nthe patch “looks” like the ﬁlter, the higher the value of the convolution operation is. For\\nconvenience, there’s also a bias parameter b associated with each ﬁlter F which is added to\\nthe result of a convolution before applying the nonlinearity.\\n\\nOne layer of a CNN consists of multiple convolution ﬁlters (each with its own bias parameter),\\njust like one layer in a vanilla FFNN consists of multiple units. Each ﬁlter of the ﬁrst\\n(leftmost) layer slides — or convolves — across the input image, left to right, top to bottom,\\nand convolution is computed at each iteration.\\n\\nImage\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\nConv\\xa01\\n\\nConv\\xa02\\n\\nConv\\xa03\\n\\nFilter \\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\nBias\\n\\n1\\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\n1\\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\n1\\n\\nOutput\\xa0before\\xa0nonlinearity \\n\\n4 \\n\\n4 \\n\\n-1 \\n\\nConv\\xa04\\n\\nConv\\xa05\\n\\n4 \\n\\n-1 \\n\\n7 \\n\\nConv\\xa06\\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\n1\\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\n1\\n\\n-1 \\n\\n2 \\n\\n4 \\n\\n-2 \\n\\n1\\n\\n-1 \\n\\n7 \\n\\n4 \\n\\n2 \\n\\n4 \\n\\n2 \\n\\n-1 \\n\\n7 \\n\\n7 \\n\\n4 \\n\\n2 \\n\\n-1 \\n\\n7 \\n\\n7 \\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\n1\\n\\n0 \\n\\n0 \\n\\n1\\n\\nFigure 2: A ﬁlter convolving across an image.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cAn illustration of the process is given in ﬁg. 2 where 6 steps of a ﬁlter convolving across an\\nimage are shown.\\n\\nThe numbers in the ﬁlter matrix, for each ﬁlter F in each layer, as well as the value of\\nthe bias term b, are found by the gradient descent with backpropagation, based on data by\\nminimizing the cost function.\\n\\nA nonlinearity is applied to the sum of the convolution and the bias term. Typically, the\\nReLU activation function is used in all hidden layers. The activation function of the output\\nlayer depends on the task.\\n\\nSince we can have sizel ﬁlters in each layer l, the output of the convolution layer l would\\nconsist of sizel matrices, one for each ﬁlter.\\n\\nIf the CNN has one convolution layer following another convolution layer, then the subsequent\\nlayer l + 1 treats the output of the preceding layer l as a collection of sizel image matrices.\\nSuch a collection is called a volume. Each ﬁlter of layer l + 1 convolves the whole volume. The\\nconvolution of a patch of a volume is simply the sum of convolutions of the corresponding\\npatches of individual matrices the volume consists of.\\n\\nVolume\\n\\n3\\n\\n4\\n\\n2\\n\\n1 \\n\\n1 \\n\\n2\\n\\n-2 \\n\\n0\\n\\n1 \\n\\n1 \\n\\n-2\\n\\n-1 \\n\\n4\\n\\n5 \\n\\n0 \\n\\n2\\n\\n2\\n\\n-3\\n\\n-1\\n\\n1 \\n\\n-1 \\n\\n1 \\n\\n2\\n\\n0\\n\\n3 \\n\\n0\\n\\n-3 \\n\\n-2\\n\\n1 \\n\\n2 \\n\\n1\\n\\n2\\n\\n1\\n\\n2 \\n\\n-5\\n\\n1 \\n\\n-3\\n\\n-1 \\n\\n1 \\n\\n-1 \\n\\n-1\\n\\n0\\n\\n4 \\n\\n1 \\n\\nOutput\\xa0before\\xa0nonlinearity \\n\\n-3 \\n\\n2\\n\\n0 \\n\\n3 \\n\\n-2\\n\\nFilter \\n\\n-2 \\n\\n3 \\n\\n5 \\n\\n-1 \\n\\nBias\\n\\n-2\\n\\nFigure 3: Convolution of a volume consisting of three matrices.\\n\\nAn example of a convolution of a patch of a volume consisting of three matrices is shown in\\nﬁg. 3. The value of the convolution,\\n1) +\\n1) + 5\\n2\\n(\\n≠\\n\\n3, was obtained as (\\n1) + 5\\n\\n3 + 3\\n·\\n(\\n1\\n≠\\n≠\\n\\n1 + 5\\n1)) + (\\n\\n≠\\n1) + (\\n\\n2\\n·\\n2 +\\n\\n≠\\n2).\\n\\n2 + 3\\n\\n1 + 3\\n\\n1\\n≠\\n\\n2\\n≠\\n\\n(\\n≠\\n\\n(\\n≠\\n\\n(\\n≠\\n\\n3) +\\n\\n≠\\n·\\n\\n4 +\\n\\n≠\\n\\n1\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cIn computer vision, CNNs often get volumes as input, since an image is usually represented\\nby three channels: R, G, and B, each channel being a monochrome picture.\\n\\nBy now, you should have a good high-level understanding of the CNN\\narchitecture. We didn’t discuss some essential features of CNNs though,\\nsuch as strides, padding, and pooling. Strides and padding are two\\nimportant hyperparameters of the convolution ﬁlter and the sliding\\nwindow, while pooling is a technique that works very well in practice\\nby reducing the number of parameters of a CNN even more.\\n\\n6.2.2 Recurrent Neural Network\\n\\nRecurrent neural networks (RNNs) are used to label, classify, or generate sequences. A\\nsequence is a matrix, each row of which is a feature vector and the order of rows matters.\\nLabeling a sequence means predicting a class to each feature vector in a sequence. Classifying\\na sequence means predicting a class for the entire sequence. Generating a sequence means\\nto output another sequence (of a possibly di\\x00erent length) somehow relevant to the input\\nsequence.\\n\\nRNNs are often used in text processing because sentences and texts are naturally sequences\\nof either words/punctuation marks or sequences of characters. For the same reason, recurrent\\nneural networks are also used in speech processing.\\n\\nA recurrent neural network is not feed-forward, because it contains loops. The idea is that\\neach unit u of recurrent layer l has a real-valued state hl,u. The state can be seen as the\\nmemory of the unit. In RNN, each unit u in each recurrent layer l receives two inputs: a\\nvector of outputs from the previous layer l\\n1 and the vector of states from this same layer l\\nfrom the previous time step.\\n\\n≠\\n\\nTo illustrate the idea, let’s consider the ﬁrst and the second recurrent layers of an RNN. The\\nﬁrst (leftmost) layer receives a feature vector as input. The second layer receives the output\\nof the ﬁrst layer as input.\\n\\nThis situation is schematically depicted in ﬁg. 4. As I said above, each training example is\\na matrix in which each row is a feature vector. For simplicity, let’s illustrate this matrix\\nas a sequence of vectors X = [x1, x2, . . . , xt\\n1, xt, xt+1, . . . , xlengthX], where lengthX is the\\n≠\\nlength of the input sequence. If our input example X is a text sentence, then feature vector\\nxt for each t = 1, . . . , lengthX represents a word in the sentence at position t.\\n\\nAs depicted in ﬁg. 4, in an RNN, the input example is “read” by the neural network one\\nfeature vector at a timestep. The index t denotes a timestep. To update the state ht\\nl,u at each\\ntimestep t in each unit u of each layer l we ﬁrst calculate a linear combination of the input\\nfeature vector with the state vector ht\\n1.\\nThe linear combination of two vectors is calculated using two parameter vectors wl,u, ul,u\\n\\nl,u of this same layer from the previous timestep, t\\n≠\\n\\n≠\\n\\n1\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0c)\\n2\\nc\\n+\\n\\nt\\n\\n2\\nh\\n2\\nV\\n\\n(\\n2\\ng\\n←\\n\\nt\\n\\n2\\ny\\n\\n)\\n1\\nc\\n+\\n\\nt\\n\\n1\\nh\\n1\\nV\\n\\n(\\n2\\ng\\n\\n←\\n\\nt\\n\\n1\\ny\\n\\nt\\n\\n]\\n2\\n,\\n2\\nh\\n,\\n1\\n,\\n2\\nh\\n[\\n←\\n\\nt\\n\\nt\\n\\n2\\nh\\n\\n,\\n\\nt\\n\\n]\\n2\\n1\\nh\\n,\\n1\\n1\\nh\\n[\\n\\nt\\n\\n,\\n\\n←\\n\\nt\\n\\n1\\nh\\n\\nt\\n\\n2\\ny\\n\\n2\\n\\nr\\ne\\ny\\na\\nl\\n\\nt\\n\\n1\\ny\\n\\n1\\n\\nr\\ne\\ny\\na\\nl\\n\\n1\\n,\\n2\\n\\nt\\n\\nh\\n\\n)\\n1\\n,\\n\\n2\\nb\\n+\\n\\n2\\n\\n,\\n\\nt\\n\\n]\\n2\\n1\\nh\\n,\\n1\\n1\\nh\\n[\\n\\nt\\n\\n,\\n\\n←\\n\\nt\\n\\n1\\nh\\n\\n1\\n,\\n1\\n\\nt\\n\\nh\\n\\n]\\n\\nt\\n,\\n)\\n2\\n(\\nx\\n,\\nt\\n,\\n)\\n1\\n(\\nx\\n[\\n\\n←\\nx\\n\\nt\\n\\nt\\n,\\n)\\n1\\n(\\nx\\n\\nt\\n,\\n)\\n2\\n(\\nx\\n\\n1\\n\\xad\\nt\\n\\n,\\n\\nh\\n1\\n2\\nu\\n+\\n\\nt\\n\\n1\\nh\\n1\\n2\\n\\n,\\n\\nw\\n\\n(\\n1\\ng\\n\\n←\\n\\n,\\n\\n1\\n2\\nh\\n\\nt\\n\\n1\\n,\\n2\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n2\\n,\\n2\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n)\\n1\\n,\\n1\\nb\\n+\\n\\n1\\n\\n1\\n\\xad\\nt\\n\\nh\\n1\\n,\\n1\\nu\\n+\\n\\nt\\n\\nx\\n1\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n1\\n,\\n1\\nh\\n\\nt\\n\\n1\\n\\n,\\n1\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n2\\n,\\nl\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n.\\ns\\nt\\ni\\nn\\nu\\n\\no\\nw\\nt\\n\\ns\\na\\nh\\n\\nr\\ne\\ny\\na\\nl\\n\\nh\\nc\\na\\ne\\n\\n;\\nl\\na\\nn\\no\\ni\\ns\\nn\\ne\\nm\\nd\\n-\\no\\nw\\nt\\n\\ni\\n\\ns\\ni\\n\\nr\\no\\nt\\nc\\ne\\nv\\n\\ne\\nr\\nu\\nt\\na\\ne\\nf\\n\\nt\\nu\\np\\nn\\n\\ni\\n\\ne\\nh\\nT\\n\\n.\\n\\nN\\nN\\nR\\nn\\na\\n\\nf\\no\\n\\ns\\nr\\ne\\ny\\na\\nl\\n\\no\\nw\\nt\\n\\nt\\ns\\nr\\nﬁ\\n\\ne\\nh\\nT\\n\\n:\\n4\\n\\ne\\nr\\nu\\ng\\ni\\nF\\n\\n2\\n,\\n2\\n\\nt\\n\\nh\\n\\n)\\n2\\n,\\n\\n2\\nb\\n+\\n\\n,\\n\\nt\\n\\n]\\n2\\n1\\nh\\n,\\n1\\n1\\nh\\n[\\n\\n,\\n\\nt\\n\\n←\\n\\nt\\n\\n1\\nh\\n\\n2\\n,\\n1\\n\\nt\\n\\nh\\n\\n]\\n\\nt\\n,\\n)\\n2\\n(\\nx\\n,\\nt\\n,\\n)\\n1\\n(\\nx\\n[\\n\\n←\\nx\\n\\nt\\n\\nt\\n,\\n)\\n1\\n(\\nx\\n\\nt\\n,\\n)\\n2\\n(\\nx\\n\\n2\\n\\n1\\n\\xad\\nt\\n\\n,\\n\\nh\\n2\\n2\\nu\\n+\\n\\nt\\n\\n1\\nh\\n2\\n2\\n\\n,\\n\\nw\\n\\n(\\n1\\ng\\n\\n←\\n\\n,\\n\\n1\\n2\\nh\\n\\nt\\n\\n1\\n,\\n2\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n2\\n,\\n2\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n)\\n2\\n,\\n1\\nb\\n+\\n\\n1\\n\\n1\\n\\xad\\nt\\n\\nh\\n2\\n,\\n1\\nu\\n+\\n\\nt\\n\\nx\\n2\\n,\\n1\\n\\nw\\n\\n(\\n1\\ng\\n←\\n\\n2\\n,\\n1\\nh\\n\\nt\\n\\n1\\n\\n2\\n\\n,\\n1\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\n,\\n1\\n\\n1\\n\\xad\\nt\\n\\nh\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\x0cand a parameter bl,u. The value of ht\\nl,u is then obtained by applying an activation function\\ng1 to the result of the linear combination. A typical choice for function g1 is tanh. The\\noutput yt\\nl is typically a vector calculated for the whole layer l at once. To obtain yt\\nl , we use\\nan activation function g2 that takes a vector as input and returns a di\\x00erent vector of the\\nsame dimensionality. The function g2 is applied to a linear combination of the state vector\\nvalues ht\\nl,u calculated using a parameter matrix Vl and a parameter vector cl,u. A typical\\nchoice for g2 is the softmax function:\\n\\n‡(z)\\n\\n= [‡(1), . . . ,‡ (D)], where ‡(j) def\\ndef\\n=\\n\\nz(j)\\n\\nexp\\nD\\nk=1 exp\\n\\n!\\n\\nz(k)\\n\"\\n\\n.\\n\\nThe softmax function is a generalization of the sigmoid function to multidimensional data. It\\nhas the property that\\n\\nq\\nD\\nj=1 ‡(j) = 1 and ‡(j) > 0 for all j.\\n\\n!\\n\\n\"\\n\\nq\\n\\nThe dimensionality of Vl is chosen by the data analyst such that multiplication of matrix Vl\\nby the vector ht\\nl results in a vector of the same dimensionality as that of the vector cl. This\\nchoice depends on the dimensionality for the output label y in your training data. (Until\\nnow we only saw one-dimensional labels, but we will see in the future chapters that labels\\ncan be multidimensional as well.)\\n\\nThe values of wl,u, ul,u, bl,u, Vl,u, and cl,u are computed from the training data using gradient\\ndescent with backpropagation. To train RNN models, a special version of backpropagation is\\nused called backpropagation through time.\\n\\nBoth tanh and softmax su\\x00er from the vanishing gradient problem. Even if our RNN has just\\none or two recurrent layers, because of the sequential nature of the input, backpropagation\\nhas to “unfold” the network over time. From the point of view of the gradient calculation, in\\npractice this means that the longer is the input sequence, the deeper is the unfolded network.\\n\\nAnother problem RNNs have is that of handling long-term dependencies. As the length of\\nthe input sequence grows, the feature vectors from the beginning of the sequence tend to\\nbe “forgotten,” because the state of each unit, which serves as network’s memory, becomes\\nsigniﬁcantly a\\x00ected by the feature vectors read more recently. Therefore, in text or speech\\nprocessing, the cause-e\\x00ect link between distant words in a long sentence can be lost.\\n\\nThe most e\\x00ective recurrent neural network models used in practice are gated RNNs. These\\ninclude the long short-term memory (LSTM) networks and networks based on the gated\\nrecurrent unit (GRU).\\n\\nThe beauty of using gated units in RNNs is that such networks can store information in their\\nunits for future use, much like bits in a computer’s memory. The di\\x00erence with the real\\nmemory is that reading, writing, and erasure of information stored in each unit is controlled\\nby activation functions that take values in the range (0, 1). The trained neural network can\\n“read” the input sequence of feature vectors and decide at some early time step t to keep\\nspeciﬁc information about the feature vectors. That information about the earlier feature\\nvectors can later be used by the model to process the feature vectors from near the end of\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0cthe input sequence. For example, if the input text starts with the word she, a language\\nprocessing RNN model could decide to store the information about the gender to interpret\\ncorrectly the word her seen later in the sentence.\\n\\nUnits make decisions about what information to store, and when to allow reads, writes, and\\nerasures. These decisions are learned from data and implemented through the concept of\\ngates. There are several architectures of gated units. The simplest one (working well in\\npractice) is called the minimal gated GRU and is composed of a memory cell, and a forget\\ngate.\\n\\nLet’s look at the math of a GRU unit on an example of the ﬁrst layer of the RNN (the one\\nthat takes the sequence of feature vectors as input). A minimal gated GRU unit u in layer\\nl takes two inputs: the vector of the memory cell values from all units in the same layer\\nfrom the previous timestep, ht\\n, and a feature vector xt. It then uses these two vectors like\\nl\\nfollows (all operations in the below sequence are executed in the unit one after another):\\n\\n≠\\n\\n1\\n\\n˜ht\\nl,u Ω\\n\\x00t\\nl,u Ω\\nht\\nl,u Ω\\nht\\nl Ω\\nyt\\nl Ω\\n\\n1\\n\\ng1(wl,uxt + ul,uht\\nl + bl,u),\\n≠\\ng2(ml,uxt + ol,uht\\n1 + al,u),\\n≠\\n˜ht\\nl,u)ht\\n\\x00t\\n\\x00t\\n,\\nl + (1\\n≠\\nl,u\\nl\\nl,1, . . . , ht\\n[ht\\ng3(Vlht\\n\\n≠\\nl,sizel ]\\nl + cl,u),\\n\\n1\\n\\nwhere g1 is the tanh activation function, g2 is called the gate function and is implemented as\\nthe sigmoid function. The sigmoid function takes values in the range of (0, 1). If the gate\\n\\x00l,u is close to 0, then the memory cell keeps its value from the previous time step, ht\\n. On\\n≠\\nl\\nthe other hand, if the gate \\x00l,u is close to 1, the value of the memory cell is overwritten by a\\nnew value ˜ht\\nl,u (this happens in the third assignment from the top). Just like in standard\\nRNNs, g3 is usually softmax.\\n\\n1\\n\\nA gated unit takes an input and stores it for some time. This is equivalent to applying the\\nidentity function (f (x) = x) to the input. Because the derivative of the identity function is\\nconstant, when a network with gated units is trained with backpropagation through time,\\nthe gradient does not vanish.\\n\\nOther important extensions to RNNs include bi-directional RNNs,\\nRNNs with attention and sequence-to-sequence RNN models.\\nSequence-to-sequence RNNs, in particular, are frequently used to build\\nneural machine translation models and other models for text to text\\ntransformations. A generalization of RNNs is a recursive neural\\nnetwork model.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n14\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c7 Problems and Solutions\\n\\n7.1 Kernel Regression\\n\\nWe talked about linear regression, but what if our data doesn’t have the form of a straight\\nN\\nline? Polynomial regression could help. Let’s say we have a one-dimensional data\\ni=1.\\nWe could try to ﬁt a quadratic line y = w1xi + w2x2\\ni + b to our data. By deﬁning the mean\\nsquared error cost function, we could apply gradient descent and ﬁnd the values of parameters\\nw1, w2, and b that minimize this cost function. In one- or two-dimensional space, we can\\neasily see whether the function ﬁts the data. However, if our input is a D-dimensional feature\\nvector, with D > 3, ﬁnding the right polynomial would be hard.\\n\\n(xi, yi)\\n}\\n{\\n\\nKernel regression is a non-parametric method. That means that there are no parameters to\\nlearn. The model is based on the data itself (like in kNN). In its simplest form, in kernel\\nregression we look for a model like this:\\n\\nf (x) =\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nwiyi, where wi =\\n\\nN k( xi≠\\nx\\n)\\nb\\nN\\nk=1 k( xk≠\\n\\nb\\n\\nx\\n\\n.\\n\\n)\\n\\n(1)\\n\\nThe function k(\\n) is a kernel. It can have di\\x00erent forms, the most frequently used one is the\\n·\\nGaussian kernel:\\n\\nq\\n\\nk(z) =\\n\\n1\\nÔ2ﬁ\\n\\nexp\\n\\nz2\\n≠\\n2\\n\\n.\\n\\n4\\n\\n3\\n\\nGood ﬁt\\n\\nSlight overﬁt\\n\\nStrong overﬁt\\n\\nFigure 1: Example of kernel regression line with a Gaussian kernel for three values of b.\\n\\nThe value b is a hyperparameter that we tune using the validation set (by running the model\\nbuilt with a speciﬁc value of b on the validation set examples and calculating the mean\\nsquared error). You can see an illustration of the inﬂuence b has on the shape of the regression\\nline in ﬁg. 1.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cIf your inputs are multi-dimensional feature vectors, the terms xi ≠\\nhave to be replaced by Euclidean distance\\n\\nand\\n\\nx\\n\\nx\\n\\nx and xk ≠\\n\\nrespectively.\\n\\nxi ≠\\nÎ\\n\\nÎ\\n\\nxk ≠\\nÎ\\n\\nÎ\\n\\nx in eq. 1\\n\\n7.2 Multiclass Classiﬁcation\\n\\nIn multiclass classiﬁcation, the label can be one of the C classes: y\\n. Many\\n}\\nmachine learning algorithms are binary; SVM is an example. Some algorithms can naturally\\nbe extended to handle multiclass problems. ID3 and other decision tree learning algorithms\\ncan be simply changed like this:\\n\\n1, . . . , C\\n\\nœ{\\n\\nf S\\nID3\\n\\ndef\\n= Pr(yi = c\\nx) =\\n|\\n\\n1\\nS\\n|\\n\\ny,\\n\\ny\\n\\n| ÿ{\\n(x,y)\\nœ\\n|\\n\\nS,y=c\\n\\n}\\n\\nfor all c\\n\\n1, . . . , C\\n\\n.\\n}\\n\\nœ{\\n\\nLogistic regression can be naturally extended to multiclass learning problems by replacing\\nthe sigmoid function with the softmax function which we already saw in Chapter 6.\\n\\nThe kNN algorithm is also straightforward to extend to the multiclass case: when we ﬁnd\\nthe k closest examples for the input x and examine them, we return the class that we saw\\nthe most among the k examples.\\n\\nSVM cannot be naturally extended to multiclass problems. Some algorithms can be imple-\\nmented more e\\x00ciently in the binary case. What should you do if you have a multiclass\\nproblem but a binary classiﬁcation learning algorithm? One common strategy is called one\\nversus rest. The idea is to transform a multiclass problem into C binary classiﬁcation\\nproblems and build C binary classiﬁers. For example, if we have three classes, y\\n1, 2, 3\\n,\\n}\\nwe create copies of the original datasets and modify them. In the ﬁrst copy, we replace all\\nlabels not equal to 1 by 0. In the second copy, we replace all labels not equal to 2 by 0. In the\\nthird copy, we replace all labels not equal to 3 by 0. Now we have three binary classiﬁcation\\nproblems where we have to learn to distinguish between labels 1 and 0, 2 and 0, and between\\nlabels 3 and 0.\\n\\nœ{\\n\\nOnce we have the three models and we need to classify the new input feature vector x,\\nwe apply the three models to the input, and we get three predictions. We then pick the\\nprediction of a non-zero class which is the most certain. Remember that in logistic regression,\\nthe model returns not a label but a score (0, 1) that can be interpreted as the probability\\nthat the label is positive. We can also interpret this score as the certainty of prediction. In\\nSVM, the analog of certainty is the distance from the input x to the decision boundary. This\\ndistance is given by,\\n\\nd =\\n\\nwúx + bú\\nw\\n\\n.\\n\\nÎ\\n\\nÎ\\n\\nThe larger the distance, the more certain is the prediction. Most learning algorithm either\\ncan be naturally converted to a multiclass case, or they return a score we can use in the one\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cversus rest strategy.\\n\\n7.3 One-Class Classiﬁcation\\n\\nOne-class classiﬁcation, also known as unary classiﬁcation or class modeling, tries to\\nidentify objects of a speciﬁc class among all objects, by learning from a training set containing\\nonly the objects of that class. That is di\\x00erent from and more di\\x00cult than the traditional\\nclassiﬁcation problem, which tries to distinguish between two or more classes with the\\ntraining set containing objects from all classes. A typical one-class classiﬁcation problem is\\nthe classiﬁcation of the tra\\x00c in a secure network as normal. In this scenario, there are few,\\nif any, examples of the tra\\x00c under an attack or during an intrusion. However, the examples\\nof normal tra\\x00c are often in abundance. One-class classiﬁcation learning algorithms are used\\nfor outlier detection, anomaly detection, and novelty detection.\\n\\nThere are several one-class learning algorithms. The most widely used in practice are\\none-class Gaussian, one-class kmeans, one-class kNN, and one-class SVM.\\n\\nThe idea behind the one-class gaussian is that we model our data as if it came from a Gaussian\\ndistribution, more precisely multivariate normal distribution (MND). The probability density\\nfunction (pdf) for MND is given by the following equation:\\n\\nfµ,\\x00(x) =\\n\\nexp\\n\\n1\\n2 (x\\n\\n≠\\n\\n!\\n\\n1(x\\n\\n≠\\n(2ﬁ)D\\n\\nµ)T\\x00≠\\n\\x00\\n|\\n|\\n\\nµ)\\n\\n≠\\n\\n,\\n\\n\"\\n\\n\\uf8ff\\nwhere fµ,\\x00(x) returns the probability density corresponding to the input feature vector x.\\nProbability density can be interpreted as the likelihood that example x was drawn from the\\nprobability distribution we model as an MND. Values µ (a vector) and \\x00 (a matrix) are\\nthe parameters we have to learn. The maximum likelihood criterion (similarly to how\\nwe solved the logistic regression learning problem) is optimized to ﬁnd the optimal values\\n= det \\x00 is the determinant of the matrix \\x00; the notation aT\\nfor these two parameters.\\n1 is the inverse of the matrix \\x00.\\nmeans the transpose of the vector a, and \\x00≠\\n\\n\\x00\\n|\\n|\\n\\nIf the terms determinant, transpose, and inverse are new to you, don’t worry. These are\\nstandard operations on vector and matrices from the branch of mathematics called matrix\\ntheory. If you feel the need to know what they are, Wikipedia explains these concepts very\\nwell.\\n\\nIn practice, the numbers in the vector µ determine the place where the curve of our Gaussian\\ndistribution is centered, while the numbers in \\x00 determine the shape of the curve. For\\na training set consisting of two-dimensional feature vectors, an example of the one-class\\nGaussian model is given in ﬁg 2.\\n\\nOnce we have our model parametrized by µ and \\x00 learned from the data, we predict the\\nlikelihood of every input x by using fµ,\\x00(x). Only if the likelihood is above a certain\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cFigure 2: One-class classiﬁcation solved using the one-class gaussian method. Left: two-\\ndimensional feature vectors. Right: the MND curve that maximizes the likelihood of the\\nexamples on the left.\\n\\nthreshold, we predict that the example belongs to our class; otherwise, it is classiﬁed as the\\noutlier. The value of the threshold is found experimentally or using an “educated guess.”\\n\\nWhen the data has a more complex shape, a more advanced algorithm can use a combination\\nof several Gaussians (called a mixture of Gaussians). In this case, there are more parameters\\nto learn from data: one µ and one \\x00 for each Gaussian as well as the parameters that allow\\ncombining multiple Gaussians to form one pdf. In Chapter 9, we consider a mixture of\\nGaussians with an application to clustering.\\n\\nOne-class kmeans and one-class kNN are based on a similar principle as that of one-class\\nGaussian: build some model of the data and then deﬁne a threshold to decide whether our\\nnew feature vector looks similar to other examples according to the model. In the former,\\nall training examples are clustered using the kmeans clustering algorithm and, when a new\\nexample x is observed, the distance d(x) is calculated as the minimum distance between x\\nand the center of each cluster. If d(x) is less than a particular threshold, then x belongs to\\nthe class.\\n\\nOne-class SVM, depending on formulation, tries either 1) to separate all\\ntraining examples from the origin (in the feature space) and maximize\\nthe distance from the hyperplane to the origin, or 2) to obtain a spherical\\nboundary around the data by minimizing the volume of this hypersphere.\\nI leave the description of the one-class kNN algorithm, as well as the\\ndetails of the one-class kmeans and one-class SVM for the complementary\\nreading.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cFigure 3: A picture labeled as “people”, “concert”, and “nature”.\\n\\n7.4 Multi-Label Classiﬁcation\\n\\nIn multi-label classiﬁcation, each training example doesn’t just have one label, but several\\nof them. For instance, if we want to describe an image, we could assign several labels to it:\\n“people,” “concert,” “nature,” all three at the same time (ﬁg. 3).\\n\\nIf the number of possible values for labels is high, but they are all of the same nature, like\\ntags, we can transform each labeled example into several labeled examples, one per label.\\nThese new examples all have the same feature vector and only one label. That becomes a\\nmulticlass classiﬁcation problem. We can solve it using the one versus rest strategy. The\\nonly di\\x00erence with the usual multiclass problem is that now we have a new hyperparameter:\\nthreshold. If the prediction score for some label is above the threshold, this label is predicted\\nfor the input feature vector. In this scenario, multiple labels can be predicted for one feature\\nvector. The value of the threshold is chosen using the validation set.\\n\\nAnalogously, algorithms that naturally can be made multiclass (decision trees, logistic\\nregression and neural networks among others) can be applied to multi-label classiﬁcation\\nproblems. Because they return the score for each class, we can deﬁne a threshold and then\\nassign multiple labels to one feature vector if the threshold is above some value chosen\\nexperimentally using the validation set.\\n\\nNeural networks algorithms can naturally train multi-label classiﬁcation models by using the\\nbinary cross-entropy cost function. The output layer of the neural network, in this case,\\nhas one unit per label. Each unit of the output layer has the sigmoid activation function.\\n), where l = 1, . . . , L and i = 1, . . . , N . The\\n0, 1\\nAccordingly, each label l is binary (yi,l œ{\\nbinary cross-entropy of predicting the probability ˆyi,l that example xi has label l is deﬁned\\nˆyi,l)). The minimization criterion is simply the average of\\nas\\n\\n(yi,l ln(ˆyi,l) + (1\\n\\nyi,l) ln(1\\n\\n}\\n\\n≠\\n\\n≠\\n\\n≠\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0call binary cross-entropy terms across all training examples and all labels of those examples.\\n\\nIn cases where the number of possible values each label can take is small, one can convert\\nmultilabel into a multiclass problem using a di\\x00erent approach. Imagine the following problem.\\nWe want to label images and labels can be of two types. The ﬁrst type of label can have\\n; the label of the second type can have three possible\\ntwo possible values:\\nvalues\\n. We can create a new fake class for each combination of\\n}\\nthe two original classes, like this:\\n\\nportrait, paysage, other\\n{\\n\\nphoto, painting\\n{\\n\\n}\\n\\nFake Class Real Class 1 Real Class 2\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nphoto\\nphoto\\nphoto\\npainting\\npainting\\npainting\\n\\nportrait\\npaysage\\nother\\nportrait\\npaysage\\nother\\n\\nNow we have the same labeled examples, but we replace real multi-labels with one fake label\\nwith values from 1 to 6. This approach works well in practice when there are not too many\\npossible combinations of classes. Otherwise, you need to use much more training data to\\ncompensate for an increased set of classes.\\n\\nThe primary advantage of this latter approach is that you keep your labels correlated,\\ncontrary to the previously seen methods that predict each label independently of one another.\\nCorrelation between labels can be an essential property in many problems. For example, if\\nyou want to predict for an email message whether it’s spam or not_spam at the same time\\nas you predict whether it’s ordinary or priority email. You would like to avoid predictions\\nlike [spam, priority].\\n\\n7.5 Ensemble Learning\\n\\nEnsemble learning is a learning paradigm that, instead of trying to learn one super-accurate\\nmodel, focuses on training a large number of low-accuracy models and then combining the\\npredictions given by those weak models to obtain a high-accuracy meta-model.\\n\\nLow-accuracy models are usually learned by weak learners, that is learning algorithms that\\ncannot learn complex models, and thus are typically fast at the training and at the prediction\\ntime. The most frequently used weak learner is a decision tree learning algorithm in which\\nwe often stop splitting the training set after just a few iterations. The obtained trees are\\nshallow and not particularly accurate, but the idea behind ensemble learning is that if the\\ntrees are not identical and each tree is at least slightly better than random guessing, then we\\ncan obtain high accuracy by combining a large number of such trees.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cTo obtain the prediction for input x, the predictions of each weak model are combined using\\nsome sort of weighted voting. The speciﬁc form of vote weighting depends on the algorithm,\\nbut, independently of the algorithm, the idea is the same: if the council of weak models\\npredicts that the message is spam, then we assign the label spam to x.\\n\\nTwo most widely used and e\\x00ective ensemble learning algorithms are random forest and\\ngradient boosting.\\n\\n7.5.1 Random Forest\\n\\nThere are two ensemble learning paradigms: bagging and boosting. Bagging consists of\\ncreating many “copies” of the training data (each copy is slightly di\\x00erent from another) and\\nthen apply the weak learner to each copy to obtain multiple weak models and then combine\\nthem. The bagging paradigm is behind the random forest learning algorithm.\\n\\nThe “vanilla” bagging algorithm works like follows. Given a training set, we create B random\\nsamples Sb (for each b = 1, . . . , B) of the training set and build a decision tree model fb\\nusing each sample Sb as the training set. To sample Sb for some b, we do the sampling with\\nreplacement. This means that we start with an empty set, and then pick at random an\\nexample from the training set and put its exact copy to Sb by keeping the original example\\nin the original training set. We keep picking examples at random until the\\n\\n= N .\\n\\nSb|\\n|\\n\\nAfter training, we have B decision trees. The prediction for a new example x is obtained as\\nthe average of B predictions:\\n\\nˆf (x)\\n\\ndef\\n=\\n\\ny\\n\\nΩ\\n\\n1\\nB\\n\\nB\\n\\nÿb=1\\n\\nfb(x),\\n\\nin the case of regression, or by taking the majority vote in the case of classiﬁcation.\\n\\nThe random forest algorithm is di\\x00erent from the vanilla bagging in just one way. It uses\\na modiﬁed tree learning algorithm that inspects, at each split in the learning process, a\\nrandom subset of the features. The reason for doing this is to avoid the correlation of the\\ntrees: if one or a few features are very strong predictors for the target, these features will\\nbe selected to split examples in many trees. This would result in many correlated trees in\\nour “forest.” Correlated predictors cannot help in improving the accuracy of prediction. The\\nmain reason behind a better performance of model ensembling is that models that are good\\nwill likely agree on the same prediction, while bad models will likely disagree on di\\x00erent\\nones. Correlation will make bad models more likely to agree, which will hamper the majority\\nvote or the average.\\n\\nThe most important hyperparameters to tune are the number of trees, B, and the size of the\\nrandom subset of the features to consider at each split.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cRandom forest is one of the most widely used ensemble learning algorithms. Why is it so\\ne\\x00ective? The reason is that by using multiple samples of the original dataset, we reduce\\nthe variance of the ﬁnal model. Remember that the low variance means low overﬁtting.\\nOverﬁtting happens when our model tries to explain small variations in the dataset because our\\ndataset is just a small sample of the population of all possible examples of the phenomenon we\\ntry to model. If we were unlucky with how our training set was sampled, then it could contain\\nsome undesirable (but unavoidable) artifacts: noise, outliers and over- or underrepresented\\nexamples. By creating multiple random samples with replacement of our training set, we\\nreduce the e\\x00ect of these artifacts.\\n\\n7.5.2 Gradient Boosting\\n\\nAnother e\\x00ective ensemble learning algorithm is gradient boosting. Let’s ﬁrst look at gradient\\nboosting for regression. To build a strong regressor, we start with a constant model f = f0\\n(just like we did in ID3):\\n\\nf = f0(x)\\n\\ndef\\n=\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nyi.\\n\\nThen we modify labels of each example i = 1, . . . , N in our training set like follows:\\n\\nˆyi Ω\\n\\nyi ≠\\n\\nf (xi),\\n\\n(2)\\n\\nwhere ˆyi, called the residual, is the new label for example xi.\\n\\nNow we use the modiﬁed training set, with residuals instead of original labels, to build a new\\ndecision tree model, f1. The boosting model is now deﬁned as f def\\n= f0 + –f1, where – is the\\nlearning rate (a hyperparameter).\\n\\nThen we recompute the residuals using eq. 2 and replace the labels in the training data once\\nagain, train the new decision tree model f2, redeﬁne the boosting model as f def\\n= f0 +–f1 +–f2\\nand the process continues until the maximum of M (another hyperparameter) trees are\\ncombined.\\n\\nIntuitively, what’s happening here? By computing the residuals, we ﬁnd how well (or poorly)\\nthe target of each training example is predicted by the current model f . We then train\\nanother tree to ﬁx the errors of the current model (this is why we use residuals instead if\\nreal labels) and add this new tree to the existing model with some weight –. Therefore, each\\nadditional tree added to the model partially ﬁxes the errors made by the previous trees until\\nthe maximum number of trees are combined.\\n\\nNow you should reasonably ask why the algorithm is called gradient boosting? In gradient\\nboosting, we don’t calculate any gradient contrary to what we did in Chapter 4 for linear\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cregression. To see the similarity between gradient boosting and gradient descent remember\\nwhy we calculated the gradient in linear regression: we did that to get an idea on where we\\nshould move the values of our parameters so that the MSE cost function reaches its minimum.\\nThe gradient showed the direction, but we didn’t know how far we should go in this direction,\\nso we used a small step – at each iteration and then reevaluated our direction. The same\\nhappens in gradient boosting. However, instead of getting the gradient directly, we use its\\nproxy in the form of residuals: they show us how the model has to be adjusted so that the\\nerror (the residual) is reduced.\\n\\nThe three principal hyperparameters to tune in gradient boosting are the number of trees,\\nthe learning rate, and the depth of trees — all three a\\x00ect model accuracy. The depth of\\ntrees also a\\x00ects the speed of training and prediction: the shorter, the faster.\\n\\nIt can be shown that training on residuals optimizes the overall model f for the mean squared\\nerror criterion. You can see the di\\x00erence with bagging here: boosting reduces the bias (or\\nunderﬁtting) instead of the variance. As such, boosting can overﬁt. However, by tuning the\\ndepth and the number of trees, overﬁtting can be largely avoided.\\n\\nThe gradient boosting algorithm for classiﬁcation is similar, but the steps are slightly di\\x00erent.\\nLet’s consider the binary case. Assume we have M regression decision trees. Similarly to\\nlogistic regression, the prediction of the ensemble of decision trees is modeled using the\\nsigmoid function:\\n\\nx, f )\\nPr(y = 1\\n|\\n\\ndef\\n=\\n\\n1\\n1 + e≠\\n\\nf (x)\\n\\n,\\n\\nwhere f (x) =\\n\\nM\\nm=1 fm(x) and fm is a regression tree.\\n\\nq\\n\\nAgain, like in logistic regression, we apply the maximum likelihood principle by trying to\\nﬁnd such an f that maximizes Lf =\\nxi, f )). Again, to avoid numerical\\n|\\noverﬂow, we maximize the sum of log-likelihoods rather than the product of likelihoods.\\nN\\nThe algorithm starts with the initial constant model f = f0 = p\\ni=1 yi.\\n1\\n≠\\n(It can be shown that such initialization is optimal for the sigmoid function.) Then at each\\niteration m, a new tree fm is added to the model. To ﬁnd the best fm, ﬁrst the partial\\nderivative gi of the current model is calculated for each i = 1, . . . , N :\\n\\nN\\ni=1 ln(Pr(yi = 1\\n\\np , where p = 1\\n\\nq\\n\\nq\\n\\nN\\n\\ngi =\\n\\ndLf\\ndf\\n\\n,\\n\\nwhere f is the ensemble classiﬁer model built at the previous iteration m\\nwe need to ﬁnd the derivatives of ln(Pr(yi = 1\\ndef\\nln(Pr(yi = 1\\n= ln(\\nequation with respect to f equals to\\n\\n1. To calculate gi\\nxi, f )) with respect to f for all i. Notice that\\n|\\n1\\nf (xi) ). The derivative of the right-hand term in the previous\\n1+e≠\\n\\nxi, f ))\\n|\\n\\n≠\\n\\n1\\nef (xi)+1 .\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0cWe then transform our training set by replacing the original label yi with the corresponding\\npartial derivative gi, and we build a new tree fm using the transformed training set. Then\\nwe ﬁnd the optimal update step ﬂm as:\\n\\nﬂm = arg max\\n\\nﬂ\\n\\nLf +ﬂfm.\\n\\nAt the end of iteration m, we update the ensemble model f by adding the new tree fm:\\n\\nf\\n\\nΩ\\n\\nf + –ﬂmfm.\\n\\nWe iterate until m = M , then we stop and return the ensemble model f .\\n\\nGradient boosting is one of the most powerful machines learning algorithms. Not just because\\nit creates very accurate models, but also because it is capable of handling huge datasets with\\nmillions of examples and features. It usually outperforms random forest in accuracy but,\\nbecause of its sequential nature, can be signiﬁcantly slower in training.\\n\\n7.6 Learning to Label Sequences\\n\\nA sequence is one the most frequently observed types of structured data. We communicate\\nusing sequences of words and sentences, we execute tasks in sequences, our genes, the music\\nwe listen and videos we watch, our observations of a continuous process, such as a moving\\ncar or the price of a stock are all sequential.\\n\\nIn sequence labeling, a labeled sequential example is a pair of lists (X, Y ), where X is a list\\nof feature vectors, one per time step, Y is a list of the same length of labels. For example, X\\ncould represent words in a sentence such as [“big”, “beautiful”, “car”], and Y would be the\\nlist of the corresponding parts of speech, such as [“adjective”, “adjective”, “noun”]). More\\nformally, in an example i, Xi = [x1\\n], where sizei is the length of the sequence\\nof the example i, Yi = [y1\\n\\ni , . . . , xsizei\\n] and yi œ{\\nYou have already seen that an RNN can be used to annotate a sequence. At each time step t,\\nit reads an input feature vector x(t)\\nlast (in the\\ni\\ncase of binary labeling) or y(t)\\n\\n, and the last recurrent layer outputs a label y(t)\\n\\nlast (in the case of multiclass or multilabel labeling).\\n\\ni , x2\\ni , . . . , ysizei\\ni\\n\\n1, 2, . . . , C\\n\\ni , y2\\n\\n.\\n}\\n\\ni\\n\\nHowever, RNN is not the only possible model for sequence labeling. The model called\\nConditional Random Fields (CRF) is a very e\\x00ective alternative that often performs well\\nin practice for the feature vectors that have many informative features. For example, imagine\\nwe have the task of named entity extraction and we want to build a model that would\\nlabel each word in the sentence such as “I go to San Francisco” with one of the following\\n. If our feature vectors (which represent\\nclasses:\\n}\\nwords) contain such binary features as “whether or not the word starts with a capital letter”\\nand “whether or not the word can be found in the list of locations,” such features would\\n\\nlocation, name, company_name, other\\n{\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0cbe very informative and help to classify the words San and Francisco as location. Building\\nhandcrafted features is known to be a labor-intensive process that requires a signiﬁcant level\\nof domain expertise.\\n\\nCRF is an interesting model and can be seen as a generalization of\\nlogistic regression to sequences. However, in practice, it has been\\noutperformed by bidirectional deep gated RNN for sequence labeling\\ntasks. CRFs are also signiﬁcantly slower in training which makes them\\ndi\\x00cult to apply to large training sets (with hundreds of thousands of\\nexamples). Additionally, a large training set is where a deep neural\\nnetwork thrives.\\n\\n7.7 Sequence-to-Sequence Learning\\n\\nSequence-to-sequence learning (often abbreviated as seq2seq learning) is a generalization\\nof the sequence labeling problem. In seq2seq, Xi and Yi can have di\\x00erent length. seq2seq\\nmodels have found application in machine translation (where, for example, the input is\\nan English sentence, and the output is the corresponding French sentence), conversational\\ninterfaces (where the input is a question typed by the user, and the output is the answer\\nfrom the machine), text summarization, spelling correction, and many others.\\n\\nMany but not most sequence-to-sequence learning problems are currently best solved by\\nneural networks. Machine translation is a notorious example. There are multiple neural\\nnetwork architectures for seq2seq which perform better than others depending on the task.\\nAll those network architectures have one property in common: they have two parts, an\\nencoder and a decoder (for this reason they are also known as encoder-decoder neural\\nnetworks).\\n\\nIn seq2seq learning, the encoder is a neural network that accepts sequential input. It can\\nbe an RNN, but also a CNN or some other architecture. The role of the encoder is to read\\nthe input and generate some sort of state (similar to the state in RNN) that can be seen\\nas a numerical representation of the meaning of the input the machine can work with. The\\nmeaning of some entity, whether it be an image, a text or a video, is usually a vector or a\\nmatrix that contains real numbers. This vector (or matrix) is called in the machine learning\\njargon the embedding of the input.\\n\\nThe decoder in seq2seq learning is another neural network that takes an embedding as input\\nand is capable of generating a sequence of outputs. As you could have already guessed, that\\nembedding comes from the encoder. To produce a sequence of outputs, the decoder takes a\\nstart of sequence input feature vector x(0) (typically all zeroes), produces the ﬁrst output\\ny(1), updates its state by combining the embedding and the input x(0), and then uses the\\noutput y(1) as its next input x(1). For simplicity, the dimensionality of y(t) can be the same\\nas that of x(t); however, it is not strictly necessary. As we saw in Chapter 6, each layer of an\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0cRNN can produce many simultaneous outputs: one can be used to generate the label y(t),\\nwhile another one, of di\\x00erent dimensionality, can be used as the x(t).\\n\\nEncoder\\n\\nDecoder\\n\\nIl\\n\\nfait\\n\\nbeau\\n\\nThe\\n\\nweather\\n\\nt\\xa0=\\n\\n1\\n\\n2\\n\\nis\\n\\n3\\n\\nfine\\n\\n4\\n\\n<\\xa0start\\xa0>\\n\\n1\\n\\n2\\n\\n3\\n\\nFigure 4: A traditional seq2seq architecture.\\n\\nBoth encoder and decoder are trained simultaneously using the training data. The errors at\\nthe decoder output are propagated to the encoder via backpropagation.\\n\\nA traditional seq2seq architecture is illustrated in ﬁg. 4. More accurate predictions can be\\nobtained using an architecture with attention. Attention mechanism is implemented by an\\nadditional set of parameters that combine some information from the encoder (in RNNs,\\nthis information is the list of state vectors of the last recurrent layer from all encoder time\\nsteps) and the current state of the decoder to generate the label. That allows for even better\\nretention of long-term dependencies than provided by gated units and bidirectional RNN. A\\nseq2seq architecture with attention is illustrated in ﬁg. 5.\\n\\nSequence-to-sequence learning is a relatively new research domain. Novel\\nnetwork architectures are regularly discovered and published. Training\\nsuch architectures can be challenging as the number of hyperparame-\\nters to tune and other architectural decisions can be overwhelming. I\\nrecommend consulting the book’s wiki for the state of the art material,\\ntutorials and code samples.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n14\\n\\n\\x0cAttention\\n\\nIl\\n\\nfait\\n\\nbeau\\n\\nThe\\n\\nweather\\n\\nt\\xa0=\\n\\n1\\n\\n2\\n\\nis\\n\\n3\\n\\nfine\\n\\n4\\n\\n<\\xa0start\\xa0>\\n\\n1\\n\\n2\\n\\n3\\n\\nFigure 5: A seq2seq architecture with attention.\\n\\n7.8 Active Learning\\n\\nActive learning is an interesting supervised learning paradigm. It is usually applied when\\nobtaining labeled examples is costly. That is often the case in the medical or ﬁnancial\\ndomains, where the opinion of an expert may be required to annotate patients’ or customers’\\ndata. The idea is that we start the learning with relatively few labeled examples, and a large\\nnumber of unlabeled ones, and then add labels only to those examples that contribute the\\nmost to the model quality.\\n\\nThere are multiple strategies of active learning. Here, we discuss only the following two:\\n\\n1) data density and uncertainty based, and\\n2) support vector-based.\\n\\nThe former strategy applies the current model f , trained using the existing labeled examples,\\nto each of the remaining unlabelled examples (or, to save the computing time, to some\\nrandom sample of them). For each unlabeled example x, the following importance score is\\ncomputed: density(x)\\nuncertaintyf (x). Density reﬂects how many examples surround x in\\nits close neighborhood, while uncertaintyf (x) reﬂects how uncertain the prediction of the\\nmodel f is for x. In binary classiﬁcation with sigmoid, the closer the prediction score is to\\n0.5, the more uncertain is the prediction. In SVM, the closer the example is to the decision\\nboundary, the most uncertain is the prediction.\\n\\n·\\n\\nIn multiclass classiﬁcation, entropy can be used as a typical measure of uncertainty:\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n15\\n\\n\\x0cHf (x) =\\n\\nC\\n\\n≠\\n\\nc=1\\nÿ\\n\\nPr(y(c); f (x)) ln Pr(y(c); f (x)),\\n\\nwhere Pr(y(c); f (x)) is the probability score the model f assigns to class y(c) when classifying\\nx. You can see that if for each y(c), f (y(c)) = 1\\nC then the model is the most uncertain and\\nthe entropy is at its maximum of 1; on the other hand, if for some y(c), f (y(c)) = 1, then the\\nmodel is certain about the class y(c) and the entropy is at its minimum of 0.\\n\\nDensity for the example x can be obtained by taking the average of the distance from x to\\neach of its k nearest neighbors (with k being a hyperparameter).\\n\\nOnce we know the importance score of each unlabeled example, we pick the one with the\\nhighest importance score and ask the expert to annotate it. Then we add the new annotated\\nexample to the training set, rebuild the model and continue the process until some stopping\\ncriterion is satisﬁed. A stopping criterion can be chosen in advance (the maximum number\\nof requests to the expert based on the available budget) or depend on how well our model\\nperforms according to some metric.\\n\\nThe support vector-based active learning strategy consists in building an SVM model using\\nthe labeled data. We then ask our expert to annotate the unlabeled example that lies the\\nclosest to the hyperplane that separates the two classes. The idea is that if the example lies\\nclosest to the hyperplane, then it is the least certain and would contribute the most to the\\nreduction of possible places where the true (the one we look for) hyperplane could lie.\\n\\nSome active learning strategies can incorporate the cost of asking an\\nexpert for a label. Others learn to ask expert’s opinion. The “query by\\ncommittee” strategy consists of training multiple models using di\\x00erent\\nmethods and then asking an expert to label example on which those\\nmodels disagree the most. Some strategies try to select examples to\\nlabel so that the variance or the bias of the model are reduced the most.\\n\\n7.9 Semi-Supervised Learning\\n\\nIn semi-supervised learning (SSL) we also have labeled a small fraction of the dataset;\\nmost of the remaining examples are unlabeled. Our goal is to leverage a large number of\\nunlabeled examples to improve the model performance without asking an expert for additional\\nlabeled examples.\\n\\nHistorically, there were multiple attempts at solving this problem. None of them could be\\ncalled universally acclaimed and frequently used in practice. For example, one frequently\\ncited SSL method is called “self-learning.” In self-learning, we use a learning algorithm to\\nbuild the initial model using the labeled examples. Then we apply the model to all unlabeled\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n16\\n\\n\\x0cexamples and label them using the model. If the conﬁdence score of prediction for some\\nunlabeled example x is higher than some threshold (chosen experimentally), then we add this\\nlabeled example to our training set, retrain the model and continue like this until a stopping\\ncriterion is satisﬁed. We could stop, for example, if the accuracy of the model has not been\\nimproved during the last m iterations.\\n\\nThe above method can bring some improvement to the model compared to just using the\\ninitially labeled dataset, but the increase in performance usually is not very impressive.\\nFurthermore, in practice, the quality of the model could even decrease. That depends on the\\nproperties of the statistical distribution the data was drawn from, which we usually do not\\nknow.\\n\\nOn the other hand, the recent advancements in neural network learning brought some\\nimpressive results. For example, it was shown that for some datasets, such as MNIST (a\\nfrequent testbench in computer vision that consists of labeled images of handwritten digits\\nfrom 0 to 9) the model trained in a semi-supervised way has an almost perfect performance\\nwith just 10 labeled examples per class (100 labeled examples overall). For comparison,\\nMNIST contains 70,000 labeled examples (60,000 for training and 10,000 for test). The\\nneural network architecture that attained such a remarkable performance is called a ladder\\nnetwork. To understand ladder networks you have to understand what an autoencoder is.\\n\\nAn autoencoder is a feed-forward neural network with an encoder-decoder architecture. It\\nis trained to reconstruct its input. So the training example is a pair (x, x). We want the\\noutput ˆx of the model f (x) to be as similar to the input x as possible.\\n\\nEncoder\\n\\nDecoder\\n\\nx\\n\\ng\\nn\\ni\\nd\\nd\\ne\\nb\\nm\\nE\\n\\nx̂  \\n\\nFigure 6: Autoencoder.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n17\\n\\n\\x0cAn important detail here is that an autoencoder’s network looks like an hourglass with a\\nbottleneck layer in the middle that contains the embedding of the D-dimensional input\\nvector; the embedding layer usually has much fewer units than D. The goal of the decoder is\\nto reconstruct the input feature vector from this embedding. Theoretically, it is su\\x00cient\\nto have 10 units in the bottleneck layer to successfully encode MNIST images. In a typical\\nautoencoder schematically depicted in ﬁg. 6, the cost function is usually either the mean\\nsquared error (when features can be any number) or the negative log-likelihood (when features\\nare binary and the units of the last layer of the decoder have the sigmoid activation function).\\nIf the cost is the mean squared error, then it is given by:\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nxi ≠\\nÎ\\n\\n2,\\nf (xi)\\nÎ\\n\\nwhere\\n\\nxi ≠\\nÎ\\n\\nf (xi)\\nÎ\\n\\nis the Euclidean distance between two vectors.\\n\\nA denoising autoencoder corrupts the left-hand side x in the training example (x, x) by\\nadding some random perturbation to the features. If our examples are grayscale images with\\npixels represented as values between 0 and 1, usually a normal Gaussian noise is added to\\neach feature. For each feature j of the input feature vector x the noise value n(j) is sampled\\nfrom the following distribution:\\n\\nn(j)\\n\\n1\\n‡Ô2ﬁ\\n\\n≥\\n\\nexp\\n\\nµ)2\\n(\\n≠\\n2‡2\\n\\n,\\n\\n4\\n\\n≠\\n\\n3\\n\\nmeans “sampled from,” ﬁ is the constant 3.14159 . . . and µ is a\\nwhere the notation\\nhyperparameter that has to be tuned. The new, corrupted value of the feature x(j) is given\\nby x(j) + n(j).\\n\\n≥\\n\\nA ladder network is a denoising autoencoder with an upgrade. The encoder and the decoder\\nhave the same number of layers. The bottleneck layer is used directly to predict the label\\n(using the softmax activation function). The network has several cost functions. For each\\nlayer l of the encoder and the corresponding layer l of the decoder, one cost C l\\nd penalizes\\nthe di\\x00erence between the outputs of the two layers (using the squared Euclidean distance).\\nWhen a labeled example is used during training, another cost function, Cc, penalizes the error\\nin prediction of the label (the negative log-likelihood cost function is used). The combined\\ncost function, Cc +\\nd (averaged over all examples in the batch), is optimized by the\\nstochastic gradient descent with backpropagation. The hyperparameters ⁄l for each layer l\\ndetermine the tradeo\\x00 between the classiﬁcation and encoding-decoding cost.\\n\\nL\\nl=1 ⁄lC l\\n\\nq\\n\\nIn the ladder network, not just the input is corrupted with the noise, but also the output of\\neach encoder layer (during training). When we apply the trained model to the new input x\\nto predict its label, we do not corrupt the input.\\n\\nOther semi-supervised learning techniques, not related to training neural networks, exist.\\nOne of them implies building the model using the labeled data and then cluster the unlabeled\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n18\\n\\n\\x0cand labeled examples together using any clustering technique (we consider some of them in\\nChapter 9).\\n\\nFor each new example, we then output as a prediction the majority label\\nin the cluster it belongs to. Another technique, called S3VM, is based\\non using SVM. We build one SVM model for each possible labeling of\\nunlabeled examples and then we pick the model with the largest margin.\\nThe paper on S3VM describes an approach that allows solving this\\nproblem without actually enumerating all possible labelings.\\n\\n7.10 One-Shot Learning\\n\\nThis chapter would be incomplete without mentioning two other important supervised learning\\nparadigms. One of them is one-shot learning. In one-shot learning, typically applied in\\nface recognition, we want to build a model that can recognize that two photos of the same\\nperson represent that same person. If we present to the model two photos of two di\\x00erent\\npeople, we expect the model to recognize that the two people are di\\x00erent.\\n\\nOne way to build such a model is to train a siamese neural network (SNN). An SNN can\\nbe implemented as any kind of neural network, a CNN, an RNN, or an MLP. What matters\\nis how we train the network.\\n\\nTo train an SNN, we use the triplet loss function. For example, let us have three images of\\na face: the image A (for anchor), the image P (for positive) and the image N (for negative).\\nA and P are two di\\x00erent pictures of the same person; N is a picture of another person.\\nEach training example i is now a triplet (Ai, Pi, Ni).\\n\\nLet’s say we have a neural network model f that can take a picture of a face as input and\\noutput an embedding of this picture. The triplet loss for one example is deﬁned as,\\n\\nf (Ai)\\nmax(\\nÎ\\n\\n≠\\n\\n2\\nf (Pi)\\nÎ\\n\\n≠ Î\\n\\nf (Ai)\\n\\n2 + –, 0).\\nf (Ni)\\nÎ\\n\\n≠\\n\\n(3)\\n\\nThe cost function is deﬁned as the average triplet loss:\\n\\n1\\nN\\n\\nN\\n\\ni=1\\nÿ\\n\\nf (Ai)\\nmax(\\nÎ\\n\\n≠\\n\\n2\\n\\nf (Pi)\\nÎ\\n\\n≠ Î\\n\\nf (Ai)\\n\\nf (Ni)\\nÎ\\n\\n≠\\n\\n2 + –, 0),\\n\\n2 is low when our neural\\nf (P )\\nwhere – is a positive hyperparameter. Intuitively,\\nÎ\\n2 is high when the\\nnetwork outputs similar embedding vectors for A and P ;\\nf (Ni)\\nÎ\\n≠\\nembedding for pictures of two di\\x00erent people are di\\x00erent. If our model works the way\\n2 will always be negative,\\n2\\nwe want, then the term m =\\nf (Pi)\\nÎ\\nbecause we subtract a high value from a small value. By setting – higher, we force the term\\n\\n≠\\nf (Ai)\\nÎ\\n\\nf (Ni)\\nÎ\\n\\nf (Ai)\\n\\nf (Ai)\\n\\nf (A)\\n\\n≠ Î\\n\\n≠\\n\\n≠\\n\\nÎ\\n\\nÎ\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n19\\n\\n\\x0cm to be even smaller, to make sure that the model learned to recognize the two same faces\\nand two di\\x00erent faces with a high margin. If m is not small enough, then because of – the\\ncost will be positive, and the model parameters will be adjusted in backpropagation.\\n\\nRather than randomly choose an image for N , a better way to create triplets for training is\\nto use the current model after several epochs of learning and ﬁnd candidates for N that are\\nsimilar to A and P according to that model. Using random examples as N would signiﬁcantly\\nslow down the training because the neural network will easily see the di\\x00erence between\\npictures of two random people, so the average triplet loss will be low most of the time and\\nthe parameters will not be updated fast enough.\\n\\nTo build an SNN, we ﬁrst decide on the architecture of our neural network. For example,\\nCNN is a typical choice if our inputs are images. Given an example, to calculate the average\\ntriplet loss, we apply, consecutively, the model to A, then to P , then to N , and then we\\ncompute the loss for that example using eq. 3. We repeat that for all triplets in the batch and\\nthen compute the cost; gradient descent with backpropagation propagates the cost through\\nthe network to update its parameters.\\n\\nIt’s a common misconception that for one-shot learning we need only one example of each\\nentity for training. In practice, we need much more than one example of each person for the\\nperson identiﬁcation model to be accurate. It’s called one-shot because of the most frequent\\napplication of such a model: face-based authentication. For example, such a model could be\\nused to unlock your phone. If your model is good, then you only need to have one picture\\nof you on your phone and it will recognize you, and also it will recognize that someone else\\nis not you. When we have the model, to decide whether two pictures A and ˆA belong to\\nf ( ˆA)\\n2 is less than some threshold · , which is another\\nthe same person, we check if\\nÎ\\nÎ\\nhyperparameter of the model.\\n\\nf (A)\\n\\n≠\\n\\n7.11 Zero-Shot Learning\\n\\nWe ﬁnish this chapter with zero-shot learning. It is a relatively new\\nresearch area, so there are no algorithms that proved to have a signiﬁcant\\npractical utility yet. Therefore, I only outline here the basic idea and\\nleave the details of various algorithms for further reading. In zero-shot\\nlearning (ZSL) we want to train a model to assign labels to objects. The\\nmost frequent application is to learn to assign labels to images.\\n\\nHowever, we want the model to be able to predict labels that we didn’t have in the training\\ndata. How is that possible?\\n\\nThe trick is to use embeddings not just to represent the input x but also to represent the\\noutput y. Imagine that we have a model that for any word in English can generate an\\nembedding vector with the following property: if a word yi has a similar meaning to the\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n20\\n\\n\\x0cword yk, then the embedding vectors for these two words will be similar. For example, if yi is\\nParis and yk is Rome, then they will have embeddings that are similar; on the other hand, if\\nyk is potato, then the embeddings of yi and yk will be dissimilar. Such embedding vectors are\\ncalled “word embeddings,” and they are usually compared using cosine similarity metrics1.\\n\\nWord embeddings have such a property that each dimension of the embedding represents a\\nspeciﬁc feature of the meaning of the word. For example, if our word embedding has four\\ndimensions (usually they are much wider, between 50 and 300 dimensions), then these four\\ndimensions could represent such features of the meaning as animalness, abstractness, sourness,\\nand yellowness (yes, sounds funny, but it’s just an example). So the word bee would have an\\nembedding like this [1, 0, 0, 1], the word yellow like this [0, 1, 0, 1], the word unicorn like this\\n[1, 1, 0, 0]. The values for each embedding are obtained using a speciﬁc training procedure\\napplied to a vast text corpus.\\n\\nNow, in our classiﬁcation problem, we can replace the label yi for each example i in our\\ntraining set with its word embedding and train a multi-label model that predicts word\\nembeddings. To get the label for a new example x, we apply our model f to x, get the\\nembedding ˆy and then search among all English words those whose embeddings are the most\\nsimilar to ˆy using cosine similarity.\\n\\nWhy does that work? Take a zebra for example. It is white, it is a mammal, and it has stripes.\\nTake a clownﬁsh: it is orange, not a mammal, and has stripes. Now take a tiger: it is orange,\\nit has stripes, and it is a mammal. If these three features are present in word embeddings,\\nthe CNN would learn to detect these same features in pictures. Even if the label tiger was\\nabsent in the training data, but other objects including zebras and clownﬁsh were, then the\\nCNN will most likely learn the notion of mammalness, orangeness, and stripeness to predict\\nlabels of those objects. Once we present the picture of a tiger to the model, those features\\nwill be correctly identiﬁed from the image and most likely the closest word embedding from\\nour English dictionary to the predicted embedding will be that of tiger.\\n\\n1I will show in Chapter 10 how to learn words embeddings from data.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n21\\n\\n\\x0cAndriy Burkov\\'s\\n\\n\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c8 Advanced Practice\\n\\nThis chapter contains the description of techniques that you could ﬁnd useful in your practice\\nin some contexts. It’s called “Advanced Practice” not because the presented techniques are\\nmore complex, but rather because they are applied in some very speciﬁc contexts. In many\\npractical situations, you will most likely not need to resort to using these techniques, but\\nsometimes they are very helpful.\\n\\n8.1 Handling Imbalanced Datasets\\n\\nIn many practical situations, your labeled dataset will have underrepresented the examples\\nof some class. This is the case, for example, when your classiﬁer has to distinguish between\\ngenuine and fraudulent e-commerce transactions: the examples of genuine transactions are\\nmuch more frequent. If you use SVM with soft margin, you can deﬁne a cost for misclassiﬁed\\nexamples. Because noise is always present in the training data, there are high chances that\\nmany examples of genuine transactions would end up on the wrong side of the decision\\nboundary by contributing to the cost.\\n\\n)\\n2\\n(\\nx\\n\\n)\\n2\\n(\\nx\\n\\nx(1)\\n(a)\\n\\nx(1)\\n\\n(b)\\n\\nFigure 1: An illustration of an imbalanced problem. (a) Both classes have the same weight;\\n(b) examples of the minority class have a higher weight.\\n\\nThe SVM algorithm will try to move the hyperplane to avoid as much as possible misclassiﬁed\\nexamples. The “fraudulent” examples, which are in the minority, risk being misclassiﬁed in\\norder to classify more numerous examples of the majority class correctly. This situation is\\nillustrated in Figure 1a. This problem is observed for most learning algorithms applied to\\nimbalanced datasets.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cIf you set the cost of misclassiﬁcation of examples of the minority class higher, then the\\nmodel will try harder to avoid misclassifying those examples, obviously for the cost of\\nmisclassiﬁcation of some examples of the majority class, as illustrated in Figure 1b.\\n\\nSome SVM implementations (including SVC in scikit-learn) allow you to provide weights for\\nevery class. The learning algorithm takes this information into account when looking for the\\nbest hyperplane.\\n\\nIf your learning algorithm doesn’t allow weighting classes, you can try to increase the\\nimportance of examples of some class by making multiple copies of the examples of this class\\n(this is called oversampling).\\n\\nAn opposite approach is to randomly remove from the training set some examples of the\\nmajority class (undersampling).\\n\\nYou might also try to create synthetic examples by randomly sampling feature values of\\nseveral examples of the minority class and combining them to obtain a new example of\\nthat class. There two popular algorithms that oversample the minority class by creating\\nsynthetic examples: the synthetic minority oversampling technique (SMOTE) and the\\nadaptive synthetic sampling method (ADASYN).\\n\\nSMOTE and ADASYN work similarly in many ways. For a given example xi of the minority\\nclass, they pick k nearest neighbors of this example (let’s call this set of k examples\\nSk) and\\nthen create a synthetic example xnew as xi + ⁄(xzi ≠\\nxi), where xzi is an example of the\\nSk. The interpolation hyperparameter ⁄ is a random\\nminority class chosen randomly from\\nnumber in the range [0, 1].\\n\\nBoth SMOTE and ADASYN randomly pick all possible xi in the dataset. In ADASYN,\\nthe number of synthetic examples generated for each xi is proportional to the number of\\nexamples in\\nSk which are not from the minority class. Therefore, more synthetic examples\\nare generated in the area where the examples of the minority class are rare.\\n\\nSome algorithms are less sensitive to the problem of an imbalanced dataset. Decision trees,\\nas well as random forest and gradient boosting, often perform well on imbalanced datasets.\\n\\n8.2 Combining Models\\n\\nEnsemble algorithms, like Random Forest, typically combine models of the same nature. They\\nboost performance by combining hundreds of weak models. In practice, we can sometimes\\nget an additional performance gain by combining strong models made with di\\x00erent learning\\nalgorithms. In this case, we usually use only two or three models.\\n\\nThere are three typical ways to combine models:\\n\\n1) averaging,\\n2) majority vote, and\\n3) stacking.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cAveraging works for regression as well as those classiﬁcation models that return classiﬁcation\\nscores. You simply apply all your models, let’s call them base models, to the input x and\\nthen average the predictions. To see if the averaged model works better than each individual\\nalgorithm, you test it on the validation set using a metric of your choice.\\n\\nMajority vote works for classiﬁcation models. You apply all your base models to the input\\nx and then return the majority class among all predictions. In the case of a tie, you either\\nrandomly pick one of the classes, or, you return an error message (if the fact of misclassifying\\nwould incur a signiﬁcant cost).\\n\\nStacking consists of building a meta-model that takes the output of your base models as\\ninput. Let’s say you want to combine a classiﬁer f1 and a classiﬁer f2, both predicting the\\nsame set of classes. To create a training example (ˆxi, ˆyi) for the stacked model, you set\\nˆxi = [f1(x), f2(x)] and ˆyi = yi.\\n\\nIf some of your base models return not just a class, but also a score for each class, you can\\nuse these values as features too.\\n\\nTo train the stacked model, it is recommended to use examples from the training set and\\ntune the hyperparameters of the stacked model using cross-validation.\\n\\nObviously, you have to make sure that your stacked model performs better on the validation\\nset than each of the base models you stacked.\\n\\nThe reason that combining multiple models can bring better performance overall is the\\nobservation that when several uncorrelated strong models agree they are more likely to agree\\non the correct outcome. The keyword here is “uncorrelated.” Ideally, di\\x00erent strong models\\nhave to be obtained using di\\x00erent features or using algorithms of a di\\x00erent nature — for\\nexample, SVMs and Random Forest. Combining di\\x00erent versions of decision tree learning\\nalgorithm, or several SVMs with di\\x00erent hyperparameters may not result in a signiﬁcant\\nperformance boosting.\\n\\n8.3 Training Neural Networks\\n\\nIn neural network training, one challenging aspect is to convert your data into the input the\\nnetwork can work with. If your input is images, ﬁrst of all, you have to resize all images so\\nthat they have the same dimensions. After that, pixels are usually ﬁrst standardized and\\nthen normalized to the range [0, 1].\\n\\nTexts have to be tokenized (that is split into pieces, such as words, punctuation marks, and\\nother symbols). For CNN and RNN, each token is converted into a vector using the one-hot\\nencoding, so the text becomes a list of one-hot vectors. Another, often a better way to\\nrepresent tokens is by using word embeddings. For multilayer perceptron, to convert texts\\nto vectors the bag of words approach may work well, especially for larger texts (larger than\\nSMS messages and tweets).\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cThe choice of speciﬁc neural network architecture is a di\\x00cult one. For the same problem,\\nlike seq2seq learning, there is a variety of architectures, and new ones are proposed almost\\nevery year. I recommend making the research on the state of the art solutions for your\\nproblem using Google Scholar or Microsoft Academic search engines that allow searching for\\nscientiﬁc publications using keywords and time range. If you don’t mind working with less\\nmodern architecture, I recommend looking for implemented architectures on GitHub and\\nﬁnd one that could be applied to your data with minor modiﬁcations.\\n\\nIn practice, the advantage of modern architecture over an older one becomes less signiﬁcant\\nas you preprocess, clean and normalize your data, and create a larger training set. Many\\nmodern neural network architectures are a result of the collaboration of several scientists\\nfrom several labs and companies; such models could be very complex to implement them on\\nyour own and usually require much computational power to train. The time spent on trying\\nto replicate the results from a recent scientiﬁc paper may not be worth it. This time could\\nbetter be spent on building the solution around a less modern but stable model and getting\\nmore training data.\\n\\nOnce you decided on the architecture of your network, you have to decide on the number\\nof layers, their type, and size. It is recommended to start with one or two layers, train a\\nmodel and see if it ﬁts the training data well (has a low bias). If not, gradually increase the\\nsize of each layer and the number of layers until the model perfectly ﬁts the training data.\\nOnce this is the case, if the model doesn’t perform well on the validation data (has a high\\nvariance), you should add regularization to your model. If, after you added regularization,\\nthe model doesn’t ﬁt the training data anymore, you slightly increase the size of the network\\nonce again, and you continue to work iteratively like this until the model ﬁts both training\\nand validation data well enough according to your metric.\\n\\n8.4 Advanced Regularization\\n\\nIn neural networks, besides L1 and L2 regularization, you can use neural network speciﬁc\\nregularizers: dropout, batch normalization, and early stopping. Batch normalization\\nis technically not a regularization technique, but it often has a regularization e\\x00ect on the\\nmodel.\\n\\nThe concept of dropout is very simple. Each time you run a training example through the\\nnetwork, you temporarily exclude at random some units from the computation. The higher\\nthe percentage of units excluded the higher the regularization e\\x00ect. Neural network libraries\\nallow you to add a dropout layer between two successive layers, or you can specify the dropout\\nparameter for the layer. The dropout parameter is in the range [0, 1] and it has to be found\\nexperimentally by tuning it on the validation data.\\n\\nBatch normalization (which rather has to be called batch standardization) is a technique that\\nconsists of standardizing the outputs of each layer before the units of the subsequent layer\\nreceive them as input. In practice, batch normalization results in a faster and more stable\\ntraining, as well as in some regularization e\\x00ect. So it’s always a good idea to try to use\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cbatch normalization. In neural network libraries, you can often insert a batch normalization\\nlayer between two layers.\\n\\nEarly stopping is the way to train a neural network by saving the preliminary model after\\nevery epoch and assessing the performance of the preliminary model on the validation set. As\\nyou remember from the section about gradient descent in Chapter 4, as the number of epochs\\nincreases, the cost decreases. The decreased cost means that the model ﬁts the training data\\nwell. However, at some point, after some epoch e, the model can start overﬁtting: the cost\\nkeeps decreasing, but the performance of the model on the validation data deteriorates. If\\nyou keep, in a ﬁle, the version of the model after each epoch, you can stop the training once\\nyou start observing a decreased performance on the validation set. Alternatively, you can\\nkeep running the training process for a ﬁxed number of epochs and then, in the end, you\\npick the best model. Models saved after each epoch are called checkpoints. Some machine\\nlearning practitioners rely on this technique very often; others try to properly regularize the\\nmodel to avoid such undesirable behavior.\\n\\nAnother regularization technique that can be applied not just to neural networks, but to\\nvirtually any learning algorithm, is called data augmentation. This technique is often\\nused to regularize models that work with images. Once you have your original labeled\\ntraining set, you can create a synthetic example from an original example by applying various\\ntransformations to the original image: zooming it slightly, rotating, ﬂipping, darkening, and\\nso on. You keep the original label in these synthetic examples. In practice, this often results\\nin increased performance of the model.\\n\\n8.5 Handling Multiple Inputs\\n\\nIn many of your practical problems, you will work with multimodal data. For example, your\\ninput could be an image and text and the binary output could indicate whether the text\\ndescribes this image or not.\\n\\nShallow learning algorithms are not particularly well suited to work with multimodal data.\\nHowever, it doesn’t mean that it is impossible. For example, you can train one model on the\\nimage and another one on the text. Then you can use a model combination technique we\\ndiscussed above.\\n\\nIf you cannot divide your problem into two independent subproblems, you can try to vectorize\\neach input (by applying the corresponding feature engineering method) and then simply\\nconcatenate two feature vectors together to form one wider feature vector. For example,\\nif your image has features [i(1), i(2), i(3)] and your text has features [t(1), t(2), t(3), t(4)] your\\nconcatenated feature vector will be [i(1), i(2), i(3), t(1), t(2), t(3), t(4)].\\n\\nWith neural networks, you have more ﬂexibility. You can build two subnetworks, one for\\neach type of input. For example, a CNN subnetwork would read the image while an RNN\\nsubnetwork would read the text. Both subnetworks have as their last layer an embedding:\\nCNN has an embedding for the image, while RNN has an embedding for the text. You can\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cthen concatenate two embeddings and then add a classiﬁcation layer, such as softmax or\\nsigmoid, on top of the concatenated embeddings. Neural network libraries provide simple to\\nuse tools that allow concatenating or averaging layers from several subnetworks.\\n\\n8.6 Handling Multiple Outputs\\n\\nIn some problems, you would like to predict multiple outputs for one input. We considered\\nmulti-label classiﬁcation in the previous chapter. Some problems with multiple outputs can\\nbe e\\x00ectively converted into a multi-label classiﬁcation problem. Especially those that have\\nlabels of the same nature (like tags) or fake labels can be created as a full enumeration of\\ncombinations of original labels.\\n\\nHowever, in some cases the outputs are multimodal, and their combinations cannot be\\ne\\x00ectively enumerated. Consider the following example: you want to build a model that\\ndetects an object on an image and returns its coordinates. The same model has to also return\\nthe label of the object, such as “person,” “cat,” or “hamster.” Your training examples will\\nhave an image as input and one vector with coordinates of an object and another vector with\\na one-hot encoded label.\\n\\nTo handle a situation like this, you can create one subnetwork that would work as an\\nencoder. It will read the input image using, for example, one or several convolution layers.\\nThe encoder’s last layer would be the embedding of the image. Then you add two other\\nsubnetworks on top of the embedding layer: one that takes the embedding vector as input\\nand predicts the coordinates of an object. This ﬁrst subnetwork can have a ReLU as the last\\nlayer, which is a good choice for predicting positive real numbers, such as coordinates; this\\nsubnetwork could use the mean squared error cost C1. The second subnetwork will take the\\nsame embedding vector as input and predict the probabilities for each label. This second\\nsubnetwork can have a softmax as the last layer, which is appropriate for the probabilistic\\noutput, and use the averaged negative log-likelihood cost C2 (also called cross-entropy cost).\\n\\nObviously, you are interested in both accurately predicted coordinates and the label. However,\\nit is impossible to optimize the two cost functions at the same time. By trying to optimize one,\\nyou risk hurting the second one and the other way around. What you can do is add another\\n“)C2.\\nhyperparameter “ in the range (0, 1) and deﬁne the combined cost function as “C1 + (1\\nThen you tune the value for “ on the validation data just like any other hyperparameter.\\n\\n≠\\n\\n8.7 Transfer Learning\\n\\nTransfer learning is probably where neural networks have a unique advantage over the\\nshallow models. In transfer learning, you pick an existing model trained on some dataset,\\nand you adapt this model to predict examples from another dataset, di\\x00erent from the one\\nthe model was built on. This second dataset is not like hold-out sets you use for validation\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cand test. It may represent some other phenomenon, or, as machine learning scientists say, it\\nmay come from another statistical distribution.\\n\\nFor example, imagine you have trained your model to recognize (and label) wild animals on a\\nbig labeled dataset. After some time, you have another problem to solve: you need to build a\\nmodel that would recognize domestic animals. With shallow learning algorithms, you do not\\nhave many options: you have to build another big labeled dataset, now for domestic animals.\\n\\nWith neural networks, the situation is much more favorable. Transfer learning in neural\\nnetworks works like this.\\n\\n1. You build a deep model on the original big dataset (wild animals).\\n2. You compile a much smaller labeled dataset for your second model (domestic animals).\\n3. You remove the last one or several layers from the ﬁrst model. Usually, these are layers\\nresponsible for the classiﬁcation or regression; they usually follow the embedding layer.\\n\\n4. You replace the removed layers with new layers adapted for your new problem.\\n5. You “freeze” the parameters of the layers remaining from the ﬁrst model.\\n6. You use your smaller labeled dataset and gradient descent to train the parameters of\\n\\nonly the new layers.\\n\\nUsually, there is an abundance of deep models for visual problems available online. You can\\nﬁnd one that has high chances to be of use for your problem, download that model, remove\\nseveral last layers (the quantity of layers to remove is a hyperparameter), put your own\\nprediction layers and train your model.\\n\\nEven if you don’t have an existing model, transfer learning can still help you in situations when\\nyour problem requires a labeled dataset very costly to obtain, but you can get another dataset\\nfor which labels are more readily available. Let’s say you build a document classiﬁcation\\nmodel. You got the taxonomy of labels from your employer, and it contains a thousand\\ncategories. In this case, you would need to pay someone to a) read, understand and memorize\\nthe di\\x00erences between categories and b) read up to a million documents and annotate them.\\nThat doesn’t sound good.\\n\\nTo save on labeling so many examples, you could consider using Wikipedia pages as the dataset\\nto build your ﬁrst model. The labels for a Wikipedia page can be obtained automatically by\\ntaking the category the Wikipedia page belongs to. Once your ﬁrst model has learned to\\npredict Wikipedia categories well, you can transfer this learning to predict the categories of\\nyour employer’s taxonomy. Usually, you will need much fewer annotated examples for your\\nemployer’s problem than you would need if you started solving your original problem from\\nscratch.\\n\\n8.8 Algorithmic E\\x00ciency\\n\\nNot all algorithms capable of solving a problem are practical. Some can be fast; some can be\\ntoo slow. Some problems can be solved by a fast algorithm, for others, no fast algorithms\\ncan exist.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0c1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\nThe subﬁeld of computer science called analysis of algorithms is concerned with determining\\nand comparing the complexity of algorithms. The big O notation is used to classify algorithms\\naccording to how their running time or space requirements grow as the input size grows.\\n\\nFor example, let’s say we have the problem of ﬁnding the two most distant one-dimensional\\nof size N . One algorithm we could craft to solve this\\nexamples in the set of examples\\nproblem would look like this (here and below, in Python):\\n\\nS\\n\\ndef find_max_distance(S):\\n\\nresult = None\\nmax_distance = 0\\nfor x1 in S:\\n\\nfor x2 in S:\\n\\nif abs(x1 - x2) >= max_distance:\\n\\nmax_distance = abs(x1 - x2)\\nresult = (x1, x2)\\n\\nreturn result\\n\\nS\\n\\n, and at every iteration of the ﬁrst loop, we\\nIn the above algorithm, we loop over all values in\\nonce again. Therefore, the above algorithm makes N 2 comparisons\\nloop over all values in\\nof numbers. If we take as a unit time the time the comparison (once), abs (twice) and\\nassignment (twice) operations take, then the time complexity (or, simply, complexity) of this\\nalgorithm is at most 5N 2. When the complexity of an algorithm is measured in the worst\\ncase, the big O notation is used. For the above algorithm, using the big O notation, we write\\nthat the algorithm’s complexity is O(N 2) (the constants, like 5, are ignored).\\n\\nS\\n\\nFor the same problem, we can craft another algorithm like this:\\n\\ndef find_max_distance(S):\\n\\nresult = None\\nmin_x = float(\"inf\")\\nmax_x = float(\"-inf\")\\nfor x in S:\\n\\nif x < min_x:\\nmin_x = x\\n\\nelif x > max_x:\\n\\nmax_x = x\\n\\nresult = (max_x, min_x)\\nreturn result\\n\\nIn the above algorithm, we loop over all values in\\nis O(N ). In this case, we say that the latter algorithm is more e\\x00cient than the former.\\n\\nonly once, so the algorithm’s complexity\\n\\nS\\n\\nUsually, an algorithm is called e\\x00cient when its complexity in big O notation is polynomial\\nin the size of the input. Therefore both O(N ) and O(N 2) are e\\x00cient. However, for very\\nlarge inputs an O(N 2) algorithm can still be very slow. In the big data era, scientists often\\nlook for O(logN ) algorithms.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cFrom a practical standpoint, when you implement your algorithm, you should avoid using\\nloops whenever possible. For example, you should use operations on matrices and vectors,\\ninstead of loops. In Python, to compute wx, you should write\\n\\n1\\n\\n2\\n\\n1\\n\\n2\\n\\n3\\n\\nimport numpy\\nwx = numpy.dot(w,x)\\n\\nand not\\n\\nwx = 0\\nfor i in range(N):\\n\\nwx += w[i]*x[i]\\n\\nUse appropriate data structures. If the order of elements in a collection doesn’t matter, use\\nset instead of list. In Python, the operation of verifying whether a speciﬁc example x belongs\\nis declared as a set and is ine\\x00cient when\\nto\\n\\nis declared as a list.\\n\\nis e\\x00cient when\\n\\nS\\n\\nS\\n\\nS\\n\\nAnother important data structure, which you can use to make your Python code more e\\x00cient\\nis dict. It is called a dictionary or a hashmap in other languages. It allows you to deﬁne a\\ncollection of key-value pairs with very fast lookups for keys.\\n\\nUnless you know exactly what you do, always prefer using popular libraries to writing your\\nown scientiﬁc code. Scientiﬁc Python packages like numpy, scipy, and scikit-learn were built\\nby experienced scientists and engineers with e\\x00ciency in mind. They have many methods\\nimplemented in the C programming language for maximum speed.\\n\\nIf you need to iterate over a vast collection of elements, use generators that create a function\\nthat returns one element at a time rather than all the elements at once.\\n\\nUse cProﬁle package in Python to ﬁnd ine\\x00ciencies in your code.\\n\\nFinally, when nothing can be improved in your code from the algorithmic perspective, you\\ncan further boost the speed of your code by using:\\n\\n• multiprocessing package to run computations in parallel, and\\n\\n• PyPy, Numba or similar tools to compile your Python code into fast, optimized machine\\n\\ncode.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c9 Unsupervised Learning\\n\\nUnsupervised learning deals with problems in which your dataset doesn’t have labels. This\\nproperty is what makes it very problematic for many practical applications. The absence\\nof labels which represent the desired behavior for your model means the absence of a solid\\nreference point to judge the quality of your model. In this book, I only present unsupervised\\nlearning methods that allow building models that can be evaluated based on data as opposed\\nto human judgment.\\n\\n9.1 Density Estimation\\n\\nDensity estimation is a problem of modeling the probability density function (pdf) of the\\nunknown probability distribution from which the dataset has been drawn. It can be useful for\\nmany applications, in particular for novelty or intrusion detection. In Chapter 7, we already\\nestimated the pdf to solve the one-class classiﬁcation problem. To do that, we decided that\\nour model would be parametric, more precisely a multivariate normal distribution (MVN).\\nThis decision was somewhat arbitrary because if the real distribution from which our dataset\\nwas drawn is di\\x00erent from the MVN, our model will be very likely far from perfect. We\\nalso know that models can be nonparametric. We used a nonparametric model in kernel\\nregression. It turns out that the same approach can work for density estimation.\\n\\nxi}\\n{\\n\\nN\\ni=1 be a one-dimensional dataset (a multi-dimensional case is similar) whose examples\\nLet\\nwere drawn from a distribution with an unknown pdf f with xi œ\\nR for all i = 1, . . . , N . We\\nare interested in modeling the shape of this function f . Our kernel model of f , let’s denote it\\nas ˆf , is given by,\\n\\nˆfb(x) =\\n\\n1\\nN b\\n\\nN\\n\\nx\\n\\nk\\n\\n≠\\nb\\n\\nxi\\n\\n,\\n\\n(1)\\n\\ni=1\\nÿ\\nwhere b is a hyperparameter that controls the tradeo\\x00 between bias and variance of our\\nmodel and k is a kernel. Again, like in Chapter 7, we use a Gaussian kernel:\\n\\n3\\n\\n4\\n\\nk(z) =\\n\\n1\\nÔ2ﬁ\\n\\nexp\\n\\nz2\\n≠\\n2\\n\\n.\\n\\n4\\n\\n3\\n\\nWe look for such a value of b that minimizes the di\\x00erence between the real shape of f and\\nthe shape of our model ˆfb. A reasonable choice of measure of this di\\x00erence is called the\\nmean integrated squared error:\\n\\nMISE(b) = E\\n\\n( ˆfb(x)\\n\\n≠\\n\\nf (x))2 dx\\n6\\n\\n.\\n\\n5 ⁄R\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n(2)\\n\\n3\\n\\n\\x0cIntuitively, you see in eq. 2 that we square the di\\x00erence between the real pdf f and our\\nN\\nmodel of it ˆf . The integral\\ni=1 we employed in the average\\nsquared error, while the expectation operator E replaces the average 1\\nN .\\n\\nreplaces the summation\\n\\nR\\n\\ns\\n\\nq\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\n(d)\\n\\nFigure 1: Kernel density estimation: (a) good ﬁt; (b) overﬁtting; (c) underﬁtting; (d) the\\ncurve of grid search for the best value for b.\\n\\nIndeed, when our loss is a function with a continuous domain, such as ( ˆfb(x)\\nf (x))2, we\\nhave to replace the summation with the integral. The expectation operation E means that\\nN\\nwe want b to be optimal for all possible realizations of our training set\\ni=1. That is\\nimportant because ˆfb is deﬁned on a ﬁnite sample of some probability distribution, while the\\nreal pdf f is deﬁned on an inﬁnite domain (the set R).\\n\\n≠\\nxi}\\n{\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cNow, we can rewrite the right-hand side term in eq. 2 like this:\\n\\nE\\n\\n5⁄R\\n\\nˆf 2\\nb (x)dx\\n6\\n\\n2E\\n\\n≠\\n\\n5⁄R\\n\\nˆfb(x)f (x)dx\\n6\\n\\n+ E\\n\\n5⁄R\\n\\nf (x)2dx\\n6\\n\\n.\\n\\nThe third term in the above summation is independent of b and thus can be ignored. An\\nˆf 2\\nb (x)dx while the unbiased estimator of\\nunbiased estimator of the ﬁrst term is given by\\nb (xi), where ˆf (i)\\nˆf (i)\\nis a kernel model of\\n\\nthe second term can be approximated by\\nf computed on our training set with the example xi excluded.\\n\\nR\\nN\\ns\\ni=1\\n\\n2\\nN\\n\\n≠\\n\\nb\\n\\nN\\ni=1\\n\\nˆf (i)\\nb (xi) is known in statistics as the leave one out estimate, a form of cross-\\nThe term\\nvalidation in which each fold consists of one example. You could have noticed that the term\\nˆfb(x)f (x)dx (let’s call it a) is the expected value of the function ˆfb, because f is a pdf. It\\n\\nR\\ncan be demonstrated that the leave one out estimate is an unbiased estimator of Ea.\\ns\\nNow, to ﬁnd the optimal value bú for b, we want to minimize the cost deﬁned as:\\n\\nq\\n\\nq\\n\\nˆf 2\\nb (x)dx\\n\\n2\\nN\\n\\n≠\\n\\n⁄R\\n\\nN\\n\\ni=1\\nÿ\\n\\nˆf (i)\\nb (xi).\\n\\nxi\\nWe can ﬁnd bú using grid search. For D-dimensional feature vectors x, the error term x\\nin eq. 1 can be replaced by the Euclidean distance\\n. In ﬁg. 1 you can see the\\nestimates for the same pdf obtained with three di\\x00erent values of b from a dataset containing\\n100 examples, as well as the grid search curve. We pick bú at the minimum of the grid search\\ncurve.\\n\\nxiÎ\\n\\nx\\nÎ\\n\\n≠\\n\\n≠\\n\\n9.2 Clustering\\n\\nClustering is a problem of learning to assign a label to examples by leveraging an unlabeled\\ndataset. Because the dataset is completely unlabeled, deciding on whether the learned model\\nis optimal is much more complicated than in supervised learning.\\n\\nThere is a variety of clustering algorithms, and, unfortunately, it’s hard to tell which one is\\nbetter in quality for your dataset. Usually, the performance of each algorithm depends on\\nthe unknown properties of the probability distribution the dataset was drawn from.\\n\\n9.2.1 K-Means\\n\\nThe k-means clustering algorithm works as follows. First, the analyst has to choose k — the\\nnumber of classes (or clusters). Then we randomly put k feature vectors, called centroids,\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cto the feature space1. We then compute the distance from each example x to each centroid c\\nusing some metric, like the Euclidean distance. Then we assign the closest centroid to each\\nexample (like if we labeled each example with a centroid id as the label). For each centroid,\\nwe calculate the average feature vector of the examples labeled with it. These average feature\\nvectors become the new locations of the centroids.\\n\\n(a) original data\\n\\n(b) iteration 1\\n\\n(c) iteration 3\\n\\n(d) iteration 5\\n\\nFigure 2: The progress of the kmeans algorithm for k = 3. The circles are two-dimensional\\nfeature vectors; the squares are moving centroids.\\n\\n1Some variants of k-means compute the initial positions of centroids based on some properties of the\\n\\ndataset.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cWe recompute the distance from each example to each centroid, modify the assignment and\\nrepeat the procedure until the assignments don’t change after the centroid locations were\\nrecomputed. The model is the list of assignments of centroids IDs to the examples.\\n\\nThe initial position of centroids inﬂuence the ﬁnal positions, so two runs of k-means can\\nresult in two di\\x00erent models. One run of the k-means algorithm is illustrated in ﬁg. 2.\\nDi\\x00erent background colors represent regions in which all points belong to the same cluster.\\n\\nThe value of k, the number of clusters, is a hyperparameter that has to be tuned by the data\\nanalyst. There are some techniques for selecting k. None of them is proven optimal. Most of\\nthem require from the analyst to make an “educated guess” by looking at some metrics or\\nby examining cluster assignments visually. Later in this chapter, we consider one technique\\nwhich allows choosing a reasonably good value for k without looking at the data and making\\nguesses.\\n\\n9.2.2 DBSCAN and HDBSCAN\\n\\nWhile k-means and similar algorithms are centroid-based, DBSCAN is a density-based\\nclustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN,\\nyou deﬁne two hyperparameters: ‘ and n. You start by picking an example x from your\\ndataset at random and assign it to cluster 1. Then you count how many examples have\\nthe distance from x less than or equal to ‘. If this quantity is greater than or equal to n,\\nthen you put all these ‘-neighbors to the same cluster 1. You then examine each member of\\ncluster 1 and ﬁnd their respective ‘-neighbors. If some member of cluster 1 has n or more\\n‘-neighbors, you expand cluster 1 by putting those ‘-neighbors to the cluster. You continue\\nexpanding cluster 1 until there are no more examples to put in it. In the latter case, you pick\\nfrom the dataset another example not belonging to any cluster and put it to cluster 2. You\\ncontinue like this until all examples either belong to some cluster or are marked as outliers.\\nAn outlier is an example whose ‘-neighborhood contains less than n examples.\\n\\nThe advantage of DBSCAN is that it can build clusters that have an arbitrary shape, while k-\\nmeans and other centroid-based algorithms create clusters that have a shape of a hypersphere.\\nAn obvious drawback of DBSCAN is that it has two hyperparameters and choosing good\\nvalues for them (especially ‘) could be challenging. Furthermore, having ‘ ﬁxed, the clustering\\nalgorithm cannot e\\x00ectively deal with clusters of varying density.\\n\\nHDBSCAN is the clustering algorithm that keeps the advantages of DBSCAN, by removing\\nthe need to decide on the value of ‘. The algorithm is capable of building clusters of\\nvarying density. HDBSCAN is an ingenious combination of multiple ideas and describing the\\nalgorithm in full is out of the scope of this book.\\n\\nHDBSCAN only has one important hyperparameter: n, that is the minimum number of\\nexamples to put in a cluster. This hyperparameter is relatively simple to choose by intuition.\\nHDBSCAN has very fast implementations: it can deal with millions of examples e\\x00ectively.\\nModern implementations of k-means are much faster than HDBSCAN though, but the qualities\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cof the latter may outweigh its drawbacks for many practical tasks. It is recommended to\\nalways try HDBSCAN on your data ﬁrst.\\n\\n9.2.3 Determining the Number of Clusters\\n\\nThe most important question is how many clusters does your dataset have? When the feature\\nvectors are one-, two- or three-dimensional, you can look at the data and see “clouds” of\\npoints in the feature space. Each cloud is a potential cluster. However, for D-dimensional\\ndata, with D > 3, looking at the data is problematic2.\\n\\nThere’s one practically useful method of determining the reasonable number of clusters based\\non the concept of prediction strength. The idea is to split the data into training and test\\nset, similarly to how we do in supervised learning. Once you have the training and test sets,\\nSte of size Nte respectively, you ﬁx k, the number of clusters, and run a\\nStr of size Ntr and\\nclustering algorithm C on sets\\nStr, k) and\\nC(\\n\\nSte and obtain the clustering results C(\\n\\nStr and\\n\\nSte, k).\\n\\nLet A be the clustering C(\\nStr, k) built using the training set. Note that the clusters in A\\ncan be deﬁned by some regions. If an example falls within one of those regions, then this\\nexample belongs to some speciﬁc cluster. For example, if we apply the k-means algorithm to\\nsome dataset, it results in a partition of the feature space into k polygonal regions, as we saw\\nin ﬁg. 2.\\nSte](i,iÕ) = 1 if and\\nDeﬁne the Nte ◊\\nonly if examples xi and xiÕ from the test set belong to the same cluster according to the\\nclustering A. Otherwise D[A,\\n\\nSte] as follows: D[A,\\n\\nNte co-membership matrix D[A,\\n\\nSte](i,iÕ) = 0.\\n\\nLet’s take a break and see what we have here. We have built, using the training set of\\nexamples, a clustering A that has k clusters. Then we have built the co-membership matrix\\nthat indicates whether two examples from the test set belong to the same cluster in A.\\n\\nIntuitively, if the quantity k is the reasonable number of clusters, then two examples that\\nbelong to the same cluster in clustering C(\\nSte, k) will most likely belong to the same cluster\\nin clustering C(\\nStr, k). On the other hand, if k is not reasonable (too high or too low), then\\ntraining data-based and test data-based clusterings will likely be less consistent.\\n\\n2Some analysts look at multiple two-dimensional scatter plots, in which only a pair of features are present\\nat the same time. It might give an intuition about the number of clusters. However, such an approach su\\x00ers\\nfrom subjectivity, is prone to error and counts as an educated guess rather than a scientiﬁc method.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0cfull dataset\\n\\ntraining set\\n\\ntest set\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 3: The clustering for k = 4: (a) training data clustering; (b) test data clustering; (c)\\ntest data plotted over the training clustering.\\n\\nThe idea is illustrated in ﬁg. 3. The plots in ﬁg. 3a and 3b show respectively C(\\nStr, 4) and\\nC(\\nSte, 4) with their respective cluster regions. Fig. 3c shows the test examples plotted over\\nthe training data cluster regions. You can see in 3c that orange test examples don’t belong\\nanymore to the same cluster according to the clustering regions obtained from the training\\ndata. This will result in many zeroes in the matrix D[A,\\nSte] which, in turn, is an indicator\\nthat k = 4 is likely not the best number of clusters.\\n\\nMore formally, the prediction strength for the number of clusters k is given by,\\n\\nps(k)\\n\\ndef\\n= min\\n\\nj=1,...,k\\n\\n1\\nAj|≠\\nAj|\\n(\\n|\\n|\\n\\n1)\\n\\nAj\\nÿi,iÕœ\\n\\nD[A,\\n\\nSte](i,iÕ),\\n\\nwhere A def\\n= C(\\nof examples in cluster Aj.\\n\\nStr, k), Aj is jth cluster from the clustering C(\\n\\nSte, k) and\\n\\nAj|\\n|\\n\\nis the number\\n\\nStr, k), for each test cluster, we compute the proportion of observation\\nGiven a clustering C(\\npairs in that cluster that are also assigned to the same cluster by the training set centroids.\\nThe prediction strength is the minimum of this quantity over the k test clusters.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cFigure 4: Predictive strength for di\\x00erent values of k for two-, three- and four-cluster data.\\n\\nExperiments suggest that a reasonable number of clusters is the largest k such that ps(k) is\\nabove 0.8. You can see in ﬁg. examples of predictive strength for di\\x00erent values of k for\\ntwo-, three- and four-cluster data.\\n\\nFor non-deterministic clustering algorithms, such as k-means, which\\ncan generate di\\x00erent clusterings depending on the initial positions\\nof centroids, it is recommended to do multiple runs of the clustering\\nalgorithm for the same k and compute the average prediction strength\\n¯ps(k) over multiple runs. Another e\\x00ective method to estimate the\\nnumber of clusters is the gap statistic method. Other, less automatic\\nmethods, which some analysts still use, include the elbow method\\nand the average silhouette method.\\n\\n9.2.4 Other Clustering Algorithms\\n\\nDBSCAN and k-means compute so-called hard clustering, in which each example can\\nbelong to only one cluster. Gaussian mixture model (GMM) allow each example to be a\\nmember of several clusters with di\\x00erent membership score (HDBSCAN allows that too,\\nby the way). Computing a GMM is very similar to doing model-based density estimation.\\nIn GMM, instead of having just one multivariate normal distribution (MND), we have a\\nweighted sum of several MNDs:\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0ck\\n\\nfX =\\n\\n„jfµj ,\\x00j ,\\n\\nj=1\\nÿ\\n\\nwhere fµj ,\\x00j is a MND j, and „j is its weight in the sum. The values of parameters µj, \\x00j,\\nand „j, for all j = 1, . . . , k are obtained using the expectation maximization algorithm\\n(EM) to optimize the maximum likelihood criterion.\\n\\nAgain, for simplicity, let us look at the one-dimensional data. Also assume that there are two\\nclusters: k = 2. In this case, we have two Gaussian distributions,\\n\\nf (x\\n\\n|\\n\\nµ1,‡ 2\\n\\n1) =\\n\\n(x\\n\\nµ1)2\\n≠\\n2‡2\\n1\\n\\ne≠\\n\\n1\\n2ﬁ‡2\\n1\\n\\nand f (x\\n\\nµ2,‡ 2\\n\\n2) =\\n\\n|\\n\\n(x\\n\\nµ2)2\\n≠\\n2‡2\\n2\\n\\n,\\n\\ne≠\\n\\n1\\n2ﬁ‡2\\n2\\n\\n(3)\\n\\nwhere f (x\\nX = x.\\n\\n|\\n\\nµ1,‡ 2\\n\\n\\uf8ff\\n1) and f (x\\n\\n|\\n\\nµ2,‡ 2\\n\\n\\uf8ff\\n2) are two parametrized pdf deﬁning the likelihood of\\n\\nWe use the EM algorithm to estimate µ1, ‡2\\n1, µ2, ‡2\\n2, „1, and „2. The parameters „1 and „2\\nof the GMM are useful for the density estimation task and less useful for the clustering task,\\nas we will see below.\\n\\nEM works like follows. In the beginning, we guess the initial values for µ1, ‡2\\nand set „1 = „2 = 1\\n\\n2 (in general, it’s 1\\n\\nk for each „k).\\n\\n1, µ2, and ‡2\\n2,\\n\\nAt each iteration of EM, the following four steps are executed:\\n\\n1. For all i = 1, . . . , N , calculate the likelihood of each xi using eq. 3:\\n\\nf (xi |\\n\\nµ1,‡ 2\\n1)\\n\\nΩ\\n\\n1\\n2ﬁ‡2\\n1\\n\\ne≠\\n\\n(xi≠\\n\\nµ1)2\\n\\n2‡2\\n1\\n\\nand f (xi |\\n\\nµ2,‡ 2)\\n\\nΩ\\n\\n1\\n2ﬁ‡2\\n2\\n\\ne≠\\n\\n(xi≠\\n\\nµ2)2\\n\\n2‡2\\n2\\n\\n.\\n\\n2. Using Bayes’ Rule, for each example xi, calculate the likelihood b(j)\\n\\n\\uf8ff\\n\\n\\uf8ff\\n\\nthat the example\\n(in other words, the likelihood that the example was drawn\\n\\ni\\n\\nbelongs to cluster j\\nœ{\\nfrom the Gaussian j):\\n\\n1, 2\\n}\\n\\nb(j)\\ni Ω\\n\\nµj,‡ 2\\nf (xi |\\nj )„j\\n1)„1 + f (xi |\\n\\nµ1,‡ 2\\n\\nf (xi |\\n\\n.\\n\\nµ2,‡ 2\\n\\n2)„2\\n\\nThe parameter „j reﬂects how likely is that our Gaussian distribution j with parameters µj\\nand “j may have produced our dataset. That is why in the beginning we set „1 = „2 = 1\\n2 : we\\ndon’t know how each of the two Gaussians is likely, and we reﬂect our ignorance by setting\\nthe likelihood of both to one half.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0citeration 1\\n\\niteration 2\\n\\niteration 10\\n\\niteration 40\\n\\nFigure 5: The progress of the Gaussian mixture model estimation using the EM algorithm\\nfor two clusters (k = 2).\\n\\n3. Compute the new values of µj and ‡2\\n\\nj , j\\n\\n1, 2\\n}\\n\\nœ{\\n\\nas,\\n\\nµj Ω\\n\\n4. Update „j, j\\n\\n1, 2\\n}\\n\\nœ{\\n\\nN\\n\\ni=1 b(j)\\ni xi\\ni=1 b(j)\\n\\nN\\n\\ni\\n\\nq\\nas,\\nq\\n\\nand ‡2\\n\\nj Ω\\n\\nN\\n\\ni=1 b(j)\\n\\ni (xi ≠\\ni=1 b(j)\\n\\nN\\n\\ni\\n\\nq\\n\\nq\\n\\nµj)2\\n\\n.\\n\\n(4)\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0c„j Ω\\n\\n1\\nN\\n\\nb(j)\\ni\\n\\n.\\n\\nN\\n\\ni=1\\nÿ\\n\\nThe steps 1\\nexample, the change is below some threshold ‘. Fig. 5 illustrates this process.\\n\\n4 are executed iteratively until the values µj and ‡2\\n\\n≠\\n\\nj don’t change much: for\\n\\nYou may have noticed that the EM algorithm is very similar to the k-means algorithm: start\\nwith random clusters, then iteratively update each cluster’s parameters by averaging the\\ndata that is assigned to that cluster. The only di\\x00erence in the case of the GMM is that the\\nassignment of an example xi to the cluster j is soft: xi belongs to cluster j with probability\\nb(j)\\nj in eq. 4 not as an average (used\\ni\\nin k-means) but as a weighted average with weights b(j)\\nOnce we have learned the parameters µj and ‡2\\nexample x in cluster j is given by f (x\\nj ).\\n\\n. This is why we calculate the new values for µj and ‡2\\n\\nj for each cluster j, the membership score of\\n\\nµj,‡ 2\\n\\n.\\n\\ni\\n\\n|\\n\\nThe extension to D-dimensional data (D > 1) is straightforward. The only di\\x00erence is\\nthat instead of the variance ‡2, we now have the covariance matrix \\x00 that parametrizes the\\nmultinomial normal distribution (MND). The advantage of GMM over k-means is that the\\nclusters in GMM can have a form of an ellipse that can have an arbitrary elongation and\\nrotation. The values in the covariance matrix control these properties.\\n\\nHow to choose k in GMM? Unfortunately, there’s no universally recognized method. What\\nis usually recommended to do is to split your dataset into training and test set. Then try\\ndi\\x00erent k and build a di\\x00erent model f k\\ntr for each k on the training data. Then choose the\\nmodel that maximizes the likelihood of examples in the test set:\\n\\narg max\\nk\\n\\nNte\\n\\ni=1\\nŸ\\n\\nf k\\ntr(xi),\\n\\nwhere Nte is the size of the test set.\\n\\nThere is a variety of clustering algorithms described in the literature.\\nWorth mentioning are spectral clustering and hierarchical cluster-\\ning. For some datasets, you may ﬁnd those more appropriate. However,\\nin most practical cases, kmeans, HDBSCAN and the Gaussian mixture\\nmodel would satisfy your needs.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n13\\n\\n\\x0c9.3 Dimensionality Reduction\\n\\nMany modern machine learning algorithms, such as ensemble algorithms and neural networks\\nhandle well very high-dimensional examples, up to millions of features. With modern\\ncomputers and graphical processing units (GPUs), dimensionality reduction techniques are\\nused much less in practice than in the past. The most frequent use case for dimensionality\\nreduction is data visualization: humans can only interpret on a plot the maximum of three\\ndimensions.\\n\\nAnother situation in which you could beneﬁt from dimensionality reduction is when you\\nhave to build an interpretable model and to do so you are limited in your choice of learning\\nalgorithms. For example, you can only use decision tree learning or linear regression. By\\nreducing your data to lower dimensionality and by ﬁguring out which quality of the original\\nexample each new feature in the reduced feature space reﬂects, one can use simpler algorithms.\\nDimensionality reduction removes redundant or highly correlated features; it also reduces the\\nnoise in the data — all that contributes to the interpretability of the model.\\n\\nThe three most widely used techniques of dimensionality reduction are principal com-\\nponent analysis (PCA), uniform manifold approximation and projection (UMAP),\\nand autoencoders.\\n\\nI already explained autoencoders in Chapter 7. You can use the low-dimensional output of the\\nbottleneck layer of the autoencoder as the vector of reduced dimensionality that represents\\nthe high-dimensional input feature vector. You know that this low-dimensional vector\\nrepresents the essential information contained in the input vector because the autoencoder is\\ncapable of reconstructing the input feature vector based on the bottleneck layer output alone.\\n\\n9.3.1 Principal Component Analysis\\n\\nPrincipal component analysis or PCA is one of the oldest methods. The math behind it\\ninvolves operation on matrices that I didn’t explain in Chapter 2, so I leave the math of\\nPCA for your further reading. Here, I only provide intuition and illustrate the method on an\\nexample.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n14\\n\\n\\x0c(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 6: PCA: (a) the original data; (b) two principal components displayed as vectors; (c)\\nthe data projected on the ﬁrst principal component.\\n\\nConsider a two-dimensional data as shown in ﬁg. 6a. Principal components are vectors that\\ndeﬁne a new coordinate system in which the ﬁrst axis goes in the direction of the highest\\nvariance in the data. The second axis is orthogonal to the ﬁrst one and goes in the direction\\nof the second highest variance in the data. If our data was three-dimensional, the third axis\\nwould be orthogonal to both the ﬁrst and the second axes and go in the direction of the third\\nhighest variance, and so on. In ﬁg. 6b, the two principal components are shown as arrows.\\nThe length of the arrow reﬂects the variance in this direction.\\n\\nNow, if we want to reduce the dimensionality of our data to Dnew < D, we pick Dnew\\nlargest principal components and project our data points on them. For our two-dimensional\\nillustration, we can set Dnew = 1 and project our examples to the ﬁrst principal component\\nto obtain the orange points in ﬁg. 6c.\\n\\nTo describe each orange point, we need only one coordinate instead\\nof two: the coordinate with respect to the ﬁrst principal component.\\nWhen our data is very high-dimensional, it often happens in practice\\nthat the ﬁrst two or three principal components account for most of the\\nvariation in the data, so by displaying the data on a 2D or 3D plot we\\ncan indeed see a very high-dimensional data and its properties.\\n\\n9.3.2 UMAP\\n\\nThe idea behind many of the modern dimensionality reduction algorithms, especially those\\ndesigned speciﬁcally for visualization purposes, such as t-SNE and UMAP, is basically\\nthe same. We ﬁrst design a similarity metric for two examples. For visualization purposes,\\nbesides the Euclidean distance between the two examples, this similarity metric often reﬂects\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n15\\n\\n\\x0csome local properties of the two examples, such as the density of other examples around\\nthem.\\n\\nIn UMAP, this similarity metric w is deﬁned as follows,\\n\\nw(xi, xj)\\n\\ndef\\n= wi(xi, xj) + wj(xj, xi)\\n\\nwi(xi, xj)wj(xj, xi).\\n\\n≠\\n\\n(5)\\n\\nThe function wi(xi, xj) is deﬁned as,\\n\\nwi(xi, xj)\\n\\ndef\\n= exp\\n\\nd(xi, xj)\\n‡i\\n\\nﬂi\\n\\n≠\\n\\n,\\n\\n4\\n\\n≠\\n\\n3\\n\\nwhere d(xi, xj) is the Euclidean distance between two examples, ﬂi is the distance from xi\\nto its closest neighbor, and ‡i is the distance from xi to its kth closest neighbor (k is a\\nhyperparameter of the algorithm).\\n\\nIt can be shown that the metric in eq. 5 varies in the range from 0 to 1 and is symmetric,\\nwhich means that w(xi, xj) = w(xj, xi).\\n\\nLet w denote the similarity of two examples in the original high dimensional space and let\\nwÕ be the similarity given by the same eq. 5 in the new low-dimensional space. Because the\\nvalues of w and wÕ lie in the range between 0 and 1, we can see them as two probability\\ndistributions. A widely used metric of similarity between two probability distributions is\\ncross-entropy:\\n\\nN\\n\\nN\\n\\nC(w, wÕ) =\\n\\nw(xi, xj) ln\\n\\ni=1\\nÿ\\n\\nj=1\\nÿ\\n\\nw(xi, xj)\\nwÕ(xÕi, xÕj) B\\n\\nA\\n\\n+ (1\\n\\n≠\\n\\nw(xi, xj)) ln\\n\\n1\\n1\\n\\n≠\\n≠\\n\\nA\\n\\nw(xi, xj)\\nwÕ(xÕi, xÕj) B\\n\\n,\\n\\n(6)\\n\\nwhere xÕ is the low-dimensional “version” of the original high-dimensional example x.\\n\\nAs you can see from eq. 6, when w(xi, xj) is similar to wÕ(xÕi, xÕj), for all pairs (i, j), then\\nC(w, wÕ) is minimized. And this is precisely what we want: for any two examples xi and\\nxj, we want their similarity metric in the original and the lower-dimensional spaces to be as\\nsimilar as possible.\\n\\nIn eq. 6 the unknown parameters are xÕi (for all i = 1, . . . , N ) and we can compute them by\\ngradient descent by minimizing C(w, wÕ).\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n16\\n\\n\\x0cPCA\\n\\nUMAP\\n\\nAutoencoder\\n\\nFigure 7: Dimensionality reduction of the MNIST dataset using three di\\x00erent techniques.\\n\\nIn ﬁg. 7, you can see the result of dimensionality reduction applied to the MNIST dataset of\\nhandwritten digits. MNIST is commonly used for benchmarking various image processing\\nsystems; it contains 70,000 labeled examples. Ten di\\x00erent colors on the plot correspond to\\nten classes. Each point on the plot corresponds a speciﬁc example in the dataset. As you can\\nsee, UMAP separates examples visually better (remember, it doesn’t have access to labels).\\nIn practice, UMAP is slightly slower than PCA but faster than autoencoder.\\n\\n9.4 Outlier Detection\\n\\nOutlier detection is the problem of detecting in the dataset the examples that are very\\ndi\\x00erent from what a typical example in the dataset looks like. We have already seen several\\ntechniques that could help to solve this problem: autoencoder and one-class classiﬁer learning.\\nIf we use autoencoder, we train it on our dataset. Then, if we want to predict whether an\\nexample is an outlier, we can use the autoencoder model to reconstruct the example from\\nthe bottleneck layer. The model will unlikely be capable of reconstructing an outlier.\\n\\nIn one-class classiﬁcation, the model either predicts that the input example belongs to the\\nclass, or it’s an outlier.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n17\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c10 Other Forms of Learning\\n\\n10.1 Metric Learning\\n\\nI mentioned that the most frequently used metrics of similarity (or dissimilarity) between\\ntwo feature vectors are Euclidean distance and cosine similarity. Such choices of metric\\nseem logical but arbitrary, just like the choice of the squared error in linear regression. The\\nfact that one metric can work better than another depending on the dataset is an indicator\\nthat none of them is perfect.\\n\\nYou can create your metric that would work better for your dataset. It’s then possible to\\nintegrate your metric into any learning algorithm that needs a metric, like k-means or kNN.\\nHow can you know, without trying all possibilities, which equation would be a good metric?\\nYou can train your metric from data.\\n\\nRemember the Euclidean distance between two feature vectors x and xÕ:\\n\\nd(x, xÕ)\\n\\ndef\\n=\\n\\n(x\\n\\n≠\\n\\nxÕ)2 =\\n\\n(x\\n\\n≠\\n\\nxÕ)(x\\n\\n≠\\n\\nxÕ).\\n\\nWe can slightly modify this metric to make it parametrizable and then learn these parameters\\nfrom data. Consider the following modiﬁcation:\\n\\n\\uf8ff\\n\\n\\uf8ff\\n\\ndA(x, xÕ) =\\n\\nx\\n\\nÎ\\n\\n≠\\n\\nxÕ\\n\\nÎA\\n\\ndef\\n=\\n\\n(x\\n\\nxÕ)€A(x\\n\\nxÕ),\\n\\n≠\\n\\n≠\\n\\nÒ\\nD matrix. Let’s say D = 3. If we let A be the identity matrix,\\n\\nwhere A is a D\\n\\n◊\\n\\n1\\n0\\nS\\n0\\nU\\nthen dA becomes the Euclidean distance. If we have a general diagonal matrix, like this,\\n\\n0\\n0\\nT\\n1\\nV\\n\\n0\\n1\\n0\\n\\ndef\\n=\\n\\nA\\n\\n,\\n\\nA\\n\\ndef\\n=\\n\\n0\\n8\\n0\\n\\n2\\n0\\nS\\n0\\nU\\n\\n,\\n\\n0\\n0\\nT\\n1\\nV\\n\\nthen di\\x00erent dimensions have di\\x00erent importance in the metric. (In the above example,\\nthe second dimension is the most important in the metric calculation.) More generally, to be\\ncalled a metric a function of two variables has to satisfy three conditions:\\n\\n1. d(x, xÕ)\\n2. d(x, xÕ)\\n3. d(x, xÕ) = d(xÕ, x)\\n\\n0\\nd(x, xÕ) + d(xÕ, z)\\n\\nØ\\nÆ\\n\\nnonnegativity\\ntriangle inequality\\nsymmetry\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0cTo satisfy the ﬁrst two conditions, the matrix A has to be positive semideﬁnite. You can\\nsee a positive semideﬁnite matrix as the generalization of the notion of a nonnegative real\\nnumber to matrices. Any positive semideﬁnite matrix M satisﬁes:\\n\\nz€Mz\\n\\n0.\\n\\nØ\\n\\nThe above property follows from the deﬁnition of a positive semideﬁnite matrix. The proof\\nthat the second condition is satisﬁed when the matrix A is positive semideﬁnite can be found\\non the book’s companion website.\\n\\nTo satisfy the third condition, we can simply take (d(x, xÕ) + d(xÕ, x))/2.\\n\\nLet’s say we have an unannotated set\\nmetric learning problem, we manually create two sets. The ﬁrst set\\nexamples (xi, xk) belongs to set\\nThe second set\\ndissimilar.\\n\\nN\\ni=1. To build the training data for our\\nis such that a pair of\\nif xi and xk are similar (from our subjective perspective).\\nif xi and xk are\\n\\nis such that a pair of examples (xi, xk) belongs to set\\n\\nxi}\\n\\n=\\n\\nD\\n\\nD\\n\\nX\\n\\nS\\n\\nS\\n\\n{\\n\\nTo train the matrix of parameters A from the data, we want to ﬁnd a positive semideﬁnite\\nmatrix A that solves the following optimization problem:\\n\\nmin\\nA\\n\\nÿ(xi,xk)\\n\\nœS\\n\\nx\\n\\nÎ\\n\\n≠\\n\\nxÕ\\n\\n2\\nA such that\\nÎ\\n\\nÿ(xi,xk)\\n\\nœD\\n\\nx\\nÎ\\n\\n≠\\n\\nxÕ\\n\\nÎA Ø\\n\\nc,\\n\\nwhere c is a positive constant (can be any number).\\n\\nThe solution to this optimization problem is found by gradient descent\\nwith a modiﬁcation that ensures that the found matrix A is positive\\nsemideﬁnite. We leave the description of the algorithm out of the scope\\nof this book for further reading. You should know that there are many\\nother ways to learn a metric, including non-linear and kernel-based.\\nHowever, the one presented in this book gives a good result for most\\npractical problems.\\n\\n10.2 Learning to Rank\\n\\nLearning to rank is a supervised learning problem. Among others, one frequent problem\\nsolved using learning to rank is the optimization of search results returned by a search engine\\nfor a query. In search result ranking optimization, a labeled example\\nXi in the training set\\nof size N is a ranked collection of documents of size ri (labels are ranks of documents). A\\nfeature vector represents each document in the collection. The goal of the learning is to ﬁnd\\na ranking function f which outputs values that can be used to rank documents. For each\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0ctraining example, an ideal function f would output values that induce the same ranking of\\ndocuments as given by labels.\\n\\nis a collection of feature vectors with labels:\\n\\nXi, i = 1, . . . , N ,\\ni,j could represent how recent is the document, x(2)\\n\\nEach example\\nXi =\\nri\\nj=1. Features in a feature vector xi,j represent the document j = 1, . . . , ri.\\n(xi,j, yi,j)\\n{\\n}\\nFor example, x(1)\\ni,j would reﬂect whether the\\nwords of the query can be found in the document title, x(3)\\ni,j could represent the size of the\\ndocument, and so on. The label yi,j could be the rank (1, 2, . . . , ri) or a score. For example,\\nthe lower the score, the higher the document should be ranked.\\n\\nThere are three principal approaches to solve such a learning problem: pointwise, pairwise,\\nand listwise.\\n\\nPointwise approach transforms each training example into multiple examples: one example\\nper document. The learning problem becomes a standard supervised learning problem, either\\nregression or logistic regression. In each example (x, y) of the pointwise learning problem,\\nx is the feature vector of some document, and y is the original score (if yi,j is a score) or a\\nsynthetic score obtained from the ranking (the higher the rank, the lower is the synthetic\\nscore). Any supervised learning algorithm can be used in this case. The solution is usually\\nfar from perfect. Principally, this is because each document is considered in isolation, while\\nthe original ranking (given by the labels yi,j of the original training set) could optimize the\\npositions of the whole set of documents. For example, if we have already given a high rank to\\na Wikipedia page in some collection of documents, we would not give a high rank to another\\nWikipedia page for the same query.\\n\\nIn the pairwise approach, the problem also considers documents in isolation, however, in this\\ncase, a pair of documents is considered at once. Given a pair of documents (xi, xk) we want\\nto build a model f that, given (xi, xk) as input, outputs a value close to 1 if xi has to be put\\nhigher than xk in the ranking. Otherwise, f outputs a value close to 0. At the test time,\\ngiven a model, the ﬁnal ranking for an unlabeled example\\nis obtained by aggregating the\\n. Such an approach works better than pointwise,\\npredictions for all pairs of documents in\\nbut still far from perfect.\\n\\nX\\n\\nX\\n\\nThe state of the art rank learning algorithms, such as LambdaMART, implement the\\nlistwise approach. In the listwise approach, we try to optimize the model directly on some\\nmetric that reﬂects the quality of ranking. There are various metrics for assessing search\\nengine result ranking, including precision and recall. One popular metric that combines both\\nprecision and recall is called mean average precision (MAP).\\n\\nTo deﬁne MAP, let us ask judges (Google call those people rankers) to examine a collection\\nof search results for a query and assign relevancy labels to each search result. Labels could\\nbe binary (1 for “relevant” and 0 for “irrelevant”) or on some scale, say from 1 to 5: the\\nhigher the value, the more relevant the document is to the search query. Let our judges build\\nsuch relevancy labeling for a collection of 100 queries. Now, let us test our ranking model on\\nthis collection. The precision of our model for some query is given by:\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cprecision = |{\\n\\nrelevant documents\\n\\nretrieved documents\\n\\n}ﬂ{\\nretrieved documents\\n\\n}|\\n\\n|{\\n\\n}|\\n\\n,\\n\\nwhere the notation\\ndeﬁned for a ranked collection of documents returned by a search engine for a query q as,\\n\\nmeans “the number of.” The average precision metric, AveP, is\\n\\n| · |\\n\\nAveP(q) =\\n\\nn\\nk=1(P (k)\\n\\nrel(k))\\nrelevant documents\\nq\\n|{\\n\\n◊\\n\\n}|\\n\\n,\\n\\nwhere n is the number of retrieved documents, P (k) denotes the precision computed for\\nthe top k search results returned by our ranking model for the query, rel(k) is an indicator\\nfunction equaling 1 if the item at rank k is a relevant document (according to judges) and\\nzero otherwise. Finally, the MAP for a collection of search queries of size Q is given by,\\n\\nMAP =\\n\\nQ\\nq=1 AveP(q)\\nQ\\n\\nq\\n\\n.\\n\\nNow we get back to LambdaMART. This algorithm implements a pairwise approach, and it\\nuses gradient boosting to train the ranking function h(x). Then the binary model f (xi, xk)\\nthat predicts whether the document xi should have a higher rank than the document xk (for\\nthe same search query) is given by a sigmoid with a hyperparameter –,\\n\\nf (xi, xk)\\n\\ndef\\n=\\n\\n1\\n1 + exp((h(xi)\\n\\n.\\n\\nh(xk))–\\n\\n≠\\n\\nAgain, as with many models that predict probability, the cost function is cross-entropy\\ncomputed using the model f . In our gradient boosting, we combine multiple regression trees\\nto build the function h by trying to minimize the cost. Remember that in gradient boosting\\nwe add a tree to the model to reduce the error that the current model makes on the training\\ndata. For the classiﬁcation problem, we computed the derivative of the cost function to\\nreplace real labels of training examples with these derivatives. LambdaMART works similarly,\\nwith one exception. It replaces the real gradient with a combination of the gradient and\\nanother factor that depends on the metric, such as MAP. This factor modiﬁes the original\\ngradient by increasing or decreasing it so that the metric value is improved.\\n\\nThat is a very bright idea and not many supervised learning algorithms can boast that they\\noptimize a metric directly. Optimizing a metric is what we really want, but what we do in a\\ntypical supervised learning algorithm is we optimize the cost instead of the metric. Usually,\\nin supervised learning, as soon as we have found a model that optimizes the cost function, we\\ntry to tweak hyperparameters to improve the value of the metric. LambdaMART optimizes\\nthe metric directly.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0cThe remaining question is how do we build the ranked list of results based on the predictions\\nof the model f which predicts whether its ﬁrst input has to be ranked higher than the second\\ninput. It’s generally a computationally hard problem, and there are multiple implementations\\nof rankers capable of transforming pairwise comparisons into a ranking list. The most\\nstraightforward approach is to use an existing sorting algorithm.\\n\\nSorting algorithms sort a collection of numbers in increasing or decreas-\\ning order. (The simplest sorting algorithm is called bubble sort. It’s\\nusually taught in engineering schools.) Typically, sorting algorithms\\niteratively compare a pair of numbers in the collection and change their\\npositions in the list based on the result of that comparison. If we plug\\nour function f into a sorting algorithm to execute this comparison, the\\nsorting algorithm will sort documents and not numbers.\\n\\n10.3 Learning to Recommend\\n\\nLeaning to recommend is an approach to build recommender systems. Usually, we have a\\nuser who consumes some content. We have the history of consumption, and we want to\\nsuggest this user new content that the user would like. It could be a movie on Netﬂix or a\\nbook on Amazon.\\n\\nTraditionally, two approaches were used to give recommendations: content-based ﬁltering\\nand collaborative ﬁltering.\\n\\nContent-based ﬁltering is based on learning what do users like based on the description of\\nthe content they consume. For example, if the user of a news site often reads news articles on\\nscience and technology, then we would suggest to this user more documents on science and\\ntechnology. More generally, we could create one training set per user and add news articles\\nto this dataset as a feature vector x and whether the user recently read this news article as a\\nlabel y. Then we build the model of each user and can regularly examine each new piece of\\ncontent to determine whether a speciﬁc user would read it or not.\\n\\nThe content-based approach has many limitations. For example, the user can be trapped in\\nthe so-called ﬁlter bubble: the system will always suggest to that user the information that\\nlooks very similar to what user already consumed. That could result in complete isolation of\\nthe user from information that disagrees with their viewpoints or expands them. On a more\\npractical side, the users might just get recommendations of items they already know about,\\nwhich is undesirable.\\n\\nCollaborative ﬁltering has a signiﬁcant advantage over content-based ﬁltering: the recommen-\\ndations to one user are computed based on what other users consume or rate. For instance,\\nif two users gave high ratings to the same ten movies, then it’s more likely that user 1 will\\nappreciate new movies recommended based on the tastes of the user 2 and vice versa. The\\ndrawback of this approach is that the content of the recommended items is ignored.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n7\\n\\n\\x0cIn collaborative ﬁltering, the information on user preferences is organized in a matrix. Each\\nrow corresponds to a user, and each column corresponds to a piece of content that user rated\\nor consumed. Usually, this matrix is huge and extremely sparse, which means that most of\\nits cells aren’t ﬁlled (or ﬁlled with a zero). The reason for such a sparsity is that most users\\nconsume or rate just a tiny fraction of available content items. It’s is very hard to make\\nmeaningful recommendations based on such sparse data.\\n\\nMost real-world recommender systems use a hybrid approach: they combine recommendations\\nobtained by the content-based and collaborative ﬁltering models.\\n\\nI already mentioned that content-based recommender model could be built using a classiﬁca-\\ntion or regression model that predicts whether a user will like the content based on content’s\\nfeatures. Examples of features could include the words in books or news articles the user\\nliked, the price, the recency of the content, the identity of the content author and so on.\\n\\nTwo e\\x00ective collaborative-ﬁltering learning algorithms are factorization machines (FM)\\nand denoising autoencoders (DAE).\\n\\n10.3.1 Factorization Machines\\n\\nFactorization machines is a relatively new kind of algorithm. It was explicitly designed for\\nsparse datasets. Let’s illustrate the problem.\\n\\nuser\\n\\nmovie\\n\\nrated\\xa0movies\\n\\nEd Al Zak\\n\\nx1\\n\\nx2\\n\\nx3\\n\\nx(1)\\n\\nx(2)\\n\\nx(3)\\n\\nx(4)\\n\\nx(5)\\n\\nx(6)\\n\\n...\\n\\nx(D)\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n...\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n...\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n...\\n\\n0\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\nIt Up Jaws Her\\n\\nIt Up Jaws Her\\n\\nx21\\n\\nx22\\n\\nx23\\n\\nx24\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n...\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n...\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n...\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n...\\n\\n0\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\nx40\\n\\nx41\\n\\nx42 x43\\n\\n0.2\\n\\n0.8 0.4\\n\\n0.2\\n\\n0.8\\n\\n0.4\\n\\n0\\n\\n0\\n\\n0.2\\n\\n0.8\\xa0 0.4\\n\\n0.7\\n\\n0\\n\\n0\\n\\n0.8\\n\\n...\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n...\\n\\n0\\n\\n0.7\\n\\n0.1\\n\\n0.7\\n\\n0.1\\n\\n0\\n\\n...\\n\\n1\\n\\n0.6\\n\\n...\\n\\n0\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\n...\\n\\nx99 x100\\n\\n0.3\\n\\n0.8\\n\\n0.3\\n\\n0.8\\n\\n0.3\\n\\n0.8\\xa0\\n\\n0.35\\n\\n0.78\\n\\n0.35\\n\\n0.78\\n\\n0.5\\n\\n0.77\\n\\n...\\n\\n...\\n\\n0.95\\n\\n0.85\\n\\ny \\n\\n1\\n\\n3\\n\\n2\\n\\n3\\n\\n1\\n\\n4\\n\\n...\\n\\n5\\n\\ny(1)\\n\\ny(2)\\n\\ny(3)\\n\\ny(4)\\n\\ny(5)\\n\\ny(6)\\n\\n...\\n\\ny(D)\\n\\nFigure 1: Example for sparse feature vectors x and their respective labels y.\\n\\nIn ﬁg. 1 you see an example of sparse feature vectors with labels. Each feature vector\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n8\\n\\n\\x0crepresents information about one speciﬁc user and one speciﬁc movie. Features in the blue\\nsection represent a user. Users are encoded as one-hot vectors. Features in the green section\\nrepresent a movie. Movies are also encoded as one-hot vectors. Features in the yellow section\\nrepresent scores the user in green gave to each movie they rated. Feature x99 represents\\nthe ratio of movies with an Oscar among those the speciﬁc user has watched. Feature x100\\nrepresents the percentage of the movie watched by the user in blue before they scored the\\nmovie in green. The target y represents the score given by the user in blue to the movie in\\ngreen.\\n\\nIn real recommender systems, the number of users can count in millions, so the matrix in\\nﬁg. 1 would count hundreds of millions of rows. The number of features could be hundreds\\nof thousands, depending on how reach is the choice of content and how creative you, as a\\ndata analyst, are in feature engineering. Features x99 and x100 were handcrafted during the\\nfeature engineering process, and I only show two features for the illustration purposes.\\n\\nTrying to ﬁt a regression or classiﬁcation model to such an extremely sparse dataset would in\\npractice result in very poor generalization. Factorization machines approach this problem in\\na di\\x00erent way.\\n\\nThe factorization machine model is deﬁned as follows:\\n\\nf (x)\\n\\ndef\\n= b +\\n\\nD\\n\\nD\\n\\nD\\n\\nwixi +\\n\\n(vivj)xixj,\\n\\ni=1\\nÿ\\n\\ni=1\\nÿ\\n\\nj=i+1\\nÿ\\n\\nwhere b and wi, i = 1, . . . , D are scalar parameters similar to those used in linear regression.\\nVectors vi, i = 1, . . . , D, are k-dimensional vectors of factors. k is a hyperparameter and\\nis usually much smaller than D. The expression (vivj) is a dot-product of the ith and\\njth vectors of factors. As you can see, instead of trying to ﬁnd just one wide vector of\\nparameters which can reﬂect poorly interactions between features because of sparsity, we\\ncomplete it by additional parameters that apply to pairwise interactions xixj between\\nfeatures. However, instead of having a parameter wi,j for each interaction, which would add\\nan enormous1 quantity of new parameters to the model, we factorize wi,j into vivj by adding\\nonly Dk\\n\\n1) parameters to the model2.\\n\\nD(D\\n\\nπ\\n\\n≠\\n\\nDepending on the problem, the loss function could be squared error loss (for regression) or\\nhinge loss. For classiﬁcation with y\\n, with hinge loss or logistic loss the prediction\\nis made as y = sign(f (x)). The logistic loss is deﬁned as,\\n\\n1, +1\\n}\\n\\nœ {≠\\n\\nloss(f (x), y) =\\n\\n1\\nln 2\\n\\nln(1 + e≠\\n\\nyf (x)).\\n\\nGradient descent can be used to optimize the average loss. In the example in ﬁg. 1, the\\n, so it’s a multiclass problem. We can use one versus rest strategy\\nlabels are in\\n}\\n\\n1, 2, 3, 4, 5\\n{\\n\\n1To be more precise we would add D(D\\n2The notation\\n\\n≠\\nmeans “much less than.”\\n\\nπ\\n\\n1) parameters wi,j .\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n9\\n\\n\\x0cto convert this multiclass problem into ﬁve binary classiﬁcation problems.\\n\\n10.3.2 Denoising Autoencoders\\n\\nFrom Chapter 7, you know what a denoising autoencoder is:\\nit’s a neural network that\\nreconstructs its input from the bottleneck layer. The fact that the input is corrupted by\\nnoise while the output shouldn’t be, makes denoising autoencoders an ideal tool to build a\\nrecommender model.\\n\\nThe idea is very straightforward: new movies a user could like are seen as if they were\\nremoved from the complete set of preferred movies by some corruption process. The goal of\\nthe denoising autoencoder is to reconstruct those removed items.\\n\\nTo prepare the training set for our denoising autoencoder, remove the blue and green features\\nfrom the training set in ﬁg. 1. Because now some examples become duplicates, keep only the\\nunique ones.\\n\\nAt the training time, randomly replace some of the non-zero yellow features in the input\\nfeature vectors with zeros. Train the autoencoder to reconstruct the uncorrupted input.\\n\\nAt the prediction time, build a feature vector for the user. The feature vector will include\\nuncorrupted yellow features as well as the handcrafted features like x99 and x100. Use the\\ntrained DAE model to reconstruct the uncorrupted input. Recommend to the user movies\\nthat have the highest scores at the model’s output.\\n\\nAnother e\\x00ective collaborative-ﬁltering model is a feed-forward neural\\nnetwork with two inputs and one output. Remember from Chapter 8\\nthat neural networks are good at handling multiple simultaneous inputs.\\nA training example here is a triplet (u, m, r). The input vector u is a\\none-hot encoding of a user. The second input vector m is a one-hot\\nencoding of a movie. The output layer could be either a sigmoid (in\\nwhich case the label r is in [0, 1]) or ReLU, in which case r can be in\\nsome typical range, [1, 5] for example.\\n\\n10.4 Self-Supervised Learning: Word Embeddings\\n\\nWe have already discussed word embeddings in Chapter 7. Recall that word embeddings are\\nfeature vectors that represent words. They have the property that similar words have similar\\nword vectors. The question that you probably want to ask is where these word embedding\\ncome from. The answer is: they are learned from data.\\n\\nThere are many algorithms to learn word embeddings. Here, we consider in detail only one of\\nthem: word2vec, and only one version of word2vec called skip-gram, which works very well\\nin practice. Pretrained word2vec embeddings for many languages are available to download\\nonline.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n10\\n\\n\\x0cIn word embedding learning, our goal is to build a model which we can use to convert a\\none-hot encoding of a word into a word embedding. Let our dictionary contain 10000 words.\\nThe one-hot vector for each word is a 10000-dimensional vector of all zeroes except for one\\ndimension that contains a 1. Di\\x00erent words have 1 in di\\x00erent dimensions.\\n\\nConsider a sentence: “I almost ﬁnished reading the book on machine learning.” Now, consider\\nthe same sentence from which we have removed some word, say “book.” Our sentence becomes:\\non machine learning.” Now let’s only keep the three words\\n“I almost ﬁnished reading the\\non machine learning.” Looking\\nbefore the\\nat this seven-word window around the\\nstands for, you would\\nprobably say: “book,” “article,” or “paper.” That is how the context words let you predict\\nthe word they surround. It’s also how the machine can discover that words “book,” “paper,”\\nand “article” have a similar meaning: because they share similar contexts in multiple texts.\\n\\n, if I ask you to guess what\\n·\\n\\nand three words after it: “ﬁnished reading the\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\nIt turns out that it works the other way around too: a word can predict the context that\\nsurrounds it. The piece “ﬁnished reading the\\non machine learning” is called a skip-gram\\nwith window size 7 (3 + 1 + 3). By using the documents available on the Web, we can easily\\ncreate hundreds of millions of skip-grams.\\n\\n·\\n\\n1, x, x+1, x+2, x+3]. In our above\\nLet’s denote words in a skip-gram like this: [x\\nexample of the sentence, x\\n2 corresponds to “reading,”\\nx is the skipped word (\\n), x+1 is “on” and so on. A skip-gram with window size 5 will look\\n·\\nlike this: [x\\n\\n3 is the one-hot vector for “ﬁnished,” x\\n\\n1, x, x+1, x+2].\\n\\n3, x\\n\\n2, x\\n\\n2, x\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\n≠\\n\\nThe skip-gram model with window size 5 is schematically depicted in ﬁg. 2. It is a fully-\\nconnected network, like the multilayer perceptron. The input word is the one denoted as\\nin\\nour skip-gram. The neural network has to learn to predict the context words of the skip-gram\\ngiven the central word.\\n\\n·\\n\\nYou can see now why the learning of this kind is called self-supervised: the labeled examples\\nget extracted from the unlabeled data such as text.\\n\\nThe activation function used in the output layer is softmax. The cost function is the negative\\nlog-likelihood. The embedding for a word is obtained as the output of the embedding layer\\nwhen the one-hot encoding of this word is given as the input to the model.\\n\\nBecause of the large number of parameters in the word2vec models, two\\ntechniques are used to make the computation more e\\x00cient: hierarchical\\nsoftmax (an e\\x00cient way of computing softmax that consists in repre-\\nsenting the outputs of softmax as leaves of a binary tree) and negative\\nsampling (essentially, the idea is only to update a random sample of all\\noutputs per iteration of gradient descent). We leave these for further\\nreading.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n11\\n\\n\\x0cinput\\nword\\n\\n1\\n\\n0\\n\\n0\\n\\n...\\n\\n1\\n\\n...\\n\\n0\\n\\n0\\n\\n0\\n\\nx\\n\\ninput\\nlayer\\n\\n1\\n\\n...\\n\\n...\\n\\nembedding\\nlayer\\n\\n1\\n\\n...\\n\\n300\\n\\n10000\\n\\n10000\\n\\noutput\\nlayer\\n\\n1\\n\\n...\\n\\n.\\n.\\n.\\n\\nx\\xad2\\n\\n10000\\n\\n1\\n\\n...\\n\\n.\\n.\\n.\\n\\nx\\xad1\\n\\n10000\\n\\n1\\n\\n...\\n\\n.\\n.\\n.\\n\\nx+1\\n\\n10000\\n\\n1\\n\\n...\\n\\n.\\n.\\n.\\n\\nx+2\\n\\n10000\\n\\nFigure 2: The skip-gram model with window size 5 and the embedding layer of 300 units.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n12\\n\\n\\x0cTheHundred-PageMachineLearningBookAndriy Burkov\\x0c“All models are wrong, but some are useful.”\\n— George Box\\n\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n\\x0c11 Conclusion\\n\\nWow, that was fast! You are really good if you got here and managed to understand most of\\nthe book’s material.\\n\\nIf you look at the number at the bottom of this page, you see that we have overspent paper,\\nwhich means that the title of the book was slightly misleading. I hope that you forgive me\\nthis little marketing trick. After all, if I wanted to make this book exactly a hundred pages, I\\ncould reduce font size, white margins, and line spacing, or remove the section on UMAP and\\nleave you on your own with the original paper. Believe me: you would not want to be left\\nalone with the original paper on UMAP!\\n\\nHowever, by stopping now, I feel conﬁdent that you have got everything you need to become\\na great modern data analyst or machine learning engineer. That doesn’t mean that I covered\\neverything, but what I covered in a hundred pages you would ﬁnd in a bunch of books, each\\nthousand-page thick. Much of what I covered is not in the books at all: typical machine\\nlearning books are conservative and academic, while I emphasize those algorithms and\\nmethods that you will ﬁnd useful in your day to day work.\\n\\nWhat exactly I didn’t cover, but would have covered if it was a thousand-page machine\\nlearning book?\\n\\n11.1 Topic Modeling\\n\\nIn text analysis, topic modeling is a prevalent unsupervised learning problem. You have a\\ncollection of text documents, and you would like to discover topics present in each document.\\nLatent Dirichlet Allocation (LDA) is a very e\\x00ective algorithm of topic discovery. You\\ndecide how many topics are present in your collection of documents and the algorithm assigns\\na topic to each word in this collection. Then, to extract the topics from a document, you\\nsimply count how many words of each topic are present in that document.\\n\\n11.2 Gaussian Processes\\n\\nGaussian processes (GP) is a supervised learning method that competes with kernel\\nregression. It has some advantages over the latter. For example, it provides conﬁdence\\nintervals for the regression line in each point. I decided not to explain GP because I could\\nnot ﬁgure out a simple way to explain them, but you deﬁnitely could spend some time to\\nlearn about GP. It will be time well spent.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n3\\n\\n\\x0c11.3 Generalized Linear Models\\n\\nGeneralized linear model (GLM) is a generalization of the linear regression to modeling\\nvarious forms of dependency between the input feature vector and the target. Logistic\\nregression, for instance, is one form of GLMs. If you are interested in regression and you\\nlook for simple and explainable models, you should deﬁnitely read more on GLM.\\n\\n11.4 Probabilistic Graphical Models\\n\\nWe have mentioned one example of probabilistic graphical models (PGMs) in Chapter 7:\\nconditional random ﬁelds (CRF). With CRF we can model the input sequence of words\\nand relationships between the features and labels in this sequence as a sequential dependency\\ngraph. More generally, a PGM can be any graph. A graph is a structure consisting of a\\ncollection of nodes and edges that join a pair of nodes. Each node in PGM represents some\\nrandom variable (values of which can be observed or unobserved), and edges represent the\\nconditional dependence of one random variable on another random variable. For example,\\nthe random variable “sidewalk wetness” depends on the random variable “weather condition.”\\nBy observing values of some random variables, an optimization algorithm can learn from\\ndata the dependency between observed and unobserved variables.\\n\\nPGMs allow the data analyst to see how the values of one feature depend on the values of\\nother features. If the edges of the dependency graph are directed, it becomes possible to infer\\ncausality. Unfortunately, constructing such models by hand require a substantial amount of\\ndomain expertise and a strong understanding of probability theory and statistics. The latter\\nis often a problem for many domain experts. Some algorithms allow learning the structure\\nof dependency graphs from data, but the learned models are often hard to interpret by a\\nhuman and thus they aren’t beneﬁcial for understanding complex probabilistic processes that\\ngenerated the data. CRF is by far the most used PGM with applications mostly in text and\\nimage processing. However, in these two domains, they were surpassed by neural networks.\\nAnother graphical model, hidden Markov model or HMM, in the past was frequently used\\nin speech recognition, time series analysis, and other temporal inference tasks, but, again\\nHMM lost to neural networks.\\n\\nPGMs are also known under names of Bayesian networks, belief networks, and probabilistic\\nindependence networks.\\n\\n11.5 Markov Chain Monte Carlo\\n\\nIf you work with graphical models and want to sample examples from a very complex\\ndistribution deﬁned by the dependency graph, you could use Markov chain Monte Carlo\\n(MCMC) algorithms. MCMC is a class of algorithms for sampling from any probability\\ndistribution deﬁned mathematically. Remember that when we talked about the denoising\\nautoencoder, we sampled the noise from the normal distribution. Sampling from standard\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n4\\n\\n\\x0cdistributions, such as normal or uniform, is relatively easy because their properties are well\\nknown. However, the task of sampling becomes signiﬁcantly more complicated when the\\nprobability distribution can have an arbitrary form deﬁned by a dependency graph learned\\nfrom data.\\n\\n11.6 Genetic Algorithms\\n\\nGenetic algorithms (GA) are a numerical optimization technique used to optimize undif-\\nferentiable optimization objective functions. They use concepts from evolutionary biology\\nto search for a global optimum (minimum or maximum) of an optimization problem, by\\nmimicking evolutionary biological processes.\\n\\nGA work by starting with an initial generation of candidate solutions. If we look for optimal\\nvalues of the parameters of our model, we ﬁrst randomly generate multiple combinations of\\nparameter values. We then test each combination of parameter values against the objective\\nfunction. Imagine each combination of parameter values as a point in a multi-dimensional\\nspace. We then generate a subsequent generation of points from the previous generation by\\napplying such concepts as “selection,” “crossover,” and “mutation.”\\n\\nIn a nutshell, this results in each new generation keeping more points similar to those points\\nfrom the previous generation that performed the best against the objective. In the new\\ngeneration, the points that performed the worst in the previous generation are replaced by\\n“mutations” and “crossovers” of the points that performed the best. A mutation of a point is\\nobtained by a random distortion of some attributes of the original point. A crossover is a\\ncertain combination of several points (for example, an average).\\n\\nGenetic algorithms allow ﬁnding solutions to any measurable optimization criteria. For\\nexample, GA can be used to optimize the hyperparameters of a learning algorithm. They are\\ntypically much slower than gradient-based optimization techniques.\\n\\n11.7 Reinforcement Learning\\n\\nAs we already discussed, reinforcement learning (RL) solves a very speciﬁc kind of problems\\nwhere the decision making is sequential. Usually, there’s an agent acting in an unknown\\nenvironment. Each action brings a reward and moves the agent to another state of the\\nenvironment (usually, as a result of some random process with unknown properties). The\\ngoal of the agent is to optimize its long-term reward.\\n\\nReinforcement learning algorithms, such as Q-learning, as well as its neural network based\\ncounterparts, are used in learning to play video games, robotic navigation and coordination,\\ninventory and supply chain management, optimization of complex electric power systems\\n(power grids), and learning ﬁnancial trading strategies.\\n\\nú ú ú\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n5\\n\\n\\x0cThe book stops here. Don’t forget to occasionally visit the book’s companion wiki to stay\\nupdated on new developments in each machine learning area considered in the book. As I\\nsaid in Preface, this book, thanks to the constantly updated wiki, like a good wine keeps\\ngetting better after you buy it. Oh, and don’t forget that the book is distributed on the read\\nﬁrst, buy later principle. That means that if while reading these words you look at a digital\\nscreen, you are probably the right person for buying this book.\\n\\nAndriy Burkov\\n\\nThe Hundred-Page Machine Learning Book - Draft\\n\\n6\\n\\n\\x0c'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b0ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset1 = extract_text(\"D:\\\\Learnbay\\\\Books & Reference Material\\\\Books\\\\Introduction to ML ops.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa90087d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\x0cMastering MLOps \\nwith Dataiku\\n\\nDataiku is the only platform that provides one simple, consistent UI \\nfor data connection, wrangling, mining, visualization, machine \\nlearning, deployment, and model monitoring, all at enterprise scale.\\n\\nKEY FEATURES FOR A SCALABLE MLOPS STRATEGY INCLUDE:\\n\\n1\\n\\n2\\n\\nModel input drift detection that looks at the recent data the model has had to score and statistically \\ncompares it with the data on which the model was evaluated.\\n\\nEasier creation of validation feedback loops via Dataiku Evaluation Recipes to compute the true \\nperformance of a saved model against a new validation dataset, plus automated retraining and \\nredeployment.\\n\\n Everyday AI, \\n\\n3\\n\\nDashboard interfaces dedicated to the monitoring of global pipelines.\\n\\n4\\n\\n...and more! See MLOps features with Dataiku in action at dataiku.com\\n\\n Extraordinary People \\n\\nElastic Architecture Built for the Cloud\\n\\nMachine Learning\\n\\nVisualization\\n\\nData Preparation\\n\\nAge\\n\\nInteger\\n\\n22\\n\\n38\\n\\n26\\n\\n35\\n\\n35\\n\\n29\\n\\nName\\n\\nNatural lang.\\n\\nBraund, Mr. Owen Harris\\n\\nMoran, Mr. James\\n\\nHeikkinen, Miss. Laina\\n\\nRemove rows containing Mr.\\n\\nSex\\n\\nGender\\n\\nmale\\n\\nmale\\n\\nfemale\\n\\nFutrelle, Mrs. Jacques Heath\\n\\nKeep only rows containing Mr.\\n\\nfemale\\n\\nAllen, Mr. William Henry\\n\\nMcCarthy, Mr. Robert\\n\\nSplit column on Mr.\\n\\nHewlett, Mrs (Mary D Kingcome)\\n\\nReplace Mr. by ...\\n\\nmale\\n\\nmale\\n\\nRemove rows equal to Moran, Mr. James\\n\\nKeep only rows equal to Moran, Mr. James\\n\\nClear cells equal to Moran, Mr. James\\n\\nFilter on Moran, Mr. James\\n\\nFilter on Mr.\\n\\nToggle row highlight\\n\\nShow complete value\\n\\nDataOps\\n\\nGovernance & MLOps\\n\\nApplications\\n\\n450+\\n\\nCUSTOMERS\\n\\n45,000+\\n\\nACTIVE USERS\\n\\n©Dataiku2021 | dataiku.com\\n\\nDataiku is the world’s leading platform for Everyday AI, systemizing the use of data for \\n\\nexceptional business results. Organizations that use Dataiku elevate their people (whether \\n\\ntechnical and working in code or on the business side and low- or no-code) to extraordinary, \\n\\narming them with the ability to make better day-to-day decisions with data.\\n\\n©2021 dataiku | dataiku.com\\n\\n\\x0cIntroducing MLOps\\nHow to Scale Machine Learning in the Enterprise\\n\\nMark Treveil and the Dataiku Team\\n\\nBeijing\\nBeijing\\n\\nBoston\\nBoston\\n\\nFarnham Sebastopol\\nFarnham Sebastopol\\n\\nTokyo\\nTokyo\\n\\n\\x0cIntroducing MLOps\\nby Mark Treveil, and the Dataiku Team\\n\\nCopyright © 2020 Dataiku. All rights reserved.\\n\\nPrinted in the United States of America.\\n\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\n\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\n\\nIndexer: Ellen Troutman-Zaig\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\n\\nAcquisitions Editor: Rebecca Novack\\nDevelopment Editor: Angela Rufino\\nProduction Editor: Katherine Tozer\\nCopyeditor: Penelope Perkins\\nProofreader: Kim Wimpsett\\n\\nDecember 2020:\\n\\n First Edition\\n\\nRevision History for the First Edition\\n2020-11-30:  First Release\\n\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492083290 for release details.\\n\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introducing MLOps, the cover image,\\nand related trade dress are trademarks of O’Reilly Media, Inc.\\n\\nThe  views  expressed  in  this  work  are  those  of  the  authors,  and  do  not  represent  the  publisher’s  views.\\nWhile  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\\nlicenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\\nthereof complies with such licenses and/or rights.\\n\\nThis  work  is  part  of  a  collaboration  between  O’Reilly  and  Dataiku.  See  our  statement  of  editorial\\nindependence.\\n\\n978-1-492-08330-6\\n\\n[LSI]\\n\\n\\x0cTable of Contents\\n\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   ix\\n\\nPart I.  MLOps: What and Why\\n\\n1. Why Now and Challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nDefining MLOps and Its Challenges                                                                             4\\nMLOps to Mitigate Risk                                                                                                  7\\nRisk Assessment                                                                                                            8\\nRisk Mitigation                                                                                                              9\\nMLOps for Responsible AI                                                                                          9\\nMLOps for Scale                                                                                                             10\\nClosing Thoughts                                                                                                           11\\n\\n2. People of MLOps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13\\nSubject Matter Experts                                                                                                   15\\nData Scientists                                                                                                                 17\\nData Engineers                                                                                                                19\\nSoftware Engineers                                                                                                         20\\nDevOps                                                                                                                            20\\nModel Risk Manager/Auditor                                                                                       21\\nMachine Learning Architect                                                                                         21\\nClosing Thoughts                                                                                                           22\\n\\n3. Key MLOps Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   23\\nA Primer on Machine Learning                                                                                   23\\nModel Development                                                                                                       24\\nEstablishing Business Objectives                                                                              24\\n\\niii\\n\\n\\x0cData Sources and Exploratory Data Analysis                                                         24\\nFeature Engineering and Selection                                                                           25\\nTraining and Evaluation                                                                                            26\\nReproducibility                                                                                                            26\\nResponsible AI                                                                                                            26\\nProductionalization and Deployment                                                                         27\\nModel Deployment Types and Contents                                                                 28\\nModel Deployment Requirements                                                                           29\\nMonitoring                                                                                                                      29\\nDevOps Concerns                                                                                                       30\\nData Scientist Concerns                                                                                             30\\nBusiness Concerns                                                                                                      31\\nIteration and Life Cycle                                                                                                 32\\nIteration                                                                                                                        32\\nThe Feedback Loop                                                                                                    33\\nGovernance                                                                                                                     34\\nData Governance                                                                                                        36\\nProcess Governance                                                                                                   37\\nClosing Thoughts                                                                                                           38\\n\\nPart II.  MLOps: How\\n\\n4. Developing Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41\\nWhat Is a Machine Learning Model?                                                                           42\\nIn Theory                                                                                                                     42\\nIn Practice                                                                                                                    43\\nRequired Components                                                                                               44\\nDifferent ML Algorithms, Different MLOps Challenges                                      45\\nData Exploration                                                                                                            46\\nFeature Engineering and Selection                                                                              47\\nFeature Engineering Techniques                                                                              47\\nHow Feature Selection Impacts MLOps Strategy                                                  48\\nExperimentation                                                                                                             49\\nEvaluating and Comparing Models                                                                             51\\nChoosing Evaluation Metrics                                                                                    51\\nCross-Checking Model Behavior                                                                             53\\nImpact of Responsible AI on Modeling                                                                   53\\nVersion Management and Reproducibility                                                                 56\\nClosing Thoughts                                                                                                           58\\n\\niv \\n\\n| \\n\\nTable of Contents\\n\\n\\x0c5. Preparing for Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   59\\nRuntime Environments                                                                                                 60\\nAdaptation from Development to Production Environments                             60\\nData Access Before Validation and Launch to Production                                   62\\nFinal Thoughts on Runtime Environments                                                            62\\nModel Risk Evaluation                                                                                                   63\\nThe Purpose of Model Validation                                                                            63\\nThe Origins of ML Model Risk                                                                                 64\\nQuality Assurance for Machine Learning                                                                   64\\nKey Testing Considerations                                                                                          65\\nReproducibility and Auditability                                                                                 66\\nMachine Learning Security                                                                                           67\\nAdversarial Attacks                                                                                                     68\\nOther Vulnerabilities                                                                                                  68\\nModel Risk Mitigation                                                                                                   69\\nChanging Environments                                                                                            70\\nInteractions Between Models                                                                                    70\\nModel Misbehavior                                                                                                     71\\nClosing Thoughts                                                                                                           72\\n\\n6. Deploying to Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\n 73\\nCI/CD Pipelines                                                                                                              73\\nBuilding ML Artifacts                                                                                                    75\\nWhat’s in an ML Artifact?                                                                                          75\\nThe Testing Pipeline                                                                                                   75\\nDeployment Strategies                                                                                                   77\\nCategories of Model Deployment                                                                             77\\nConsiderations When Sending Models to Production                                         78\\nMaintenance in Production                                                                                      79\\nContainerization                                                                                                             79\\nScaling Deployments                                                                                                     81\\nRequirements and Challenges                                                                                      83\\nClosing Thoughts                                                                                                           84\\n\\n7. Monitoring and Feedback Loop. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85\\nHow Often Should Models Be Retrained?                                                                  86\\nUnderstanding Model Degradation                                                                             89\\nGround Truth Evaluation                                                                                          89\\nInput Drift Detection                                                                                                 91\\nDrift Detection in Practice                                                                                            92\\nExample Causes of Data Drift                                                                                   93\\nInput Drift Detection Techniques                                                                            93\\n\\nTable of Contents \\n\\n| \\n\\nv\\n\\n\\x0cThe Feedback Loop                                                                                                        95\\nLogging                                                                                                                         96\\nModel Evaluation                                                                                                        97\\nOnline Evaluation                                                                                                       99\\nClosing Thoughts                                                                                                         103\\n\\n8. Model Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\n 105\\nWho Decides What Governance the Organization Needs?                                   105\\nMatching Governance with Risk Level                                                                     107\\nCurrent Regulations Driving MLOps Governance                                                 108\\nPharmaceutical Regulation in the US: GxP                                                          109\\nFinancial Model Risk Management Regulation                                                   109\\nGDPR and CCPA Data Privacy Regulations                                                        110\\nThe New Wave of AI-Specific Regulations                                                               111\\nThe Emergence of Responsible AI                                                                             112\\nKey Elements of Responsible AI                                                                                113\\nElement 1: Data                                                                                                         113\\nElement 2: Bias                                                                                                          114\\nElement 3: Inclusiveness                                                                                          115\\nElement 4: Model Management at Scale                                                                116\\nElement 5: Governance                                                                                            116\\nA Template for MLOps Governance                                                                         117\\nStep 1: Understand and Classify the Analytics Use Cases                                  118\\nStep 2: Establish an Ethical Position                                                                      118\\nStep 3: Establish Responsibilities                                                                            119\\nStep 4: Determine Governance Policies                                                                120\\nStep 5: Integrate Policies into the MLOps Process                                               121\\nStep 6: Select the Tools for Centralized Governance Management                   122\\nStep 7: Engage and Educate                                                                                     123\\nStep 8: Monitor and Refine                                                                                     124\\nClosing Thoughts                                                                                                         125\\n\\nPart III.  MLOps: Real-World Examples\\n\\n9. MLOps in Practice: Consumer Credit Risk Management. . . . . . . . . . . . . . . . . . . . . . . . .  129\\nBackground: The Business Use Case                                                                         129\\nModel Development                                                                                                    130\\nModel Bias Considerations                                                                                         131\\nPrepare for Production                                                                                                131\\nDeploy to Production                                                                                                  132\\nClosing Thoughts                                                                                                         133\\n\\nvi \\n\\n| \\n\\nTable of Contents\\n\\n\\x0c10. MLOps in Practice: Marketing Recommendation Engines. . . . . . . . . . . . . . . . . . . . . . .   135\\nThe Rise of Recommendation Engines                                                                     135\\nThe Role of Machine Learning                                                                               136\\nPush or Pull?                                                                                                              136\\nData Preparation                                                                                                          137\\nDesign and Manage Experiments                                                                              138\\nModel Training and Deployment                                                                              138\\nScalability and Customizability                                                                              140\\nMonitoring and Retraining Strategy                                                                      140\\nReal-Time Scoring                                                                                                    140\\nAbility to Turn Recommendations On and Off                                                   141\\nPipeline Structure and Deployment Strategy                                                           141\\nMonitoring and Feedback                                                                                           142\\nRetraining Models                                                                                                    142\\nUpdating Models                                                                                                      143\\nRuns Overnight, Sleeps During Daytime                                                              143\\nOption to Manually Control Models                                                                     143\\nOption to Automatically Control Models                                                             144\\nMonitoring Performance                                                                                         144\\nClosing Thoughts                                                                                                         145\\n\\n11. MLOps in Practice: Consumption Forecast. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   147\\nPower Systems                                                                                                              147\\nData Collection                                                                                                             149\\nProblem Definition: Machine Learning, or Not Machine Learning?                   151\\nSpatial and Temporal Resolution                                                                               151\\nImplementation                                                                                                            153\\nModeling                                                                                                                        153\\nDeployment                                                                                                                   155\\nMonitoring                                                                                                                    156\\nClosing Thoughts                                                                                                         157\\n\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   159\\n\\nTable of Contents \\n\\n| \\n\\nvii\\n\\n\\x0c\\x0cPreface\\n\\nWe’ve reached a turning point in the story of machine learning where the technology\\nhas moved from the realm of theory and academics and into the “real world”—that is,\\nbusinesses  providing  all  kinds  of  services  and  products  to  people  across  the  globe.\\nWhile  this  shift  is  exciting,  it’s  also  challenging,  as  it  combines  the  complexities  of\\nmachine learning models with the complexities of the modern organization.\\n\\nOne difficulty, as organizations move from experimenting with machine learning to\\nscaling it in production environments, is maintenance. How can companies go from\\nmanaging just one model to managing tens, hundreds, or even thousands? This is not\\nonly where MLOps comes into play, but it’s also where the aforementioned complexi‐\\nties, both on the technical and business sides, appear. This book will introduce read‐\\ners to the challenges at hand, while also offering practical insights and solutions for\\ndeveloping MLOps capabilities.\\n\\nWho This Book Is For\\nWe wrote this book specifically for analytics and IT operations team managers, that\\nis, the people directly facing the task of scaling machine learning (ML) in production.\\nGiven  that  MLOps  is  a  new  field,  we  developed  this  book  as  a  guide  for  creating  a\\nsuccessful  MLOps  environment,  from  the  organizational  to  the  technical  challenges\\ninvolved.\\n\\nHow This Book Is Organized\\nThis  book  is  divided  into  three  parts.  The  first  is  an  introduction  to  the  topic  of\\nMLOps, diving into how (and why) it has developed as a discipline, who needs to be\\ninvolved to execute MLOps successfully, and what components are required.\\n\\nThe second part roughly follows the machine learning model life cycle, with chapters\\non developing models, preparing for production, deploying to production, monitor‐\\ning,  and  governance.  These  chapters  cover  not  only  general  considerations,  but\\n\\nix\\n\\n\\x0cMLOps  considerations  at  each  stage  of  the  life  cycle,  providing  more  detail  on  the\\ntopics touched on in Chapter 3.\\n\\nThe final part provides tangible examples of how MLOps looks in companies today,\\nso  that  readers  can  understand  the  setup  and  implications  in  practice.  Though  the\\ncompany names are fictitious, the stories are based on real-life companies’ experience\\nwith MLOps and model management at scale.\\n\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\n\\nItalic\\n\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\n\\nConstant width\\n\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments  such  as  variable  or  function  names,  databases,  data  types,  environment\\nvariables, statements, and keywords.\\n\\nConstant width bold\\n\\nShows commands or other text that should be typed literally by the user.\\n\\nConstant width italic\\n\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\n\\nO’Reilly Online Learning\\n\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy  and  business  training,  knowledge,  and  insight  to  help\\ncompanies succeed.\\n\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\\n\\nx \\n\\n|  Preface\\n\\n\\x0cHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\n\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\n\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at https://oreil.ly/intro-mlops.\\n\\nEmail  bookquestions@oreilly.com  to  comment  or  ask  technical  questions  about  this\\nbook.\\n\\nFor news and information about our books and courses, visit http://oreilly.com.\\n\\nFind us on Facebook: http://facebook.com/oreilly\\n\\nFollow us on Twitter: http://twitter.com/oreillymedia\\n\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\n\\nAcknowledgments\\nWe would like to thank the entire Dataiku team for their support in developing this\\nbook,  from  conception  to  completion.  It’s  been  a  true  team  effort  and,  like  most\\nthings we do at Dataiku, rooted in fundamental collaboration between countless peo‐\\nple and teams.\\n\\nThanks to those who supported our vision from the beginning of writing this book\\nwith  O’Reilly.  Thanks  to  those  who  stepped  in  to  help  with  writing  and  editing.\\nThanks  to  those  who  provided  honest  feedback  (even  when  it  meant  more  writing\\nand rewriting and re-rewriting). Thanks to those who were internal cheerleaders and,\\nof course, those who helped us promote the finished product to the world.\\n\\nPreface \\n\\n| \\n\\nxi\\n\\n\\x0c\\x0cPART I\\nMLOps: What and Why\\n\\n\\x0c\\x0cCHAPTER 1\\nWhy Now and Challenges\\n\\nMachine  learning  operations  (MLOps)  is  quickly  becoming  a  critical  component  of\\nsuccessful data science project deployment in the enterprise (Figure 1-1). It’s a process\\nthat  helps  organizations  and  business  leaders  generate  long-term  value  and  reduce\\nrisk associated with data science, machine learning, and AI initiatives. Yet it’s a rela‐\\ntively new concept; so why has it seemingly skyrocketed into the data science lexicon\\novernight?  This  introductory  chapter  delves  into  what  MLOps  is  at  a  high  level,  its\\nchallenges,  why  it  has  become  essential  to  a  successful  data  science  strategy  in  the\\nenterprise, and, critically, why it is coming to the forefront now.\\n\\nMLOps Versus ModelOps Versus AIOps\\nMLOps  (or  ModelOps)  is  a  relatively  new  discipline,  emerging  under  these  names\\nparticularly in late 2018 and 2019. The two—MLOps and ModelOps—are, at the time\\nthis book is being written, largely being used interchangeably. However, some argue\\nthat ModelOps is more general than MLOps, as it’s not only about machine learning\\nmodels but any kind of model (e.g., rule-based models). For the purpose of this book,\\nwe’ll be specifically discussing the machine learning model life cycle and will thus use\\nthe term “MLOps.”\\n\\nAIOps, though sometimes confused with MLOps, is another topic entirely and refers\\nto  the  process  of  solving  operational  challenges  through  the  use  of  artificial  intelli‐\\ngence (i.e., AI for DevOps). An example would be a form of predictive maintenance\\nfor  network  failures,  alerting  DevOps  teams  to  possible  problems  before  they  arise.\\nWhile important and interesting in its own right, AIOps is outside the scope of this\\nbook.\\n\\n3\\n\\n\\x0cFigure 1-1. Representation of the exponential growth of MLOps (not the parallel growth\\nof the term “ModelOps”)\\n\\nDefining MLOps and Its Challenges\\nAt its core, MLOps is the standardization and streamlining of machine learning life\\ncycle management (Figure 1-2). But taking a step back, why does the machine learn‐\\ning life cycle need to be streamlined? On the surface, just looking at the steps to go\\nfrom  business  problem  to  a  machine  learning  model  at  a  very  high  level,  it  seems\\nstraightforward.\\n\\nFor  most  traditional  organizations,  the  development  of  multiple  machine  learning\\nmodels and their deployment in a production environment are relatively new. Until\\nrecently, the number of models may have been manageable at a small scale, or there\\nwas  simply  less  interest  in  understanding  these  models  and  their  dependencies  at  a\\ncompany-wide  level.  With  decision  automation  (that  is,  an  increasing  prevalence  of\\ndecision  making  that  happens  without  human  intervention),  models  become  more\\ncritical,  and,  in  parallel,  managing  model  risks  becomes  more  important  at  the  top\\nlevel.\\n\\nThe  reality  of  the  machine  learning  life  cycle  in  an  enterprise  setting  is  much  more\\ncomplex, in terms of needs and tooling (Figure 1-3).\\n\\n4 \\n\\n| \\n\\nChapter 1: Why Now and Challenges\\n\\n\\x0cFigure 1-2. A simple representation of the machine learning model life cycle, which often\\nunderplays the need for MLOps, compared to Figure 1-3\\n\\nThere  are  three  key  reasons  that  managing  machine  learning  life  cycles  at  scale  is\\nchallenging:\\n\\n• There are many dependencies. Not only is data constantly changing, but business\\nneeds shift as well. Results need to be continually relayed back to the business to\\nensure that the reality of the model in production and on production data aligns\\nwith  expectations  and,  critically,  addresses  the  original  problem  or  meets  the\\noriginal goal.\\n\\n• Not everyone speaks the same language. Even though the machine learning life\\ncycle involves people from the business, data science, and IT teams, none of these\\ngroups  are  using  the  same  tools  or  even,  in  many  cases,  share  the  same  funda‐\\nmental skills to serve as a baseline of communication.\\n\\n• Data scientists are not software engineers. Most are specialized in model building\\nand  assessment,  and  they  are  not  necessarily  experts  in  writing  applications.\\nThough this may start to shift over time as some data scientists become special‐\\nists  more  on  the  deployment  or  operational  side,  for  now  many  data  scientists\\nfind themselves having to juggle many roles, making it challenging to do any of\\nthem  thoroughly.  Data  scientists  being  stretched  too  thin  becomes  especially\\nproblematic at scale with increasingly more models to manage. The complexity\\nbecomes exponential when considering the turnover of staff on data teams and,\\nsuddenly, data scientists have to manage models they did not create.\\n\\nDefining MLOps and Its Challenges \\n\\n| \\n\\n5\\n\\n\\x0cFigure 1-3. The realistic picture of a machine learning model life cycle inside an average\\norganization today, which involves many different people with completely different skill\\nsets and who are often using entirely different tools.\\n\\nIf  the  definition  (or  even  the  name  MLOps)  sounds  familiar,  that’s  because  it  pulls\\nheavily  from  the  concept  of  DevOps,  which  streamlines  the  practice  of  software\\nchanges and updates. Indeed, the two have quite a bit in common. For example, they\\nboth center around:\\n\\n• Robust automation and trust between teams\\n\\n• The idea of collaboration and increased communication between teams\\n\\n6 \\n\\n| \\n\\nChapter 1: Why Now and Challenges\\n\\n\\x0c• The end-to-end service life cycle (build, test, release)\\n\\n• Prioritizing continuous delivery and high quality\\n\\nYet there is one critical difference between MLOps and DevOps that makes the latter\\nnot immediately transferable to data science teams: deploying software code into pro‐\\nduction is fundamentally different than deploying machine learning models into pro‐\\nduction.  While  software  code  is  relatively  static  (“relatively”  because  many  modern\\nsoftware-as-a-service [SaaS] companies do have DevOps teams that can iterate quite\\nquickly  and  deploy  in  production  multiple  times  per  day),  data  is  always  changing,\\nwhich means machine learning models are constantly learning and adapting—or not,\\nas the case may be—to new inputs. The complexity of this environment, including the\\nfact that machine learning models are made up of both code and data, is what makes\\nMLOps a new and unique discipline.\\n\\nWhat About DataOps?\\nTo  add  to  the  complexity  of  MLOps  versus  DevOps,  there  is  also  DataOps,  a  term\\nintroduced  in  2014  by  IBM.  DataOps  seeks  to  provide  business-ready  data  that  is\\nquickly  available  for  use,  with  a  large  focus  on  data  quality  and  metadata  manage‐\\nment. For example, if there’s a sudden change in data that a model relies on, a Data‐\\nOps  system  would  alert  the  business  team  to  deal  more  carefully  with  the  latest\\ninsights,  and  the  data  team  would  be  notified  to  investigate  the  change  or  revert  a\\nlibrary upgrade and rebuild the related partition.\\n\\nThe rise of MLOps, therefore, intersects with DataOps at some level, though MLOps\\ngoes a step further and brings even more robustness through additional key features\\n(discussed in more detail in Chapter 3).\\n\\nAs was the case with DevOps and later DataOps, until recently teams have been able\\nto get by without defined and centralized processes mostly because—at an enterprise\\nlevel—they  weren’t  deploying  machine  learning  models  into  production  at  a  large\\nenough scale. Now, the tables are turning and teams are increasingly looking for ways\\nto  formalize  a  multi-stage,  multi-discipline,  multi-phase  process  with  a  heterogene‐\\nous environment and a framework for MLOps best practices, which is no small task.\\nPart II of this book, “MLOps: How,” will provide this guidance.\\n\\nMLOps to Mitigate Risk\\nMLOps  is  important  to  any  team  that  has  even  one  model  in  production  because,\\ndepending on the model, continuous performance monitoring and adjusting is essen‐\\ntial.  By  allowing  safe  and  reliable  operations,  MLOps  is  key  in  mitigating  the  risks\\n\\nMLOps to Mitigate Risk \\n\\n| \\n\\n7\\n\\n\\x0cinduced by the use of ML models. However, MLOps practices do come at a cost, so a\\nproper cost-benefit evaluation should be performed for each use case.\\n\\nRisk Assessment\\nWhen  it  comes  to  machine  learning  models,  risks  vary  widely.  For  example,  the\\nstakes  are  much  lower  for  a  recommendation  engine  used  once  a  month  to  decide\\nwhich  marketing  offer  to  send  a  customer  than  for  a  travel  site  whose  pricing  and\\nrevenue depend on a machine learning model. Therefore, when looking at MLOps as\\na way to mitigate risk, an analysis should cover:\\n\\n• The risk that the model is unavailable for a given period of time\\n\\n• The risk that the model returns a bad prediction for a given sample\\n\\n• The risk that the model accuracy or fairness decreases over time\\n\\n• The risk that the skills necessary to maintain the model (i.e., data science talent)\\n\\nare lost\\n\\nRisks are usually larger for models that are deployed widely and used outside of the\\norganization. As shown in Figure 1-4, risk assessment is generally based on two met‐\\nrics:  the  probability  and  the  impact  of  the  adverse  event.  Mitigation  measures  are\\ngenerally  based  on  the  combination  of  the  two,  i.e.,  the  model’s  severity.  Risk\\nassessment should be performed at the beginning of each project and reassessed peri‐\\nodically, as models may be used in ways that were not foreseen initially.\\n\\nFigure 1-4. A table that helps decision makers with quantitative risk analysis\\n\\n8 \\n\\n| \\n\\nChapter 1: Why Now and Challenges\\n\\n\\x0cRisk Mitigation\\nMLOps  really  tips  the  scales  as  critical  for  risk  mitigation  when  a  centralized  team\\n(with unique reporting of its activities, meaning that there can be multiple such teams\\nat any given enterprise) has more than a handful of operational models. At this point,\\nit  becomes  difficult  to  have  a  global  view  of  the  states  of  these  models  without  the\\nstandardization that allows the appropriate mitigation measures to be taken for each\\nof them (see “Matching Governance with Risk Level” on page 107).\\n\\nPushing machine learning models into production without MLOps infrastructure is\\nrisky  for  many  reasons,  but  first  and  foremost  because  fully  assessing  the  perfor‐\\nmance of a machine learning model can often only be done in the production envi‐\\nronment.  Why?  Because  prediction  models  are  only  as  good  as  the  data  they  are\\ntrained  on,  which  means  the  training  data  must  be  a  good  reflection  of  the  data\\nencountered in the production environment. If the production environment changes,\\nthen the model performance is likely to decrease rapidly (see Chapter 5 for details).\\n\\nAnother major risk factor is that machine learning model performance is often very\\nsensitive  to  the  production  environment  it  is  running  in,  including  the  versions  of\\nsoftware and operating systems in use. They tend not to be buggy in the classic soft‐\\nware  sense,  because  most  weren’t  written  by  hand,  but  rather  were  machine-\\ngenerated. Instead, the problem is that they are often built on a pile of open source\\nsoftware  (e.g.,  libraries,  like  scikit-learn,  Python,  or  Linux),  and  having  versions  of\\nthis software in production that match those that the model was verified on is criti‐\\ncally important.\\n\\nUltimately, pushing models into production is not the final step of the machine learn‐\\ning life cycle—far from it. It’s often just the beginning of monitoring its performance\\nand ensuring that it behaves as expected. As more data scientists start pushing more\\nmachine learning models into production, MLOps becomes critical in mitigating the\\npotential risks, which (depending on the model) can be devastating for the business if\\nthings  go  wrong.  Monitoring  is  also  essential  so  that  the  organization  has  a  precise\\nknowledge of how broadly each model is used.\\n\\nMLOps for Responsible AI\\nA  responsible  use  of  machine  learning  (more  commonly  referred  to  as  Responsible\\nAI) covers two main dimensions:\\n\\nIntentionality\\n\\nEnsuring  that  models  are  designed  and  behave  in  ways  aligned  with  their  pur‐\\npose. This includes assurance that data used for AI projects comes from compli‐\\nant  and  unbiased  sources  plus  a  collaborative  approach  to  AI  projects  that\\nensures multiple checks and balances on potential model bias. Intentionality also\\n\\nMLOps to Mitigate Risk \\n\\n| \\n\\n9\\n\\n\\x0cincludes explainability, meaning the results of AI systems should be explainable\\nby humans (ideally, not just the humans who created the system).\\n\\nAccountability\\n\\nCentrally  controlling,  managing,  and  auditing  the  enterprise  AI  effort—no\\nshadow  IT!  Accountability  is  about  having  an  overall  view  of  which  teams  are\\nusing what data, how, and in which models. It also includes the need for trust that\\ndata  is  reliable  and  being  collected  in  accordance  with  regulations  as  well  as  a\\ncentralized understanding of which models are used for what business processes.\\nThis  is  closely  tied  to  traceability:  if  something  goes  wrong,  is  it  easy  to  find\\nwhere in the pipeline it happened?\\n\\nThese principles may seem obvious, but it’s important to consider that machine learn‐\\ning models lack the transparency of traditional imperative code. In other words, it is\\nmuch harder to understand what features are used to determine a prediction, which\\nin turn can make it much harder to demonstrate that models comply with the neces‐\\nsary regulatory or internal governance requirements.\\n\\nThe  reality  is  that  introducing  automation  vis-à-vis  machine  learning  models  shifts\\nthe fundamental onus of accountability from the bottom of the hierarchy to the top.\\nThat is, decisions that were perhaps previously made by individual contributors who\\noperated within a margin of guidelines (for example, what the price of a given prod‐\\nuct  should  be  or  whether  or  not  a  person  should  be  accepted  for  a  loan)  are  now\\nbeing made by a model. The person responsible for the automated decisions of said\\nmodel is likely a data team manager or even executive, and that brings the concept of\\nResponsible AI even more to the forefront.\\n\\nGiven the previously discussed risks as well as these particular challenges and princi‐\\nples,  it’s  easy  to  see  the  interplay  between  MLOps  and  Responsible  AI.  Teams  must\\nhave good MLOps principles to practice Responsible AI, and Responsible AI necessi‐\\ntates MLOps strategies. Given the gravity of this topic, we’ll come back to it multiple\\ntimes throughout this book, examining how it should be addressed at each stage of\\nthe ML model life cycle.\\n\\nMLOps for Scale\\nMLOps  isn’t  just  important  because  it  helps  mitigate  the  risk  of  machine  learning\\nmodels  in  production;  it  is  also  an  essential  component  to  massively  deploying\\nmachine learning efforts (and in turn benefiting from the corresponding economies\\nof scale). Going from one or a handful of models in production to tens, hundreds, or\\nthousands that have a positive business impact requires MLOps discipline.\\n\\n10 \\n\\n| \\n\\nChapter 1: Why Now and Challenges\\n\\n\\x0cGood MLOps practices will help teams at a minimum:\\n\\n• Keep track of versioning, especially with experiments in the design phase\\n\\n• Understand whether retrained models are better than the previous versions (and\\n\\npromoting models to production that are performing better)\\n\\n• Ensure (at defined periods—daily, monthly, etc.) that model performance is not\\n\\ndegrading in production\\n\\nClosing Thoughts\\nKey features will be discussed at length in Chapter 3, but the point here is that these\\nare not optional practices. They are essential tasks for not only efficiently scaling data\\nscience and machine learning at the enterprise level, but also doing it in a way that\\ndoesn’t  put  the  business  at  risk.  Teams  that  attempt  to  deploy  data  science  without\\nproper MLOps practices in place will face issues with model quality and continuity—\\nor, worse, they will introduce models that have a real, negative impact on the business\\n(e.g., a model that makes biased predictions that reflect poorly on the company).\\n\\nMLOps is also, at a higher level, a critical part of transparent strategies for machine\\nlearning. Upper management and the C-suite should be able to understand as well as\\ndata  scientists  what  machine  learning  models  are  deployed  in  production  and  what\\neffect  they’re  having  on  the  business.  Beyond  that,  they  should  arguably  be  able  to\\ndrill down to understand the whole data pipeline (i.e., the steps taken to go from raw\\ndata to final output) behind those machine learning models. MLOps, as described in\\nthis book, can provide this level of transparency and accountability.\\n\\nClosing Thoughts \\n\\n| \\n\\n11\\n\\n\\x0c\\x0cCHAPTER 2\\nPeople of MLOps\\n\\nEven though machine learning models are primarily built by data scientists, it’s a mis‐\\nconception  that  only  data  scientists  can  benefit  from  robust  MLOps  processes  and\\nsystems.  In  fact,  MLOps  is  an  essential  piece  of  enterprise  AI  strategy  and  affects\\neveryone working on, or benefiting from, the machine learning model life cycle.\\n\\nThis chapter covers the roles each of these people plays in the machine learning life\\ncycle, who they should ideally be connected and working together with under a top-\\nnotch  MLOps  program  to  achieve  the  best  possible  results  from  machine  learning\\nefforts, and what MLOps requirements they may have.\\n\\nIt’s important to note that this field is constantly evolving, bringing with it many new\\njob titles that may not be listed here and presenting new challenges (or overlaps) in\\nMLOps responsibilities.\\n\\nBefore  we  dive  into  the  details,  let’s  look  at  the  following  table,  which  provides  an\\noverview:\\n\\n13\\n\\n\\x0cRole\\nSubject\\nmatter\\nexperts\\n\\nData\\nscientists\\n\\nData\\nengineers\\n\\nSoftware\\nengineers\\n\\nDevOps\\n\\nModel risk\\nmanagers/\\nauditors\\n\\nMachine\\nlearning\\narchitects\\n\\nRole in machine learning model life cycle\\n\\nMLOps requirements\\n\\n• Provide business questions, goals, or KPIs around\\n\\n• Easy way to understand deployed model\\n\\nwhich ML models should be framed.\\n\\nperformance in business terms.\\n\\n• Continually evaluate and ensure that model\\n\\n• Mechanism or feedback loop for flagging model\\n\\nperformance aligns with or resolves the initial need.\\n\\nresults that don’t align with business\\nexpectations.\\n\\n• Build models that address the business question or\\n\\nneeds brought by subject matter experts.\\n\\n• Deliver operationalizable models so that they can be\\nproperly used in the production environment and\\nwith production data.\\n\\n• Assess model quality (of both original and tests) in\\ntandem with subject matter experts to ensure they\\nanswer initial business questions or needs.\\n\\n• Automated model packaging and delivery for\\nquick and easy (yet safe) deployment to\\nproduction.\\n\\n• Ability to develop tests to determine the quality\\nof deployed models and to make continual\\nimprovements.\\n\\n• Visibility into the performance of all deployed\\nmodels (including side-by-side for tests) from\\none central location.\\n\\n• Ability to investigate data pipelines of each\\nmodel to make quick assessments and\\nadjustments regardless of who originally built\\nthe model.\\n\\n• Optimize the retrieval and use of data to power ML\\n\\n• Visibility into performance of all deployed\\n\\nmodels.\\n\\nmodels.\\n\\n• Ability to see the full details of individual data\\npipelines to address underlying data plumbing\\nissues.\\n\\n• Integrate ML models in the company’s applications\\n\\nand systems.\\n\\n• Versioning and automatic tests.\\n• The ability to work in parallel on the same\\n\\n• Ensure that ML models work seamlessly with other\\n\\napplication.\\n\\nnon-machine-learning-based applications.\\n\\n• Conduct and build operational systems and test for\\n\\n• Seamless integration of MLOps into the larger\\n\\nsecurity, performance, availability.\\n\\nDevOps strategy of the enterprise.\\n\\n• Continuous Integration/Continuous Delivery (CI/CD)\\n\\n• Seamless deployment pipeline.\\n\\npipeline management.\\n\\n• Minimize overall risk to the company as a result of\\n\\n• Robust, likely automated, reporting tools on all\\n\\nML models in production.\\n\\n• Ensure compliance with internal and external\\nrequirements before pushing ML models to\\nproduction.\\n\\n• Ensure a scalable and flexible environment for ML\\nmodel pipelines, from design to development and\\nmonitoring.\\n\\n• Introduce new technologies when appropriate that\\nimprove ML model performance in production.\\n\\nmodels (currently or ever in production),\\nincluding data lineage.\\n\\n• High-level overview of models and their\\n\\nresources consumed.\\n\\n• Ability to drill down into data pipelines to assess\\n\\nand adjust infrastructure needs.\\n\\n14 \\n\\n| \\n\\nChapter 2: People of MLOps\\n\\n\\x0cSubject Matter Experts\\nThe  first  profile  to  consider  as  part  of  MLOps  efforts  is  the  subject  matter  experts\\n(SMEs); after all, the ML model life cycle starts and ends with them. While the data-\\noriented profiles (data scientist, engineer, architect, etc.) have expertise across many\\nareas,  they  tend  to  lack  a  deep  understanding  of  the  business  and  the  problems  or\\nquestions that need to be addressed using machine learning.\\n\\nSubject matter experts usually come to the table—or, at least, they should come to the\\ntable—with clearly defined goals, business questions, and/or key performance indica‐\\ntors  (KPIs)  that  they  want  to  achieve  or  address.  In  some  cases,  they  might  be\\nextremely well defined (e.g., “To hit our numbers for the quarter, we need to reduce\\ncustomer churn by 10%” or “We’re losing $N per quarter due to unscheduled mainte‐\\nnance;  how  can  we  better  predict  downtime?”).  In  other  cases,  the  goals  and  ques‐\\ntions may be less well defined (e.g., “Our service staff needs to better understand our\\ncustomers to upsell them” or “How can we get people to buy more widgets?”).\\n\\nIn  organizations  with  healthy  processes,  starting  the  machine  learning  model  life\\ncycle with a more defined business question isn’t necessarily always an imperative, or\\neven  an  ideal,  scenario.  Working  with  a  less  defined  business  goal  can  be  a  good\\nopportunity for subject matter experts to work directly with data scientists up front to\\nbetter  frame  the  problem  and  brainstorm  possible  solutions  before  even  beginning\\nany data exploration or model experimentation.\\n\\nWithout this critical starting point from subject matter experts, other data professio‐\\nnals (particularly data scientists) risk starting the machine learning life cycle process\\ntrying  to  solve  problems  or  provide  solutions  that  don’t  serve  the  larger  business.\\nUltimately,  this  is  detrimental  not  only  to  the  subject  matter  experts  who  need  to\\npartner with data scientists and other data experts to build solutions, but to data sci‐\\nentists themselves who might struggle to provide larger value.\\n\\nAnother negative outcome when SMEs are not involved in the ML life cycle is that,\\nwithout real business outcomes, data teams subsequently struggle to gain traction and\\nadditional  budget  or  support  to  continue  advanced  analytics  initiatives.  Ultimately,\\nthis is bad for data teams, for SMEs, and for the business as a whole.\\n\\nTo add more structure around SME involvement, business decision modeling meth‐\\nodologies can be applied to formalize the business problems to be solved and frame\\nthe role of machine learning in the solution.\\n\\nSubject Matter Experts \\n\\n| \\n\\n15\\n\\n\\x0cBusiness Decision Modeling\\nDecision  modeling  creates  a  business  blueprint  of  the  decision-making  process,\\nallowing  subject  matter  experts  to  directly  structure  and  describe  their  needs.  Deci‐\\nsion models can be helpful because they put machine learning in context for subject\\nmatter experts. This allows the models to be integrated with the business rules, as well\\nas helps the SMEs to fully understand decision contexts and the potential impact of\\nmodel changes.\\n\\nMLOps strategies that include a component of business decision modeling for subject\\nmatter experts can be an effective tool for ensuring that real-world machine learning\\nmodel results are properly contextualized for those who don’t have deep knowledge of\\nhow the underlying models themselves work.1\\n\\nSubject matter experts have a role to play not only at the beginning of the ML model\\nlife  cycle,  but  at  the  end  (post-production)  as  well.  Oftentimes,  to  understand  if  an\\nML  model  is  performing  well  or  as  expected,  data  scientists  need  subject  matter\\nexperts  to  close  the  feedback  loop  because  traditional  metrics  (accuracy,  precision,\\nrecall, etc.) are not enough.\\n\\nFor  example,  data  scientists  could  build  a  simple  churn  prediction  model  that  has\\nvery high accuracy in a production environment; however, marketing does not man‐\\nage  to  prevent  anyone  from  churning.  From  a  business  perspective,  that  means  the\\nmodel didn’t work, and that’s important information that needs to make its way back\\nto those building the ML model so that they can find another possible solution, such\\nas introducing uplift modeling that helps marketing better target potential churners\\nwho might be receptive to marketing messaging.\\n\\nGiven the role of SMEs in the ML model life cycle, it’s critical when building MLOps\\nprocesses to have an easy way for them to understand deployed model performance\\nin  business  terms.  That  is,  they  need  to  understand  not  just  model  accuracy,  preci‐\\nsion, and recall, but the results or impact of the model on the business process identi‐\\nfied up front. In addition, when there are unexpected shifts in performance, subject\\nmatter experts need a scalable way, through MLOps processes, to flag model results\\nthat don’t align with business expectations.\\n\\nOn  top  of  these  explicit  feedback  mechanisms,  more  generally,  MLOps  should  be\\nbuilt  in  a  way  that  increases  transparency  for  subject  matter  experts.  That  is,  they\\nshould be able to use MLOps processes as a jumping-off point for exploring the data\\n\\n1 Decision requirements models are based on Decision Model and Notation, a framework for improving pro‐\\n\\ncesses, effectively managing business rules projects, framing predictive analytics efforts, and ensuring decision\\nsupport systems and dashboards are action-oriented.\\n\\n16 \\n\\n| \\n\\nChapter 2: People of MLOps\\n\\n\\x0cpipelines  behind  the  models,  understanding  what  data  is  being  used,  how  it’s  being\\ntransformed and enhanced, and what kind of machine learning techniques are being\\napplied.\\n\\nFor subject matter experts who are also concerned with compliance of machine learn‐\\ning models with internal or external regulations, MLOps serves as an additional way\\nto bring transparency and understanding to these processes. This includes being able\\nto dig into individual decisions made by a model to understand why the model came\\nto  that  decision.  This  should  be  complementary  to  statistical  and  aggregated\\nfeedback.\\n\\nUltimately, MLOps is most relevant for subject matter experts as a feedback mecha‐\\nnism  and  a  platform  for  communication  with  data  scientists  about  the  models  they\\nare  building.  However,  there  are  other  MLOps  needs  as  well—specifically  around\\ntransparency,  which  ties  into  Responsible  AI—that  are  relevant  for  subject  matter\\nexperts and make them an important part of the MLOps picture.\\n\\nData Scientists\\nThe needs of data scientists are the most critical ones to consider when building an\\nMLOps strategy. To be sure, they have a lot to gain; data scientists at most organiza‐\\ntions  today  often  deal  with  siloed  data,  processes,  and  tools,  making  it  difficult  to\\neffectively scale their efforts. MLOps is well positioned to change this.\\n\\nThough most see data scientists’ role in the ML model life cycle as strictly the model\\nbuilding portion, it is—or at least, it should be—much wider. From the very begin‐\\nning, data scientists need to be involved with subject matter experts, understanding\\nand  helping  to  frame  business  problems  in  such  a  way  that  they  can  build  a  viable\\nmachine learning solution.\\n\\nThe reality is that this very first, critical step in the ML model life cycle is often the\\nhardest.  It’s  challenging  particularly  for  data  scientists  because  it’s  not  where  their\\ntraining  lies.  Both  formal  and  informal  data  science  programs  in  universities  and\\nonline heavily emphasize technical skills and not necessarily skills for communicating\\neffectively with subject matter experts from the business side of the house, who usu‐\\nally are not intimately familiar with machine learning techniques. Once again, busi‐\\nness decision modeling techniques can help here.\\n\\nIt’s also a challenge because it can take time. For data scientists who want to dive in\\nand get their hands dirty, spending weeks framing and outlining the problem before\\ngetting  started  on  solving  it  can  be  torture.  To  top  it  off,  data  scientists  are  often\\nsiloed (physically, culturally, or both) from the core of the business and from subject\\nmatter  experts,  so  they  simply  don’t  have  access  to  an  organizational  infrastructure\\nthat facilitates easy collaboration between these profiles. Robust MLOps systems can\\nhelp address some of these challenges.\\n\\nData Scientists \\n\\n| \\n\\n17\\n\\n\\x0cAfter overcoming the first hurdle, depending on the organization, the project might\\nget handed off to either data engineers or analysts to do some of the initial data gath‐\\nering, preparation, and exploration. In some cases, data scientists themselves manage\\nthese  parts  of  the  ML  model  life  cycle.  But  in  any  case,  data  scientists  step  back  in\\nwhen it comes time to build, test, robustify, and then deploy the model.\\n\\nFollowing deployment, data scientists’ roles include constantly assessing model qual‐\\nity to ensure the way it’s working in production answers initial business questions or\\nneeds. The underlying question in many organizations is often whether data scientists\\nmonitor  only  the  models  they  have  had  a  hand  in  building  or  whether  one  person\\nhandles  all  monitoring.  In  the  former  scenario,  what  happens  when  there  is  staff\\nturnover? In the latter scenario, building good MLOps practices is critical, as the per‐\\nson monitoring also needs to be able to quickly jump in and take action should the\\nmodel drift and start negatively affecting the business. If they weren’t the ones who\\nbuilt it, how can MLOps make this process seamless?\\n\\nOperationalization and MLOps\\nThroughout  2018  and  the  beginning  of  2019,  operationalization  was  the  key  buzz‐\\nword when it came to ML model life cycles and AI in the enterprise. Put simply, oper‐\\nationalization  of  data  science  is  the  process  of  pushing  models  to  production  and\\nmeasuring their performance against business goals. So how does operationalization\\nfit into the MLOps story? MLOps takes operationalization one step further, encom‐\\npassing  not  just  the  push  to  production  but  the  maintenance  of  those  models—and\\nthe entire data pipeline—in production.\\n\\nThough  they  are  distinct,  MLOps  might  be  considered  the  new  operationalization.\\nThat is, where many of the major hurdles for businesses to operationalize have disap‐\\npeared, MLOps is the next frontier and presents the next big challenge for machine\\nlearning efforts in the enterprise.\\n\\nAll  of  the  questions  in  the  previous  section  lead  directly  here:  data  scientists’  needs\\nwhen  it  comes  to  MLOps.  Starting  from  the  end  of  the  process  and  working  back‐\\nward, MLOps must provide data scientists with visibility into the performance of all\\ndeployed models as well as any models being A/B tested. But taking that one step fur‐\\nther, it’s not just about monitoring—it’s also about action. Top-notch MLOps should\\nallow  data  scientists  the  flexibility  to  select  winning  models  from  tests  and  easily\\ndeploy them.\\n\\nTransparency is an overarching theme in MLOps, so it’s no surprise that it’s also a key\\nneed for data scientists. The ability to drill down into data pipelines and make quick\\nassessments and adjustments (regardless of who originally built the model) is critical. \\nAutomated model packaging and delivery for quick and easy (yet safe) deployment to\\nproduction is another important point for transparency, and it’s a crucial component\\n\\n18 \\n\\n| \\n\\nChapter 2: People of MLOps\\n\\n\\x0cof MLOps, especially to bring data scientists together to a place of trust with software\\nengineers and DevOps teams.\\n\\nIn addition to transparency, another theme for mastering MLOps—especially when it\\ncomes to meeting the needs of data scientists—is pure efficiency. In an enterprise set‐\\nting, agility and speed matter. It’s true for DevOps, and the story for MLOps is no dif‐\\nferent.  Of  course,  data  scientists  can  deploy,  test,  and  monitor  models  in  an  ad  hoc\\nfashion.  But  they  will  spend  enormous  amounts  of  time  reinventing  the  wheel  with\\nevery single ML model, and that will never add up to scalable ML processes for the\\norganization.\\n\\nData Engineers\\nData pipelines are at the core of the ML model life cycle, and data engineers are, in\\nturn,  at  the  core  of  data  pipelines.  Because  data  pipelines  can  be  abstract  and  com‐\\nplex, data engineers have a lot of efficiencies to gain from MLOps.\\n\\nIn  large  organizations,  managing  the  flow  of  data,  outside  of  the  application  of  ML\\nmodels, is a full-time job.  Depending on the technical stack and organizational struc‐\\nture of the enterprise, data engineers might, therefore, be more focused on the data‐\\nbases  themselves  than  on  pipelines  (especially  if  the  company  is  leveraging  data\\nscience and ML platforms that facilitate the visual building of pipelines by other data\\npractitioners, like business analysts).\\n\\nUltimately,  despite  these  slight  variations  in  the  role  by  an  organization,  the  role  of\\ndata engineers in the life cycle is to optimize the retrieval and use of data to eventually\\npower ML models. Generally, this means working closely with business teams, partic‐\\nularly  subject  matter  experts,  to  identify  the  right  data  for  the  project  at  hand  and\\npossibly also prepare it for use. On the other end, they work closely with data scien‐\\ntists to resolve any data plumbing issues that might cause a model to behave undesira‐\\nbly in production.\\n\\nGiven data engineers’ central role in the ML model life cycle, underpinning both the\\nbuilding and monitoring portions, MLOps can bring significant efficiency gains. Data\\nengineers require not only visibility into the performance of all models deployed in\\nproduction,  but  the  ability  to  take  it  one  step  further  and  directly  drill  down  into\\nindividual data pipelines to address any underlying issues.\\n\\nIdeally, for maximum efficiency for the data engineer profile (and for others as well,\\nincluding  data  scientists),  MLOps  must  not  consist  of  simple  monitoring,  but  be  a\\nbridge to underlying systems for investigating and tweaking ML models.\\n\\nData Engineers \\n\\n| \\n\\n19\\n\\n\\x0cSoftware Engineers\\nIt would be easy to exclude classical software engineers from MLOps consideration,\\nbut  it  is  crucial  from  a  wider  organizational  perspective  to  consider  their  needs  to\\nbuild a cohesive enterprise-wide strategy for machine learning.\\n\\nSoftware  engineers  don’t  usually  build  ML  models,  but,  on  the  other  hand,  most\\norganizations  are  not  only  producing  ML  models,  but  classic  software  and  applica‐\\ntions as well. It’s important that software engineers and data scientists work together\\nto ensure the functioning of the larger system. After all, ML models aren’t just stand‐\\nalone experiments; the machine learning code, training, testing, and deployment have\\nto  fit  into  the  Continuous  Integration/Continuous  Delivery  (CI/CD)  pipelines  that\\nthe rest of the software is using.\\n\\nFor example, consider a retail company that has built an ML-based recommendation\\nengine  for  their  website.  The  ML  model  was  built  by  the  data  scientist,  but  to  inte‐\\ngrate it into the larger functioning of the site, software engineers will necessarily need\\nto  be  involved.  Similarly,  software  engineers  are  responsible  for  the  maintenance  of\\nthe  website  as  a  whole,  and  a  large  part  of  that  includes  the  functioning  of  the  ML\\nmodels in production.\\n\\nGiven  this  interplay,  software  engineers  need  MLOps  to  provide  them  with  model\\nperformance  details  as  part  of  a  larger  picture  of  software  application  performance\\nfor the enterprise. MLOps is a way for data scientists and software engineers to speak\\nthe same language and have the same baseline understanding of how different models\\ndeployed across the silos of the enterprise are working together in production.\\n\\nOther  important  features  for  software  engineers  include  versioning,  to  be  sure  of\\nwhat  they  are  currently  dealing  with;  automatic  tests,  to  be  as  sure  as  possible  that\\nwhat they are currently dealing with is working; and the ability to work in parallel on\\nthe same application (thanks to a system that allows branches and merges like Git). \\n\\nDevOps\\nMLOps was born out of DevOps principles, but that doesn’t mean they can be run in\\nparallel as completely separate and siloed systems.\\n\\nDevOps teams have two primary roles in the ML model life cycle. First, they are the\\npeople conducting and building operational systems as well as tests to ensure security,\\nperformance, and availability of ML models. Second, they are responsible for CI/CD\\npipeline management. Both of these roles require tight collaboration with data scien‐\\ntists, data engineers, and data architects. Tight collaboration is, of course, easier said\\nthan done, but that is where MLOps can add value.\\n\\n20 \\n\\n| \\n\\nChapter 2: People of MLOps\\n\\n\\x0cFor DevOps teams, MLOps needs to be integrated into the larger DevOps strategy of\\nthe  enterprise,  bridging  the  gap  between  traditional  CI/CD  and  modern  ML.  That\\nmeans systems that are fundamentally complementary and that allow DevOps teams\\nto automate tests for ML just as they can automate tests for traditional software.\\n\\nModel Risk Manager/Auditor\\nIn certain industries (particularly the financial services sector), the model risk man‐\\nagement (MRM) function is crucial for regulatory compliance. But it’s not only highly\\nregulated industries that should be concerned or that should have a similar function;\\nMRM  can  protect  companies  in  any  industry  from  catastrophic  loss  introduced  by\\npoorly  performing  ML  models.  What’s  more,  audits  play  a  role  in  many  industries\\nand can be labor intensive, which is where MLOps comes into the picture.\\n\\nWhen it comes to the ML model life cycle, model risk managers play the critical role\\nof analyzing not just model outcomes, but the initial goal and business questions ML\\nmodels  seek  to  resolve  to  minimize  overall  risk  to  the  company.  They  should  be\\ninvolved  along  with  subject  matter  experts  at  the  very  beginning  of  the  life  cycle  to\\nensure that an automated, ML-based approach in and of itself doesn’t present risk.\\n\\nAnd, of course, they have a role to play in monitoring—their more traditional place\\nin the model life cycle—to ensure that risks are kept at bay once models are in pro‐\\nduction.  In  between  conception  and  monitoring,  MRM  also  is  a  factor  post-model\\ndevelopment and preproduction, ensuring initial compliance with internal and exter‐\\nnal requirements.\\n\\nMRM professionals and teams have a lot to gain from MLOps, because their work is\\noften painstakingly manual. As MRM and the teams with which they work often use\\ndifferent tools, standardization can offer a huge leg up in the speed at which auditing\\nand risk management can occur.\\n\\nWhen  it  comes  to  specific  MLOps  needs,  robust  reporting  tools  on  all  models\\n(whether they are currently in production or have been in production in the past) is\\nthe primary one. This reporting should include not just performance details, but the\\nability to see data lineage. Automated reporting adds an extra layer of efficiency for\\nMRM and audit teams in MLOps systems and processes.\\n\\nMachine Learning Architect\\nTraditional  data  architects  are  responsible  for  understanding  the  overall  enterprise\\narchitecture and ensuring that it meets the requirements for data needs from across\\nthe  business.  They  generally  play  a  role  in  defining  how  data  will  be  stored  and\\nconsumed.\\n\\nModel Risk Manager/Auditor \\n\\n| \\n\\n21\\n\\n\\x0cToday, demands on architects are much greater, and they often have to be knowledge‐\\nable not only on the ins and outs of data storage and consumption, but on how ML\\nmodels work in tandem. This adds a lot of complexity to the role and increases their\\nresponsibility  in  the  MLOps  life  cycle,  and  it’s  why  in  this  section,  we  have  called\\nthem machine learning architects instead of the more traditional “data architect” title.\\n\\nMachine learning architects play a critical role in the ML model life cycle, ensuring a\\nscalable and flexible environment for model pipelines. In addition, data teams need\\ntheir  expertise  to  introduce  new  technologies  (when  appropriate)  that  improve  ML\\nmodel performance in production. It is for this reason that the data architect title isn’t\\nenough; they need to have an intimate understanding of machine learning, not just\\nenterprise architecture, to play this key role in the ML model life cycle.\\n\\nThis role requires collaboration across the enterprise, from data scientists and engi‐\\nneers  to  DevOps  and  software  engineers.  Without  a  complete  understanding  of  the\\nneeds of each of these people and teams, machine learning architects cannot properly\\nallocate resources to ensure optimal performance of ML models in production.\\n\\nWhen  it  comes  to  MLOps,  the  machine  learning  architects’  role  is  about  having  a\\ncentralized  view  of  resource  allocation.  As  they  have  a  strategic,  tactical  role,  they\\nneed an overview of the situation to identify bottlenecks and use that information to\\nfind long-term improvements. Their role is one of pinpointing possible new technol‐\\nogy or infrastructure for investment, not necessarily operational quick fixes that don’t\\naddress the heart of the scalability of the system.\\n\\nClosing Thoughts\\nMLOps isn’t just for data scientists; a diverse group of experts across the organization\\nhas a role to play not only in the ML model life cycle, but the MLOps strategy as well.\\nIn fact, each person—from the subject matter expert on the business side to the most\\ntechnical machine learning architect—plays a critical part in the maintenance of ML\\nmodels in production. This is ultimately important not only to ensure the best possi‐\\nble results from ML models (good results generally lead to more trust in ML-based\\nsystems as well as increased budget to build more), but, perhaps more pointedly, to\\nprotect the business from the risks outlined in Chapter 1.\\n\\n22 \\n\\n| \\n\\nChapter 2: People of MLOps\\n\\n\\x0cCHAPTER 3\\nKey MLOps Features\\n\\nMark Treveil\\n\\nMLOps affects many different roles across the organization and, in turn, many parts\\nof the machine learning life cycle. This chapter introduces the five key components of\\nMLOps (development, deployment, monitoring, iteration, and governance) at a high\\nlevel as a foundation for Chapters 4 through 8, which delve into the more technical\\ndetails and requirements of these components.\\n\\nA Primer on Machine Learning\\nTo  understand  the  key  features  of  MLOps,  it’s  essential  first  to  understand  how\\nmachine learning works and be intimately familiar with its specificities. Though often\\noverlooked  in  its  role  as  a  part  of  MLOps,  ultimately  algorithm  selection  (or  how\\nmachine learning models are built) can have a direct impact on MLOps processes.\\n\\nAt its core, machine learning is the science of computer algorithms that automatically\\nlearn  and  improve  from  experience  rather  than  being  explicitly  programmed.  The\\nalgorithms  analyze  sample  data,  known  as  training  data,  to  build  a  software  model\\nthat can make predictions.\\n\\nFor example, an image recognition model might be able to identify the type of elec‐\\ntricity meter from a photograph by searching for key patterns in the image that dis‐\\ntinguish each type of meter. Another example is an insurance recommender model,\\nwhich might suggest additional insurance products that a specific existing customer\\nis most likely to buy based on the previous behavior of similar customers.\\n\\nWhen faced with unseen data, be it a photo or a customer, the ML model uses what it\\nhas  learned  from  previous  data  to  make  the  best  prediction  it  can  based  on  the\\nassumption that the unseen data is somehow related to the previous data.\\n\\n23\\n\\n\\x0cML algorithms use a wide range of mathematical techniques, and the models can take\\nmany different forms, from simple decision trees to logistic regression algorithms to\\nmuch  more  complex  deep  learning  models  (see  “What  Is  a  Machine  Learning\\nModel?” on page 42 for details).\\n\\nModel Development\\nLet’s  take  a  deeper  look  into  ML  model  development  as  a  whole  for  an  even  more\\ncomplete  understanding  of  its  components,  all  of  which  can  have  an  impact  on\\nMLOps after deployment.\\n\\nEstablishing Business Objectives\\nThe process of developing a machine learning model typically starts with a business\\nobjective,  which  can  be  as  simple  as  reducing  fraudulent  transactions  to  <  0.1%  or\\nhaving  the  ability  to  identify  people’s  faces  on  their  social  media  photos.  Business\\nobjectives naturally come with performance targets, technical infrastructure require‐\\nments, and cost constraints; all of these factors can be captured as key performance\\nindicators, or KPIs, which will ultimately enable the business performance of models\\nin production to be monitored.\\n\\nIt’s important to recognize that ML projects don’t happen in a vacuum. They are gen‐\\nerally part of a larger project that in turn impacts technologies, processes, and people.\\nThat means part of establishing objectives also includes change management, which\\nmay even provide some guidance for how the ML model should be built. For exam‐\\nple,  the  required  degree  of  transparency  will  strongly  influence  the  choice  of  algo‐\\nrithms and may drive the need to provide explanations together with predictions so\\nthat predictions are turned into valuable decisions at the business level.\\n\\nData Sources and Exploratory Data Analysis\\nWith  clear  business  objectives  defined,  it  is  time  to  bring  together  subject  matter\\nexperts  and  data  scientists  to  begin  the  journey  of  developing  the  ML  model.  This\\nstarts with the search for suitable input data. Finding data sounds simple, but in prac‐\\ntice, it can be the most arduous part of the journey.\\n\\nKey questions for finding data to build ML models include:\\n\\n• What relevant datasets are available?\\n\\n• Is this data sufficiently accurate and reliable?\\n\\n• How can stakeholders get access to this data?\\n\\n• What  data  properties  (known  as  features)  can  be  made  available  by  combining\\n\\nmultiple sources of data?\\n\\n24 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0c• Will this data be available in real time?\\n\\n• Is there a need to label some of the data with the “ground truth” that is to be pre‐\\ndicted, or does unsupervised learning make sense? If so, how much will this cost\\nin terms of time and resources?\\n\\n• What platform should be used?\\n\\n• How will data be updated once the model is deployed?\\n\\n• Will the use of the model itself reduce the representativeness of the data?\\n\\n• How will the KPIs, which were established along with the business objectives, be\\n\\nmeasured?\\n\\nThe constraints of data governance bring even more questions, including:\\n\\n• Can the selected datasets be used for this purpose?\\n\\n• What are the terms of use?\\n\\n• Is there personally identifiable information (PII) that must be redacted or anony‐\\n\\nmized?\\n\\n• Are  there  features,  such  as  gender,  that  legally  cannot  be  used  in  this  business\\n\\ncontext?\\n\\n• Are minority populations sufficiently well represented that the model has equiva‐\\n\\nlent performances on each group?\\n\\nSince data is the essential ingredient to power ML algorithms, it always helps to build\\nan understanding of the patterns in data before attempting to train models. Explora‐\\ntory data analysis (EDA) techniques can help build hypotheses about the data, iden‐\\ntify  data  cleaning  requirements,  and  inform  the  process  of  selecting  potentially\\nsignificant features. EDA can be carried out visually for intuitive insight and statisti‐\\ncally if more rigor is required.\\n\\nFeature Engineering and Selection\\nEDA leads naturally into feature engineering and feature selection. Feature engineer‐\\ning  is  the  process  of  taking  raw  data  from  the  selected  datasets  and  transforming  it\\ninto “features” that better represent the underlying problem to be solved. “Features”\\nare arrays of numbers of fixed size, as it is the only object that ML algorithms under‐\\nstand.  Feature  engineering  includes  data  cleansing,  which  can  represent  the  largest\\npart  of  an  ML  project  in  terms  of  time  spent.  For  details,  see  “Feature  Engineering\\nand Selection” on page 47.\\n\\nModel Development \\n\\n| \\n\\n25\\n\\n\\x0cTraining and Evaluation\\nAfter  data  preparation  by  way  of  feature  engineering  and  selection,  the  next  step  is\\ntraining. The process of training and optimizing a new ML model is iterative; several\\nalgorithms may be tested, features can be automatically generated, feature selections\\nmay be adapted, and algorithm hyperparameters tuned. In addition to—or in many\\ncases  because  of—its  iterative  nature,  training  is  also  the  most  intensive  step  of  the\\nML model life cycle when it comes to computing power.\\n\\nKeeping  track  of  the  results  of  each  experiment  when  iterating  becomes  complex\\nquickly. Nothing is more frustrating to data scientists than not being able to re-create\\nthe best results because they cannot remember the precise configuration. An experi‐\\nment tracking tool can greatly simplify the process of remembering the data, the fea‐\\ntures  selection,  and  model  parameters  alongside  the  performance  metrics.  These\\nenable  experiments  to  be  compared  side-by-side,  highlighting  the  differences  in\\nperformance.\\n\\nDeciding  what  is  the  best  solution  requires  both  quantitative  criteria,  such  as  accu‐\\nracy or average error, and qualitative criteria regarding the explainability of the algo‐\\nrithm or its ease of deployment.\\n\\nReproducibility\\nWhile many experiments may be short-lived, significant versions of a model need to\\nbe  saved  for  possible  later  use.  The  challenge  here  is  reproducibility,  which  is  an\\nimportant  concept  in  experimental  science  in  general.  The  aim  in  ML  is  to  save\\nenough information about the environment the model was developed in so that the\\nmodel can be reproduced with the same results from scratch.\\n\\nWithout reproducibility, data scientists have little chance of being able to confidently\\niterate on models, and worse, they are unlikely to be able to hand over the model to\\nDevOps to see if what was created in the lab can be faithfully reproduced in produc‐\\ntion.  True  reproducibility  requires  version  control  of  all  the  assets  and  parameters\\ninvolved, including the data used to train and evaluate the model, as well as a record\\nof  the  software  environment  (see  “Version  Management  and  Reproducibility”  on\\npage 56 for details).\\n\\nResponsible AI\\nBeing  able  to  reproduce  the  model  is  only  part  of  the  operationalization  challenge;\\nthe DevOps team also needs to understand how to verify the model (i.e., what does\\nthe model do, how should it be tested, and what are the expected results?). Those in\\nhighly regulated industries are likely required to document even more detail, includ‐\\ning how the model was built and how it was tuned. In critical cases, the model may be\\nindependently recoded and rebuilt.\\n\\n26 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cDocumentation is still the standard  solution to this communication challenge. Auto‐\\nmated model document generation, whereby a tool automatically creates documenta‐\\ntion associated with any trained model, can make the task less onerous. But in almost\\nall cases, some documentation will need to be written by hand to explain the choices\\nmade.\\n\\nIt is a fundamental consequence of their statistical nature that ML models are chal‐\\nlenging  to  understand.  While  model  algorithms  come  with  standard  performance\\nmeasures  to  assess  their  efficacy,  these  don’t  explain  how  the  predictions  are  made.\\nThe  “how”  is  important  as  a  way  to  sanity-check  the  model  or  help  better  engineer\\nfeatures,  and  it  may  be  necessary  to  ensure  that  fairness  requirements  (e.g.,  around\\nfeatures like sex, age, or race) have been met. This is the field of explainability, which\\nis connected to Responsible AI as discussed in Chapter 1 and which will be discussed\\nin further detail in Chapter 4.\\n\\nExplainability  techniques  are  becoming  increasingly  important  as  global  concerns\\ngrow about the impact of unbridled AI. They offer a way to mitigate uncertainty and\\nhelp prevent unintended consequences. The techniques most commonly used today\\ninclude:\\n\\n• Partial  dependence  plots,  which  look  at  the  marginal  impact  of  features  on  the\\n\\npredicted outcome \\n\\n• Subpopulation analyses, which look at how the model treats specific subpopula‐\\n\\ntions and that are the basis of many fairness analyses\\n\\n• Individual  model  predictions,  such  as  Shapley  values,  which  explain  how  the\\n\\nvalue of each feature contributes to a specific prediction\\n\\n• What-if analysis, which helps the ML model user to understand the sensitivity of\\n\\nthe prediction to its inputs\\n\\nAs we’ve seen in this section, even though model development happens very early on,\\nit’s still an important place to incorporate MLOps practices. Any MLOps work done\\nup front during the model development stage will make the models easier to manage\\ndown the line (especially when pushing to production).\\n\\nProductionalization and Deployment\\nProductionalizing and deploying models is a key component of MLOps that presents\\nan  entirely  different  set  of  technical  challenges  than  developing  the  model.  It  is  the\\ndomain of the software engineer and the DevOps team, and the organizational chal‐\\nlenges  in  managing  the  information  exchange  between  the  data  scientists  and  these\\nteams must not be underestimated. As touched on in Chapter 1, without effective col‐\\nlaboration between the teams, delays or failures to deploy are inevitable.\\n\\nProductionalization and Deployment \\n\\n| \\n\\n27\\n\\n\\x0cModel Deployment Types and Contents\\nTo understand what happens in these phases, it’s helpful to take a step back and ask:\\nwhat exactly is going into production, and what does a model consist of? There are\\ncommonly two types of model deployment:\\n\\nModel-as-a-service, or live-scoring model\\n\\nTypically the model is deployed into a simple framework to provide a REST API\\nendpoint  (the  means  from  which  the  API  can  access  the  resources  it  needs  to\\nperform the task) that responds to requests in real time.\\n\\nEmbedded model\\n\\nHere the model is packaged into an application, which is then published. A com‐\\nmon example is an application that provides batch-scoring of requests.\\n\\nWhat  to-be-deployed  models  consist  of  depends,  of  course,  on  the  technology\\nchosen, but typically they comprise a set of code (commonly Python, R, or Java) and\\ndata artifacts. Any of these can have version dependencies on runtimes and packages\\nthat need to match in the production environment because the use of different ver‐\\nsions may cause model predictions to differ.\\n\\nOne approach to reducing dependencies on the production environment is to export\\nthe model to a portable format such as PMML, PFA, ONNX, or POJO. These aim to\\nimprove model portability between systems and simplify deployment. However, they\\ncome  at  a  cost:  each  format  supports  a  limited  range  of  algorithms,  and  sometimes\\nthe portable models behave in subtly different ways than the original. Whether or not\\nto use a portable format is a choice to be made based on a thorough understanding of\\nthe technological and business context.\\n\\nContainerization\\nContainerization  is  an  increasingly  popular  solution  to  the  headaches  of  dependen‐\\ncies  when  deploying  ML  models.  Container  technologies  such  as  Docker  are  light‐\\nweight  alternatives  to  virtual  machines,  allowing  applications  to  be  deployed  in\\nindependent, self-contained environments, matching the exact requirements of each\\nmodel.\\n\\nThey also enable new models to be seamlessly deployed using the blue-green deploy‐\\nment technique.1 Compute resources for models can be scaled elastically using multi‐\\nple containers, too. Orchestrating many containers is the role of technologies such as\\nKubernetes and can be used both in the cloud and on-premise.\\n\\n1 Describing the blue-green deployment technique will require more space than we have here. For more infor‐\\n\\nmation, see Martin Fowler’s blog.\\n\\n28 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cModel Deployment Requirements\\nSo  what  about  the  productionalization  process  between  completing  model  develop‐\\nment  and  physically  deploying  into  production—what  needs  to  be  addressed?  One\\nthing is for sure: rapid, automated deployment is always preferred to labor-intensive\\nprocesses.\\n\\nFor  short-lifetime,  self-service  applications,  there  often  isn’t  much  need  to  worry\\nabout testing and validation. If the maximum resource demands of the model can be\\nsecurely capped by technologies such as Linux cgroups, then a fully automated single-\\nstep push-to-production may be entirely adequate. It is even possible to handle sim‐\\nple  user  interfaces  with  frameworks  like  Flask  when  using  this  lightweight\\ndeployment  mode.  Along  with  integrated  data  science  and  machine  learning  plat‐\\nforms, some business rule management systems may also allow some sort of autono‐\\nmous deployment of basic ML models.\\n\\nIn  customer-facing,  mission-critical  use  cases,  a  more  robust  CI/CD  pipeline  is\\nrequired. This typically involves:\\n\\n1. Ensuring all coding, documentation and sign-off standards have been met\\n\\n2. Re-creating the model in something approaching the production environment\\n\\n3. Revalidating the model accuracy\\n\\n4. Performing explainability checks\\n\\n5. Ensuring all governance requirements have been met\\n\\n6. Checking the quality of any data artifacts\\n\\n7. Testing resource usage under load\\n\\n8. Embedding into a more complex application, including integration tests\\n\\nIn  heavily  regulated  industries  (e.g.,  finance  and  pharmaceuticals),  governance  and\\nregulatory checks will be extensive and are likely to involve manual intervention. The\\ndesire in MLOps, just as in DevOps, is to automate the CI/CD pipeline as far as possi‐\\nble.  This  not  only  speeds  up  the  deployment  process,  but  it  enables  more  extensive\\nregression testing and reduces the likelihood of errors in the deployment. \\n\\nMonitoring\\nOnce a model is deployed to production, it is crucial that it continue to perform well\\nover time. But good performance means different things to different people, in par‐\\nticular to the DevOps team, to data scientists, and to the business.\\n\\nMonitoring \\n\\n| \\n\\n29\\n\\n\\x0cDevOps Concerns\\nThe concerns of the DevOps team are very familiar and include questions like:\\n\\n• Is the model getting the job done quickly enough?\\n\\n• Is it using a sensible amount of memory and processing time?\\n\\nThis is traditional IT performance monitoring, and DevOps teams know how to do\\nthis well already. The resource demands of ML models are not so different from tra‐\\nditional software in this respect.\\n\\nScalability  of  compute  resources  can  be  an  important  consideration,  for  example,  if\\nyou are retraining models in production. Deep learning models have greater resource\\ndemands  than  much  simpler  decision  trees.  But  overall,  the  existing  expertise  in\\nDevOps teams for monitoring and managing resources can be readily applied to ML\\nmodels.\\n\\nData Scientist Concerns\\nThe data scientist is interested in monitoring ML models for a new, more challenging\\nreason:  they  can  degrade  over  time,  since  ML  models  are  effectively  models  of  the\\ndata they were trained on. This is not a problem faced by traditional software, but it is\\ninherent to machine learning. ML mathematics builds a concise representation of the\\nimportant patterns in the training data with the hope that this is a good reflection of\\nthe real world. If the training data reflects the real world well, then the model should\\nbe accurate and, thus, useful.\\n\\nBut the real world doesn’t stand still. The training data used to build a fraud detection\\nmodel six months ago won’t reflect a new type of fraud that has started to occur in the\\nlast  three  months.  If  a  given  website  starts  to  attract  an  increasingly  younger  user\\nbase, then a model that generates advertisements is likely to produce less and less rel‐\\nevant adverts. At some point, the performance will become unacceptable, and model\\nretraining  becomes  necessary.  How  soon  models  need  to  be  retrained  depends  on\\nhow fast the real world is changing and how accurate the model needs to be, but also,\\nimportantly, on how easy it is to build and deploy a better model.\\n\\nBut  first,  how  can  data  scientists  tell  a  model’s  performance  is  degrading?  It’s  not\\nalways easy. There are two common approaches, one based on ground truth and the\\nother on input drift.\\n\\nGround truth\\n\\nThe  ground  truth,  put  simply,  is  the  correct  answer  to  the  question  that  the  model\\nwas asked to solve—for example, “Is this credit card transaction actually fraudulent?”\\n\\n30 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cIn knowing the ground truth for all the predictions a model has made, one can judge\\nhow well that model is performing.\\n\\nSometimes ground truth is obtained rapidly after a prediction—for example, in mod‐\\nels that decide which advertisements to display to a user on a web page. The user is\\nlikely to click on the advertisements within seconds, or not at all. However, in many\\nuse cases, obtaining the ground truth is much slower. If a model predicts that a trans‐\\naction is fraudulent, how can this be confirmed? In some cases, verification may only\\ntake a few minutes, such as a phone call placed to the cardholder. But what about the\\ntransactions the model thought were OK but actually weren’t? The best hope is that\\nthey will be reported by the cardholder when they review their monthly transactions,\\nbut this could happen up to a month after the event (or not at all).\\n\\nIn the fraud example, ground truth isn’t going to enable data science teams to moni‐\\ntor performance accurately on a daily basis. If the situation requires rapid feedback,\\nthen input drift may be a better approach.\\n\\nInput drift\\n\\nInput drift is based on the principle that a model is only going to predict accurately if\\nthe data it was trained on is an accurate reflection of the real world. So if a compari‐\\nson  of  recent  requests  to  a  deployed  model  against  the  training  data  shows  distinct\\ndifferences, then there is a strong likelihood that the model performance is compro‐\\nmised. This is the basis of input drift monitoring. The beauty of this approach is that\\nall the data required for this test already exists, so there is no need to wait for ground\\ntruth or any other information.\\n\\nIdentifying  drift  is  one  of  the  most  important  components  of  an  adaptable  MLOps\\nstrategy, and one that can bring agility to the organization’s enterprise AI efforts over‐\\nall. Chapter 7 will go into more technical depth about data scientists’ concerns when\\nit comes to model monitoring.\\n\\nBusiness Concerns\\nThe business has a holistic outlook on monitoring, and some of its concerns might\\ninclude questions like:\\n\\n• Is the model delivering value to the enterprise?\\n\\n• Do the benefits of the model outweigh the cost of developing and deploying it?\\n\\n(And how can we measure this?)\\n\\nThe  KPIs  identified  for  the  original  business  objective  are  one  part  of  this  process.\\nWhere  possible,  these  should  be  monitored  automatically,  but  this  is  rarely  trivial.\\nThe  objective  of  reducing  fraud  to  less  than  0.1%  of  transactions  in  our  previous\\n\\nMonitoring \\n\\n| \\n\\n31\\n\\n\\x0cexample is reliant on establishing the ground truth. But even monitoring this doesn’t\\nanswer the question: what is the net gain to the business in dollars?\\n\\nThis  is  an  age-old  challenge  for  software,  and  with  ever-increasing  expenditure  on\\nML, the pressure for data scientists to demonstrate value is only going to grow. In the\\nabsence  of  a  “dollar-o-meter,”  effectively  monitoring  the  business  KPIs  is  the  best\\noption available (see “Design and Manage Experiments” on page 138). The choice of\\nthe baseline is important here and should ideally allow for differentiation of the value\\nof the ML subproject specifically, rather than that of the global project. For example,\\nthe  ML  performance  can  be  assessed  with  respect  to  a  rule-based  decision  model\\nbased on subject matter expertise to set apart the contribution of decision automation\\nand of ML per se.\\n\\nIteration and Life Cycle\\nDeveloping  and  deploying  improved  versions  of  a  model  is  an  essential  part  of  the\\nMLOps  life  cycle,  and  one  of  the  more  challenging.  There  are  various  reasons  to\\ndevelop a new model version, one of which is model performance degradation due to\\nmodel  drift,  as  discussed  in  the  prior  section.  Sometimes  there  is  a  need  to  reflect\\nrefined business objectives and KPIs, and other times, it’s just that the data scientists\\nhave come up with a better way to design the model.\\n\\nIteration\\nIn  some  fast-moving  business  environments,  new  training  data  becomes  available\\nevery  day.  Daily  retraining  and  redeployment  of  the  model  are  often  automated  to\\nensure that the model reflects recent experience as closely as possible.\\n\\nRetraining an existing model with the latest training data is the simplest scenario for\\niterating a new model version. But while there are no changes to feature selection or\\nalgorithm, there are still plenty of pitfalls. In particular:\\n\\n• Does  the  new  training  data  look  as  expected?  Automated  validation  of  the  new\\n\\ndata through predefined metrics and checks is essential.\\n\\n• Is the data complete and consistent?\\n\\n• Are the distributions of features broadly similar to those in the previous training\\n\\nset? Remember that the goal is to refine the model, not radically change it.\\n\\nWith a new model version built, the next step is to compare the metrics with the cur‐\\nrent live model version. Doing so requires evaluating both models on the same devel‐\\nopment  dataset,  whether  it  be  the  previous  or  latest  version.  If  metrics  and  checks\\nsuggest a wide variation between the models, automated scripts should not be rede‐\\nployed, and manual intervention should be sought.\\n\\n32 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cEven in the “simple” automated retraining scenario with new training data, there is a\\nneed  for  multiple  development  datasets  based  on  scoring  data  reconciliation  (with\\nground truth when it becomes available), data cleaning and validation, the previous\\nmodel version, and a set of carefully considered checks. Retraining in other scenarios\\nis likely to be even more complicated, rendering automated redeployment unlikely.\\n\\nAs  an  example,  consider  retraining  motivated  by  the  detection  of  significant  input\\ndrift. How can the model be improved? If new training data is available, then retrain‐\\ning with this data is the action with the highest cost-benefit ratio, and it may suffice.\\nHowever,  in  environments  where  it’s  slow  to  obtain  the  ground  truth,  there  may  be\\nlittle new labeled data.\\n\\nThis case requires direct invention from data scientists who need to understand the\\ncause  of  the  drift  and  work  out  how  the  existing  training  data  could  be  adjusted  to\\nmore  accurately  reflect  the  latest  input  data.  Evaluating  a  model  generated  by  such\\nchanges is difficult. The data scientist has to spend time assessing the situation—time\\nthat  increases  with  the  amount  of  modeling  debt—as  well  as  estimate  the  potential\\nimpact  on  performance  and  design  custom  mitigation  measures.  For  example,\\nremoving a specific feature or sampling the existing rows of training data may lead to\\na better-tuned model.\\n\\nThe Feedback Loop\\nIn large enterprises, DevOps best practices typically dictate that the live model scor‐\\ning environment and the model retraining environment are distinct. As a result, the\\nevaluation of a new model version on the retraining environment is likely to be com‐\\npromised.\\n\\nOne approach to mitigating this uncertainty is shadow testing, where the new model\\nversion  is  deployed  into  the  live  environment  alongside  the  existing  model.  All  live\\nscoring  is  handled  by  the  incumbent  model  version,  but  each  new  request  is  then\\nscored again by the new model version and the results logged, but not returned to the\\nrequestor. Once sufficient requests have been scored by both versions, the results can\\nbe  compared  statistically.  Shadow  scoring  also  gives  more  visibility  to  the  SMEs  on\\nthe future versions of the model and may thus allow for a smoother transition.\\n\\nIn the advertisement generation model previously discussed, it is impossible to tell if\\nthe  ads  selected  by  the  model  are  good  or  bad  without  allowing  the  end  user  the\\nchance to click on them. In this use case, shadow testing has limited benefits, and A/B\\ntesting is more common.\\n\\nIn  A/B  testing,  both  models  are  deployed  into  the  live  environment,  but  input\\nrequests  are  split  between  the  two  models.  Each  request  is  processed  by  one  or  the\\nother model, not both. Results from the two models are logged for analysis (but never\\n\\nIteration and Life Cycle \\n\\n| \\n\\n33\\n\\n\\x0cfor the same request). Drawing statistically meaningful conclusions from an A/B test\\nrequires careful planning of the test.\\n\\nChapter 7 will cover the how-to of A/B testing in more detail, but as a preview, the\\nsimplest form of A/B testing is often referred to as a fixed-horizon test. That’s because\\nin the search for a statistically meaningful conclusion, one has to wait until the care‐\\nfully  predetermined  number  of  samples  have  been  tested.  “Peeking”  at  the  result\\nbefore the test is finished is unreliable. However, if the test is running live in a com‐\\nmercial environment, every bad prediction is likely to cost money, so not being able\\nto stop a test early could be expensive.\\n\\nBayesian,  and  in  particular  multi-armed  bandit,  tests  are  an  increasingly  popular\\nalternative  to  the  “frequentist”  fixed-horizon  test,  with  the  aim  of  drawing  conclu‐\\nsions more quickly. Multi-armed bandit testing is adaptive: the algorithm that decides\\nthe split between models adapts according to live results and reduces the workload of\\nunderperforming models. While multi-armed bandit testing is more complex, it can\\nreduce the business cost of sending traffic to a poorly performing model.\\n\\nIterating on the Edge\\nIterating on an ML model deployed to millions of devices, such as smartphones, sen‐\\nsors, or cars, presents different challenges to iteration in a corporate IT environment.\\nOne  approach  is  to  relay  all  the  feedback  from  the  millions  of  model  instances  to  a\\ncentral point and perform training centrally. Tesla’s autopilot system, running in more\\nthan 500,000 cars, does exactly this. Full retraining of their 50 or so neural networks\\ntakes 70,000 GPU hours.\\n\\nGoogle  has  taken  a  different  approach  with  its  smartphone  keyboard  software,\\nGBoard.  Instead  of  centralized  retraining,  every  smartphone  retrains  the  model\\nlocally and sends a summary of the improvements it has found to Google centrally.\\nThese improvements from every device are averaged and the shared model updated. \\nThis  federated  learning  approach  means  that  an  individual  user’s  personal  data\\ndoesn’t need to be collected centrally, the improved model on each phone can be used\\nimmediately, and the overall power consumption goes down. \\n\\nGovernance\\nGovernance is the set of controls placed on a business to ensure that it delivers on its\\nresponsibilities  to  all  stakeholders,  from  shareholders  and  employees  to  the  public\\nand national governments. These responsibilities include financial, legal, and ethical\\nobligations. Underpinning all three of these is the fundamental principle of fairness.\\n\\nLegal obligations are the easiest to understand. Businesses were constrained by regu‐\\nlations long before the advent of machine learning. Many regulations target specific\\n\\n34 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cindustries;  for  example,  financial  regulations  aim  to  protect  the  public  and  wider\\neconomy  from  finance  mismanagement  and  fraud,  while  pharmaceutical  industries\\nmust  comply  with  rules  to  safeguard  the  health  of  the  public.  Business  practice  is\\nimpacted by broader legislation to protect vulnerable sectors of society and to ensure\\na level playing field on criteria such as sex, race, age, or religion.\\n\\nRecently, governments across the world have imposed regulations to protect the pub‐\\nlic from the impact of the use of personal data by businesses.  The 2016 EU General\\nData Protection Regulation (GDPR) and the 2018 California Consumer Privacy Act\\n(CCPA) typify this trend, and their impact on ML—with its total dependency on data\\n—has  been  immense.  For  example,  GDPR  attempts  to  protect  personal  data  from\\nindustrial  misuse  with  a  goal  of  limiting  the  potential  discrimination  against\\nindividuals.\\n\\nGDPR Principles\\nThe GDPR sets out principles for the processing of personal data, and it’s worth not‐\\ning that the CCPA was built to closely mirror its principles, though it does have some\\nsignificant differences.2 Processing includes the collection, storage, alteration, and use\\nof personal data. These principles are:\\n\\n• Lawfulness, fairness, and transparency\\n\\n• Purpose limitation\\n\\n• Data minimization\\n\\n• Accuracy\\n\\n• Storage limitation\\n\\n• Integrity and confidentiality (security)\\n\\n• Accountability\\n\\nGovernments are now starting to turn their regulatory eye to ML specifically, hoping\\nto  mitigate  the  negative  impact  of  its  use.  The  European  Union  is  leading  the  way\\nwith planned legislation to define the acceptable uses of various forms of AI. This is\\nnot necessarily about reducing use; for example, it may enable beneficial applications\\nof facial recognition technology that are currently restricted by data privacy regula‐\\ntions. But what is clear is that businesses will have to take heed of yet more regulation\\nwhen applying ML.\\n\\nDo businesses care about moral responsibilities to society, beyond formal legislation?\\nIncreasingly, the answer is yes, as seen in the current development of environmental,\\n\\n2 Delve into the differences between GDPR and CCPA.\\n\\nGovernance \\n\\n| \\n\\n35\\n\\n\\x0csocial,  and  governance  (ESG)  performance  indicators.  Trust  matters  to  consumers,\\nand a lack of trust is bad for business. With increasing public activism on the subject,\\nbusinesses  are  engaging  with  ideas  of  Responsible  AI,  the  ethical,  transparent,  and\\naccountable application of AI technology. Trust matters to shareholders, too, and full\\ndisclosure of ML risks is on its way.\\n\\nApplying good governance to MLOPs is challenging. The processes are complex, the\\ntechnology is opaque, and the dependence on data is fundamental. Governance ini‐\\ntiatives in MLOps broadly fall into one of two categories:\\n\\nData governance\\n\\nA framework for ensuring appropriate use and management of data\\n\\nProcess governance\\n\\nThe  use  of  well-defined  processes  to  ensure  all  governance  considerations  have\\nbeen addressed at the correct point in the life cycle of the model and that a full\\nand accurate record has been kept\\n\\nData Governance\\nData  governance  concerns  itself  with  the  data  being  used,  especially  that  for  model\\ntraining, and it can address questions like:\\n\\n• What is the data’s provenance?\\n\\n• How was the original data collected and under what terms of use?\\n\\n• Is the data accurate and up to date?\\n\\n• Is there PII or other forms of sensitive data that should not be used?\\n\\nML projects usually involve significant pipelines, consisting of data cleaning, combi‐\\nnation,  and  transformation  steps.  Understanding  the  data  lineage  is  complex,  espe‐\\ncially  at  the  feature  level,  but  it  is  essential  for  compliance  with  GDPR-style\\nregulations. How can teams—and more broadly organizations, as it matters at the top\\nas well—be sure that no PII is used to train a given model? Anonymizing or pseudo-\\nanonymizing data is not always a sufficient solution to managing personal informa‐\\ntion.  If  these  processes  are  not  performed  correctly,  it  can  still  be  possible  to  single\\nout an individual and their data, contrary to the requirements of GDPR.3\\n\\nInappropriate biases in models can arise quite accidentally despite the best intentions\\nof data scientists. An ML recruitment model famously discriminated against women\\nby identifying that certain schools—all-female schools—were less well represented in\\n\\n3 For more on anonymization, pseudo-anonymization, and why they don’t solve all data privacy woes, we rec‐\\n\\nommend Executing Data Privacy-Compliant Data Projects by Dataiku.\\n\\n36 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cthe company’s upper management, which reflected the historical dominance of men\\nin  the  organization.4  The  point  is  that  making  predictions  based  on  experience  is  a\\npowerful  technique,  but  sometimes  the  consequences  are  not  only  counter-\\nproductive, but illegal.\\n\\nData  governance  tools  that  can  address  these  problems  are  in  their  infancy.  Most\\nfocus on answering these two questions about data lineage:\\n\\n• Where did the information in this dataset come from, and what does this tell me\\n\\nabout how I can use it?\\n\\n• How is this dataset used, and if I change it in some way, what are the implications\\n\\ndownstream?\\n\\nNeither question is easy to answer fully and accurately in real-world data preparation\\npipelines. For example, if a data scientist writes a Python function to in-memory pro‐\\ncess several input datasets and output a single dataset, how can one be sure from what\\ninformation each cell of the new dataset was derived?\\n\\nProcess Governance\\nProcess governance focuses on formalizing the steps in the MLOps process and asso‐\\nciating actions with them. Typically these actions are reviews, sign-offs, and the cap‐\\nture of supporting materials, such as documentation. The aim is twofold:\\n\\n• To ensure every governance-related consideration is made at the correct time and\\ncorrectly acted upon. For example, models should not be deployed to production\\nuntil all validation checks have been passed.\\n\\n• To  enable  oversight  from  outside  of  the  strict  MLOps  process.  Auditors,  risk\\nmanagers, compliance officers, and the business as a whole all have an interest in\\nbeing able to track progress and review decisions at a later stage.\\n\\nEffective implementation of process governance is hard:\\n\\n• Formal  processes  for  the  ML  life  cycle  are  rarely  easy  to  define  accurately.  The\\nunderstanding of the complete process is usually spread across the many teams\\ninvolved,  often  with  no  one  person  having  a  detailed  understanding  of  it  as  a\\nwhole.\\n\\n• For the process to be applied successfully, every team must be willing to adopt it\\n\\nwholeheartedly.\\n\\n4 In 2018, Amazon famously scrapped an AI recruiting tool because of its bias against women.\\n\\nGovernance \\n\\n| \\n\\n37\\n\\n\\x0c• If  the  process  is  just  too  heavy-weight  for  some  use-cases,  teams  will  certainly\\n\\nsubvert it, and much of the benefit will be lost.\\n\\nToday,  process  governance  is  most  commonly  found  in  organizations  with  a  tradi‐\\ntionally  heavy  burden  of  regulation  and  compliance,  such  as  finance.  Outside  of\\nthese,  it  is  rare.  With  ML  creeping  into  all  spheres  of  commercial  activity  and  with\\nrising  concern  about  Responsible  AI,  we  will  need  new  and  innovative  solutions  to\\nthis problem that can work for all businesses. \\n\\nClosing Thoughts\\nGiven  this  overview  of  features  required  for  and  processes  affected  by  MLOps,  it’s\\nclearly not something data teams—or even the data-driven organization at large—can\\nignore.  Nor  is  it  an  item  to  check  off  of  a  list  (“yes,  we  do  MLOps!”),  but  rather  a\\ncomplex  interplay  between  technologies,  processes,  and  people  that  requires  disci‐\\npline and time to do correctly.\\n\\nThe following chapters go deeper into each of the ML model life cycle components at\\nplay in MLOps, providing a look at how each should be done to get closer to the ideal\\nMLOps implementation.\\n\\n38 \\n\\n| \\n\\nChapter 3: Key MLOps Features\\n\\n\\x0cPART II\\nMLOps: How\\n\\n\\x0c\\x0cCHAPTER 4\\nDeveloping Models\\n\\nAdrien Lavoillotte\\n\\nAnyone who wants to be serious about MLOps needs to have at least a cursory under‐\\nstanding  of  the  model  development  process,  which  is  presented  in  Figure  4-1  as  an\\nelement  of  the  larger  ML  project  life  cycle.  Depending  on  the  situation,  the  model\\ndevelopment process can range from quite simple to extremely complex, and it dic‐\\ntates the constraints of subsequent usage, monitoring, and maintenance of models.\\n\\nFigure 4-1. Model development highlighted in the larger context of the ML project life\\ncycle\\n\\nThe implications of the data collection process on the rest of the model’s life is quite\\nstraightforward, and one easily sees how a model can become stale. For other parts of\\nthe model, the effects may be less obvious.\\n\\nFor  example,  take  feature  creation,  where  feeding  a  date  to  the  model  versus  a  flag\\nindicating whether the day is a public holiday may make a big difference in perfor‐\\nmance, but also comes with significantly different constraints on updating the model.\\nOr consider how the metrics used for evaluating and comparing models may enable\\n\\n41\\n\\n\\x0cautomatic switching to the best possible version down the line, should the situation\\nrequire it.\\n\\nThis  chapter  therefore  covers  the  basics  of  model  development,  specifically  in  the\\ncontext  of  MLOps,  that  is,  how  models  might  be  built  and  developed  in  ways  that\\nmake MLOps considerations easier to implement down the line.\\n\\nWhat Is a Machine Learning Model?\\nMachine learning models are leveraged both in academia and in the real world (i.e.,\\nbusiness contexts), so it’s important to distinguish what they represent in theory ver‐\\nsus how they are implemented in practice. Let’s dive into both, building on what we’ve\\nalready seen in Chapter 3.\\n\\nIn Theory\\nA machine learning model is a projection of reality; that is, it’s a partial and approxi‐\\nmate  representation  of  some  aspect  (or  aspects)  of  a  real  thing  or  process.  Which\\naspects  are  represented  often  depends  on  what  is  available  and  useful.  A  machine\\nlearning model, once trained, boils down a mathematical formula that yields a result\\nwhen fed some inputs—say, a probability estimation of some event happening or the\\nestimated value of a raw number.\\n\\nMachine learning models are based on statistical theory, and machine learning algo‐\\nrithms are the tools that build models from training data. Their goal is to find a syn‐\\nthetic representation of the data they are fed, and this data represents the world as it\\nwas  at  the  time  of  collection.  Therefore,  machine  learning  models  can  be  used  to\\nmake predictions when the future looks like the past, because their synthetic repre‐\\nsentation is still valid.\\n\\nGeneralization Capacity\\nMachine  learning  models’  ability  to  accurately  predict  for  cases  that  are  not  exactly\\nlike the input data is called their generalization capacity. Even when they yield outputs\\nlike horses with zebra stripes1 that do not exist in training datasets, they do it by mod‐\\neling a probability distribution that allows them to have this kind of surprising gener‐\\nalization capacity.\\n\\n1 CycleGAN is the implementation of recent research by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\\n\\nA. Efros.\\n\\n42 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cAn often-used example for how machine learning models can predict and generalize\\nis the price of a house. Of course, the selling price of a house will depend on too many\\nfactors too complex to model precisely, but getting close enough to be useful is not so\\ndifficult. The input data for that model may be things inherent to the house like sur‐\\nface  area,  number  of  bedrooms  and  bathrooms,  year  of  construction,  location,  etc.,\\nbut also other more contextual information like the state of the housing market at the\\ntime of sale, whether the seller is in a hurry, and so on. With complete enough histor‐\\nical data, and provided the market conditions do not change too much, an algorithm\\ncan compute a formula that provides a reasonable estimate.\\n\\nAnother  frequent  example  is  a  health  diagnosis  or  prediction  that  someone  will\\ndevelop a certain disease within a given timeframe. This kind of classification model\\noften  outputs  the  probability  of  some  event,  sometimes  also  with  a  confidence\\ninterval.\\n\\nIn Practice\\nA model is the set of parameters necessary to rebuild and apply the formula. It is usu‐\\nally  stateless  and  deterministic  (i.e.,  the  same  inputs  always  give  the  same  outputs,\\nwith some exceptions; see “Online Learning” on page 88).\\n\\nThis  includes  the  parameters  of  the  end  formula  itself,  but  it  also  includes  all  the\\ntransformations  to  go  from  the  input  data  that  will  be  fed  to  the  model  to  the  end\\nformula that will yield a value plus the possible derived data (like a classification or a\\ndecision).  Given  this  description  in  practice,  it  usually  does  not  make  a  difference\\nwhether the model is ML-based or not: it is just a computable mathematical function\\napplied to the input data, one row at a time.\\n\\nIn the house price case, for instance, it may not be practical to gather enough pricing\\ndata for every zip code to get a model that’s accurate enough in all target locations.\\nInstead,  maybe  the  zip  codes  will  be  replaced  with  some  derived  inputs  that  are\\ndeemed  to  have  the  most  influence  on  price—say,  average  income,  population  den‐\\nsity,  or  proximity  to  some  amenities.  But  since  end  users  will  continue  to  input  the\\nzip code and not these derived inputs, for all intents and purposes, all of this transfor‐\\nmation is also part of the pricing model.\\n\\nOutputs  can  also  be  richer  than  a  single  number.  A  system  that  detects  fraud,  for\\nexample, will often provide some kind of probability (and in some cases maybe also a\\nconfidence interval) rather than a binary answer. Depending on the acceptability of\\nfraud and the cost of subsequent verification or denial of the transaction, it may be set\\nup  to  only  classify  fraudulent  instances  where  the  probability  reaches  some  fine-\\ntuned threshold. Some models even include recommendations or decisions, such as\\nwhich product to show a visitor to maximize spending or which treatment provides\\nthe most probable recovery.\\n\\nWhat Is a Machine Learning Model? \\n\\n| \\n\\n43\\n\\n\\x0cAll of these transformations and associated data are part of the model to some degree;\\nhowever, this does not mean they are always bundled in a monolithic package, as one\\nsingle  artifact  compiled  together.  This  could  quickly  get  unwieldy,  and,  in  addition,\\nsome parts of this information come with varying constraints (different refresh rates,\\nexternal sources, etc.).\\n\\nRequired Components\\nBuilding  a  machine  learning  model  requires  many  components  as  outlined  in\\nTable 4-1.\\n\\nTable 4-1. Required components of a machine learning model\\n\\nML component\\nTraining data\\n\\nA performance\\nmetric\\n\\nML algorithm\\n\\nHyperparameters\\n\\nDescription\\nTraining data is usually labeled for the prediction case with examples of what is being modeled\\n(supervised learning). It might sound obvious, but it’s important to have good training data. An\\nillustrative example of when this was not the case was data from damaged planes during World War II,\\nwhich suffered from survivor bias and therefore was not good training data.\\nA performance metric is what the model being developed seeks to optimize. It should be chosen\\ncarefully to avoid unintended consequences, like the cobra effect (named for a famous anecdote, where\\na reward for dead cobras led some to breed them). For example, if 95% of the data has class A,\\noptimizing for raw accuracy may produce a model that always predicts A and is 95% accurate.\\nThere are a variety of models that work in various ways and have different pros and cons. It is\\nimportant to note that some algorithms are more suited to certain tasks than others, but their selection\\nalso depends on what needs to be prioritized: performance, stability, interpretability, computation cost,\\netc.\\nHyperparameters are configurations for ML algorithms. The algorithm contains the basic formula, the\\nparameters it learns are the operations and operands that make up this formula for that particular\\nprediction task, and the hyperparameters are the ways that the algorithm may go to find these\\nparameters.\\nFor example, in a decision tree (where data continues to be split in two according to what looks to be\\nthe best predictor in the subset that reached this path), one hyperparameter is the depth of the tree\\n(i.e., the number of splits).\\n\\nEvaluation dataset When using labeled data, an evaluation dataset that is different from the training set will be required\\n\\nto evaluate how the model performs on unseen data (i.e., how well it can generalize).\\n\\nThe sheer number and complexity of each individual component is part of what can\\nmake good MLOps a challenging undertaking. But the complexity doesn’t stop here,\\nas algorithm choice is yet another piece of the puzzle.\\n\\n44 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cDifferent ML Algorithms, Different MLOps Challenges\\nWhat ML algorithms all have in common is that they model patterns in past data to\\nmake inferences, and the quality and relevance of this experience are the key factors\\nin  their  effectiveness.  Where  they  differ  is  that  each  style  of  algorithm  has  specific\\ncharacteristics and presents different challenges in MLOps (outlined in Table 4-2).\\n\\nTable 4-2. MLOps considerations by algorithm type\\n\\nAlgorithm\\ntype\\nLinear\\n\\nTree-based\\n\\nDeep learning\\n\\nName\\n\\nMLOps considerations\\n\\nLinear\\nregression\\nLogistic\\nregression\\nDecision tree\\n\\nRandom forest\\n\\nGradient\\nboosting\\nNeural\\nnetworks\\n\\nThere is a tendency for overfitting.\\n\\nThere is a tendency for overfitting.\\n\\nCan be unstable—small changes in data can lead to a large change in the structure of the\\noptimal decision tree.\\nPredictions can be difficult to understand, which is challenging from a Responsible AI\\nperspective. Random forest models can also be relatively slow to output predictions, which\\ncan present challenges for applications.\\nLike random forest, predictions can be difficult to understand. Also, a small change in the\\nfeature or training set can create radical changes in the model.\\nIn terms of interpretability, deep learning models are almost impossible to understand.\\nDeep learning algorithms, including neural networks, are also extremely slow to train and\\nrequire a lot of power (and data). Is it worth the resources, or would a simpler model work\\njust as well?\\n\\nSome ML algorithms can best support specific use cases, but governance considera‐\\ntions may also play a part in the choice of algorithm. In particular, highly regulated\\nenvironments where decisions must be explained (e.g., financial services) cannot use\\nopaque algorithms such as neural networks; rather, they have to favor simpler techni‐\\nques, such as decision trees. In many use cases, it’s not so much a trade-off on perfor‐\\nmance but rather a trade-off on cost. That is, simpler techniques usually require more\\ncostly  manual  feature  engineering  to  reach  the  same  level  of  performance  as  more\\ncomplex techniques.\\n\\nComputing Power\\nWhen talking about components of machine learning model development, it’s impos‐\\nsible to ignore computing power. Some say planes fly thanks to human ingenuity, but\\nit’s also thanks to a lot of fuel. This holds true with machine learning as well: its devel‐\\nopment is inversely proportional to the cost of computing power.\\n\\nFrom hand-computed linear regression of the early twentieth century to today’s larg‐\\nest deep learning models, new algorithms arose when the required computing power\\n\\nWhat Is a Machine Learning Model? \\n\\n| \\n\\n45\\n\\n \\n\\x0cbecame available. For example, mainstream algorithms like random forest and gradi‐\\nent boosting both require a computing power that was expensive 20 years ago.\\n\\nIn exchange, they brought an ease of use that considerably lowered the cost of devel‐\\noping ML models, thus putting new use cases within the reach of the average organi‐\\nzation. The decrease in the cost of data also helped, but it was not the first driver: very\\nfew algorithms leverage big data technology in which both data and computation are\\ndistributed over a large number of computers; rather, most of them still operate with\\nall the training data in memory.\\n\\nData Exploration\\nWhen data scientists or analysts consider data sources to train a model, they need to\\nfirst  get  a  grasp  of  what  that  data  looks  like.  Even  a  model  trained  using  the  most\\neffective  algorithm  is  only  as  good  as  its  training  data.  At  this  stage,  a  number  of\\nissues can prevent all or part of the data from being useful, including incompleteness,\\ninaccuracy, inconsistency, etc.\\n\\nExamples of such processes can include:\\n\\n• Documenting  how  the  data  was  collected  and  what  assumptions  were  already\\n\\nmade\\n\\n• Looking  at  summarizing  statistics  of  the  data:  What  is  the  domain  of  each  col‐\\numn? Are there some rows with missing values? Obvious mistakes? Strange out‐\\nliers? No outliers at all?\\n\\n• Taking a closer look at the distribution of the data\\n\\n• Cleaning, filling, reshaping, filtering, clipping, sampling, etc.\\n\\n• Checking correlations between the different columns, running statistical tests on\\n\\nsome subpopulations, fitting distribution curves\\n\\n• Comparing that data to other data or models in the literature: Is there some usual\\n\\ninformation that seems to be missing? Is this data comparably distributed?\\n\\nOf  course,  domain  knowledge  is  required  to  make  informed  decisions  during  this\\nexploration. Some oddities may be hard to spot without specific insight, and assump‐\\ntions made can have consequences that are not obvious to the untrained eye. Indus‐\\ntrial  sensor  data  is  a  good  example:  unless  the  data  scientist  is  also  a  mechanical\\nengineer or expert in the equipment, they might not know what constitutes normal\\nversus strange outliers for a particular machine.\\n\\n46 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cFeature Engineering and Selection\\nFeatures are how data is presented to a model, serving to inform that model on things\\nit  may  not  infer  by  itself.  This  table  provides  examples  of  how  features  may  be\\nengineered:\\n\\nFeature\\nengineering\\ncategory\\nDerivatives\\nEnrichment\\nEncoding\\nCombination\\n\\nDescription\\n\\nInfer new information from existing information—e.g., what day of the week is this date?\\nAdd new external information—e.g., is this day a public holiday?\\nPresent the same information differently—e.g., day of the week or weekday versus weekend.\\nLink features together—e.g., the size of the backlog might need to be weighted by the complexity of\\nthe different items in it.\\n\\nFor instance, in trying to estimate the potential duration of a business process given\\nthe current backlog, if one of the inputs is a date, it is pretty common to derive the\\ncorresponding day of the week or how far ahead the next public holiday is from that\\ndate.  If  the  business  serves  multiple  locations  that  observe  different  business  calen‐\\ndars, that information may also be important.\\n\\nAnother example, to follow up on the house pricing scenario from the previous sec‐\\ntion, would be using average income and population density, which ideally allows the\\nmodel to better generalize and train on more diverse data than trying to segment by\\narea (i.e., zip code).\\n\\nFeature Engineering Techniques\\nA whole market exists for such complementary data that extends far beyond the open\\ndata  that  public  institutions  and  companies  share.  Some  services  provide  direct\\nenrichment that can save a lot of time and effort.\\n\\nThere are, however, many cases when information that data scientists need for their\\nmodels is not available. In this case, there are techniques like impact coding, whereby\\ndata scientists replace a modality by the average value of the target for that modality,\\nthus allowing the model to benefit from data in a similar range (at the cost of some\\ninformation loss).\\n\\nUltimately, most ML algorithms require a table of numbers as input, each row repre‐\\nsenting a sample, and all samples coming from the same dataset. When the input data\\nis not tabular, data scientists can use other tricks to transform it.\\n\\nThe most common one is one-hot encoding. For example, a feature that can take three\\nvalues (e.g., Raspberry, Blueberry, and Strawberry) is transformed into three features\\n\\nFeature Engineering and Selection \\n\\n| \\n\\n47\\n\\n\\x0cthat  can  take  only  two  values—yes  or  no  (e.g.,  Raspberry  yes/no,  Blueberry  yes/no,\\nStrawberry yes/no).\\n\\nText  or  image  inputs,  on  the  other  hand,  require  more  complex  engineering.  Deep\\nlearning  has  recently  revolutionized  this  field  by  providing  models  that  transform\\nimages and text into tables of numbers that are usable by ML algorithms. These tables\\nare  called  embeddings,  and  they  allow  data  scientists  to  perform  transfer  learning\\nbecause they can be used in domains on which they were not trained.\\n\\nTransfer Learning\\nTransfer  learning  is  the  technique  of  using  information  gained  from  solving  one\\nproblem in solving a different problem. Transfer learning can be used to significantly\\naccelerate learning of second or subsequent tasks, and it is very popular in deep learn‐\\ning, where the resources needed to train models can be enormous.\\n\\nFor example, even if a particular deep learning model was trained on images that did\\nnot contain any forks, it may give a useful embedding to be used by a model that is\\ntrained  to  detect  them,  because  a  fork  is  an  object,  and  that  model  was  trained  to\\ndetect similar human-made objects.\\n\\nHow Feature Selection Impacts MLOps Strategy\\nWhen it comes to feature creation and selection, the question of how much and when\\nto  stop  comes  up  regularly.  Adding  more  features  may  produce  a  more  accurate\\nmodel, achieve more fairness when splitting into more precise groups, or compensate\\nfor some other useful missing information. However, it also comes with downsides,\\nall of which can have a significant impact on MLOps strategies down the line:\\n\\n• The model can become more and more expensive to compute.\\n\\n• More features require more inputs and more maintenance down the line.\\n\\n• More features mean a loss of some stability.\\n\\n• The sheer number of features can raise privacy concerns.\\n\\nAutomated  feature  selection  can  help  by  using  heuristics  to  estimate  how  critical\\nsome features will be for the predictive performance of the model. For instance, one\\ncan look at the correlation with the target variable or quickly train a simple model on\\na representative subset of the data and then look at which features are the strongest\\npredictors.\\n\\nWhich inputs to use, how to encode them, how they interact or interfere with each\\nother—such  decisions  require  a  certain  understanding  of  the  inner  workings  of  the\\nML algorithm. The good news is that some of this can be partly automated, e.g., by\\n\\n48 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cusing tools such as Auto-sklearn or AutoML applications that cross-reference features\\nagainst  a  given  target  to  estimate  which  features,  derivatives,  or  combinations  are\\nlikely  to  yield  the  best  results,  leaving  out  all  the  features  that  would  probably  not\\nmake that much of a difference.\\n\\nOther  choices  still  require  human  intervention,  such  as  deciding  whether  to  try  to\\ncollect additional information that might improve the model. Spending time to build\\nbusiness-friendly  features  will  often  improve  the  final  performance  and  ease  the\\nadoption  by  end  users,  as  model  explanations  are  likely  to  be  simpler.  It  can  also\\nreduce  modeling  debt,  allowing  data  scientists  to  understand  the  main  prediction\\ndrivers  and  ensure  that  they  are  robust.  Of  course,  there  are  trade-offs  to  consider\\nbetween  the  cost  of  time  spent  to  understand  the  model  and  the  expected  value,  as\\nwell as risks associated with the model’s use.\\n\\nFeature Stores\\nFeature  factories,  or  feature  stores,  are  repositories  of  different  features  associated\\nwith business entities that are created and stored in a central location for easier reuse.\\nThey usually combine an offline part (slower, but potentially more powerful) and an\\nonline part (quicker and more useful for real-time needs), making sure they remain\\nconsistent with each other.\\n\\nGiven  how  time-consuming  feature  engineering  is  for  data  scientists,  feature  stores\\nhave huge potential to free up their time for even more valuable tasks. Machine learn‐\\ning  is  still  often  the  “high-interest  credit  card  of  technical  debt”.  Reversing  this  will\\nrequire huge efficiency gains in the data-to-model-to-production process and in the\\nMLOps process, and feature stores are one way to get there.\\n\\nThe bottom line is that when building models, the process of engineering and select‐\\ning  features,  like  many  other  ML  model  components,  is  a  delicate  balance  between\\nconsidering MLOps components and performance.\\n\\nExperimentation\\nExperimentation takes place throughout the entire model development process, and\\nusually every important decision or assumption comes with at least some experiment\\nor previous research to justify it. Experimentation can take many shapes, from build‐\\ning full-fledged predictive ML models to doing statistical tests or charting data. Goals\\nof experimentation include:\\n\\n• Assessing  how  useful  or  how  good  of  a  model  can  be  built  given  the  elements\\noutlined in Table 4-1. (The next section will cover model evaluation and compar‐\\nison in more detail.)\\n\\nExperimentation \\n\\n| \\n\\n49\\n\\n\\x0c• Finding the best modeling parameters (algorithms, hyperparameters, feature pre‐\\n\\nprocessing, etc.).\\n\\n• Tuning the bias/variance trade-off for a given training cost to fit that definition of\\n\\n“best.”\\n\\n• Finding  a  balance  between  model  improvement  and  improved  computation\\ncosts. (Since there’s always room for improvement, how good is good enough?)\\n\\nBias and Variance\\nA high bias model (also known as underfitting) fails to discover some of the rules that\\ncould  have  been  learned  from  the  training  data,  possibly  because  of  reductive\\nassumptions making the model overly simplistic.\\n\\nA  high  variance  model  (or  overfitting)  sees  patterns  in  noise  and  seeks  to  predict\\nevery  single  variation,  resulting  in  a  complex  model  that  does  not  generalize  well\\nbeyond its training data.\\n\\nWhen experimenting, data scientists need to be able to quickly iterate through all the\\npossibilities for each of the model building blocks outlined in Table 4-1. Fortunately,\\nthere are tools to do all of this semiautomatically, where you only need to define what\\nshould  be  tested  (the  space  of  possibilities)  depending  on  prior  knowledge  (what\\nmakes sense) and the constraints (e.g., computation, budget).\\n\\nSome tools allow for even more automation, for instance by offering stratified model\\ntraining. For example, say the business wants to predict customer demand for prod‐\\nucts to optimize inventory, but behavior varies a lot from one store to the next. Strati‐\\nfied modeling consists of training one model per store that can be better optimized\\nfor each store rather than a model that tries to predict in all stores.\\n\\nTrying  all  combinations  of  every  possible  hyperparameter,  feature  handling,  etc.,\\nquickly becomes untraceable. Therefore, it is useful to define a time and/or computa‐\\ntion budget for experiments as well as an acceptability threshold for usefulness of the\\nmodel (more on that notion in the next section).\\n\\nNotably, all or part of this process may need to be repeated every time anything in the\\nsituation  changes  (including  whenever  the  data  and/or  problem  constraints  change\\nsignificantly; see “Drift Detection in Practice” on page 92). Ultimately, this means that\\nall experiments that informed the final decisions data scientists made to arrive at the\\nmodel as well as all the assumptions and conclusions along the way may need to be\\nrerun and reexamined.\\n\\nFortunately,  more  and  more  data  science  and  machine  learning  platforms  allow  for\\nautomating these workflows not only on the first run, but also to preserve all the pro‐\\n\\n50 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0ccessing operations for repeatability. Some also allow for the use of version control and\\nexperimental branch spin-off to test theories and then merge, discard, or keep them\\n(see “Version Management and Reproducibility” on page 56).\\n\\nEvaluating and Comparing Models\\nGeorge E. P. Box, a twentieth century British statistician, once said that all models are\\nwrong, but some are useful. In other words, a model should not aim to be perfect, but\\nit  should  pass  the  bar  of  “good  enough  to  be  useful”  while  keeping  an  eye  on  the\\nuncanny valley—typically a model that looks like it’s doing a good job but does a bad\\n(or  catastrophic)  job  for  a  specific  subset  of  cases  (say,  an  underrepresented\\npopulation).\\n\\nWith this in mind, it’s important to evaluate a model in context and have some ability\\nto compare it to what existed before the model—whether a previous model or a rules-\\nbased process—to get an idea of what the outcome would be if the current model or\\ndecision process were replaced by the new one.\\n\\nA  model  with  an  absolute  performance  that  could  technically  be  considered  disap‐\\npointing  can  still  possibly  enhance  an  existing  situation.  For  instance,  having  a\\nslightly more accurate forecast of demand for a certain product or service may result\\nin huge cost-savings.\\n\\nConversely,  a  model  that  gets  a  perfect  score  is  suspicious,  as  most  problems  have\\nnoise in the data that’s at least somewhat hard to predict. A perfect or nearly-perfect\\nscore may be a sign that there is a leak in the data (i.e., that the target to be predicted\\nis also in the input data or that an input feature is very correlated to the target but,\\npractically,  available  only  once  the  target  is  known)  or  that  the  model  overfits  the\\ntraining data and will not generalize well.\\n\\nChoosing Evaluation Metrics\\nChoosing the proper metric by which to evaluate and compare different models for a\\ngiven problem can lead to very different models (think of the cobra effect mentioned\\nin Table 4-1). A simple example: accuracy is often used for automated classification\\nproblems but is rarely the best fit when the classes are unbalanced (i.e., when one of\\nthe outcomes is very unlikely compared to the other). In a binary classification prob‐\\nlem where the positive class (i.e., the one that is interesting to predict because its pre‐\\ndiction  triggers  an  action)  is  rare,  say  5%  of  occurrences,  a  model  that  constantly\\npredicts the negative class is therefore 95% accurate, while also utterly useless.\\n\\nUnfortunately, there is no one-size-fits-all metric. You need to pick one that matches\\nthe  problem  at  hand,  which  means  understanding  the  limits  and  trade-offs  of  the\\nmetric (the mathematics side) and their impact on the optimization of the model (the\\nbusiness side).\\n\\nEvaluating and Comparing Models \\n\\n| \\n\\n51\\n\\n\\x0cTo get an idea of how well a model will generalize, that metric should be evaluated on\\na  part  of  the  data  that  was  not  used  for  the  model’s  training  (a  holdout  dataset),  a\\nmethod called cross-testing. There can be multiple steps where some data is held for\\nevaluation and the rest is used for training or optimizing, such as metric evaluation or\\nhyperparameter  optimization.  There  are  different  strategies  as  well,  not  necessarily\\njust  a  simple  split.  In  k-fold  cross-validation,  for  example,  data  scientists  rotate  the\\nparts that they hold out to evaluate and train multiple times. This multiplies the train‐\\ning time but gives an idea of the stability of the metric.\\n\\nWith a simple split, the holdout dataset can consist of the most recent records instead\\nof randomly chosen ones. Indeed, as models are usually used for future predictions, it\\nis  likely  that  assessing  them  as  if  they  were  used  for  prediction  on  the  most  recent\\ndata leads to more realistic estimations. In addition, it allows one to assess whether\\nthe data drifted between the training and the holdout dataset (see “Drift Detection in\\nPractice” on page 92 for more details).\\n\\nAs  an  example,  Figure  4-2  shows  a  scheme  in  which  a  test  dataset  is  a  holdout  (in\\ngray) in order to perform the evaluation. The remaining data is split into three parts\\nto find the best hyperparameter combination by training the model three times with a\\ngiven  combination  on  each  of  the  blue  datasets,  and  validating  its  performance  on\\ntheir respective green datasets. The gray dataset is used only once with the best hyper‐\\nparameter combination, while the other datasets are used with all of them.\\n\\nFigure 4-2. An example of dataset split for model evaluation\\n\\nOftentimes,  data  scientists  want  to  periodically  retrain  models  with  the  same  algo‐\\nrithms, hyperparameters, features, etc., but on more recent data. Naturally, the next\\nstep  is  to  compare  the  two  models  and  see  how  the  new  version  fares.  But  it’s  also\\nimportant  to  make  sure  all  previous  assumptions  still  hold:  that  the  problem  hasn’t\\nfundamentally  shifted,  that  the  modeling  choices  made  previously  still  fit  the  data,\\netc.  This  is  more  specifically  part  of  performance  and  drift  monitoring  (find  more\\ndetails on this in Chapter 7).\\n\\n52 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cCross-Checking Model Behavior\\nBeyond  the  raw  metrics,  when  evaluating  a  model,  it’s  critical  to  understand  how  it\\nwill behave. Depending on the impact of the model’s predictions, decisions, or classi‐\\nfications, a more or less deep understanding may be required. For example, data sci‐\\nentists  should  take  reasonable  steps  (with  respect  to  that  impact)  to  ensure  that  the\\nmodel is not actively harmful: a model that would predict that all patients need to be\\nchecked by a doctor may score high in terms of raw prevention, but not so much on\\nrealistic resource allocation.\\n\\nExamples of these reasonable steps include:\\n\\n• Cross-checking different metrics (and not only the ones on which the model was\\n\\ninitially optimized)\\n\\n• Checking how the model reacts to different inputs—e.g., plot the average predic‐\\ntion (or probability for classification models) for different values of some inputs\\nand see whether there are oddities or extreme variability\\n\\n• Splitting  one  particular  dimension  and  checking  the  difference  in  behavior  and\\nmetrics across different subpopulations—e.g., is the error rate the same for males\\nand females?\\n\\nThese kinds of global analyses should not be understood as causality, just as correla‐\\ntion. They do not necessarily imply a specific causal relationship between some vari‐\\nables  and  an  outcome;  they  merely  show  how  the  model  sees  that  relationship.  In\\nother words, the model should be used with care for what-if analysis. If one feature\\nvalue is changed, the model prediction is likely to be wrong if the new feature value\\nhas never been seen in the training dataset or if it has never been seen in combination\\nwith the values of the other features in this dataset.\\n\\nWhen comparing models, those different aspects should be accessible to data scien‐\\ntists, who need to be able to go deeper than a single metric. That means the full envi‐\\nronment  (interactive  tooling,  data,  etc.)  needs  to  be  available  for  all  models,  ideally\\nallowing for comparison from all angles and between all components. For example,\\nfor  drift,  the  comparison  might  use  the  same  settings  but  different  data,  while  for\\nmodeling performance, it might use the same data but different settings.\\n\\nImpact of Responsible AI on Modeling\\nDepending on the situation (and sometimes depending on the industry or sector of\\nthe  business),  on  top  of  a  general  understanding  of  model  behavior,  data  scientists\\nmay also need models’ individual predictions to be explainable, including having an\\nidea of what specific features pushed the prediction one way or the other. Sometimes\\npredictions may be very different for a specific record than on average. Popular meth‐\\nods to compute individual prediction explanations include Shapley value (the average\\n\\nEvaluating and Comparing Models \\n\\n| \\n\\n53\\n\\n\\x0cmarginal contribution of a feature value across all possible coalitions) and individual\\nconditional  expectation  (ICE)  computations,  which  show  the  dependence  between\\nthe target functions and features of interest.\\n\\nFor example, the measured level of a specific hormone could generally push a model\\nto predict someone has a health issue, but for a pregnant woman, that level makes the\\nmodel  infer  she  is  at  no  such  risk.  Some  legal  frameworks  mandate  some  kind  of\\nexplainability for decisions made by a model that have consequences on humans, like\\nrecommending a loan to be denied. “Element 2: Bias” on page 114 discusses this topic\\nin detail.\\n\\nNote  that  the  notion  of  explainability  has  several  dimensions.  In  particular,  deep\\nlearning networks are sometimes called black-box models because of their complexity\\n(though when reading the model coefficients, a model is fully specified, and it is usu‐\\nally a conceptually remarkably simple formula, but a very large formula that becomes\\nimpossible to intuitively apprehend). Conversely, global and local explanation tools—\\nsuch as partial dependence plots or Shapley value computations—give some insights\\nbut  arguably  do  not  make  the  model  intuitive.  To  actually  communicate  a  rigorous\\nand  intuitive  understanding  of  what  exactly  the  model  is  doing,  limiting  the  model\\ncomplexity is required.\\n\\nFairness  requirements  can  also  have  dimensioning  constraints  on  model  develop‐\\nment. Consider this theoretical example to better understand what is at stake when it\\ncomes to bias: a US-based organization regularly hires people who do the same types\\nof  jobs.  Data  scientists  could  train  a  model  to  predict  the  workers’  performance\\naccording  to  various  characteristics,  and  people  would  then  be  hired  based  on  the\\nprobability that they would be high-performing workers.\\n\\nThough this seems like a simple problem, unfortunately, it’s fraught with pitfalls. To\\nmake  this  problem  completely  hypothetical  and  to  detach  it  from  the  complexities\\nand problems of the real world, let’s say everyone in the working population belongs\\nto one of two groups: Weequay or Togruta.\\n\\nFor  this  hypothetical  example,  let’s  claim  that  a  far  larger  population  of  Weequay\\nattend  university.  Off  the  bat,  there  would  be  an  initial  bias  in  favor  of  Weequay\\n(amplified by the fact they would have been able to develop their skills through years\\nof experience).\\n\\nAs a result, there would not only be more Weequay than Togruta in the pool of appli‐\\ncants, but Weequay applicants would tend to be more qualified. The employer has to\\nhire 10 people during the month to come. What should it do?\\n\\n• As an equal opportunity employer, it should ensure the fairness of its recruitment\\nprocess  as  it  controls  it.  That  means  in  mathematical  terms,  for  each  applicant\\nand all things being equal, being hired (or not) should not depend on their group\\naffiliation (in this case, Weequay or Togruta). However, this results in bias in and\\n\\n54 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cof itself, as Weequay are more qualified. Note that “all things being equal” can be\\ninterpreted in various ways, but the usual interpretation is that the organization\\nis likely not considered accountable for processes it does not control.\\n\\n• The  employer  may  also  have  to  avoid  disparate  impact,  that  is,  practices  in\\nemployment that adversely affect one group of people of a protected characteris‐\\ntic  more  than  another.  Disparate  impact  is  assessed  on  subpopulations  and  not\\non individuals; practically, it assesses whether proportionally speaking, the com‐\\npany has hired as many Weequay as Togruta. Once again, the target proportions\\nmay be those of the applicants or those of the general population, though the for‐\\nmer  is  more  likely,  as  again,  the  organization  can’t  be  accountable  for  biases  in\\nprocesses out of its control.\\n\\nThe two objectives are mutually exclusive. In this scenario, equal opportunity would\\nlead to hiring 60% (or more) Weequay and 40% (or fewer) Togruta. As a result, the\\nprocess  has  a  disparate  impact  on  the  two  populations,  because  the  hiring  rates  are\\ndifferent.\\n\\nConversely,  if  the  process  is  corrected  so  that  40%  of  people  hired  are  Togruta  to\\navoid  disparate  impact,  it  means  that  some  rejected  Weequay  applicants  will  have\\nbeen predicted as more qualified than some accepted Togruta applicants (contradict‐\\ning the equal opportunity assertion).\\n\\nThere needs to be a trade-off—the law sometimes referred to as the 80% rule. In this\\nexample, it would mean that the hiring rate of Togruta should be equal to or larger\\nthan 80% of the hiring rate of Weequay. In this example, it means that it would be OK\\nto hire up to 65% Weequay.\\n\\nThe  point  here  is  that  defining  these  objectives  cannot  be  a  decision  made  by  data\\nscientists  alone.  But  even  once  the  objectives  are  defined,  the  implementation  itself\\nmay also be problematic:\\n\\n• Without any indications, data scientists naturally try to build equal opportunity\\nmodels because they correspond to models of the world as it is. Most of the tools\\ndata scientists employ also try to achieve this because it is the most mathemati‐\\ncally  sound  option.  Yet  some  ways  to  achieve  this  goal  may  be  unlawful.  For\\nexample,  the  data  scientist  may  choose  to  implement  two  independent  models:\\none for Weequay and one for Togruta. This could be a reasonable way to address\\nthe biases induced by a training dataset in which Weequay are overrepresented,\\nbut  it  would  induce  a  disparate  treatment  of  the  two  that  could  be  considered\\ndiscriminatory.\\n\\n• To let data scientists use their tools in the way they were designed (i.e., to model\\nthe world as it is), they may decide to post-process the predictions so that they fit\\nwith  the  organization’s  vision  of  the  world  as  it  should  be.  The  simplest  way  of\\ndoing this is to choose a higher threshold for Weequay than for Togruta. The gap\\n\\nEvaluating and Comparing Models \\n\\n| \\n\\n55\\n\\n\\x0cbetween  them  will  adjust  the  trade-off  between  “equal  opportunity”  and  “equal\\nimpact”; however, it may still be considered discriminatory because of the dispa‐\\nrate treatment.\\n\\nData  scientists  are  unlikely  to  be  able  to  sort  this  problem  out  alone  (see  “Key  Ele‐\\nments of Responsible AI” on page 113 for a broader view on the subject). This simple\\nexample illustrates the complexity of the subject, which may be even more complex\\ngiven that there may be many protected attributes, and the fact that bias is as much a\\nbusiness question as a technical question.\\n\\nConsequently, the solution heavily depends on the context. For instance, this example\\nof Weequay and Togruta is representative of processes that give access to privileges.\\nThe situation is different if the process has negative impacts on the user (like fraud\\nprediction that leads to transaction rejection) or is neutral (like disease prediction).\\n\\nVersion Management and Reproducibility\\nDiscussing the evaluation and comparison of models (for fairness as discussed imme‐\\ndiately before, but also a host of other factors) necessarily brings up the idea of ver‐\\nsion control and the reproducibility of different model versions. With data scientists\\nbuilding, testing, and iterating on several versions of models, they need to be able to\\nkeep all the versions straight.\\n\\nVersion management and reproducibility address two different needs:\\n\\n• During  the  experimentation  phase,  data  scientists  may  find  themselves  going\\nback  and  forth  on  different  decisions,  trying  out  different  combinations,  and\\nreverting  when  they  don’t  produce  the  desired  results.  That  means  having  the\\nability  to  go  back  to  different  “branches”  of  the  experiments—for  example,\\nrestoring a previous state of a project when the experimentation process led to a\\ndead end.\\n\\n• Data scientists or others (auditors, managers, etc.) may need to be able to replay\\nthe computations that led to model deployment for an audit team several years\\nafter the experimentation was first done.\\n\\nVersioning has arguably been somewhat solved when everything is code-based, with\\nsource version control technology. Modern data processing platforms typically offer\\nsimilar capabilities for data transformation pipelines, model configuration, etc. Merg‐\\ning several parts is, of course, less straightforward than merging code that diverged,\\nbut the basic need is to be able to go back to some specific experiment, if only to be\\nable to copy its settings to replicate them in another branch.\\n\\nAnother very important property of a model is reproducibility. After a lot of experi‐\\nments and tweaking, data scientists may arrive at a model that fits the bill. But after\\n\\n56 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cthat, operationalization necessitates model reproduction not only in another environ‐\\nment,  but  also  possibly  from  a  different  starting  point.  Repeatability  also  makes\\ndebugging much easier (sometimes even simply possible). To this end, all facets of the\\nmodel need to be documented and reusable, including:\\n\\nAssumptions\\n\\nWhen  a  data  scientist  makes  decisions  and  assumptions  about  the  problem  at\\nhand, its scope, the data, etc., they should all be explicit and logged so that they\\ncan be checked against any new information down the line.\\n\\nRandomness\\n\\nA  lot  of  ML  algorithms  and  processes,  such  as  sampling,  make  use  of  pseudo-\\nrandom numbers. Being able to precisely reproduce an experiment, such as for\\ndebugging, means to have control over that pseudo-randomness, most often by\\ncontrolling  the  “seed”  of  the  generator  (i.e.,  the  same  generator  initialized  with\\nthe same seed would yield the same sequence of pseudo-random numbers).\\n\\nData\\n\\nTo  get  repeatability,  the  same  data  must  be  available.  This  can  sometimes  be\\ntricky  because  the  storage  capacity  required  to  version  data  can  be  prohibitive\\ndepending on the rate of update and quantity. Also, branching on data does not\\nyet have as rich an ecosystem of tools as branching on code.\\n\\nSettings\\n\\nThis one is a given: all processing that has been done must be reproducible with\\nthe same settings.\\n\\nResults\\n\\nWhile developers use merging tools to compare and merge different text file ver‐\\nsions,  data  scientists  need  to  be  able  to  compare  in-depth  analysis  of  models\\n(from confusion matrices to partial dependence plots) to obtain models that sat‐\\nisfy the requirements.\\n\\nImplementation\\n\\nEver-so-slightly  different  implementations  of  the  same  model  can  actually  yield\\ndifferent models, enough to change the predictions on some close calls. And the\\nmore  sophisticated  the  model,  the  higher  the  chances  that  these  discrepancies\\nhappen. On the other hand, scoring a dataset in bulk with a model comes with\\ndifferent  constraints  than  scoring  a  single  record  live  in  an  API,  so  different\\nimplementations  may  sometimes  be  warranted  for  the  same  model.  But  when\\ndebugging and comparing, data scientists need to keep the possible differences in\\nmind.\\n\\nVersion Management and Reproducibility \\n\\n| \\n\\n57\\n\\n\\x0cEnvironment\\n\\nGiven  all  the  steps  covered  in  this  chapter,  it’s  clear  that  a  model  is  not  just  its\\nalgorithm and parameters. From the data preparation to the scoring implementa‐\\ntion, including feature selection, feature encoding, enrichment, etc., the environ‐\\nment in which several of those steps run may be more or less implicitly tied to the\\nresults. For instance, a slightly different version of a Python package involved in\\none step may change the results in ways that can be hard to predict. Preferably,\\ndata scientists should make sure that the runtime environment is also repeatable.\\nGiven the pace at which ML is evolving, this might require techniques that freeze\\nthe computation environments.\\n\\nFortunately,  part  of  the  underlying  documentation  tasks  associated  with  versioning\\nand  reproducibility  can  be  automated,  and  the  use  of  an  integrated  platform  for\\ndesign  and  deployment  can  greatly  decrease  the  reproducibility  costs  by  ensuring\\nstructured information transfer.\\n\\nClearly, while maybe not the sexiest part of model development, version management\\nand  reproducibility  are  critical  to  building  machine  learning  efforts  in  real-world\\norganizational settings where governance—including audits—matters.\\n\\nClosing Thoughts\\nModel  development  is  one  of  the  most  critical  and  consequential  steps  of  MLOps.\\nThe  many  technical  questions  that  are  necessarily  answered  during  this  phase  have\\nbig repercussions on all aspects of the MLOps process throughout the life of the mod‐\\nels.  Therefore,  exposure,  transparency,  and  collaboration  are  crucial  to  long-term\\nsuccess.\\n\\nThe model development stage is also the one that has been practiced the most by pro‐\\nfiles like data scientists and, in the pre-MLOps world, often represents the whole ML\\neffort,  yielding  a  model  that  will  then  be  used  as  is  (with  all  its  consequences  and\\nlimitations).\\n\\n58 \\n\\n| \\n\\nChapter 4: Developing Models\\n\\n\\x0cCHAPTER 5\\nPreparing for Production\\n\\nJoachim Zentici\\n\\nConfirming that something works in the laboratory has never been a sure sign it will\\nwork well in the real world, and machine learning models are no different. Not only\\nis  the  production  environment  typically  very  different  from  the  development  envi‐\\nronment,  but  the  commercial  risks  associated  with  models  in  production  are  much\\ngreater.  It  is  important  that  the  complexities  of  the  transition  to  production  are\\nunderstood and tested and that the potential risks have been adequately mitigated.\\n\\nThis chapter explores the steps required to prepare for production (highlighted in the\\ncontext of the entire life cycle in Figure 5-1). The goal is to illustrate, by extension, the\\nelements that must be considered for robust MLOps systems.\\n\\nFigure 5-1. Preparing for production highlighted in the larger context of the ML project\\nlife cycle\\n\\n59\\n\\n\\x0cRuntime Environments\\nThe first step in sending a model to production is making sure it’s technically possi‐\\nble. As discussed in Chapter 3, ideal MLOps systems favor rapid, automated deploy‐\\nment over labor-intensive processes, and runtime environments can have a big effect\\non which approach prevails.\\n\\nProduction  environments  take  a  wide  variety  of  forms:  custom-built  services,  data\\nscience  platforms,  dedicated  services  like  TensorFlow  Serving,  low-level  infrastruc‐\\nture like Kubernetes clusters, JVMs on embedded systems, etc. To make things even\\nmore complex, consider that in some organizations, multiple heterogeneous produc‐\\ntion environments coexist.\\n\\nIdeally, models running in the development environment would be validated and sent\\nas is to production; this minimizes the amount of adaptation work and improves the\\nchances that the model in production will behave as it did in development. Unfortu‐\\nnately,  this  ideal  scenario  is  not  always  possible,  and  it’s  not  unheard  of  that  teams\\nfinish a long-term project only to realize it can’t be put in production.\\n\\nAdaptation from Development to Production Environments\\nIn terms of adaptation work, on one end of the spectrum, the development and pro‐\\nduction platforms are from the same vendor or are otherwise interoperable, and the\\ndev model can run without any modification in production. In this case, the technical\\nsteps required to push the model into production are reduced to a few clicks or com‐\\nmands, and all efforts can be focused on validation.\\n\\nOn the other end of the spectrum, there are cases where the model needs to be reim‐\\nplemented  from  scratch—possibly  by  another  team,  and  possibly  in  another  pro‐\\ngramming language. Given the resources and time required, there are few cases today\\nwhere this approach makes sense. However, it’s still the reality in many organizations\\nand is often a consequence of the lack of appropriate tooling and processes. The real‐\\nity is that handing over a model for another team to reimplement and adapt for the\\nproduction  environment  means  that  model  won’t  reach  production  for  months\\n(maybe years), if at all.\\n\\nBetween  these  two  extreme  cases,  there  can  be  a  number  of  transformations  per‐\\nformed on the model or the interactions with its environment to make it compatible\\nwith  production.  In  all  cases,  it  is  crucial  to  perform  validation  in  an  environment\\nthat  mimics  production  as  closely  as  possible,  rather  than  in  the  development\\nenvironment.\\n\\n60 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cTooling considerations\\n\\nThe format required to send to production should be considered early, as it may have\\na large impact on the model itself and the quantity of work required to productional‐\\nize it. For example, when a model is developed using scikit-learn (Python) and pro‐\\nduction  is  a  Java-based  environment  that  expects  PMML  or  ONNX  as  input,\\nconversion is obviously required.\\n\\nIn this case, teams should set up tooling while developing the model, ideally before\\nthe first version of the model is finished or even started. Failure to create this pipeline\\nup  front  would  block  the  validation  process  (and,  of  course,  final  validation  should\\nnot  be  performed  on  the  scikit-learn  model,  as  it’s  not  the  one  that  will  be  put  into\\nproduction).\\n\\nPerformance considerations\\n\\nAnother common reason conversion may be required is for performance. For exam‐\\nple, a Python model will typically have higher latency for scoring a single record than\\nits  equivalent  converted  to  C++.  The  resulting  model  will  likely  be  dozens  of  times\\nfaster  (although  obviously  it  depends  on  many  factors,  and  the  result  can  also  be  a\\nmodel that is dozens of times slower).\\n\\nPerformance  also  comes  into  play  when  the  production  model  must  run  on  a  low-\\npower device. In the specific case of deep neural networks, for example, trained mod‐\\nels  can  become  extremely  large  with  billions  or  hundreds  of  billions  of  parameters.\\nRunning them on small devices is simply impossible, and running them on standard\\nservers can be slow and expensive.\\n\\nFor these models, an optimized runtime is not enough. To obtain better performance,\\nthe  model  definition  must  be  optimized.  One  solution  is  to  use  compression\\ntechniques:\\n\\n• With quantization, the model can be trained using 32-bit floating-point numbers\\nand used for inference at a lower precision so that the model requires less mem‐\\nory and is faster while accuracy is mostly preserved.\\n\\n• With pruning, one simply removes weights (or even entire layers) from the neu‐\\nral  network.  This  is  a  rather  radical  approach,  but  some  methods  allow  for  the\\npreservation of accuracy.\\n\\n• With distillation, a smaller “student” network is trained to mimic a bigger, more\\npowerful  network.  Done  appropriately,  this  can  lead  to  better  models  (as  com‐\\npared to trying to train the smaller network directly from the data).\\n\\nThese methods are efficient if the initial model is trained in a way that reduces infor‐\\nmation loss while performing them, so these operations are not simply conversions of\\nthe  trained  model  post  hoc,  but  rather  orient  the  way  the  model  is  trained.  These\\n\\nRuntime Environments \\n\\n| \\n\\n61\\n\\n\\x0cmethods are still very recent and quite advanced but already commonly used in natu‐\\nral language processing (NLP) pretrained models.\\n\\nData Access Before Validation and Launch to Production\\nAnother technical aspect that needs to be addressed before validation and launch to\\nproduction is data access. For example, a model evaluating apartment prices may use\\nthe average market price in a zip code area; however, the user or the system request‐\\ning the scoring will probably not provide this average and would most likely provide\\nsimply the zip code, meaning a lookup is necessary to fetch the value of the average.\\n\\nIn some cases, data can be frozen and bundled with the model. But when this is not\\npossible (e.g., if the dataset is too large or the enrichment data needs to always be up\\nto  date),  the  production  environment  should  access  a  database  and  thus  have  the\\nappropriate network connectivity, libraries, or drivers required to communicate with\\nthe data storage installed, and authentication credentials stored in some form of pro‐\\nduction configuration.\\n\\nManaging this setup and configuration can be quite complex in practice since, again,\\nit requires appropriate tooling and collaboration (in particular to scale to more than a\\nfew dozen models). When using external data access, model validation in situations\\nthat closely match production is even more critical as technical connectivity is a com‐\\nmon source of production malfunction.\\n\\nFinal Thoughts on Runtime Environments\\nTraining a model is usually the most impressive computation, requiring a high level\\nof software sophistication, massive data volumes, and high-end machines with pow‐\\nerful GPUs. But in the whole life cycle of a model, there is a good chance that most of\\nthe compute is spent at inference time (even if this computation is orders of magni‐\\ntude simpler and faster). This is because a model is trained once and can be used bil‐\\nlions of times for inference.\\n\\nScaling  inference  on  complex  models  can  be  expensive  and  have  significant  energy\\nand  environmental  impact.  Lowering  the  complexity  of  models  or  compressing\\nextremely  complex  models  can  lower  the  infrastructure  cost  of  operating  machine\\nlearning models.\\n\\nIt’s important to remember that not all applications require deep learning, and in fact,\\nnot  all  applications  require  machine  learning  at  all.  A  valuable  practice  to  control\\ncomplexity in production is to develop complex models only to provide a baseline for\\nwhat  seems  achievable.  What  goes  into  production  can  then  be  a  much  simpler\\nmodel, with the advantages of lowering the operating risk, increasing computational\\nperformance, and lowering power consumption. If the simple model is close enough\\n\\n62 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cto  the  high  complexity  baseline,  then  it  can  be  a  much  more  desirable  solution  for\\nproduction.\\n\\nModel Risk Evaluation\\nBefore  exploring  how  validation  should  be  done  in  an  ideal  MLOps  system,  it’s\\nimportant to consider the purpose of validation. As discussed in Chapter 4, models\\nattempt to mimic reality, but they are imperfect; their implementation can have bugs,\\nas  can  the  environment  they  are  executing  in.  The  indirect,  real-world  impact  a\\nmodel in production can have is never certain, and the malfunctioning of a seemingly\\ninsignificant cog can have tremendous consequences in a complex system.\\n\\nThe Purpose of Model Validation\\nIt is, to some extent, possible (not to mention absolutely necessary) to anticipate the\\nrisks of models in production and thus design and validate so as to minimize these\\nrisks. As organizations become more and more complex, it is essential to understand\\nthat involuntary malfunctions or malicious attacks are potentially threatening in most\\nuses  of  machine  learning  in  the  enterprise,  not  only  in  financial  or  safety-related\\napplications.\\n\\nBefore putting a model in production (and in fact constantly from the beginning of\\nthe machine learning project), teams should ask the uncomfortable questions:\\n\\n• What if the model acts in the worst imaginable way?\\n\\n• What  if  a  user  manages  to  extract  the  training  data  or  the  internal  logic  of  the\\n\\nmodel?\\n\\n• What are the financial, business, legal, safety, and reputational risks?\\n\\nFor  high-risk  applications,  it  is  essential  that  the  whole  team  (and  in  particular  the\\nengineers in charge of validation) be fully aware of these risks so that they can design\\nthe  validation  process  appropriately  and  apply  the  strictness  and  complexity  appro‐\\npriate for the magnitude of the risks.\\n\\nIn  many  ways,  machine  learning  risk  management  covers  model  risk  management\\npractices that are well established in many industries, such as banking and insurance.\\nHowever, machine learning introduces new types of risks and liabilities, and as data\\nscience gets democratized, it involves many new organizations or teams that have no\\nexperience with more traditional model risk management.\\n\\nModel Risk Evaluation \\n\\n| \\n\\n63\\n\\n\\x0cThe Origins of ML Model Risk\\nThe magnitude of risk ML models can bring is hard to model for mathematical rea‐\\nsons,  but  also  because  the  materialization  of  risks  arises  through  real-world  conse‐\\nquences. The ML metrics, and in particular the cost matrix, allow teams to evaluate\\nthe  average  cost  of  operating  a  model  in  its  “nominal”  case,  meaning  on  its  cross-\\nvalidation data, compared to operating a perfect magical model.\\n\\nBut while computing this expected cost can be very important, a wide range of things\\ncan  go  wrong  well  beyond  expected  cost.  In  some  applications,  the  risk  can  be  a\\nfinancially unbounded liability, a safety issue for individuals, or an existential threat\\nfor the organization. ML model risk originates essentially from:\\n\\n• Bugs, errors in designing, training, or evaluating the model (including data prep)\\n\\n• Bugs in the runtime framework, bugs in the model post-processing/conversion,\\n\\nor hidden incompatibilities between the model and its runtime\\n\\n• Low quality of training data\\n\\n• High difference between production data and training data\\n\\n• Expected  error  rates,  but  with  failures  that  have  higher  consequences  than\\n\\nexpected\\n\\n• Misuse of the model or misinterpretation of its outputs\\n\\n• Adversarial attacks\\n\\n• Legal  risk  originating  in  particular  from  copyright  infringement  or  liability  for\\n\\nthe model output\\n\\n• Reputational risk due to bias, unethical use of machine learning, etc.\\n\\nThe probability of materialization of the risk and its magnitude can be amplified by:\\n\\n• Broad use of the model\\n\\n• A rapidly changing environment\\n\\n• Complex interactions between models\\n\\nThe  following  sections  provide  more  details  on  these  threats  and  how  to  mitigate\\nthem,  which  should  ultimately  be  the  goal  of  any  MLOps  system  the  organization\\nputs in place.\\n\\nQuality Assurance for Machine Learning\\nSoftware engineering has developed a mature set of tools and methodologies for qual‐\\nity assurance (QA), but the equivalent for data and models is still in its infancy, which\\nmakes it challenging to incorporate into MLOps processes. The statistical methods as\\n\\n64 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cwell as documentation best practices are well known, but implementing them at scale\\nis not common.\\n\\nThough it’s being covered as a part of this chapter on preparing for production, to be\\nclear,  QA  for  machine  learning  does  not  occur  only  at  the  final  validation  stage;\\nrather, it should accompany all stages of model development. Its purpose is to ensure\\ncompliance  with  processes  as  well  as  ML  and  computational  performance  require‐\\nments, with a level of detail that is proportionate to the level of risk.\\n\\nIn the case where the people in charge of validation are not the ones who developed\\nthe  model,  it  is  essential  that  they  have  enough  training  in  machine  learning  and\\nunderstand the risks so that they can design appropriate validation or detect breaches\\nin  the  validation  proposed  by  the  development  team.  It  is  also  essential  that  the\\norganization’s  structure  and  culture  give  them  the  authority  to  appropriately  report\\nissues and contribute to continuous improvement or block passage to production if\\nthe level of risk justifies it.\\n\\nRobust MLOps practices dictate that performing QA before sending to production is\\nnot  only  about  technical  validation.  It  is  also  the  occasion  to  create  documentation\\nand validate the model against organizational guidelines. In particular, this means the\\norigin  of  all  input  datasets,  pretrained  models,  or  other  assets  should  be  known,  as\\nthey could be subject to regulations or copyrights. For this reason (and for computer\\nsecurity  reasons  in  particular),  some  organizations  choose  to  allow  only  whitelisted\\ndependencies.  While  this  can  significantly  impact  the  ability  of  data  scientists  to\\ninnovate quickly, though the list of dependencies can be reported and checked partly\\nautomatically, it can also provide additional safety.\\n\\nKey Testing Considerations\\nObviously, model testing will consist of applying the model to carefully curated data\\nand validating measurements against requirements. How the data is selected or gen‐\\nerated as well as how much data is required is crucial, but it will depend on the prob‐\\nlem tackled by the model.\\n\\nThere are some scenarios in which the test data should not always match “real-world”\\ndata. For example, it can be a good idea to prepare a certain number of scenarios, and\\nwhile some of them should match realistic situations, other data should be specifically\\ngenerated in ways that could be problematic (e.g., extreme values, missing values).\\n\\nMetrics must be collected on both statistical (accuracy, precision, recall, etc.) as well\\nas computational (average latency, 95th latency percentile, etc.) aspects, and the test\\nscenarios should fail if some assumptions on them are not verified. For example, the\\ntest  should  fail  if  the  accuracy  of  the  model  falls  below  90%,  the  average  inference\\ntime goes above 100 milliseconds, or more than 5% of inferences take more than 200\\n\\nKey Testing Considerations \\n\\n| \\n\\n65\\n\\n\\x0cmilliseconds. These assumptions can also be called expectations, checks, or assertions,\\nas in traditional software engineering.\\n\\nStatistical tests on results can also be performed but are typically used for subpopula‐\\ntions. It is also important to be able to compare the model with its previous version. It\\ncan  allow  putting  in  place  a  champion/challenger  approach  (described  in  detail  in\\n“Champion/Challenger”  on  page  100)  or  checking  that  a  metric  does  not  suddenly\\ndrop.\\n\\nSubpopulation Analysis and Model Fairness\\nIt can be useful to design test scenarios by splitting data into subpopulations based on\\na “sensitive” variable (that may or may not be used as a feature of the model). This is\\nhow fairness (typically between genders) is evaluated.\\n\\nVirtually all models that apply to people should be analyzed for fairness. Increasingly,\\nfailure to assess model fairness will have business, regulatory, and reputational impli‐\\ncations  for  organizations.  For  details  about  biases  and  fairness,  refer  to  “Impact  of\\nResponsible AI on Modeling” on page 53 and “Key Elements of Responsible AI” on\\npage 113.\\n\\nIn addition to validating the ML and computational performance metrics, model sta‐\\nbility  is  an  important  testing  property  to  consider.  When  changing  one  feature\\nslightly, one expects small changes in the outcome. While this cannot be always true,\\nit is generally a desirable model property. A very unstable model introduces a lot of\\ncomplexity  and  loopholes  in  addition  to  delivering  a  frustrating  experience,  as  the\\nmodel can feel unreliable even if it has decent performance. There is no single answer\\nto  model  stability,  but  generally  speaking,  simpler  models  or  more  regularized  ones\\nshow better stability.\\n\\nReproducibility and Auditability\\nReproducibility  in  MLOps  does  not  have  the  same  meaning  as  in  academia.  In  the\\nacademic world, reproducibility essentially means that the findings of an experiment\\nare  described  well  enough  that  another  competent  person  can  replicate  the  experi‐\\nment using the explanations alone, and if the person doesn’t make any mistakes, they\\nwill arrive at the same conclusion.\\n\\nIn general, reproducibility in MLOps also involves the ability to easily rerun the exact\\nsame experiment. It implies that the model comes with detailed documentation, the\\ndata used for training and testing, and with an artifact that bundles the implementa‐\\ntion  of  the  model  plus  the  full  specification  of  the  environment  it  was  run  in  (see\\n\\n66 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0c“Version Management and Reproducibility” on page 56). Reproducibility is essential\\nto prove model findings, but also to debug or build on a previous experiment.\\n\\nAuditability is related to reproducibility, but it adds some requirements. For a model\\nto be auditable, it must be possible to access the full history of the ML pipeline from a\\ncentral  and  reliable  storage  and  to  easily  fetch  metadata  on  all  model  versions\\nincluding:\\n\\n• The full documentation\\n\\n• An artifact that allows running the model with its exact initial environment\\n\\n• Test results, including model explanations and fairness reports\\n\\n• Detailed model logs and monitoring metadata\\n\\nAuditability  can  be  an  obligation  in  some  highly  regulated  applications,  but  it  has\\nbenefits  for  all  organizations  because  it  can  facilitate  model  debugging,  continuous\\nimprovement, and keeping track of actions and responsibilities (which is an essential\\npart of governance for responsible applications of ML, as discussed at length in Chap‐\\nter  8).  A  full  QA  toolchain  for  machine  learning—and,  thus,  MLOps  processes—\\nshould provide a clear view of model performance with regard to requirements while\\nalso facilitating auditability. \\n\\nEven when MLOps frameworks allow data scientists (or others) to find a model with\\nall its metadata, understanding the model itself can still be challenging (see “Impact\\nof Responsible AI on Modeling” on page 53 for a detailed discussion).\\n\\nTo have a strong practical impact, auditability must allow for intuitive human under‐\\nstanding of all the parts of the system and their version histories. This doesn’t change\\nthe fact that understanding a machine learning model (even a relatively simple one)\\nrequires  appropriate  training,  but  depending  on  the  criticality  of  the  application,  a\\nwider  audience  may  need  to  be  able  to  understand  the  details  of  the  model.  As  a\\nresult, full auditability comes at a cost that should be balanced with the criticality of\\nthe model itself.\\n\\nMachine Learning Security\\nAs  a  piece  of  software,  a  deployed  model  running  in  its  serving  framework  can\\npresent multiple security issues that range from low-level glitches to social engineer‐\\ning. Machine learning introduces a new range of potential threats where an attacker\\nprovides malicious data designed to cause the model to make a mistake.\\n\\nThere are numerous cases of potential attacks. For example, spam filters were an early\\napplication of machine learning essentially based on scoring words that were in a dic‐\\ntionary.  One  way  for  spam  creators  to  avoid  detection  was  to  avoid  writing  these\\nexact  words  while  still  making  their  message  easily  understandable  by  a  human\\n\\nMachine Learning Security \\n\\n| \\n\\n67\\n\\n\\x0creader (e.g., using exotic Unicode characters, voluntarily introducing typos, or using\\nimages).\\n\\nAdversarial Attacks\\nA more modern but quite analogous example of a machine learning model security\\nissue is an adversarial attack for deep neural networks in which an image modifica‐\\ntion that can seem minor or even impossible for a human eye to notice can cause the\\nmodel to drastically change its prediction. The core idea is mathematically relatively\\nsimple:  since  deep  learning  inference  is  essentially  matrix  multiplication,  carefully\\nchosen  small  perturbations  to  coefficients  can  cause  a  large  change  in  the  output\\nnumbers.\\n\\nOne example of this is that small stickers glued to road signs can confuse an autono‐\\nmous car’s computer vision system, rendering signs invisible or incorrectly classified\\nby  the  system,  while  remaining  fully  visible  and  understandable  to  a  human  being.\\nThe more the attacker knows about the system, the more likely they are to find exam‐\\nples that will confuse it.\\n\\nA  human  can  use  reason  to  find  these  examples  (in  particular  for  simple  models).\\nHowever,  for  more  complex  models  like  deep  learning,  the  attacker  will  probably\\nneed  to  perform  many  queries  and  either  use  brute  force  to  test  as  many  combina‐\\ntions as possible or use a model to search for problematic examples. The difficulty of\\ncountermeasures  is  increasing  with  the  complexity  of  models  and  their  availability.\\nSimple  models  such  as  logistic  regressions  are  essentially  immune,  while  an  open\\nsource pretrained deep neural network will basically always be vulnerable, even with\\nadvanced, built-in attack detectors.\\n\\nAdversarial attacks don’t necessarily happen at inference time. If an attacker can get\\naccess to the training data, even partially, then they get control over the system. This\\nkind of attack is traditionally known as a poisoning attack in computer security.\\n\\nOne famous example is the Twitter chatbot released by Microsoft in 2016. Just a few\\nhours after launch, the bot started to generate very offensive tweets. This was caused\\nby  the  bot  adapting  to  its  input;  when  realizing  that  some  users  submitted  a  large\\namount of offensive content, the bot started to replicate. In theory, a poisoning attack\\ncan  occur  as  a  result  of  an  intrusion  or  even,  in  a  more  sophisticated  way,  through\\npretrained models. But in practice, one should mostly care about data collected from\\neasily  manipulated  data  sources.  Tweets  sent  to  a  specific  account  are  a  particularly\\nclear example.\\n\\nOther Vulnerabilities\\nSome patterns do not exploit machine learning vulnerabilities per se, but they do use\\nthe machine learning model in ways that lead to undesirable situations. One example\\n\\n68 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cis in credit scoring: for a given amount of money, borrowers with less flexibility tend\\nto choose a longer period to lower the payments, while borrowers who are not con‐\\ncerned about their ability to pay may choose a shorter period to lower the total cost of\\ncredit. Salespeople may advise those who do not have a good enough score to shorten\\ntheir  payments.  This  increases  the  risk  for  the  borrower  and  the  bank  and  is  not  a\\nmeaningful course of action. Correlation is not causality!\\n\\nModels can also leak data in many ways. Since the machine learning models can fun‐\\ndamentally be considered a summary of the data they have been trained on, they can\\nleak more or less precise information on the training data, up to the full training set\\nin  some  cases.  Imagine,  for  example,  that  a  model  predicts  how  much  someone  is\\npaid using the nearest neighbor algorithm. If one knows the zip code, age, and pro‐\\nfession of a certain person registered on the service, it’s pretty easy to obtain that per‐\\nson’s  exact  income.  There  are  a  wide  range  of  attacks  that  can  extract  information\\nfrom models in this way.\\n\\nIn addition to technical hardening and audit, governance plays a critical role in secu‐\\nrity. Responsibilities must be assigned clearly and in a way that ensures an appropri‐\\nate balance between security and capacity of execution. It is also important to put in\\nplace feedback mechanisms, and employees and users should have an easy channel to\\ncommunicate  breaches  (including,  potentially,  “bug  bounty  programs”  that  reward\\nreporting  vulnerabilities).  It  is  also  possible,  and  necessary,  to  build  safety  nets\\naround the system to mitigate the risks.\\n\\nMachine learning security shares many common traits with general computer system\\nsecurity,  one  of  the  main  ideas  being  that  security  is  not  an  additional  independent\\nfeature  of  the  system;  that  is,  generally  you  cannot  secure  a  system  that  is  not\\ndesigned  to  be  secure,  and  the  organization  processes  must  take  into  account  the\\nnature of the threat from the beginning. Strong MLOps processes, including all of the\\nsteps  in  preparing  for  production  described  in  this  chapter,  can  help  make  this\\napproach a reality.\\n\\nModel Risk Mitigation\\nGenerally speaking, as discussed in detail in Chapter 1, the broader the model deploy‐\\nment, the greater the risk. When risk impact is high enough, it is essential to control\\nthe deployment of new versions, which is where tightly controlled MLOps processes\\ncome  into  play  in  particular.  Progressive  or  canary  rollouts  should  be  a  common\\npractice, with new versions of models being served to a small proportion of the orga‐\\nnization or customer base first and slowly increasing that proportion, while monitor‐\\ning behavior and getting human feedback if appropriate.\\n\\nModel Risk Mitigation \\n\\n| \\n\\n69\\n\\n\\x0cChanging Environments\\nRapidly changing environments also multiply risk, as mentioned earlier in this chap‐\\nter. Changes in inputs is a related and also well-identified risk, and Chapter 7 dives\\ninto these challenges and how to address them in more detail. But what’s important to\\nnote  is  that  the  speed  of  change  can  amplify  the  risk  depending  on  the  application.\\nChanges may be so fast that they have consequences even before the monitoring sys‐\\ntem sends alerts. That is to say, even with an efficient monitoring system and a proce‐\\ndure  to  retrain  models,  the  time  necessary  to  remediate  may  be  a  critical  threat,\\nespecially  if  simply  retraining  the  model  on  new  data  is  not  sufficient  and  a  new\\nmodel must be developed. During this time, the production systems misbehaving can\\ncause large losses for the organization.\\n\\nTo  control  this  risk,  monitoring  via  MLOps  should  be  reactive  enough  (typically,\\nalerting on distributions computed every week might not be enough), and the proce‐\\ndure should consider the period necessary for remediation. For example, in addition\\nto  retraining  or  rollout  strategies,  the  procedure  may  define  thresholds  that  would\\ntrigger  a  degraded  mode  for  the  system.  A  degraded  mode  may  simply  consist  of  a\\nwarning message displayed for end users, but could be as drastic as shutting down the\\ndysfunctional system to avoid harm until a stable solution can be deployed.\\n\\nLess dramatic issues that are frequent enough can also do harm that quickly becomes\\ndifficult  to  control.  If  the  environment  changes  often,  even  if  remediation  never\\nseems urgent, a model can always be slightly off, never operating within its nominal\\ncase, and the operating cost can be challenging to evaluate. This can only be detected\\nthrough dedicated MLOps, including relatively long-term monitoring and reevaluat‐\\ning the cost of operating the model.\\n\\nIn  many  cases,  retraining  the  model  on  more  data  will  increasingly  improve  the\\nmodel, and this problem will eventually disappear, but this can take time. Before this\\nconvergence, a solution might be to use a less complex model that may have a lower\\nevaluated  performance  and  may  be  more  consistent  in  a  frequently  changing\\nenvironment.\\n\\nInteractions Between Models\\nComplex  interactions  between  models  is  probably  the  most  challenging  source  of\\nrisk. This class of issue will be a growing concern as ML models become pervasive,\\nand it’s an important potential area of focus for MLOps systems. Obviously, adding\\nmodels  will  often  add  complexity  to  an  organization,  but  the  complexity  does  not\\nnecessarily grow linearly in proportion to the number of models; having two models\\nis more complicated to understand than the sum since there are potential interactions\\nbetween them.\\n\\n70 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cMoreover,  the  total  complexity  is  heavily  determined  by  how  the  interactions  with\\nmodels  are  designed  at  a  local  scale  and  governed  at  an  organizational  scale.  Using\\nmodels in chains (where a model uses inputs from another model) can create signifi‐\\ncant additional complexity as well as totally unexpected results, whereas using models\\nin independent parallel processing chains, which are each as short and explainable as\\npossible,  is  a  much  more  sustainable  way  to  design  large-scale  deployment  of\\nmachine learning.\\n\\nFirst, the absence of obvious interactions between models makes the complexity grow\\ncloser  to  linearly  (though  note  that,  in  practice,  it  is  rarely  the  case,  as  there  can\\nalways be interactions in the real world even if models are not connected). Also, mod‐\\nels used in redundant chains of processing can avoid errors—that is, if a decision is\\nbased on several independent chains of processing with methods as different as possi‐\\nble, it can be more robust.\\n\\nFinally, generally speaking, the more complex the model, the more complex its inter‐\\nactions with other systems may be, as it may have many edge cases, be less stability in\\nsome domains, overreact to the changes of an upstream model, or confuse a sensitive\\ndownstream model, etc. Here again, we see that model complexity has a cost, and a\\npotentially highly unpredictable one at that.\\n\\nModel Misbehavior\\nA  number  of  measures  can  be  implemented  to  avoid  model  misbehavior,  including\\nexamining its inputs and outputs in real time. While training a model, it is possible to\\ncharacterize  its  domain  of  applicability  by  examining  the  intervals  on  which  the\\nmodel was trained and validated. If the value of a feature at inference time is out of\\nbounds,  the  system  can  trigger  appropriate  measures  (e.g.,  rejecting  the  sample  or\\ndispatching a warning message).\\n\\nControlling feature-value intervals is a useful and simple technique, but it might be\\ninsufficient. For example, when training an algorithm to evaluate car prices, the data\\nmay  have  provided  examples  of  recent  light  cars  and  old  heavy  cars,  but  no  recent\\nheavy  cars.  The  performance  of  a  complex  model  for  these  is  unpredictable.  When\\nthe  number  of  features  is  large,  this  issue  becomes  unavoidable  due  to  the  curse  of\\ndimensionality—i.e., the number of combinations is exponential relative to the num‐\\nber of features.\\n\\nIn  these  situations,  more  sophisticated  methods  can  be  used,  including  anomaly\\ndetection  to  identify  records  where  the  model  is  used  outside  of  its  application\\ndomain. After scoring, the outputs of the model can be examined before confirming\\nthe inference. In the case of classification, many algorithms provide certainty scores\\nin  addition  to  their  prediction,  and  a  threshold  can  be  fixed  to  accept  an  inference\\noutput.  Note  that  these  certainty  scores  do  not  typically  translate  into  probabilities,\\neven if they are named this way in the model.\\n\\nModel Risk Mitigation \\n\\n| \\n\\n71\\n\\n\\x0cConformal prediction is a set of techniques that helps calibrate these scores to obtain\\nan accurate estimation of the probability of correctness. For regression, the value can\\nbe checked against a predetermined interval. For example, if the model predicts a car\\ncosts $50 or $500,000, you may not want to commit any business on this prediction.\\nThe  complexity  of  the  implemented  techniques  should  be  relevant  for  the  level  of\\nrisk: a highly complex, highly critical model will require more thorough safeguards.\\n\\nClosing Thoughts\\nIn practice, preparing models for production starts from the beginning at the devel‐\\nopment  phase;  that  is  to  say,  the  requirements  of  production  deployments,  security\\nimplications, and risk mitigation aspects should be considered when developing the\\nmodels. MLOps includes having a clear validation step before sending models to pro‐\\nduction, and the key ideas to successfully prepare models for productions are:\\n\\n• Clearly identifying the nature of the risks and their magnitudes\\n\\n• Understanding  model  complexity  and  its  impact  at  multiple  levels,  including\\nincreased  latency,  increased  memory  and  power  consumption,  lower  ability  to\\ninterpret inference in production, and a harder-to-control risk\\n\\n• Providing a simple but clear standard of quality, making sure the team is appro‐\\npriately trained and the organization structure allows for fast and reliable valida‐\\ntion processes\\n\\n• Automating all the validation that can be automated to ensure it is properly and\\n\\nconsistently performed while maintaining the ability to deploy quickly\\n\\n72 \\n\\n| \\n\\nChapter 5: Preparing for Production\\n\\n\\x0cCHAPTER 6\\nDeploying to Production\\n\\nJoachim Zentici\\n\\nBusiness leaders view the rapid deployment of new systems into production as key to\\nmaximizing business value. But this is only true if deployment can be done smoothly\\nand  at  low  risk  (software  deployment  processes  have  become  more  automated  and\\nrigorous in recent years to address this inherent conflict). This chapter dives into the\\nconcepts and considerations when deploying machine learning models to production\\nthat  impact—and  indeed,  drive—the  way  MLOps  deployment  processes  are  built\\n(Figure 6-1 presents this phase in the context of the larger life cycle).\\n\\nFigure 6-1. Deployment to production highlighted in the larger context of the ML project\\nlife cycle\\n\\nCI/CD Pipelines\\nCI/CD is a common acronym for continuous integration and continuous delivery (or\\nput more simply, deployment). The two form a modern philosophy of agile software\\ndevelopment and a set of practices and tools to release applications more often and\\nfaster, while also better controlling quality and risk.\\n\\n73\\n\\n\\x0cWhile  these  ideas  are  decades  old  and  already  used  to  various  extents  by  software\\nengineers, different people and organizations use certain terms in very different ways.\\nBefore digging into how CI/CD applies to machine learning workflows, it is essential\\nto keep in mind that these concepts should be tools to serve the purpose of delivering\\nquality  fast,  and  the  first  step  is  always  to  identify  the  specific  risks  present  at  the\\norganization.  In  other  words,  as  always,  CI/CD  methodology  should  be  adapted\\nbased on the needs of the team and the nature of the business.\\n\\nCI/CD concepts apply to traditional software engineering, but they apply just as well\\nto machine learning systems and are a critical part of MLOps strategy. After success‐\\nfully developing a model, a data scientist should push the code, metadata, and docu‐\\nmentation to a central repository and trigger a CI/CD pipeline. An example of such\\npipeline could be:\\n\\n1. Build the model\\n\\na. Build the model artifacts\\n\\nb. Send the artifacts to long-term storage\\n\\nc. Run basic checks (smoke tests/sanity checks)\\n\\nd. Generate fairness and explainability reports\\n\\n2. Deploy to a test environment\\n\\na. Run tests to validate ML performance, computational performance\\n\\nb. Validate manually\\n\\n3. Deploy to production environment\\n\\na. Deploy the model as canary\\n\\nb. Fully deploy the model\\n\\nMany scenarios are possible and depend on the application, the risks from which the\\nsystem should be protected, and the way the organization chooses to operate. Gener‐\\nally  speaking,  an  incremental  approach  to  building  a  CI/CD  pipeline  is  preferred:  a\\nsimple or even naïve workflow on which a team can iterate is often much better than\\nstarting with complex infrastructure from scratch.\\n\\nA starting project does not have the infrastructure requirements of a tech giant, and it\\ncan be hard to know up front which challenges deployments will present. There are\\ncommon tools and best practices, but there is no one-size-fits-all CI/CD methodol‐\\nogy. This means the best path forward is starting from a simple (but fully functional)\\nCI/CD  workflow  and  introducing  additional  or  more  sophisticated  steps  along  the\\nway as quality or scaling challenges appear.\\n\\n74 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cBuilding ML Artifacts\\nThe goal of a continuous integration pipeline is to avoid unnecessary effort in merg‐\\ning the work from several contributors as well as to detect bugs or development con‐\\nflicts  as  soon  as  possible.  The  very  first  step  is  using  centralized  version  control\\nsystems  (unfortunately,  working  for  weeks  on  code  stored  only  on  a  laptop  is  still\\nquite common).\\n\\nThe  most  common  version  control  system  is  Git,  an  open  source  software  initially\\ndeveloped to manage the source code for the Linux kernel. The majority of software\\nengineers across the world already use Git, and it is increasingly being adopted in sci‐\\nentific  computing  and  data  science.  It  allows  for  maintaining  a  clear  history  of\\nchanges, safe rollback to a previous version of the code, multiple contributors to work\\non their own branches of the project before merging to the main branch, etc.\\n\\nWhile Git is appropriate for code, it was not designed to store other types of assets\\ncommon  in  data  science  workflows,  such  as  large  binary  files  (for  example,  trained\\nmodel weights), or to version the data itself. Data versioning is a more complex topic\\nwith numerous solutions, including Git extensions, file formats, databases, etc.\\n\\nWhat’s in an ML Artifact?\\nOnce the code and data is in a centralized repository, a testable and deployable bun‐\\ndle of the project must be built. These bundles are usually called artifacts in the con‐\\ntext  of  CI/CD.  Each  of  the  following  elements  needs  to  be  bundled  into  an  artifact\\nthat  goes  through  a  testing  pipeline  and  is  made  available  for  deployment  to\\nproduction:\\n\\n• Code for the model and its preprocessing\\n\\n• Hyperparameters and configuration\\n\\n• Training and validation data\\n\\n• Trained model in its runnable form\\n\\n• An  environment  including  libraries  with  specific  versions,  environment  vari‐\\n\\nables, etc.\\n\\n• Documentation\\n\\n• Code and data for testing scenarios\\n\\nThe Testing Pipeline\\nAs touched on in Chapter 5, the testing pipeline can validate a wide variety of proper‐\\nties of the model contained in the artifact. One of the important operational aspects\\n\\nBuilding ML Artifacts \\n\\n| \\n\\n75\\n\\n\\x0cof  testing  is  that,  in  addition  to  verifying  compliance  with  requirements,  good  tests\\nshould make it as easy as possible to diagnose the source issue when they fail.\\n\\nFor that purpose, naming the tests is extremely important, and carefully choosing a\\nnumber of datasets to validate the model against can be valuable. For example:\\n\\n• A test on a fixed (not automatically updated) dataset with simple data and not-\\ntoo-restrictive  performance  thresholds  can  be  executed  first  and  called  “base\\ncase.” If the test reports show that this test failed, there is a strong possibility that\\nthe model is way off, and the cause may be a programming error or a misuse of\\nthe model, for example.\\n\\n• Then,  a  number  of  datasets  that  each  have  one  specific  oddity  (missing  values,\\nextreme values, etc.) could be used with tests appropriately named so that the test\\nreport immediately shows the kind of data that is likely to make the model fail.\\nThese datasets can represent realistic yet remarkable cases, but it may also be use‐\\nful to generate synthetic data that is not expected in production. This could pos‐\\nsibly  protect  the  model  from  new  situations  not  yet  encountered,  but  most\\nimportantly,  this  could  protect  the  model  from  malfunctions  in  the  system\\nquerying or from adversarial examples (as discussed in “Machine Learning Secu‐\\nrity” on page 67).\\n\\n• Then, an essential part of model validation is testing on recent production data.\\nOne or several datasets should be used, extracted from several time windows and\\nnamed appropriately. This category of tests should be performed and automati‐\\ncally analyzed when the model is already deployed to production. Chapter 7 pro‐\\nvides more specific details on how to do that.\\n\\nAutomating these tests as much as possible is essential and, indeed, is a key compo‐\\nnent  of  efficient  MLOps.  A  lack  of  automation  or  speed  wastes  time,  but,  more\\nimportantly, it discourages the development team from testing and deploying often,\\nwhich  can  delay  the  discovery  of  bugs  or  design  choices  that  make  it  impossible  to\\ndeploy to production.\\n\\nIn  extreme  cases,  a  development  team  can  hand  over  a  monthslong  project  to  a\\ndeployment team that will simply reject it because it does not satisfy requirements for\\nthe  production  infrastructure.  Also,  less  frequent  deployments  imply  larger  incre‐\\nments that are harder to manage; when many changes are deployed at once and the\\nsystem is not behaving in the desired way, isolating the origin of an issue is more time\\nconsuming.\\n\\nThe most widespread tool for software engineering continuous integration is Jenkins,\\na very flexible build system that allows for the building of CI/CD pipelines regardless\\nof the programming language, testing framework, etc. Jenkins can be used in data sci‐\\nence to orchestrate CI/CD pipelines, although there are many other options.\\n\\n76 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cDeployment Strategies\\nTo  understand  the  details  of  a  deployment  pipeline,  it  is  important  to  distinguish\\namong concepts often used inconsistently or interchangeably.\\n\\nIntegration\\n\\nThe process of merging a contribution to a central repository (typically merging\\na Git feature branch to the main branch) and performing more or less complex\\ntests.\\n\\nDelivery\\n\\nAs used in the continuous delivery (CD) part of CI/CD, the process of building a\\nfully  packaged  and  validated  version  of  the  model  ready  to  be  deployed  to\\nproduction.\\n\\nDeployment\\n\\nThe  process  of  running  a  new  model  version  on  a  target  infrastructure.  Fully\\nautomated deployment is not always practical or desirable and is a business deci‐\\nsion as much as a technical decision, whereas continuous delivery is a tool for the\\ndevelopment  team  to  improve  productivity  and  quality  as  well  as  measure  pro‐\\ngress more reliably. Continuous delivery is required for continuous deployment,\\nbut it also provides enormous value without.\\n\\nRelease\\n\\nIn principle, release is yet another step, as deploying a model version (even to the\\nproduction infrastructure) does not necessarily mean that the production work‐\\nload is directed to the new version. As we will see, multiple versions of a model\\ncan run at the same time on the production infrastructure.\\n\\nGetting everyone in the MLOps process on the same page about what these concepts\\nmean and how they apply will allow for smoother processes on both the technical and\\nbusiness sides.\\n\\nCategories of Model Deployment\\nIn addition to different deployment strategies, there are two ways to approach model\\ndeployment:\\n\\n• Batch scoring, where whole datasets are processed using a model, such as in daily\\n\\nscheduled jobs.\\n\\n• Real-time  scoring,  where  one  or  a  small  number  of  records  are  scored,  such  as\\nwhen an ad is displayed on a website and a user session is scored by models to\\ndecide what to display.\\n\\nDeployment Strategies \\n\\n| \\n\\n77\\n\\n\\x0cThere  is  a  continuum  between  these  two  approaches,  and  in  fact,  in  some  systems,\\nscoring  on  one  record  is  technically  identical  to  requesting  a  batch  of  one.  In  both\\ncases,  multiple  instances  of  the  model  can  be  deployed  to  increase  throughput  and\\npotentially lower latency.\\n\\nDeploying many real-time scoring systems is conceptually simpler since the records\\nto be scored can be dispatched between several machines (e.g., using a load balancer). \\nBatch scoring can also be parallelized, for example by using a parallel processing run‐\\ntime  like  Apache  Spark,  but  also  by  splitting  datasets  (which  is  usually  called  parti‐\\ntioning  or  sharding)  and  scoring  the  partitions  independently.  Note  that  these  two\\nconcepts of splitting the data and computation can be combined, as they can address\\ndifferent problems.\\n\\nConsiderations When Sending Models to Production\\nWhen sending a new model version to production, the first consideration is often to\\navoid downtime, in particular for real-time scoring. The basic idea is that rather than\\nshutting down the system, upgrading it, and then putting it back online, a new system\\ncan  be  set  up  next  to  the  stable  one,  and  when  it’s  functional,  the  workload  can  be\\ndirected to the newly deployed version (and if it remains healthy, the old one is shut\\ndown).  This  deployment  strategy  is  called  blue-green—or  sometimes  red-black—\\ndeployment. There are many variations and frameworks (like Kubernetes) to handle\\nthis natively.\\n\\nAnother more advanced solution to mitigate the risk is to have canary releases (also\\ncalled canary deployments). The idea is that the stable version of the model is kept in\\nproduction, but a certain percentage of the workload is redirected to the new model,\\nand results are monitored. This strategy is usually implemented for real-time scoring,\\nbut a version of it could also be considered for batch.\\n\\nA  number  of  computational  performance  and  statistical  tests  can  be  performed  to\\ndecide whether to fully switch to the new model, potentially in several workload per‐\\ncentage increments. This way, a malfunction would likely impact only a small portion\\nof the workload.\\n\\nCanary releases apply to production systems, so any malfunction is an incident, but\\nthe idea here is to limit the blast radius. Note that scoring queries that are handled by\\nthe canary model should be carefully picked, because some issues may go unnoticed\\notherwise. For example, if the canary model is serving a small percentage of a region\\nor  country  before  the  model  is  fully  released  globally,  it  could  be  the  case  that  (for\\nmachine learning or infrastructure reasons) the model does not perform as expected\\nin other regions.\\n\\n78 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cA more robust approach is to pick the portion of users served by the new model at\\nrandom,  but  then  it  is  often  desirable  for  user  experience  to  implement  an  affinity\\nmechanism so that the same user always uses the same version of the model.\\n\\nCanary testing can be used to carry out A/B testing, which is a process to compare\\ntwo  versions  of  an  application  in  terms  of  a  business  performance  metric.  The  two\\nconcepts  are  related  but  not  the  same,  as  they  don’t  operate  at  the  same  level  of\\nabstraction. A/B testing can be made possible through a canary release, but it could\\nalso  be  implemented  as  logic  directly  coded  into  a  single  version  of  an  application.\\nChapter 7 provides more details on the statistical aspects of setting up A/B testing.\\n\\nOverall,  canary  releases  are  a  powerful  tool,  but  they  require  somewhat  advanced\\ntooling to manage the deployment, gather the metrics, specify and run computations\\non them, display the results, and dispatch and process alerts.\\n\\nMaintenance in Production\\nOnce a model is released, it must be maintained. At a high level, there are three main‐\\ntenance measures:\\n\\nResource monitoring\\n\\nJust as for any application running on a server, collecting IT metrics such as CPU,\\nmemory, disk, or network usage can be useful to detect and troubleshoot issues.\\n\\nHealth check\\n\\nTo check if the model is indeed online and to analyze its latency, it is common to\\nimplement  a  health  check  mechanism  that  simply  queries  the  model  at  a  fixed\\ninterval (on the order of one minute) and logs the results.\\n\\nML metrics monitoring\\n\\nThis  is  about  analyzing  the  accuracy  of  the  model  and  comparing  it  to  another\\nversion or detecting when it is going stale. Since it may require heavy computa‐\\ntion, this is typically lower frequency, but as always, will depend on the applica‐\\ntion;  it  is  typically  done  once  a  week.  Chapter  7  details  how  to  implement  this\\nfeedback loop.\\n\\nFinally, when a malfunction is detected, a rollback to a previous version may be nec‐\\nessary. It is critical to have the rollback procedure ready and as automated as possible;\\ntesting it regularly can make sure it is indeed functional.\\n\\nContainerization\\nAs described earlier, managing the versions of a model is much more than just saving\\nits code into a version control system. In particular, it is necessary to provide an exact\\ndescription of the environment (including, for example, all the Python libraries used\\nas well as their versions, the system dependencies that need to be installed, etc.).\\n\\nContainerization \\n\\n| \\n\\n79\\n\\n\\x0cBut storing this metadata is not enough. Deploying to production should automati‐\\ncally  and  reliably  rebuild  this  environment  on  the  target  machine.  In  addition,  the\\ntarget  machine  will  typically  run  multiple  models  simultaneously,  and  two  models\\nmay have incompatible dependency versions. Finally, several models running on the\\nsame machine could compete for resources, and one misbehaving model could hurt\\nthe performance of multiple cohosted models.\\n\\nContainerization  technology  is  increasingly  used  to  tackle  these  challenges.  These\\ntools bundle an application together with all of its related configuration files, libraries,\\nand dependencies that are required for it to run across different operating environ‐\\nments.  Unlike  virtual  machines  (VMs),  containers  do  not  duplicate  the  complete\\noperating  system;  multiple  containers  share  a  common  operating  system  and  are\\ntherefore far more resource efficient. \\n\\nThe  most  well-known  containerization  technology  is  the  open  source  platform\\nDocker. Released in 2014, it has become the de facto standard. It allows an application\\nto be packaged, sent to a server (the Docker host), and run with all its dependencies\\nin isolation from other applications.\\n\\nBuilding  the  basis  of  a  model-serving  environment  that  can  accommodate  many\\nmodels, each of which may run multiple copies, may require multiple Docker hosts.\\nWhen deploying a model, the framework should solve a number of issues:\\n\\n• Which Docker host(s) should receive the container?\\n\\n• When a model is deployed in several copies, how can the workload be balanced?\\n\\n• What happens if the model becomes unresponsive, for example, if the machine\\n\\nhosting it fails? How can that be detected and a container reprovisioned?\\n\\n• How  can  a  model  running  on  multiple  machines  be  upgraded,  with  assurances\\nthat old and new versions are switched on and off, and that the load balancer is\\nupdated with a correct sequence?\\n\\nKubernetes, an open source platform that has gained a lot of traction in the past few\\nyears  and  is  becoming  the  standard  for  container  orchestration,  greatly  simplifies\\nthese issues and many others. It provides a powerful declarative API to run applica‐\\ntions  in  a  group  of  Docker  hosts,  called  a  Kubernetes  cluster.  The  word  declarative\\nmeans that rather than trying to express in code the steps to set up, monitor, upgrade,\\nstop, and connect the container (which can be complex and error prone), users spec‐\\nify in a configuration file the desired state, and Kubernetes makes it happen and then\\nmaintains it.\\n\\nFor example, users need only specify to Kubernetes “make sure four instances of this\\ncontainer run at all times,” and Kubernetes will allocate the hosts, start the containers,\\nmonitor them, and start a new instance if one of them fails. Finally, the major cloud\\nproviders all provide managed Kubernetes services; users do not even have to install\\n\\n80 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cand maintain Kubernetes itself. If an application or a model is packaged as a Docker\\ncontainer,  users  can  directly  submit  it,  and  the  service  will  provision  the  required\\nmachines to run one or several instances of the container inside Kubernetes.\\n\\nDocker  with  Kubernetes  can  provide  a  powerful  infrastructure  to  host  applications,\\nincluding ML models. Leveraging these products greatly simplifies the implementa‐\\ntion of the deployment strategies—like blue-green deployments or canary releases—\\nalthough they are not aware of the nature of the deployed applications and thus can’t\\nnatively manage the ML performance analysis. Another major advantage of this type\\nof infrastructure is the ability to easily scale the model’s deployment.\\n\\nScaling Deployments\\nAs ML adoption grows, organizations face two types of growth challenges:\\n\\n• The ability to use a model in production with high-scale data\\n\\n• The ability to train larger and larger numbers of models\\n\\nHandling more data for real-time scoring is made much easier by frameworks such as\\nKubernetes. Since most of the time trained models are essentially formulas, they can\\nbe replicated in the cluster in as many copies as necessary. With the auto-scaling fea‐\\ntures  in  Kubernetes,  both  provisioning  new  machines  and  load  balancing  are  fully\\nhandled by the framework, and setting up a system with huge scaling capabilities is\\nnow relatively simple. The major difficulty can then be to process the large amount of\\nmonitoring data; Chapter 7 provides some details on this challenge.\\n\\nScalable and Elastic Systems\\nA computational system is said to be horizontally scalable (or just scalable) if it is pos‐\\nsible to incrementally add more computers to expand its processing power. For exam‐\\nple,  a  Kubernetes  cluster  can  be  expanded  to  hundreds  of  machines.  However,  if  a\\nsystem includes only one machine, it may be challenging to incrementally upgrade it\\nsignificantly,  and  at  some  point,  a  migration  to  a  bigger  machine  or  a  horizontally\\nscalable system will be required (and may be very expensive and require interruption\\nof service).\\n\\nAn elastic system allows, in addition to being scalable, easy addition and removal of\\nresources to match the compute requirements. For example, a Kubernetes cluster in\\nthe cloud can have an auto-scaling capability that automatically adds machines when\\nthe cluster usage metrics are high and removes them when they are low. In principle,\\nelastic  systems  can  optimize  the  usage  of  resources;  they  automatically  adapt  to  an\\nincrease in usage without the need to permanently provision resources that are rarely\\nrequired.\\n\\nScaling Deployments \\n\\n| \\n\\n81\\n\\n\\x0cFor  batch  scoring,  the  situation  can  be  more  complex.  When  the  volume  of  data\\nbecomes  too  large,  there  are  essentially  two  types  of  strategies  to  distribute  the\\ncomputation:\\n\\n• Using  a  framework  that  handles  distributed  computation  natively,  in  particular\\nSpark. Spark is an open source distributed computation framework. It is useful to\\nunderstand that Spark and Kubernetes do not play similar roles and can be com‐\\nbined.  Kubernetes  orchestrates  containers,  but  Kubernetes  is  not  aware  of  what\\nthe containers are actually doing; as far as Kubernetes is concerned, they are just\\ncontainers that run an application on one specific host. (In particular, Kubernetes\\nhas no concept of data processing, as it can be used to run any kind of applica‐\\ntion.) Spark is a computation framework that can split the data and the computa‐\\ntion among its nodes. A modern way to use Spark is through Kubernetes. To run\\na  Spark  job,  the  desired  number  of  Spark  containers  are  started  by  Kubernetes;\\nonce they are started, they can communicate to complete the computation, after\\nwhich  the  containers  are  destroyed  and  the  resources  are  available  for  other\\napplications, including other Spark jobs that may have different Spark versions or\\ndependencies.\\n\\n• Another  way  to  distribute  batch  processing  is  to  partition  the  data.  There  are\\nmany ways to achieve this, but the general idea is that scoring is typically a row-\\nby-row  operation  (each  row  is  scored  one  by  one),  and  the  data  can  be  split  in\\nsome way so that several machines can each read a subset of the data and score a\\nsubset of the rows.\\n\\nIn terms of computation, scaling the number of models is somewhat simpler. The key\\nis to add more computing power and to make sure the monitoring infrastructure can\\nhandle the workload. But in terms of governance and processes, this is the most chal‐\\nlenging situation.\\n\\nIn particular, scaling the number of models means that the CI/CD pipeline must be\\nable  to  handle  large  numbers  of  deployments.  As  the  number  of  models  grows,  the\\nneed for automation and governance grows, as human verification cannot necessarily\\nbe systematic or consistent.\\n\\nIn some applications, it is possible to rely on fully automated continuous deployment\\nif the risks are well controlled by automated validation, canary releases, and automa‐\\nted canary analysis. There can be numerous infrastructure challenges since training,\\nbuilding  models,  validating  on  test  data,  etc.,  all  need  to  be  performed  on  clusters\\nrather than on a single machine. Also, with a higher number of models, the CI/CD\\npipeline of each model can vary widely, and if nothing is done, each team will have to\\ndevelop its own CI/CD pipeline for each model.\\n\\nThis is suboptimal from efficiency and governance perspectives. While some models\\nmay need highly specific validation pipelines, most projects can probably use a small\\n\\n82 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cnumber of common patterns. In addition, maintenance is made much more complex\\nas  it  may  become  impractical  to  implement  a  new  systematic  validation  step,  for\\nexample,  since  the  pipelines  would  not  necessarily  share  a  common  structure  and\\nwould then be impossible to update safely, even programmatically. Sharing practices\\nand  standardized  pipelines  can  help  limit  complexity.  A  dedicated  tool  to  manage\\nlarge numbers of pipelines can also be used; for example, Netflix released Spinnaker,\\nan open source continuous deployment and infrastructure management platform.\\n\\nRequirements and Challenges\\nWhen deploying a model, there are several possible scenarios:\\n\\n• One model deployed on one server\\n\\n• One model deployed on multiple servers\\n\\n• Multiple versions of a model deployed on one server\\n\\n• Multiple versions of a model deployed on multiple servers\\n\\n• Multiple versions of multiple models deployed on multiple servers\\n\\nAn effective logging system should be able to generate centralized datasets that can be\\nexploited by the model designer or the ML engineer, usually outside of the produc‐\\ntion environment. More specifically, it should cover all of the following situations:\\n\\n• The system can access and retrieve scoring logs from multiple servers, either in a\\n\\nreal-time scoring use case or in a batch scoring use case.\\n\\n• When a model is deployed on multiple servers, the system can handle the map‐\\n\\nping and aggregation of all information per model across servers.\\n\\n• When  different  versions  of  a  model  are  deployed,  the  system  can  handle  the\\nmapping  and  aggregation  of  all  information  per  version  of  the  model  across\\nservers.\\n\\nIn terms of challenges, for large-scale machine learning applications, the number of\\nraw event logs generated can be an issue if there are no preprocessing steps in place to\\nfilter  and  aggregate  data.  For  real-time  scoring  use  cases,  logging  streaming  data\\nrequires  setting  up  a  whole  new  set  of  tooling  that  entails  a  significant  engineering\\neffort to maintain. However, in both cases, because the goal of monitoring is usually\\nto  estimate  aggregate  metrics,  saving  only  a  subset  of  the  predictions  may  be\\nacceptable.\\n\\nRequirements and Challenges \\n\\n| \\n\\n83\\n\\n\\x0cClosing Thoughts\\nDeploying  to  production  is  a  key  component  of  MLOps,  and  as  dissected  in  this\\nchapter,  having  the  right  processes  and  tools  in  place  can  ensure  that  it  happens\\nquickly. The good news is that many of the elements of success, particularly CI/CD\\nbest  practices,  are  not  new.  Once  teams  understand  how  they  can  be  applied  to\\nmachine learning models, the organization will have a good foundation on which to\\nexpand as MLOps scales with the business.\\n\\n84 \\n\\n| \\n\\nChapter 6: Deploying to Production\\n\\n\\x0cCHAPTER 7\\nMonitoring and Feedback Loop\\n\\nDu Phan\\n\\nWhen a machine learning model is deployed in production, it can start degrading in\\nquality fast—and without warning—until it’s too late (i.e., it’s had a potentially nega‐\\ntive impact on the business). That’s why model monitoring is a crucial step in the ML\\nmodel life cycle and a critical piece of MLOps (illustrated in Figure 7-1 as a part of\\nthe overall life cycle).\\n\\nFigure 7-1. Monitoring and feedback loop highlighted in the larger context of the ML\\nproject life cycle\\n\\nMachine learning models need to be monitored at two levels:\\n\\n• At  the  resource  level,  including  ensuring  the  model  is  running  correctly  in  the\\nproduction  environment.  Key  questions  include:  Is  the  system  alive?  Are  the\\nCPU, RAM, network usage, and disk space as expected? Are requests being pro‐\\ncessed at the expected rate?\\n\\n85\\n\\n\\x0c• At the performance level, meaning monitoring the pertinence of the model over\\ntime. Key questions include: Is the model still an accurate representation of the\\npattern of new incoming data? Is it performing as well as it did during the design\\nphase?\\n\\nThe first level is a traditional DevOps topic that has been extensively addressed in the\\nliterature (and has been covered in Chapter 6). However, the latter is more compli‐\\ncated.  Why?  Because  how  well  a  model  performs  is  a  reflection  of  the  data  used  to\\ntrain it; in particular, how representative that training data is of the live request data.\\nAs the world is constantly changing, a static model cannot catch up with new patterns\\nthat  are  emerging  and  evolving  without  a  constant  source  of  new  data.  While  it  is\\npossible to detect large deviations on single predictions (see Chapter 5), smaller but\\nstill significant deviations have to be detected statistically on datasets of scored rows,\\nwith or without ground truth.\\n\\nModel performance monitoring attempts to track this degradation, and, at an appro‐\\npriate time, it will also trigger the retraining of the model with more representative\\ndata. This chapter delves into detail on how data teams should handle both monitor‐\\ning and subsequent retraining.\\n\\nHow Often Should Models Be Retrained?\\nOne  of  the  key  questions  teams  have  regarding  monitoring  and  retraining  is:  how\\noften  should  models  be  retrained?  Unfortunately,  there  is  no  easy  answer,  as  this\\nquestion depends on many factors, including:\\n\\nThe domain\\n\\nModels in areas like cybersecurity or real-time trading need to be updated regu‐\\nlarly to keep up with the constant changes inherent in these fields. Physical mod‐\\nels,  like  voice  recognition,  are  generally  more  stable,  because  the  patterns  don’t\\noften abruptly change. However, even more stable physical models need to adapt\\nto change: what happens to a voice recognition model if the person has a cough\\nand the tone of their voice changes?\\n\\nThe cost\\n\\nOrganizations  need  to  consider  whether  the  cost  of  retraining  is  worth  the\\nimprovement in performance. For example, if it takes one week to run the whole\\ndata pipeline and retrain the model, is it worth a 1% improvement?\\n\\nThe model performance\\n\\nIn some situations, the model performance is restrained by the limited number of\\ntraining examples, and thus the decision to retrain hinges on collecting enough\\nnew data.\\n\\n86 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cWhatever the domain, the delay to obtain the ground truth is key to defining a lower\\nbound to the retraining period. It is very risky to use a prediction model when there\\nis  a  possibility  that  it  drifts  faster  than  the  lag  between  prediction  time  and  ground\\ntruth obtention time. In this scenario, the model can start giving bad results without\\nany  recourse  other  than  to  withdraw  the  model  if  the  drift  is  too  significant.  What\\nthis means in practice is that it is unlikely a model with a lag of one year is retrained\\nmore than a few times a year.\\n\\nFor the same reason, it is unlikely that a model is trained on data collected during a\\nperiod  smaller  than  this  lag.  Retraining  will  not  be  performed  in  a  shorter  period,\\neither.  In  other  words,  if  the  model  retraining  occurs  way  more  often  than  the  lag,\\nthere will be almost no impact of the retraining on the performance of the model.\\n\\nThere  are  also  two  organizational  bounds  to  consider  when  it  comes  to  retraining\\nfrequency:\\n\\nAn upper bound\\n\\nIt  is  better  to  perform  retraining  once  every  year  to  ensure  that  the  team  in\\ncharge has the skills to do it (despite potential turnover—i.e., the possibility that\\nthe people retraining the model were not the ones who built it) and that the com‐\\nputing toolchain is still up.\\n\\nA lower bound\\n\\nTake, for example, a model with near-instantaneous feedback, such as a recom‐\\nmendation engine where the user clicks on the product offerings within seconds\\nafter the prediction. Advanced deployment schemes will involve shadow testing\\nor A/B testing to make sure that the model performs as anticipated. Because it is\\na statistical validation, it takes some time to gather the required information. This\\nnecessarily  sets  a  lower  bound  to  the  retraining  period.  Even  with  a  simple\\ndeployment,  the  process  will  probably  allow  for  some  human  validation  or  for\\nthe  possibility  of  manual  rollback,  which  means  it’s  unlikely  that  the  retraining\\nwill occur more than once a day.\\n\\nTherefore, it is very likely that retraining will be done between once a day and once a\\nyear. The simplest solution that consists of retraining the model in the same way and\\nin the same environment it was trained in originally is acceptable. Some critical cases\\nmay require retraining in a production environment, even though the initial training\\nwas done in a design environment, but the retraining method is usually identical to\\nthe training method so that the overall complexity is limited. As always, there is an\\nexception to this rule: online learning.\\n\\nHow Often Should Models Be Retrained? \\n\\n| \\n\\n87\\n\\n\\x0cOnline Learning\\nSometimes, the use case requires teams to go further than the automation of the exist‐\\ning manual ML pipeline by using dedicated algorithms that can train themselves iter‐\\natively. (Standard algorithms, by contrast, are retrained from scratch most of the time,\\nwith the exception of deep learning algorithms.)\\n\\nWhile  conceptually  attractive,  these  algorithms  are  more  costly  to  set  up.  The\\ndesigner has to not only test the performance of the model on a test dataset, but also\\nqualify its behavior when data changes. (The latter is required because it’s difficult to\\nmitigate bad learning once the algorithm is deployed, and it’s hard to reproduce the\\nbehavior when each training recursively relies on the previous one because one needs\\nto replay all the steps to understand the bad behavior). In addition, these algorithms\\nare not stateless: running them twice on the same data will not give the same result\\nbecause they have learned from the first run.\\n\\nThere  is  no  standard  way—similar  to  cross-validation—to  do  this  process,  so  the\\ndesign costs will be higher. Online machine learning is a vivid branch of research with\\nsome  mature  technologies  like  state-space  models,  though  they  require  significant\\nskills  to  be  used  effectively.  Online  learning  is  typically  appealing  in  streaming  use\\ncases, though mini batches may be more than enough to handle it.\\n\\nIn any case, some level of model retraining is definitely necessary—it’s not a question\\nof  if,  but  of  when.  Deploying  ML  models  without  considering  retraining  would  be\\nlike launching an unmanned aircraft from Paris in the exact right direction and hop‐\\ning it will land safely in New York City without further control.\\n\\nThe good news is that if it was possible to gather enough data to train the model the\\nfirst  time,  then  most  of  the  solutions  for  retraining  are  already  available  (with  the\\npossible  exception  of  cross-trained  models  that  are  used  in  a  different  context—for\\nexample, trained with data from one country but used in another). It is therefore crit‐\\nical for organizations to have a clear idea of deployed models’ drift and accuracy by\\nsetting up a process that allows for easy monitoring and notifications. An ideal sce‐\\nnario would be a pipeline that automatically triggers checks for degradation of model\\nperformance.\\n\\nIt’s  important  to  note  that  the  goal  of  notifications  is  not  necessarily  to  kick  off  an\\nautomated process of retraining, validation, and deployment. Model performance can\\nchange  for  a  variety  of  reasons,  and  retraining  may  not  always  be  the  answer.  The\\npoint  is  to  alert  the  data  scientist  of  the  change;  that  person  can  then  diagnose  the\\nissue and evaluate the next course of action.\\n\\nIt is therefore critical that as part of MLOps and the ML model life cycle, data scien‐\\ntists  and  their  managers  and  the  organization  as  a  whole  (which  is  ultimately  the\\n\\n88 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0centity that has to deal with the business consequences of degrading model perform‐\\nances and any subsequent changes) understand model degradation. Practically, every\\ndeployed  model  should  come  with  monitoring  metrics  and  corresponding  warning\\nthresholds  to  detect  meaningful  business  performance  drops  as  quickly  as  possible.\\nThe  following  sections  focus  on  understanding  these  metrics  to  be  able  to  define\\nthem for a particular model.\\n\\nUnderstanding Model Degradation\\nOnce a machine learning model is trained and deployed in production, there are two\\napproaches  to  monitor  its  performance  degradation:  ground  truth  evaluation  and\\ninput  drift  detection.  Understanding  the  theory  behind  and  limitations  of  these\\napproaches is critical to determining the best strategy.\\n\\nGround Truth Evaluation\\nGround truth retraining requires waiting for the label event. For example, in a fraud\\ndetection model, the ground truth would be whether or not a specific transaction was\\nactually  fraudulent.  For  a  recommendation  engine,  it  would  be  whether  or  not  the\\ncustomer clicked on—or ultimately bought—one of the recommended products.\\n\\nWith the new ground truth collected, the next step is to compute the performance of\\nthe model based on ground truth and compare it with registered metrics in the train‐\\ning  phase.  When  the  difference  surpasses  a  threshold,  the  model  can  be  deemed  as\\noutdated, and it should be retrained.\\n\\nThe metrics to be monitored can be of two varieties:\\n\\n• Statistical metrics like accuracy, ROC AUC, log loss, etc. As the model designer\\nhas probably already chosen one of these metrics to pick the best model, it is a\\nfirst-choice candidate for monitoring. For more complex models, where the aver‐\\nage performance is not enough, it may be necessary to look at metrics computed\\nby subpopulations.\\n\\n• Business  metrics,  like  cost-benefit  assessment.  For  example,  the  credit  scoring\\n\\nbusiness has developed its own specific metrics.\\n\\nThe  main  advantage  of  the  first  kind  of  metric  is  that  it  is  domain  agnostic,  so  the\\ndata  scientist  likely  feels  comfortable  setting  thresholds.  So  as  to  have  the  earliest\\nmeaningful warning, it is even possible to compute p-values to assess the probability\\nthat the observed drop is not due to random fluctuations.\\n\\nUnderstanding Model Degradation \\n\\n| \\n\\n89\\n\\n\\x0cA Stats Primer: From Null Hypothesis to p-Values\\nThe null hypothesis says that there is no relationship between the variables being com‐\\npared; any results are due to sheer chance.\\n\\nThe alternative hypothesis says that the variables being compared are related, and the\\nresults  are  significant  in  supporting  the  theory  being  considered,  and  not  due  to\\nchance.\\n\\nThe  level  of  statistical  significance  is  often  expressed  as  a  p-value  between  0  and  1.\\nThe  smaller  the  p-value,  the  stronger  the  evidence  that  one  should  reject  the  null\\nhypothesis.\\n\\nThe  drawback  is  that  the  drop  may  be  statistically  significant  without  having  any\\nnoticeable  impact.  Or  worse,  the  cost  of  retraining  and  the  risk  associated  with  a\\nredeployment  may  be  higher  than  the  expected  benefits.  Business  metrics  are  far\\nmore  interesting  because  they  ordinarily  have  a  monetary  value,  enabling  subject\\nmatter experts to better handle the cost-benefit trade-off of the retraining decision.\\n\\nWhen  available,  ground  truth  monitoring  is  the  best  solution.  However,  it  may  be\\nproblematic. There are three main challenges:\\n\\n• Ground truth is not always immediately, or even imminently, available. For some\\ntypes of models, teams need to wait months (or longer) for ground truth labels to\\nbe available, which can mean significant economic loss if the model is degrading\\nquickly. As said before, deploying a model for which the drift is faster than the\\nlag  is  risky.  However,  by  definition,  drifts  are  not  forecastable,  so  models  with\\nlong lags need mitigation measures.\\n\\n• Ground truth and prediction are decoupled. To compute the performance of the\\ndeployed model on new data, it’s necessary to be able to match ground truth with\\nthe corresponding observation. In many production environments, this is a chal‐\\nlenging task because these two pieces of information are generated and stored in\\ndifferent systems and at different timestamps. For low-cost or short-lived models,\\nit might not be worth automated ground truth collection. Note that this is rather\\nshort-sighted, because sooner or later, the model will need to be retrained.\\n\\n• Ground truth is only partially available. In some situations, it is extremely expen‐\\nsive to retrieve the ground truth for all the observations, which means choosing\\nwhich samples to label and thus inadvertently introducing bias into the system.\\n\\nFor the last challenge, fraud detection presents a clear use case. Given that each trans‐\\naction needs to be examined manually and the process takes a long time, does it make\\nsense  to  establish  ground  truth  for  only  suspect  cases  (i.e.,  cases  where  the  model\\n\\n90 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cgives  a  high  probability  of  fraud)?  At  first  glance,  the  approach  seems  reasonable;\\nhowever,  a  critical  mind  understands  that  this  creates  a  feedback  loop  that  will\\namplify the flaws of the model. Fraud patterns that were never captured by the model\\n(i.e.,  those  that  have  a  low  fraud  probability  according  to  the  model)  will  never  be\\ntaken into account in the retraining process.\\n\\nOne  solution  to  this  challenge  might  be  to  randomly  label,  establishing  a  ground\\ntruth  for  just  a  subsample  of  transactions  in  addition  to  those  that  were  flagged  as\\nsuspicious. Another solution might be to reweight the biased sample so that its char‐\\nacteristics  match  the  general  population  more  closely.  For  example,  if  the  system\\nawarded  little  credit  to  people  with  low  income,  the  model  should  reweight  them\\naccording to their importance in the applicant, or even in the general, population.\\n\\nThe  bottom  line  is  that  whatever  the  mitigation  measure,  the  labeled  sample  subset\\nmust cover all possible future predictions so that the trained model makes good pre‐\\ndictions whatever the sample; this will sometimes mean making suboptimal decisions\\nfor the sake of checking that the model continues to generalize well.\\n\\nOnce this problem is solved for retraining, the solution (reweighting, random sam‐\\npling) can be used for monitoring. Input drift detection complements this approach,\\nas it is needed to make sure that ground truth covering new, unexplored domains is\\nmade available to retrain the model.\\n\\nInput Drift Detection\\nGiven the challenges and limitations of ground truth retraining presented in the pre‐\\nvious section, a more practical approach might be input drift detection. This section\\ntakes a brief but deep dive into the underlying logic behind drift and presents differ‐\\nent scenarios that can cause models and data to drift.\\n\\nSay the goal is to predict the quality of Bordeaux wines using as training data the UCI\\nWine Quality dataset, which contains information about red and white variants of the\\nPortuguese wine vinho verde along with a quality score varying between 0 and 10.\\n\\nThe following features are provided for each wine: type, fixed acidity, volatile acidity,\\ncitric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density,\\npH, sulphates, and alcohol rate.\\n\\nTo simplify the modeling problem, say that a good wine is one with a quality score\\nequal to or greater than 7. The goal is thus to build a binary model that predicts this\\nlabel from the wine’s attributes.\\n\\nUnderstanding Model Degradation \\n\\n| \\n\\n91\\n\\n\\x0cTo demonstrate data drift, we explicitly split the original dataset into two:\\n\\n• wine_alcohol_above_11,  which  contains  all  wines  with  an  alcohol  rate  of  11%\\n\\nand above\\n\\n• wine_alcohol_below_11,  which  contains  all  wines  with  an  alcohol  rate  below\\n\\n11%\\n\\nWe split wine_alcohol_above_11 to train and score our model, and the second data‐\\nset, wine_alcohol_below_11, will be considered as new incoming data that needs to\\nbe scored once the model has been deployed.\\n\\nWe have artificially created a big problem: it is very unlikely that the quality of wine is\\nindependent from the alcohol level. Worse, the alcohol level is likely to be correlated\\ndifferently with the other features in the two datasets. As a result, what is learned on\\none dataset (“if the residual sugar is low and the pH is high, then the probability that\\nthe wine is good is high”) may be wrong on the other one because, for example, the\\nresidual sugar is not important anymore when the alcohol level is high.\\n\\nMathematically speaking, the samples of each dataset cannot be assumed to be drawn\\nfrom  the  same  distribution  (i.e.,  they  are  not  “identically  distributed”).  Another\\nmathematical  property  is  necessary  to  ensure  that  ML  algorithms  perform  as\\nexpected:  independence.  This  property  is  broken  if  samples  are  duplicated  in  the\\ndataset  or  if  it  is  possible  to  forecast  the  “next”  sample  given  the  previous  one,  for\\nexample.\\n\\nLet’s  assume  that  despite  the  obvious  problems,  we  train  the  algorithm  on  the  first\\ndataset and then deploy it on the second one. The resulting distribution shift is called\\na drift. It will be called a feature drift if the alcohol level is one of the features used by\\nthe  ML  model  (or  if  the  alcohol  level  is  correlated  with  other  features  used  by  the\\nmodel) and a concept drift if it is not.\\n\\nDrift Detection in Practice\\nAs  explained  previously,  to  be  able  to  react  in  a  timely  manner,  model  behavior\\nshould be monitored solely based on the feature values of the incoming data, without\\nwaiting for the ground truth to be available.\\n\\nThe logic is that if the data distribution (e.g., mean, standard deviation, correlations\\nbetween features) diverges between the training and testing phases1 on one side and\\nthe development phase on the other, it is a strong signal that the model’s performance\\nwon’t  be  the  same.  It  is  not  the  perfect  mitigation  measure,  as  retraining  on  the\\n\\n1 It is also advisable to assess the drift between the training and the test dataset, especially when the test dataset\\n\\nis posterior to the training dataset. See “Choosing Evaluation Metrics” on page 51 for details.\\n\\n92 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cdrifted dataset will not be an option, but it can be part of mitigation measures (e.g.,\\nreverting to a simpler model, reweighting).\\n\\nExample Causes of Data Drift\\nThere are two frequent root causes of data drift:\\n\\n• Sample selection bias, where the training sample is not representative of the pop‐\\nulation.  For  instance,  building  a  model  to  assess  the  effectiveness  of  a  discount\\nprogram  will  be  biased  if  the  best  discounts  are  proposed  for  the  best  clients.\\n  Selection  bias  often  stems  from  the  data  collection  pipeline  itself.  In  the  wine\\nexample,  the  original  dataset  sample  with  alcohol  levels  above  11%  surely  does\\nnot represent the whole population of wines—this is sample selection at its best.\\nIt could have been mitigated if a few samples of wine with an alcohol level above\\n11% had been kept and reweighted according to the expected proportion in the\\npopulation of wines to be seen by the deployed model. Note that this task is eas‐\\nier said than done in real life, as the problematic features are often unknown or\\nmaybe even not available.\\n\\n• Non-stationary environment, where training data collected from the source pop‐\\nulation  does  not  represent  the  target  population.  This  often  happens  for  time-\\ndependent tasks—such as forecasting use cases—with strong seasonality effects,\\nwhere learning a model over a given month won’t generalize to another month.\\nBack to the wine example: one can imagine a case where the original dataset sam‐\\nple only includes wines from a specific year, which might represent a particularly\\ngood (or bad) vintage. A model trained on this data may not generalize to other\\nyears.\\n\\nInput Drift Detection Techniques\\nAfter understanding the possible situations that can cause different types of drift, the\\nnext  logical  question  is:  how  can  drift  be  detected?  This  section  presents  two  com‐\\nmon  approaches.  The  choice  between  them  depends  on  the  expected  level  of\\ninterpretability.\\n\\nOrganizations  that  need  proven  and  explainable  methods  should  prefer  univariate\\nstatistical tests. If complex drift involving several features simultaneously is expected,\\nor if the data scientists want to reuse what they already know and assuming the orga‐\\nnization doesn’t dread the black box effect, the domain classifier approach may be a\\ngood option, too.\\n\\nDrift Detection in Practice \\n\\n| \\n\\n93\\n\\n\\x0cUnivariate statistical tests\\n\\nThis method requires applying a statistical test on data from the source distribution\\nand the target distribution for each feature. A warning will be raised when the results\\nof those tests are significant.\\n\\nThe choice of hypothesis tests have been extensively studied in the literature, but the\\nbasic approaches rely on these two tests:\\n\\n• For  continuous  features,  the  Kolmogorov-Smirnov  test  is  a  nonparametric\\nhypothesis  test  that  is  used  to  check  whether  two  samples  come  from  the  same\\ndistribution. It measures a distance between the empirical distribution functions.\\n\\n• For  categorical  features,  the  Chi-squared  test  is  a  practical  choice  that  checks\\nwhether  the  observed  frequencies  for  a  categorical  feature  in  the  target  data\\nmatch the expected frequencies seen from the source data.\\n\\nThe  main  advantage  of  p-values  is  that  they  help  detect  drift  as  quickly  as  possible.\\nThe main drawback is that they detect an effect, but they do not quantify the level of\\nthe effect (i.e., on large datasets, they detect very small changes, which may be com‐\\npletely without impact). As a result, if development datasets are very large, it is neces‐\\nsary  to  complement  p-values  with  business-significant  metrics.  For  example,  on  a\\nsufficiently large dataset, the average age may have significantly drifted from a statisti‐\\ncal perspective, but if the drift is only a few months, this is probably an insignificant\\nvalue for many business use cases.\\n\\nDomain classifier\\n\\nIn this approach, data scientists train a model that tries to discriminate between the\\noriginal dataset (input features and, optionally, predicted target) and the development\\ndataset. In other words, they stack the two datasets and train a classifier that aims at\\npredicting the data’s origin. The performance of the model (its accuracy, for example)\\ncan then be considered as a metric for the drift level.\\n\\nIf this model is successful in its task, and thus has a high drift score, it implies that the\\ndata used at training time and the new data can be distinguished, so it’s fair to say that\\nthe new data has drifted. To gain more insights, in particular to identify the features\\nthat  are  responsible  for  the  drift,  one  can  use  the  feature  importance  of  the  trained\\nmodel.\\n\\nInterpretation of results\\n\\nBoth domain classifier and univariate statistical tests point to the importance of fea‐\\ntures  or  of  the  target  to  explain  drift.  Drift  attributed  to  the  target  is  important  to\\nidentify because it often directly impacts the bottom line of the business. (Think, for\\nexample, of credit scores: if the scores are lower overall, the number of awarded loans\\n\\n94 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cis likely to be lower, and therefore revenue will be lower.) Drift attributed to features\\nis useful to mitigate the impact of drift, as it may hint at the need for:\\n\\n• Reweighting according to this feature (e.g., if customers above 60 now represent\\n60% of users but were only 30% in the training set, then double their weight and\\nretrain the model)\\n\\n• Removing the feature and training a new model without it\\n\\nIn all cases, it is very unlikely that automatic actions exist if drift is detected. It could\\nhappen  if  it  is  costly  to  deploy  retrained  models:  the  model  would  be  retrained  on\\nnew data only if performance based on ground truth had dropped or significant drift\\nwas detected. In this peculiar case, new data is indeed available to mitigate the drift.\\n\\nThe Feedback Loop\\nAll effective machine learning projects implement a form of data feedback loop; that\\nis, information from the production environment flows back to the model prototyp‐\\ning environment for further improvement.\\n\\nOne can see in Figure 7-2 that data collected in the monitoring and feedback loop is\\nsent  to  the  model  development  phase  (details  about  this  data  are  covered  in  Chap‐\\nter 6). From there, the system analyzes whether the model is working as expected. If it\\nis, no action is required. If the model’s performance is degrading, an update will be\\ntriggered, either automatically or manually by the data scientist. In practice, as seen at\\nthe beginning of this chapter, this usually means either retraining the model with new\\nlabeled data or developing a new model with additional features.\\n\\nFigure 7-2. Continuous delivery for end-to-end machine learning process\\n\\nThe Feedback Loop \\n\\n| \\n\\n95\\n\\n\\x0cIn either case, the goal is to be able to capture the emerging patterns and make sure\\nthat the business is not negatively impacted. This infrastructure is comprised of three\\nmain components, which in addition to the concepts discussed in the first part of this\\nchapter, are critical to robust MLOps capabilities:\\n\\n• A logging system that collects data from several production servers\\n\\n• A model evaluation store that does versioning and evaluation between different\\n\\nmodel versions\\n\\n• An  online  system  that  does  model  comparison  on  production  environments,\\neither with the shadow scoring (champion/challenger) setup or with A/B testing\\n\\nThe following sections address each of these components individually, including their\\npurpose, key features, and challenges.\\n\\nLogging\\nMonitoring a live system, with or without machine learning components, means col‐\\nlecting and aggregating data about its states. Nowadays, as production infrastructures\\nare  getting  more  and  more  complex,  with  several  models  deployed  simultaneously\\nacross several servers, an effective logging system is more important than ever.\\n\\nData from these environments needs to be centralized to be analyzed and monitored,\\neither automatically or manually. This will enable continuous improvement of the ML\\nsystem. An event log of a machine learning system is a record with a timestamp and\\nthe following information.\\n\\nModel metadata\\n\\nIdentification of the model and the version.\\n\\nModel inputs\\n\\nFeature  values  of  new  observations,  which  allow  for  verification  of  whether  the\\nnew incoming data is what the model was expecting and thus allowing for detec‐\\ntion of data drift (as explained in the previous section).\\n\\nModel outputs\\n\\nPredictions made by the model that, along with the ground truth collected later\\non,  give  a  concrete  idea  about  the  model  performance  in  a  production\\nenvironment.\\n\\nSystem action\\n\\nIt’s rare that the model prediction is the end product of a machine learning appli‐\\ncation; the more common situation is that the system will take an action based on\\nthis prediction. For example, in a fraud detection use case, when the model gives\\nhigh probability, the system can either block the transaction or send a warning to\\n\\n96 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cthe bank. This type of information is important because it affects the user reac‐\\ntion and thus indirectly affects the feedback data.\\n\\nModel explanation\\n\\nIn  some  highly  regulated  domains  such  as  finance  or  healthcare,  predictions\\nmust come with an explanation (i.e., which features have the most influence on\\nthe  prediction).  This  kind  of  information  is  usually  computed  with  techniques\\nsuch  as  Shapley  value  computation  and  should  be  logged  to  identify  potential\\nissues with the model (e.g., bias, overfitting).\\n\\nModel Evaluation\\nOnce the logging system is in place, it periodically fetches data from the production\\nenvironment for monitoring. Everything goes well until one day the data drift alert is\\ntriggered: the incoming data distribution is drifting away from the training data dis‐\\ntribution. It’s possible that the model performance is degrading.\\n\\nAfter  review,  data  scientists  decide  to  improve  the  model  by  retraining  it,  using  the\\ntechniques  described  earlier  in  this  chapter.  With  several  trained  candidate  models,\\nthe  next  step  is  to  compare  them  with  the  deployed  model.  In  practice,  this  means\\nevaluating all the models (the candidates as well as the deployed model) on the same\\ndataset.  If  one  of  the  candidate  models  outperforms  the  deployed  model,  there  are\\ntwo  ways  to  proceed:  either  update  the  model  on  the  production  environment  or\\nmove to an online evaluation via a champion/challenger or A/B testing setup.\\n\\nIn a nutshell, this is the notion of model store. It is a structure that allows data scien‐\\ntists to:\\n\\n• Compare  multiple,  newly  trained  model  versions  against  existing  deployed\\n\\nversions\\n\\n• Compare  completely  new  models  against  versions  of  other  models  on  labeled\\n\\ndata\\n\\n• Track model performance over time\\n\\nFormally,  the  model  evaluation  store  serves  as  a  structure  that  centralizes  the  data\\nrelated to model life cycle to allow comparisons (though note that comparing models\\nmakes sense only if they address the same problem). By definition, all these compari‐\\nsons are grouped under the umbrella of a logical model.\\n\\nLogical model\\n\\nBuilding  a  machine  learning  application  is  an  iterative  process,  from  deploying  to\\nproduction,  monitoring  performance,  retrieving  data,  and  looking  for  ways  to\\nimprove how the system addresses the target problem. There are many ways to iter‐\\nate, some of which have already been discussed in this chapter, including:\\n\\nThe Feedback Loop \\n\\n| \\n\\n97\\n\\n\\x0c• Retraining the same model on new data\\n\\n• Adding new features to the model\\n\\n• Developing new algorithms\\n\\nFor  those  reasons,  the  machine  learning  model  itself  is  not  a  static  object;  it  con‐\\nstantly changes with time. It is therefore helpful to have a higher abstraction level to\\nreason about machine learning applications, which is referred to as a logical model.\\n\\nA  logical  model  is  a  collection  of  model  templates  and  their  versions  that  aims  to\\nsolve a business problem. A model version is obtained by training a model template\\non a given dataset. All versions of model templates of the same logical model can usu‐\\nally be evaluated on the same kinds of datasets (i.e., on datasets with the same feature\\ndefinition and/or schema); however, this may not be the case if the problem did not\\nchange  but  the  features  available  to  solve  it  did.  Model  versions  could  be  imple‐\\nmented  using  completely  different  technologies,  and  there  could  even  be  several\\nimplementations of the same model version (Python, SQL, Java, etc.); regardless, they\\nare supposed to give the same prediction if given the same input.\\n\\nLet’s  get  back  to  the  wine  example  introduced  earlier  in  this  chapter.  Three  months\\nafter  deployment,  there  is  new  data  about  less  alcoholic  wine.  We  can  retrain  our\\nmodel  on  the  new  data,  thus  obtaining  a  new  model  version  using  the  same  model\\ntemplate. While investigating the result, we discover new patterns are emerging. We\\nmay  decide  to  create  new  features  that  capture  this  information  and  add  it  to  the\\nmodel, or we may decide to use another ML algorithm (like deep learning) instead of\\nXGBoost. This would result in a new model template.\\n\\nAs a result, our model has two model templates and three versions:\\n\\n• The first version is live in production, based on the original model template.\\n\\n• The second version is based on the original template, but trained on new data.\\n\\n• The third version uses the deep learning–based template with additional features\\n\\nand is trained on the same data as the second version.\\n\\nThe information about the evaluation of these versions on various datasets (both the\\ntest datasets used at training time and the development datasets that may be scored\\nafter training) is then stored in the model evaluation store.\\n\\nModel evaluation store\\n\\nAs a reminder, model evaluation stores are structures that centralize the data related\\nto model life cycles to allow comparisons. The two main tasks of a model evaluation\\nstore are:\\n\\n98 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0c• Versioning the evolution of a logical model through time. Each logged version of\\nthe  logical  model  must  come  with  all  the  essential  information  concerning  its\\ntraining phase, including:\\n\\n— The list of features used\\n\\n— The preprocessing techniques that are applied to each feature\\n\\n— The algorithm used, along with the chosen hyperparameters\\n\\n— The training dataset\\n\\n— The test dataset used to evaluate the trained model (this is necessary for the\\n\\nversion comparison phase)\\n\\n— Evaluation metrics\\n\\n• Comparing  the  performance  between  different  versions  of  a  logical  model.  To\\ndecide which version of a logical model to deploy, all of them (the candidates and\\nthe deployed one) must be evaluated on the same dataset.\\n\\nThe choice of dataset to evaluate is crucial. If there is enough new labeled data to give\\na reliable estimation of the model performance, this is the preferred choice because it\\nis closest to what we are expecting to receive in the production environment. Other‐\\nwise, we can use the original test dataset of the deployed model. Assuming that the\\ndata has not drifted, this gives us a concrete idea about the performance of the candi‐\\ndate models compared to the original model.\\n\\nAfter identifying the best candidate model, the job is not yet done. In practice, there\\nis often a substantial discrepancy between the offline and online performance of the\\nmodels. Therefore, it’s critical to take the testing to the production environment. This\\nonline  evaluation  gives  the  most  truthful  feedback  about  the  behavior  of  the  candi‐\\ndate model when facing real data.\\n\\nOnline Evaluation\\nOnline evaluation of models in production is critical from a business perspective, but\\ncan  be  challenging  from  a  technical  perspective.  There  two  main  modes  of  online\\nevaluation:\\n\\n• Champion/challenger (otherwise known as shadow testing), where the candidate\\n\\nmodel shadows the deployed model and scores the same live requests\\n\\n• A/B testing, where the candidate model scores a portion of the live requests and\\n\\nthe deployed model scores the others\\n\\nBoth  cases  require  ground  truth,  so  the  evaluation  will  necessarily  take  longer  than\\nthe  lag  between  prediction  and  ground  truth  obtention.  In  addition,  whenever\\n\\nThe Feedback Loop \\n\\n| \\n\\n99\\n\\n\\x0cshadow testing is possible, it should be used over A/B testing because it is far simpler\\nto understand and set up, and it detects differences more quickly.\\n\\nChampion/Challenger\\n\\nChampion/challenger involves deploying one or several additional models (the chal‐\\nlengers)  to  the  production  environment.  These  models  receive  and  score  the  same\\nincoming requests as the active model (the champion). However, they do not return\\nany  response  or  prediction  to  the  system:  that’s  still  the  job  of  the  old  model.  The\\npredictions  are  simply  logged  for  further  analysis.  That’s  why  this  method  is  also\\ncalled “shadow testing” or “dark launch.”\\n\\nThis setup allows for two things:\\n\\n• Verification that the performance of the new models is better than, or at least as\\ngood  as,  the  old  model.  Because  the  two  models  are  scoring  on  the  same  data,\\nthere  is  a  direct  comparison  of  their  accuracy  in  the  production  environment.\\nNote that this could also be done offline by using the new models on the dataset\\nmade of new requests scored by the champion model.\\n\\n• Measurement  of  how  the  new  models  handle  realistic  load.  Because  the  new\\nmodels  can  have  new  features,  new  preprocessing  techniques,  or  even  a  new\\nalgorithm, the prediction time for a request won’t be the same as that of the origi‐\\nnal model, and it is important to have a concrete idea of this change. Of course,\\nthis is the main advantage of doing it online.\\n\\nThe other advantage of this deployment scheme is that the data scientist or the ML\\nengineer  is  giving  visibility  to  other  stakeholders  on  the  future  champion  model:\\ninstead of being locked in the data science environment, the challenger model results\\nare exposed to the business leaders, which decreases the perceived risk to switch to a\\nnew model.\\n\\nTo be able to compare the champion and the challenger models, the same informa‐\\ntion must be logged for both, including input data, output data, processing time, etc.\\nThis means updating the logging system so that it can differentiate between the two\\nsources of data.\\n\\nHow long should both models be deployed before it’s clear that one is better than the\\nother?  Long  enough  that  the  metric  fluctuations  due  to  randomness  are  dampened\\nbecause  enough  predictions  have  been  made.  This  can  be  assessed  graphically  by\\nchecking  that  the  metric  estimations  are  not  fluctuating  anymore  or  by  doing  a\\nproper statistical test (as most metrics are averages of row-wise scores, the most usual\\ntest is a paired sample T-test) that yields the probability that the observation that one\\nmetric  is  higher  than  the  other  is  due  to  these  random  fluctuations.  The  wider  the\\nmetric  difference,  the  fewer  predictions  necessary  to  ensure  that  the  difference  is\\nsignificant.\\n\\n100 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cDepending on the use case and the implementation of the champion/challenger sys‐\\ntem,  server  performance  can  be  a  concern.  If  two  memory-intensive  models  are\\ncalled synchronously, they can slow the system down. This will not only have a nega‐\\ntive impact on the user experience but also corrupt the data collected about the func‐\\ntioning of the models.\\n\\nAnother concern is communication with the external system. If the two models use\\nan external API to enrich their features, that doubles the number of requests to these\\nservices, thus doubling costs. If that API has a caching system in place, then the sec‐\\nond  request  will  be  processed  much  faster  than  the  first,  which  can  bias  the  result\\nwhen comparing the total prediction time of the two models. Note that the challenger\\nmay be used only for a random subset of the incoming requests, which will alleviate\\nthe load at the expense of increased time before a conclusion can be drawn.\\n\\nFinally,  when  implementing  a  challenger  model,  it’s  important  to  ensure  it  doesn’t\\nhave any influence on the system’s actions. This implies two scenarios:\\n\\n• When  the  challenger  model  encounters  an  unexpected  issue  and  fails,  the  pro‐\\nduction environment will not experience any discontinuation or degradation in\\nterms of response time.\\n\\n• Actions  taken  by  the  system  depend  only  on  the  prediction  of  the  champion\\nmodel,  and  they  happen  only  once.  For  example,  in  a  fraud  detection  use  case,\\nimagine that by mistake the challenger model is plugged directly into the system,\\ncharging each transaction twice—a catastrophic scenario.\\n\\nIn general, some effort needs to be spent on the logging, monitoring, and serving sys‐\\ntem to ensure the production environment functions as usual and is not impacted by\\nany issues coming from the challenger model.\\n\\nA/B testing\\n\\nA/B testing (a randomized experiment testing two variants, A and B) is a widely used\\ntechnique  in  website  optimization.  For  ML  models,  it  should  be  used  only  when\\nchampion/challenger is not possible. This might happen when:\\n\\n• The ground truth cannot be evaluated for both models. For example, for a rec‐\\nommendation engine, the prediction gives a list of items on which a given cus‐\\ntomer is likely to click if they are presented. Therefore, it is impossible to know if\\nthe customer would have clicked if an item was not presented. In this case, some\\nkind of A/B testing will have to be done, in which some customers will be shown\\nthe recommendations of model A, and some the recommendations of model B.\\nSimilarly,  for  a  fraud  detection  model,  because  heavy  work  is  needed  to  obtain\\nthe ground truth, it may not be possible to do so for the positive predictions of\\ntwo models; it would increase the workload too much, because some frauds are\\n\\nThe Feedback Loop \\n\\n| \\n\\n101\\n\\n\\x0cdetected by only one model. As a result, randomly applying only the B model to a\\nsmall fraction of the requests will allow the workload to remain constant.\\n\\n• The objective to optimize is only indirectly related to the performance of the pre‐\\ndiction. Imagine an ad engine based on an ML model that predicts if a user will\\nclick on the ad. Now imagine that it is evaluated on the buy rate, i.e., whether the\\nuser  bought  the  product  or  service.  Once  again,  it  is  not  possible  to  record  the\\nreaction  of  the  user  for  two  different  models,  so  in  this  case,  A/B  testing  is  the\\nonly way.\\n\\nEntire books are dedicated to A/B testing, so this section presents only its main idea\\nand  a  simple  walkthrough.  Unlike  the  champion/challenger  framework,  with  A/B\\ntesting, the candidate model returns predictions for certain requests, and the original\\nmodel handles the other requests. Once the test period is over, statistical tests com‐\\npare the performance of the two models, and teams can make a decision based on the\\nstatistical significance of those tests.\\n\\nIn an MLOps context, some considerations need to be made. A walkthrough of these\\nconsiderations is presented in Table 7-1.\\n\\nTable 7-1. Considerations for A/B testing in MLOps\\n\\nStage\\nBefore the\\nA/B test\\n\\nMLOps consideration\\nDefine a clear goal: A quantitative business metric that needs to be optimized, such as click-through rate.\\nDefine a precise population: Carefully choose a segment for the test along with a splitting strategy that assures\\nno bias between groups. (This is the so-called experimental design or randomized control trial that’s been\\npopularized by drug studies.) This may be a random split, or it may be more complex. For example, the situation\\nmight dictate that all the requests of a particular customer are handled by the same model.\\nDefine the statistical protocol: The resulting metrics are compared using statistical tests, and the null hypothesis\\nis either rejected or retained. To make the conclusion robust, teams need to define beforehand the sample size\\nfor the desired minimum effect size, which is the minimum difference between the two models’ performance\\nmetrics. Teams must also fix a test duration (or alternatively have a method to handle multiple tests). Note that\\nwith similar sample sizes, the power to detect meaningful differences will be lower than with champion/\\nchallenger because unpaired sample tests have to be used. (It is usually impossible to match each request scored\\nwith model B to a request scored with model A, whereas with champion/challenger, this is trivial.)\\n\\nDuring the\\nA/B test\\n\\nAfter the\\nA/B test\\n\\nIt is important not to stop the experiment before the test duration is over, even if the statistical test starts to\\nreturn a significant metric difference. This practice (also called p-hacking) produces unreliable and biased results\\ndue to cherry-picking the desired outcome.\\nOnce the test duration is over, check the collected data to make sure the quality is good. From there, run the\\nstatistical tests; if the metric difference is statistically significant in favor of the candidate model, the original\\nmodel can be replaced with the new version.\\n\\n102 \\n\\n| \\n\\nChapter 7: Monitoring and Feedback Loop\\n\\n\\x0cClosing Thoughts\\nOrdinary software is built to satisfy specifications. Once an application is deployed,\\nits ability to fulfill its objective does not degrade. ML models, by contrast, have objec‐\\ntives  statistically  defined  by  their  performance  on  a  given  dataset.  As  a  result,  their\\nperformance changes, usually for the worse, when the statistical properties of the data\\nchange.\\n\\nIn  addition  to  ordinary  software  maintenance  needs  (bug  correction,  release\\nupgrades,  etc.),  this  performance  drift  has  to  be  carefully  monitored.  We  have  seen\\nthat  performance  monitoring  based  on  the  ground  truth  is  the  cornerstone,  while\\ndrift monitoring can provide early warning signals. Among possible drift mitigation\\nmeasures, the workhorse is definitely retraining on new data, while model modifica‐\\ntion remains an option. Once a new model is ready to be deployed, its improved per‐\\nformance  can  be  validated  thanks  to  shadow  scoring  or,  as  a  second  choice,  A/B\\ntesting.  This  enables  proving  that  the  new  model  is  better  in  order  to  improve  the\\nperformance of the system. \\n\\nClosing Thoughts \\n\\n| \\n\\n103\\n\\n\\x0c\\x0cCHAPTER 8\\nModel Governance\\n\\nMark Treveil\\n\\nWe explored the idea of governance as a set of controls placed on a business in Chap‐\\nter 3. These goals aim to ensure that the business delivers on its responsibilities to all\\nstakeholders,  from  shareholders  and  employees  to  the  public  and  national  govern‐\\nments. The responsibilities include financial, legal, and ethical, and are all underpin‐\\nned by the desire for fairness.\\n\\nThis chapter goes even more in depth on these topics, shifting from why they matter\\nto how organizations can incorporate them as a part of their MLOps strategy.\\n\\nWho Decides What Governance the Organization Needs?\\nNational regulations are a key part of a society’s framework for safeguarding fairness. \\nBut  these  take  considerable  time  to  be  agreed  upon  and  implemented;  they  always\\nreflect a slightly historical understanding of fairness and the challenges to it. Just as\\nwith  ML  models,  the  past  cannot  always  anticipate  the  evolving  problems  of  the\\nfuture.\\n\\nWhat most businesses want from governance is to safeguard shareholder investment\\nand to help ensure a suitable ROI, both now and in the future. That means the busi‐\\nness has to perform effectively, profitability, and sustainably. The shareholders need\\nclear visibility that customers, employees, and regulatory bodies are happy, and they\\nwant reassurances that appropriate measures are in place to detect and manage any\\ndifficulties that could occur in the future.\\n\\nNone of this is news, of course, nor specific to MLOps. What is different with ML is\\nthat it is a new and often opaque technology that carries many risks, but it is rapidly\\nbeing  embedded  in  decision-making  systems  that  impact  every  aspect  of  our  lives.\\nML  systems  invent  their  own  statistically  driven  decision-making  processes,  often\\n\\n105\\n\\n\\x0cextremely difficult to understand, based on large volumes of data that is thought to\\nrepresent the real world. It’s not hard to see what could go wrong!\\n\\nPerhaps  the  most  surprising  influence  on  the  direction  of  ML  governance  is  public\\nopinion, which evolves much faster than formal regulation. It follows no formal pro‐\\ncess or etiquette. It doesn’t have to be based on fact or reason. Public opinion deter‐\\nmines what products people buy, where they invest their money, and what rules and\\nregulations governments make. Public opinion decides what is fair and what is not.\\n\\nFor  example,  the  agricultural  biotechnology  companies  that  developed  genetically\\nmodified  crops  felt  the  power  of  public  opinion  painfully  in  the  1990s.  While  the\\narguments rage back and forth about whether there was, or was not, a risk to health,\\npublic opinion in Europe swung against genetic modification, and these crops were\\nbanned in many European countries. The parallels with ML are clear: ML offers ben‐\\nefits  to  all  and  yet  brings  risks  that  need  to  be  managed  if  the  public  is  to  trust  it.\\nWithout public trust, the benefits will not fully materialize.\\n\\nThe general public needs to be reassured that ML is fair. What is considered “fair” is\\nnot defined in a rule book, and it is not fixed; it will fluctuate based on events, and it\\nwill not always be the same across the world. Right now, opinion on ML is in the bal‐\\nance. Most people prefer getting sensibly targeted ads, they like their cars being able\\nto  read  speed-limit  signs,  and  improving  fraud  detection  ultimately  saves  them\\nmoney.\\n\\nBut  there  have  also  been  well-publicized  scandals  that  have  rocked  the  public’s\\nacceptance  of  this  technology.  The  Facebook-Cambridge  Analytica  affair,  where  the\\ncompanies  used  the  power  of  ML  to  manipulate  public  opinion  on  social  media,\\nshocked the world. This looked like ML with explicitly malicious intent. Equally wor‐\\nrying have been instances of entirely unintentional harm, where ML black box judg‐\\nments  proved  to  be  unacceptably  and  illegally  biased  on  criteria  such  as  race  or\\ngender, for example in criminal assessment systems and in recruitment tools.\\n\\nIf  businesses  and  governments  want  to  reap  the  benefits  of  ML,  they  have  to  safe‐\\nguard the public trust in it as well as proactively address the risks. For businesses, this\\nmeans developing strong governance of their MLOps process. They must assess the\\nrisks,  determine  their  own  set  of  fairness  values,  and  then  implement  the  necessary\\nprocess  to  manage  them.  Much  of  this  is  simply  about  good  housekeeping  with  an\\nadded  focus  on  mitigating  the  inherent  risks  of  ML,  addressing  topics  such  as  data\\nprovenance, transparency, bias, performance management, and reproducibility.\\n\\n106 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cMatching Governance with Risk Level\\nGovernance is not a free lunch; it takes effort, discipline, and time.\\n\\nFrom  the  business  stakeholders’  perspective,  governance  is  likely  to  slow  down  the\\ndelivery of new models, which may cost the business money. For data scientists, it can\\nlook like a lot of bureaucracy that erodes their ability to get things done. In contrast,\\nthose  responsible  for  managing  risk  and  the  DevOps  team  managing  deployment\\nwould argue that strict governance across the board should be mandatory.\\n\\nThose  responsible  for  MLOps  must  manage  the  inherent  tension  between  different\\nuser profiles, striking a balance between getting the job done efficiently and protect‐\\ning  against  all  possible  threats.  This  balance  can  be  found  by  assessing  the  specific\\nrisk of each project and matching the governance process to that risk level. There are\\nseveral dimensions to consider when assessing risk, including:\\n\\n• The audience for the model\\n\\n• The lifetime of the model and its outcomes\\n\\n• The impact of the outcomes\\n\\nThis  assessment  should  not  only  determine  the  governance  measures  applied,  but\\nalso drive the complete MLOps development and deployment tool chain.\\n\\nFor example, a self-service analytics (SSA) project (one consumed by a small internal-\\nonly audience and often built by business analysts) calls for relatively lightweight gov‐\\nernance.  Conversely,  a  model  deployed  to  a  public-facing  website  making  decisions\\nthat impact people’s lives or company finances requires a very thorough process. This\\nprocess would consider the type of KPIs chosen by the business, the type of model-\\nbuilding algorithm used for the required level of explainability, the coding tools used,\\nthe  level  of  documentation  and  reproducibility,  the  level  of  automated  testing,  the\\nresilience of the hardware platform, and the type of monitoring implemented.\\n\\nBut the business risk is not always so clear cut. An SSA project that makes a decision\\nthat has a long-term impact can also be high risk and can justify stronger governance\\nmeasures.  That’s  why  across  the  board,  teams  need  well  thought  out,  regularly\\nreviewed  strategies  for  MLOps  risk  assessment  (see  Figure  8-1  for  a  breakdown  of\\nproject criticality and operationalization approaches).\\n\\nMatching Governance with Risk Level \\n\\n| \\n\\n107\\n\\n\\x0cFigure 8-1. Choosing the right kind of operationalization model and MLOps features\\ndepending on the project’s criticality\\n\\nCurrent Regulations Driving MLOps Governance\\nThere  is  little  regulation  around  the  world  today  specifically  aimed  at  ML  and  AI.\\nMany existing regulations do, however, have a significant impact on ML governance.\\nThese take two forms:\\n\\n• Industry-specific  regulation.  This  is  particularly  significant  in  the  finance  and\\n\\npharmaceutical sectors.\\n\\n• Broad-spectrum regulation, particularly addressing data privacy.\\n\\nA few of the most pertinent regulations are outlined in the following sections. Their\\nrelevance  to  the  challenges  of  MLOps  governance  is  striking,  and  these  regulations\\ngive  a  good  indication  of  what  governance  measures  will  be  needed  broadly  across\\nthe industry to establish and maintain trust in ML.\\n\\nEven for those working in industries that don’t have specific regulations, the follow‐\\ning  sections  can  give  a  brief  idea  of  what  organizations  worldwide,  regardless  of\\nindustry,  might  face  in  the  future  in  terms  of  the  level  of  specificity  of  control  with\\nregards to machine learning.\\n\\n108 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cPharmaceutical Regulation in the US: GxP\\nGxP is a collection of quality guidelines (such as the Good Clinical Practice, or GCP,\\nguidelines)  and  regulations  established  by  the  U.S.  Food  and  Drug  Administration\\n(FDA), which aim to ensure that bio and pharmaceutical products are safe.\\n\\nGxP’s guidelines focus on:\\n\\n• Traceability, or the ability to re-create the development history of a drug or medi‐\\n\\ncal device.\\n\\n• Accountability, meaning who has contributed what to the development of a drug\\n\\nand when.\\n\\n• Data  Integrity  (DI),  or  the  reliability  of  data  used  in  development  and  testing.\\nThis  is  based  on  the  ALCOA  principle:  attributable,  legible,  contemporaneous,\\noriginal,  and  accurate,  and  considerations  include  identifying  risks  and  mitiga‐\\ntion strategies.\\n\\nFinancial Model Risk Management Regulation\\nIn finance, model risk is the risk of incurring losses when the models used for making\\ndecisions  about  tradable  assets  prove  to  be  inaccurate.  These  models,  such  as  the\\nBlack–Scholes model, existed long before the arrival of ML.\\n\\nModel risk management (MRM) regulation has been driven by the experience of the\\nimpact of extraordinary events, such as financial crashes, and the resulting harm to\\nthe public and the wider economy if severe losses are incurred. Since the financial cri‐\\nsis  of  2007–2008,  a  large  amount  of  additional  regulation  has  been  introduced  to\\nforce good MRM practices (see Figure 8-2).\\n\\nThe  UK  Prudential  Regulation  Authority’s  (PRA)  regulation,  for  example,  defines\\nfour principles for good MRM:\\n\\nModel definition\\n\\nDefine a model and record such models in inventory.\\n\\nRisk governance\\n\\nEstablish model risk governance framework, policies, procedures, and controls.\\n\\nLife cycle management\\n\\nCreate robust model development, implementation, and usage processes.\\n\\nEffective challenge\\n\\nUndertake appropriate model validation and independent review.\\n\\nCurrent Regulations Driving MLOps Governance \\n\\n| \\n\\n109\\n\\n\\x0cFigure 8-2. The history of model risk management (MRM) regulation\\n\\nGDPR and CCPA Data Privacy Regulations\\nThe EU General Data Protection Regulation (GDPR) was first implemented in 2018,\\nsetting  guidelines  for  the  collection  and  processing  of  personal  information  from\\nindividuals  who  live  in  the  European  Union.  However,  it  was  developed  with  the\\ninternet age in mind, so it actually applies for EU visitors to any website, regardless of\\nwhere  that  website  is  based.  Since  few  websites  want  to  exclude  EU  visitors,  sites\\nacross the world have been forced to meet the requirements, making GDPR a de facto\\nstandard for data protection. The regulations aim to give people control of their per‐\\nsonal data that IT systems have collected, including the rights to:\\n\\n• Be informed about data collected or processed\\n\\n• Access collected data and understand its processing\\n\\n• Correct inaccurate data\\n\\n• Be forgotten (i.e., to have data removed)\\n\\n• Restrict processing of personal data\\n\\n• Obtain collected data and reuse it elsewhere\\n\\n• Object to automated decision-making\\n\\n110 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cThe California Consumer Privacy Act (CCPA) is quite similar to GDPR in terms of\\nwho and what is protected, although the scope, territorial reach, and financial penal‐\\nties are all more limited.\\n\\nThe New Wave of AI-Specific Regulations\\nAround the world, a new wave of regulations and guidelines specifically targeting AI\\napplications (and thus all ML applications) is emerging. The European Union is lead‐\\ning the way with an attempt to establish a framework for trustworthy AI.\\n\\nIn a white paper on artificial intelligence, the EU emphasizes the potential benefits of\\nAI for all walks of life. Equally, it highlights that scandals surrounding the misuse of\\nAI  and  warnings  of  the  dangers  of  potential  advances  in  the  power  of  AI  have  not\\ngone  unnoticed.  The  EU  considers  that  regulatory  framework  based  on  its  funda‐\\nmental values “will enable it to become a global leader in innovation in the data econ‐\\nomy and its applications.”\\n\\nThe  EU  identifies  seven  key  requirements  that  AI  applications  should  respect  to  be\\nconsidered trustworthy:\\n\\n• Human agency and oversight\\n\\n• Technical robustness and safety\\n\\n• Privacy and data governance\\n\\n• Transparency\\n\\n• Diversity, non-discrimination, and fairness\\n\\n• Societal and environmental well-being\\n\\n• Accountability\\n\\nThe  EU  approach  is  not  one-size-fits-all:  it  will  primarily  impact  specific  high-risk\\nsectors,  including  healthcare,  transportation,  energy,  and  parts  of  the  public  sector.\\nThe regulations are expected to be optional for other sectors.\\n\\nAs  with  GDPR,  the  EU  approach  is  likely  to  have  a  worldwide  influence.  It  is  also\\nprobable that many large organizations will decide to opt in considering the impor‐\\ntance to their businesses of public trust in the use of AI. Even for those not opting in,\\nthe framework is likely to establish a way of thinking about governance in AI and will\\ninfluence their approach.\\n\\nTable 8-1 outlines some of the statuses of AI governance initiatives across the world.\\nAll are following an unmistakably similar route, even if the level of prescriptiveness\\nreflects their traditionally distinct approaches to regulation.\\n\\nThe New Wave of AI-Specific Regulations \\n\\n| \\n\\n111\\n\\n\\x0cTable 8-1. Status of AI governance initiatives across the world\\n\\nStage\\n\\nFocus\\n\\nComing next\\n\\nRegions &\\norganizations\\nOECD\\n\\nGuidance\\n\\n• 42 signatories\\n• 5 principles for responsible stewardship of trustworthy AI:\\n\\ninclusive growth, human-centered and fairness,\\ntransparency and explainability, robustness, and\\naccountability\\n\\n• Recommendations for national policies\\n\\n• Binding for high-risk activities (Sector X impact), optional\\n\\nwith possibility for label for others\\n\\n• Specifically targeting model fairness, robustness, and\\nauditability, mixing policies and controls, integrating\\nstrong ethical considerations on environmental and social\\nimpacts\\n\\n• Positive, nonsanctioned-based approach focusing on\\n\\npractical steps to implementation AI governance at an\\norganization level\\n\\n• Best practice center, supporting AI governance work at\\n\\nEconomic Forum level\\n\\n• Federal guidelines issued to prepare ground for industry-\\n\\nspecific guidelines or regulation\\n\\n• Focus on public trust and fairness; no broader ethics\\n\\nconsiderations\\n\\nHigh-level guidelines only; nonbinding and broad in coverage\\nDetailed guidelines issued, integrating ethical and a strong\\nfocus on end-consumer protection\\n\\n• Directive by end\\n2020/early 2021\\n• To be translated\\ninto national\\nregime\\n\\n• Regulation by\\nend 2020/early\\n2021\\n\\nEU\\n\\nGuidance,\\ncommunication,\\ndirection, and\\nregulation\\n\\nSingapore\\n\\nGuidance\\n\\nUS\\n\\nGuidance,\\ncommunication,\\nand regulation\\n\\nUK\\nAustralia\\n\\nGuidance\\nGuidance\\n\\nThe Emergence of Responsible AI\\nAs the adoption of data science, machine learning, and AI has accelerated worldwide,\\na  loose  consensus  among  AI  thinkers  has  emerged.  The  most  common  banner  for\\nthis  consensus  is  Responsible  AI:  the  idea  of  developing  machine  learning  systems\\nthat  are  accountable,  sustainable,  and  governable.  In  essence,  AI  systems  should  do\\nwhat they are supposed to, remain reliable over time, and be well controlled as well as\\nauditable. \\n\\nThere is no strict definition of Responsible AI or the terms used to frame it, but there\\nis agreement about the overarching considerations and largely about what is needed\\nto  deliver  it  (see  Table  8-2).  Despite  the  lack  of  any  single  body  driving  the  move‐\\nment, Responsible AI has already had a significant influence on collective thinking,\\nand especially on the EU’s trustworthy AI regulators.\\n\\n112 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n \\n \\n \\n \\n\\x0cTable 8-2. Components of Responsible AI, an increasingly critical part of MLOps\\n\\nIntentionality\\nMust have:\\n\\nAccountability\\nMust have:\\n\\n• Assurance that models are designed and behave in\\n\\n• Central control, management, and the ability to audit the\\n\\nways aligned with their purpose\\n\\nenterprise AI effort (no shadow IT!)\\n\\n• Assurance that data used for AI projects comes from\\ncompliant and unbiased sources plus a collaborative\\napproach to AI projects that ensures multiple checks\\nand balances on potential model bias\\n\\n• Intentionality also includes explainability, meaning\\nthe result of AI systems should be explainable by\\nhumans (ideally not just the humans that created the\\nsystem)\\n\\n• An overall view of which teams are using what data, how,\\n\\nand in which models\\n\\n• Trust that data is reliable and being collected in accordance\\nwith regulation as well as a centralized understanding of\\nwhich models are being used for which business process.\\nThis is closely tied to traceability—if something goes\\nwrong, is it easy to find where in the pipeline it happened?\\n\\nHuman-centered approach\\nProviding people with the tools and training to be aware of and then execute on both components  \\n\\nKey Elements of Responsible AI\\nResponsible  AI  is  about  the  responsibility  of  data  practitioners,  not  about  AI  itself\\nbeing responsible: this is a very important distinction.  Another important distinction\\nis that, according to Kurt Muemel of Dataiku, “It is not necessarily about intentional\\nharm, but accidental harm.”\\n\\nThis section presents five key elements that figure in Responsible AI thinking—data,\\nbias, inclusiveness, model management at scale, and governance—as well as MLOps\\nconsiderations for each element.\\n\\nElement 1: Data\\nThe dependence on data is a fundamental differentiator between ML and traditional\\nsoftware development. The quality of the data used will make the biggest impact on\\nthe accuracy of the model. Some real-world considerations are as follows:\\n\\n• Provenance is king. Understand how the data was collected and its journey to the\\n\\npoint of use.\\n\\n• Get the data off of desktops. Data must be manageable, securable, and traceable.\\n\\nPersonal data must be strictly managed.\\n\\n• The quality of data over time: consistency, completeness, and ownership.\\n\\n• Bias in, bias out. Biased input data can occur easily and unintentionally.\\n\\nKey Elements of Responsible AI \\n\\n| \\n\\n113\\n\\n \\n \\n\\x0cElement 2: Bias\\nML predictive modeling is about building a system to recognize and exploit tenden‐\\ncies in the real world. Certain types of cars, driven by certain types of people, in cer‐\\ntain places are more likely to be costlier to insurance companies than others. But is\\nmatching  a  pattern  always  considered  ethical?  When  is  such  pattern-matching  pro‐\\nportionate, and when is it an unfair bias?\\n\\nEstablishing what is fair is not clear-cut. Even using a churn model to give rebates to\\nthe customers who are more likely to leave might be considered as unfair against dor‐\\nmant customers who will pay more for the same product. Regulations are a place to\\nstart looking, but as already discussed, opinion is not universal and is not fixed. Even\\nwith a clear understanding of the fairness constraints to work toward, achieving them\\nis not simple. When the developers of the recruitment system that was biased against\\nwomen’s  schools  adapted  the  model  to  ignore  the  words  like  “women’s,”  they  found\\nthat even the tone of the language in a resume reflected the gender of the author and\\ncreated unwanted bias against women. Addressing these biases has deep implications\\non the ML model to be built (see “Impact of Responsible AI on Modeling” on page 53\\nfor a detailed example).\\n\\nTaking a step back, these bias problems are not new; for example, hiring discrimina‐\\ntion has always been an issue. What is new is that, thanks to the IT revolution, data to\\nassess biases is more available. On top of that, thanks to the automation of decision\\nmaking with machine learning, it is possible to change the behavior without having to\\ngo through the filter of individuals making subjective decisions.\\n\\nThe bottom line is that biases are not only statistical. Bias checks should be integrated\\nin governance frameworks so that issues are identified as early as possible, since they\\ndo have the potential to derail data science and machine learning projects.\\n\\nIt’s  not  all  bad  news:  there  are  many  potential  sources  of  statistical  bias  (i.e.,  of  the\\nworld as it was) that can be addressed by data scientists:\\n\\n• Is bias encoded into the training data? Is the raw material biased? Has data prepa‐\\n\\nration, sampling, or splitting introduced bias?\\n\\n• Is the problem framed properly?\\n\\n• Do we have the right target for all subpopulations? Beware that many variables\\n\\nmay be highly correlated.\\n\\n• Is feedback-loop data biased through factors such as the order in which choices\\n\\nare presented in the UI?\\n\\nIt  is  so  complex  to  prevent  the  problems  caused  by  bias  that  much  of  the  current\\nfocus  is  on  detecting  bias  before  it  causes  harm.  ML  interpretability  is  the  current\\n\\n114 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cmainstay  of  bias  detection,  bringing  understanding  to  ML  models  through  a  set  of\\ntechnical tools to analyze models including:\\n\\n• Prediction understanding: Why did a model make a specific prediction?\\n\\n• Subpopulation analysis: Is there bias among subpopulations?\\n\\n• Dependency understanding: What contributions are individual features making?\\n\\nA  very  different,  but  complementary,  approach  to  addressing  bias  is  to  leverage  as\\nbroad a range of human expertise as possible in the development process. This is one\\naspect of the idea of inclusiveness in Responsible AI.\\n\\nElement 3: Inclusiveness\\nThe human-in-the-loop (HITL) approach aims to combine the best of human intelli‐\\ngence with the best of machine intelligence. Machines are great at making smart deci‐\\nsions  from  vast  datasets,  whereas  people  are  much  better  at  making  decisions  with\\nless  information.  Human  judgment  is  particularly  effective  for  making  ethical  and\\nharm-related judgments.\\n\\nThis concept can be applied to the way models are used in production, but it can be\\nequally important in the way models are built. Formalizing human responsibility in\\nthe  MLOps  loop,  for  example  through  sign-off  processes,  can  be  simple  to  do,  but\\nhighly effective.\\n\\nThe  principle  of  inclusiveness  takes  the  idea  of  human-AI  collaboration  further:\\nbringing as diverse a set of human expertise to the ML life cycle as possible reduces\\nthe  risk  of  serious  blind  spots  and  omissions.  The  less  inclusive  the  group  building\\nthe ML, the greater the risk.\\n\\nThe perspectives of the business analyst, the subject matter expert, the data scientist,\\nthe data engineer, the risk manager, and the technical architect are all different. All of\\nthese perspectives together bring far greater clarity to managing model development\\nand deployment than relying on any single user profile, and enabling these user pro‐\\nfiles to collaborate effectively is a key factor in reducing risk and increasing the per‐\\nformance  of  MLOps  in  any  organization.  Refer  to  Chapter  2  for  clear  examples  of\\ncollaboration among different profiles for better MLOps performance.\\n\\nFull  inclusiveness  may  even  bring  the  consumer  into  the  process,  perhaps  through\\nfocus group testing. The objective of inclusiveness is to bring the appropriate human\\nexpertise  into  the  process,  regardless  of  source.  Leaving  ML  to  data  scientists  is  not\\nthe answer to managing risk.\\n\\nKey Elements of Responsible AI \\n\\n| \\n\\n115\\n\\n\\x0cElement 4: Model Management at Scale\\nManaging the risk associated with ML when there are a handful of models in produc‐\\ntion  can  afford  to  be  largely  manual.  But  as  the  volume  of  deployments  grows,  the\\nchallenges  multiply  rapidly.  Here  are  some  key  considerations  for  managing  ML  at\\nscale:\\n\\n• A scalable model life cycle needs to be largely automated as well as streamlined.\\n\\n• Errors, for example in a subset of a dataset, will propagate out rapidly and widely.\\n\\n• Existing software engineering techniques can assist ML at scale.\\n\\n• Decisions must be explainable, auditable, and traceable.\\n\\n• Reproducibility  is  key  to  understanding  what  went  wrong,  who  or  what  was\\n\\nresponsible, and who should ensure it is corrected.\\n\\n• Model  performance  will  degrade  over  time:  monitoring,  drift  management,\\n\\nretraining, and remodeling must be built into the process.\\n\\n• Technology  is  evolving  rapidly;  an  approach  to  integrating  new  technologies  is\\n\\nrequired.\\n\\nElement 5: Governance\\nResponsible AI sees strong governance as the key to achieving fairness and trustwor‐\\nthiness. The approach builds on traditional governance techniques:\\n\\n• Determine intentions at the beginning of the process\\n\\n• Formalize bringing humans in the loop\\n\\n• Clearly identify responsibilities (Figure 8-3)\\n\\n• Integrate goals that define and structure the process\\n\\n• Establish and communicate a process and rules\\n\\n• Define measurable metrics and monitor for deviation\\n\\n• Build multiple checks into the MLOps pipeline aligned with overall goals\\n\\n• Empower people through education\\n\\n• Teach builders as well as decision makers how to prevent harm\\n\\nGovernance  is,  therefore,  both  the  foundation  and  the  glue  of  MLOps  initiatives.\\nHowever,  it’s  important  to  recognize  that  it  goes  beyond  the  borders  of  traditional\\ndata governance.\\n\\n116 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cFigure 8-3. A representation of who is responsible at different levels of the organization\\nfor different parts of the Responsible AI process\\n\\nA Template for MLOps Governance\\nHaving  explored  the  key  themes  to  be  addressed  by  an  MLOps  governance,  both\\nthrough regulatory measures and the Responsible AI movement, it is time to map out\\nhow to implement a robust governance framework of MLOps.\\n\\nThere is no one-size-fits-all solution across businesses, and different use cases within\\na business justify different levels of management, but the step-by-step approach out‐\\nlined can be applied in any organization to guide the implementation process.\\n\\nThe process has eight steps:\\n\\n1. Understand and classify the analytics use cases.\\n\\n2. Establish an ethical position.\\n\\n3. Establish responsibilities.\\n\\n4. Determine governance policies.\\n\\n5. Integrate policies into the MLOps process.\\n\\n6. Select the tools for centralized governance management.\\n\\n7. Engage and educate.\\n\\n8. Monitor and refine.\\n\\nThis section will go through each of the steps in detail, including a simple definition\\nand the “how” of actually implementing the step.\\n\\nA Template for MLOps Governance \\n\\n| \\n\\n117\\n\\n\\x0cStep 1: Understand and Classify the Analytics Use Cases\\nThis step entails defining what the different classes of analytics use cases are and, sub‐\\nsequently, what the governance needs are for each.\\n\\nConsider the answers to the following questions for a representative cross-section of\\nanalytics use cases. Identify the key distinguishing features of the different use cases\\nand categorize these features. Conflate categories where appropriate. Typically, it will\\nbe necessary to associate several categories to each use case to fully describe it.\\n\\n• What  regulations  is  each  use  case  subject  to,  and  what  are  the  implications?\\n\\nSector-specific regulations, regional, PII?\\n\\n• Who  consumes  the  results  of  the  model?  The  public?  One  of  many  internal\\n\\nusers?\\n\\n• What  are  the  availability  requirements  for  the  deployed  model?  24/7  real-time\\n\\nscoring, scheduled batch scoring, ad-hoc runs (self-service analytics)?\\n\\n• What is the impact of any errors and deficiencies? Legal, financial, personal, pub‐\\n\\nlic trust?\\n\\n• What is the cadence and urgency of releases?\\n\\n• What is the lifetime of the model and the lifetime of the impact of its decision?\\n\\n• What is the likely rate of model quality decay?\\n\\n• What is the need for explainability and transparency?\\n\\nStep 2: Establish an Ethical Position\\nWe established that fairness and ethical considerations are important motivating fac‐\\ntors  for  effective  governance,  that  businesses  have  a  choice  on  their  ethical  stance,\\nand that this impacts public perception and trust. The position a business takes is a\\ntrade-off between the cost to implement the position and public perception. Respon‐\\nsible stances rarely come at zero short-term financial cost even if the long-term ROI\\nmay be positive.\\n\\nAny MLOps governance framework needs to reflect the ethical position of the com‐\\npany. While the position typically impacts what a model does and how it does it, the\\nMLOps governance process needs to ensure that deployed models match the chosen\\nethical stance. This stance is likely to influence the governance process more widely,\\nincluding the selection and verification of new models and the acceptable likelihood\\nof accidental harm.\\n\\n118 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cConsider the following ethical questions:\\n\\n• What aspects of well-being in society matter? E.g., equality, privacy, human rights\\n\\nand dignity, employment, democracy, bias\\n\\n• Is  the  potential  impact  on  human  psychology  to  be  considered?  E.g.,  human-\\n\\nhuman or human-AI relationships, deception, manipulation, exploitation\\n\\n• Is a stance on the financial impact required? E.g., market manipulation\\n\\n• How transparent should the decision making be?\\n\\n• What  level  of  accountability  for  AI-driven  mistakes  does  the  business  want  to\\n\\naccept?\\n\\nStep 3: Establish Responsibilities\\nIdentify the groups of people responsible for overseeing MLOps governance as well\\nas their roles.\\n\\n• Engage  the  whole  organization,  across  departments,  from  top  to  bottom  of  the\\n\\nmanagement hierarchy.\\n\\n• Peter  Drucker’s  famous  line  “Culture  eats  strategy  for  breakfast”  highlights  the\\n\\npower of broad engagement and shared beliefs.\\n\\n• Avoid  creating  all-new  governance  structures.  Look  at  what  structures  exist\\n\\nalready and try to incorporate MLOps governance into them.\\n\\n• Get senior management sponsorship for the governance process.\\n\\n• Think in terms of separate levels of responsibility:\\n\\n— Strategic: set out the vision\\n\\n— Tactical: implement and enforce the vision\\n\\n— Operational: execute on a daily basis\\n\\n• Consider  building  a  RACI  matrix  for  the  complete  MLOps  process  (see\\nFigure 8-4). RACI stands for responsible, accountable, consulted, informed, and it\\nhighlights the roles of different stakeholders in the overall MLOps process. It is\\nquite likely that any matrix you create at this stage will need to be refined later on\\nin the process.\\n\\nA Template for MLOps Governance \\n\\n| \\n\\n119\\n\\n\\x0cFigure 8-4. A typical RACI matrix for MLOps\\n\\nStep 4: Determine Governance Policies\\nWith an understanding of the scope and objectives for governance now established,\\nand the engagement of the responsible governance leaders, it is time to consider the\\ncore  policies  for  the  MLOps  process.  This  is  no  small  task,  and  it  is  unlikely  to  be\\nachieved in one iteration. Focus on establishing the broad areas of policy and accept\\nthat experience will help to evolve the details.\\n\\nConsider the classification of initiatives from Step 1. What governance measures do\\nthe team or organization need in each case?\\n\\nIn  initiatives  where  there  is  less  concern  about  the  risk  or  regulatory  compliance,\\nlighter-weight, cheaper measures may be appropriate. For example, “what if ” calcula‐\\ntions to determine the number of in-flight meals of different types has relatively little\\nimpact—after  all,  the  mix  was  never  right  even  before  the  introduction  of  machine\\nlearning. Even such a seemingly insignificant use case may have ethical implications\\nas meal choices are likely to be correlated to religion or gender, which are protected\\nattributes in many countries. On the other hand, the implications of calculations to\\ndetermine the level of fueling of planes carry substantially greater risk.\\n\\n120 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cGovernance considerations can be broadly grouped under the headings in Table 8-3.\\nFor each heading, there are a range of measures to consider for each class.\\n\\nTable 8-3. MLOps governance considerations\\n\\nGovernance\\nconsideration\\nReproducibility and\\ntraceability\\nAudit and\\ndocumentation\\nHuman-in-the-loop\\nsign-off\\nPreproduction\\nverification\\n\\nTransparency and\\nexplainability\\nBias and harm testing\\n\\nProduction deployment\\nmodes\\nProduction monitoring\\n\\nData quality and\\ncompliance\\n\\nExample measures\\n\\nFull VM and data snapshot for precise and rapid model re-instantiation, or ability to re-create the\\nenvironment and retrain with a data sample, or only record metrics of models deployed?\\nFull log of all changes during development including experiments run and reasons for choices made\\nor automated documentation of deployed model only or no documentation at all\\nMultiple sign-offs for every environment move (development, QA, preproduction, production)\\n\\nVerify model documentation by hand-coding the model and comparing results or full automated\\ntest pipeline re-creating in production-like environment with extensive unit and end-to-end test\\ncases or automated checks on database, software version, and naming standards only\\nUse manually-coded decision tree for maximum explainability or use regression algorithms’\\nexplainability tools such as Shapely values or accept opaque algorithms such as neural networks\\n“Red team” adversarial manual testing using multiple tools and attack vectors or automated bias\\nchecking on specific subpopulations\\nContainerized deployment to elastic scalable high-availability, multi-node configuration with\\nautomated stress/load testing prior to deployment or a single production server\\nReal-time alerting of errors, dynamic multi-armed bandit model balancing, automated nightly\\nretraining, model evaluation, and redeployment or weekly input drift monitoring and manual\\nretraining or basic infrastructure alerts, no monitoring, no feedback-based retraining\\nPII considerations including anonymization and documented and reviewed column-level lineage to\\nunderstand the source, quality, and appropriateness of the data and automated data quality checks\\nfor anomalies\\n\\nThe finalized governance policies should provide:\\n\\n• A process for determining the classification of any analytics initiative. This could\\n\\nbe implemented as a checklist or a risk assessment application.\\n\\n• A matrix of initiative classification against governance consideration, where each\\n\\ncell identifies the measures required.\\n\\nStep 5: Integrate Policies into the MLOps Process\\nOnce the governance policies for the different classes of initiatives have been identi‐\\nfied, measures to implement them need to be incorporated into the MLOps process\\nand responsibilities for actioning the measures assigned.\\n\\nWhile most businesses will have an existing MLOps process, it is quite likely that this\\nhas  not  been  defined  explicitly,  but  rather  has  evolved  in  response  to  individual\\nneeds.  Now  is  the  time  to  revisit,  enhance,  and  document  the  process.  Successful\\n\\nA Template for MLOps Governance \\n\\n| \\n\\n121\\n\\n\\x0cadoption  of  the  governance  process  can  only  happen  if  it  is  communicated  clearly\\nand buy-in is sought from each stakeholder group.\\n\\nUnderstand all of the steps in the existing process by interviewing those responsible.\\nWhere there is no existing formal process, this is often harder than it sounds because\\nthe process steps are often not explicitly defined, and ownership is unclear.\\n\\nAttempting to map the policy-driven governance measures into the understanding of\\nthe process will identify issues in the process very quickly. Within one business there\\nmay be a range of different styles of project and governance needs, such as:\\n\\n• One-off self-service analytics\\n\\n• Internally consumed models\\n\\n• Models embedded in public websites\\n\\n• Models deployed to Internet of Things devices\\n\\nIn  these  cases,  the  differences  between  some  processes  may  be  so  great  it  is  best  to\\nthink in terms of several parallel processes. Ultimately, every governance measure for\\neach  use  case  should  be  associated  with  a  process  step  and  with  a  team  that  is  ulti‐\\nmately responsible, as presented here:\\n\\nProcess step\\nBusiness scoping\\nIdeation\\n\\nDevelopment\\n\\nPreproduction\\n\\nDeployment\\n\\nExample activities and governance considerations\\nRecord objectives, define KPIs, and record sign-off: for internal governance considerations\\n\\nData discovery: data quality and regulatory compliance constraints\\nAlgorithm choice: impacted by explainability requirements\\n\\nData preparation: consider PII compliance, separation of legal regional scopes, avoid input bias\\nModel development: consider model reproducibility and auditabilityModel testing and verification:\\nbias and harm testing, explainability\\n\\nVerify performance/bias with production data\\nProduction-ready testing: verify scalability\\n\\nDeployment strategy: driven by the level of operationalization\\nDeployment verification testsUse of shadow challenger or A/B test techniques for in-production\\nverification\\n\\nMonitoring and\\nfeedback\\n\\nPerformance metrics and alerting\\nPrediction log analysis for input drift with alerting\\n\\nStep 6: Select the Tools for Centralized Governance Management\\nThe MLOps governance process impacts both the complete ML life cycle and many\\nteams across the organization. Each step requires a specific sequence of actions and\\nchecks to be executed. Traceability of both the development of the model and the exe‐\\ncution of governance activities is a complex challenge.\\n\\n122 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cMost organizations still have a “paper form” mindset for process management, where\\nforms  are  filled  in,  circulated,  signed,  and  filed.  The  forms  may  be  text  documents,\\ncirculated via email, and filed electronically, but the limitations of paper remain. It is\\nhard to track progress, associate artifacts, review many projects at once, prompt for\\naction,  and  remind  teams  of  responsibilities.  The  complete  record  of  events  is  typi‐\\ncally spread across multiple systems and owned by individual teams, making a simple\\noverview of analytics projects effectively impossible.\\n\\nWhile teams will always have tools specific to their roles, MLOps governance is much\\nmore  effective  if  the  overarching  process  is  managed  and  tracked  from  one  system.\\nThis system should:\\n\\n• Centralize the definition of the governance process flows for each class of analyt‐\\n\\nics use cases\\n\\n• Enable tracking and enforcement of the complete governance process\\n\\n• Provide a single point of reference for the discovery of analytics projects\\n\\n• Enable collaboration between teams, in particular, the transfer of work between\\n\\nteams\\n\\n• Integrate with existing tools used for project execution\\n\\nThe current workflow, project management, and MLOps tools can only partially sup‐\\nport these objectives. A new category of ML governance tools is emerging to support\\nthis need directly and more fully. These new tools focus on the specific challenges of\\nML governance, including:\\n\\n• A single view of the status of all models (otherwise known as a model registry)\\n\\n• Process gates with a sign-off mechanism to allow ready traceability of the history\\n\\nof decision making\\n\\n• Ability to track all versions of a model\\n\\n• Ability to link to artifact stores, metrics snapshots, and documentation\\n\\n• Ability to tailor processes specifically for each class of analytics use cases\\n\\n• Ability to integrate health monitoring from production systems and to track the\\n\\nperformance of models against the original business KPIs\\n\\nStep 7: Engage and Educate\\nWithout a program of engagement and training for the groups involved in overseeing\\nand executing the governance process, the chances of it being even partially adopted\\nare slim. It is essential that the importance of MLOps governance to the business, and\\nthe  necessity  of  each  team’s  contribution,  is  communicated.  Building  on  this\\nunderstanding,  every  individual  needs  to  learn  what  they  must  do,  when,  and  how.\\n\\nA Template for MLOps Governance \\n\\n| \\n\\n123\\n\\n\\x0cThis  exercise  will  require  considerable  documentation,  training,  and,  most  of  all,\\ntime.\\n\\nStart  by  communicating  the  broad  vision  for  MLOps  governance  in  the  business.\\nHighlight the dangers of the status quo, outline a process, and detail how it is tailored\\nto the range of use cases.\\n\\nEngage directly with each team involved and build a training program directly with\\nthem. Do not be afraid to leverage their experience to shape not only the training, but\\nalso the detailed implementation of their governance responsibilities. The result will\\nbe much stronger buy-in and more effective governance.\\n\\nStep 8: Monitor and Refine\\nIs the newly implemented governance working? Are the prescribed steps being imple‐\\nmented, and are the objectives being met? What actions should be taken if things are\\ngoing  poorly?  How  do  we  measure  the  gap  between  today’s  reality  and  where  the\\nbusiness needs to be?\\n\\nMeasuring success requires metrics and checks. It requires people to be tasked with\\nmonitoring and a way to address problems. The governance process and the way it is\\nimplemented will need to be refined over time, based both on lessons learned and on\\nevolving requirements (including, as discussed earlier in this chapter, evolving regula‐\\ntory requirements).\\n\\nA  big  factor  in  the  success  of  the  process  will  be  the  diligence  of  the  individuals\\nresponsible for the individual measures in the process, and incentivizing them is key.\\n\\nMonitoring the governance process starts with a clear understanding of the key per‐\\nformance  metrics  and  targets—KPIs  for  governance.  These  should  aim  to  measure\\nboth whether the process is being enacted and whether objectives are being achieved.\\nMonitoring and auditing can be time consuming, so look to automate metrics as far\\nas  possible  and  encourage  individual  teams  to  own  the  monitoring  of  metrics  that\\nrelate to their area of responsibility.\\n\\nIt’s  hard  to  make  people  carry  out  tasks  that  seemingly  deliver  nothing  concrete  to\\nthose doing the work. One popular tactic to address this is gamification. This is not\\nabout making everything look like a video game, but about introducing incentives for\\npeople to carry out tasks where the main benefit is derived by others.\\n\\nLook to gamify the governance process in simple ways: publishing KPI results widely\\nis  the  simplest  place  to  start.  Just  being  able  to  see  targets  being  met  is  a  source  of\\nsatisfaction  and  motivation.  Leaderboards,  whether  at  the  team  or  individual  level,\\ncan add some constructive element of competition. For example, people whose work\\nconsistently  passes  compliance  checks  the  first  time,  or  meets  deadlines,  should  be\\nable to feel their efforts are visible.\\n\\n124 \\n\\n| \\n\\nChapter 8: Model Governance\\n\\n\\x0cHowever, excessive competition can be disruptive and demotivating. A balance must\\nbe struck, and this is best achieved by building up gamification elements slowly over\\ntime.  Start  with  the  least  competition-oriented  and  add  new  elements  one  by  one,\\nmeasuring their effectiveness before adding the next.\\n\\nMonitoring  changes  in  the  governance  landscape  is  essential.  This  might  be  regula‐\\ntory, or it might be about public opinion. Those with responsibility for the strategic\\nvision  must  continue  to  monitor  it  as  well  as  have  a  process  to  evaluate  potential\\nchanges.\\n\\nFinally,  all  monitoring  of  the  process  is  only  worthwhile  if  issues  are  acted  upon.\\nEstablish  a  process  for  agreeing  on  change  and  for  enacting  it.  This  may  result  in\\nrevisiting  policies,  processes,  tools,  responsibilities,  education,  and  monitoring!  It’s\\nnecessary to iterate and refine, but the balance between efficiency and effectiveness is\\nhard to find; many lessons can only be learned the hard way. Build a culture where\\npeople see iteration and refinement as a measure of a successful process, not a failed\\none.\\n\\nClosing Thoughts\\nIt’s hard to separate MLOps from its governance. It’s not possible to successfully man‐\\nage the model life cycle, mitigate the risks, and deliver value at scale without gover‐\\nnance. Governance impacts everything from how the business can acceptably exploit\\nML,  to  the  data  and  algorithms  that  can  be  used,  to  the  style  of  operationalization,\\nmonitoring, and retraining.\\n\\nMLOps at scale is in its infancy. Few businesses are doing it, and even fewer are doing\\nit well. While governance is the key to improving the effectiveness of MLOps, there\\nare  few  tools  today  that  directly  address  this  challenge,  and  there  is  only  piecemeal\\nadvice.\\n\\nPublic trust in ML is at risk. Even slow-moving organizations like the EU understand\\nthis. If trust is lost, then so too will be many of the benefits to be derived from ML.\\nAdditional  legislation  is  being  prepared,  but  even  without  this,  businesses  need  to\\nworry  about  the  potential  damage  to  their  public  image  that  can  be  caused  by  an\\ninadvertently harmful model.\\n\\nWhen planning to scale MLOps, start with governance and use it to drive the process.\\nDon’t bolt it on at the end. Think through the policies; think about using tooling to\\ngive a centralized view; engage across the organization. It will take time and iteration,\\nbut  ultimately  the  business  will  be  able  to  look  back  and  be  proud  that  it  took  its\\nresponsibilities seriously.\\n\\nClosing Thoughts \\n\\n| \\n\\n125\\n\\n\\x0c\\x0cPART III\\nMLOps: Real-World Examples\\n\\n\\x0c\\x0cCHAPTER 9\\nMLOps in Practice: Consumer\\nCredit Risk Management\\n\\nIn  the  final  chapters  of  this  book,  we  explore  three  examples  of  how  MLOps  might\\nlook in practice. We explicitly chose these three examples because they represent fun‐\\ndamentally different use cases for machine learning and illustrate how MLOps meth‐\\nodology  might  differ  to  suit  the  needs  of  the  business  and  its  ML  model  life  cycle\\npractices.\\n\\nBackground: The Business Use Case\\nWhen  a  consumer  asks  for  a  loan,  the  credit  institution  has  to  make  a  decision  on\\nwhether or not to grant it. Depending on the case, the amount of automation in the\\nprocess  may  vary.  However,  it  is  very  likely  that  the  decision  will  be  informed  by\\nscores that estimate the probability that the loan will or will not be repaid as expected.\\n\\nScores are routinely used at different stages of the process:\\n\\n• At the prescreen stage, a score computed with a small number of features allows\\n\\nthe institution to quickly discard some applications.\\n\\n• At  the  underwriting  stage,  a  score  computed  with  all  the  required  information\\n\\ngives a more precise basis for the decision.\\n\\n• After the underwriting stage, scores can be used to assess the risk associated with\\n\\nloans in the portfolio.\\n\\nAnalytics  methods  have  been  used  for  decades  to  compute  these  probabilities.  For\\nexample,  the  FICO  score  has  been  used  since  1995  in  the  United  States.  Given  the\\ndirect  impact  they  have  on  the  institutions’  revenues  and  on  customers’  lives,  these\\npredictive  models  have  always  been  under  great  scrutiny.  Consequently,  processes,\\n\\n129\\n\\n\\x0cmethods,  and  skills  have  been  formalized  into  a  highly  regulated  environment  to\\nensure the sustainable performance of models.\\n\\nWhether the models are based on expert-made rules, on classical statistical models,\\nor on more recent machine learning algorithms, they all have to comply with similar\\nregulations. Consumer credit risk management can therefore be seen as a precursor\\nof  MLOps:  parallels  with  other  use  cases  as  well  as  best  practices  can  be  analyzed\\nbased on this use case.\\n\\nAt the time a credit decision is made, information about the customer’s historical and\\ncurrent situation is usually available. How much credit does the customer hold? Has\\nthe customer ever not repaid a loan (in credit jargon, is the customer a delinquent)?\\nIn  some  countries,  organizations  called  credit  bureaus  collect  this  information  and\\nmake it available to creditors either directly or through the form of a score (like the\\naforementioned FICO score).\\n\\nThe definition of the target to be predicted is more complex. A customer not repay‐\\ning as expected is a “bad” outcome in credit risk modeling. In theory, one should wait\\nfor the complete repayment to determine a “good” outcome and for the loss charge\\noff to determine a “bad” outcome. However, it may take a long time to obtain these\\nultimate figures, and waiting for them would deter reactivity to changing conditions.\\nAs a result, trade-offs are usually made, based on various indicators, to declare “bad”\\noutcomes before the losses are certain.\\n\\nModel Development\\nHistorically,  credit  risk  modeling  is  based  on  a  mix  of  rules  (“manual  feature  engi‐\\nneering” in modern ML jargon) and logistic regression. Expert knowledge is vital to\\ncreating a good model. Building adapted customer segmentation as well as studying\\nthe  influence  of  each  variable  and  the  interactions  between  variables  requires  enor‐\\nmous  time  and  effort.  Combined  with  advanced  techniques  like  two-stage  models\\nwith offset, advanced general linear models based on Tweedie distribution, or monot‐\\nonicity  constraints  on  one  side  and  financial  risk  management  techniques  on  the\\nother side, this makes the field a playground for actuaries.\\n\\nGradient boosting algorithms like XGBoost have reduced the cost to build good mod‐\\nels. However, their validation is made more complex by the black box effect: it’s hard\\nto get the feeling that such models give sensible results whatever the inputs. Never‐\\ntheless, credit risk modelers have learned to use and validate these new types of mod‐\\nels.  They  have  developed  new  validation  methodologies  based,  for  example,  on\\nindividual explanations (e.g., Shapley values) to build trust into their models, which is\\na critical component of MLOps, as we’ve explored throughout this book.\\n\\n130 \\n\\n| \\n\\nChapter 9: MLOps in Practice: Consumer Credit Risk Management\\n\\n\\x0cModel Bias Considerations\\nThe modeler also has to take into account selection biases, as the model will inevita‐\\nbly be used to reject applicants. As a result, the population to which a loan is granted\\nis not representative of the applicant population.\\n\\nBy training a model version on the population selected by the previous model version\\nwithout care, the data scientist would make a model unable to accurately predict on\\nthe rejected population because it is not represented in the training dataset, while it is\\nexactly  what  is  expected  from  the  model.  This  effect  is  called  cherry-picking.  As  a\\nresult,  special  methods,  like  reweighting  based  on  the  applicant  population  or  cali‐\\nbrating the model based on external data, have to be used.\\n\\nModels that are used for risk assessment and not only to make decisions about grant‐\\ning  loans  have  to  produce  probabilities  and  not  only  yes/no  outcomes.  Usually,  the\\nprobability produced directly by prediction models is not accurate. While it is not an\\nissue  if  data  scientists  apply  thresholding  to  obtain  a  binary  classification,  they  will\\nusually  need  a  monotonous  transformation  called  a  calibration  to  recover  “true”\\nprobabilities as evaluated on historical data.\\n\\nThe model validation for this use case typically consists of:\\n\\n• Testing  its  performance  on  out-of-sample  datasets,  chosen  after  (or,  in  some\\n\\ncases, before, as well) the training datasets.\\n\\n• Investigating the performance not only overall, but also per subpopulation. The\\nsubpopulations  would  typically  have  customer  segments  based  on  revenue,  and\\nwith  the  rise  of  Responsible  AI,  other  segmenting  variables  like  gender  or  any\\nprotected attribute according to local regulation. Risks of not doing so can result\\nin serious damages, as Apple learned the hard way in 2019 when its credit card\\nwas said to be “sexist” against women applying for credit.\\n\\nPrepare for Production\\nGiven  the  significant  impact  of  credit  risk  models,  their  validation  process  involves\\nsignificant work with regard to the modeling part of the life cycle, and it includes the\\nfull documentation of:\\n\\n• The data used\\n\\n• The model and the hypothesis made to build it\\n\\n• The validation methodology and the validation results\\n\\n• The monitoring methodology\\n\\nModel Bias Considerations \\n\\n| \\n\\n131\\n\\n\\x0cThe monitoring methodology in this scenario is twofold: data and performance drift.\\nAs the delay between the prediction and obtaining the ground truth is long (typically\\nthe duration of the loan plus a few months to take into account late payments), it is\\nnot enough to monitor the model performance: data drift also has to be monitored\\ncarefully.\\n\\nFor  example,  should  an  economic  recession  occur  or  should  the  commercial  policy\\nchange, it is likely that the applicant population would change in such a way that the\\nmodel’s performance could not be guaranteed without further validation. Data drift is \\nusually performed by customer segment with generic statistical metrics that measure\\ndistances  between  probability  distributions  (like  Kolmogorov-Smirnov  or  Wasser‐\\nstein distances) and also with metrics that are specific to financial services, like popu‐\\nlation  stability  index  and  characteristic  stability  index.  Performance  drift  is  also\\nregularly assessed on subpopulations with generic metrics (AUC) or specific metrics\\n(Kolmogorov-Smirnov, Gini).\\n\\nThe model documentation is usually reviewed by an MRM team in a very formal and\\nstandalone process. Such an independent review is a good practice to make sure that\\nthe right questions are asked of the model development team. In some critical cases,\\nthe validation team may rebuild the model from scratch given the documentation. In\\nsome  cases,  the  second  implementation  is  made  using  an  alternative  technology  to\\nestablish  confidence  in  documented  understanding  of  the  model  and  to  highlight\\nunseen bugs deriving from the original toolset.\\n\\nComplex and time-consuming model validation processes have an implication on the\\nentire MLOps life cycle. Quick-fixes and rapid model iteration are not possible with\\nsuch lengthy QA and lead to a very slow and deliberate MLOps life cycle.\\n\\nDeploy to Production\\nIn a typical large financial services organization, the production environment is not\\nonly separate from the design environment, but also likely to be based on a different\\ntechnical  stack.  The  technical  stack  for  critical  operations—like  transaction  valida‐\\ntion, but also potentially loan validation—will always evolve slowly.\\n\\nHistorically,  the  production  environments  have  mainly  supported  rules  and  linear\\nmodels  like  logistic  regression.  Some  can  handle  more  complex  models  such  as\\nPMML  or  JAR  file.  For  less  critical  use  cases,  Docker  deployment  or  deployment\\nthrough integrated data science and machine learning platforms may be possible. As\\na result, the operationalization of the model may involve operations that range from\\nclicking on a button to writing a formula based on a Microsoft Word document.\\n\\nActivity  logging  of  the  deployed  model  is  essential  for  monitoring  model  perfor‐\\nmance in such a high-value use case. Depending on the frequency of the monitoring,\\nthe  feedback  loop  may  be  automated  or  not.  For  example,  automation  may  not  be\\n\\n132 \\n\\n| \\n\\nChapter 9: MLOps in Practice: Consumer Credit Risk Management\\n\\n\\x0cnecessary if the task is performed only once or twice a year and the largest amount of\\ntime is spent asking questions of the data. On the other hand, automation might be\\nessential if the assessment is done weekly, which may be the case for short-term loans\\nwith durations of a few months.\\n\\nClosing Thoughts\\nFinancial services have been developing schemes for prediction model validation and\\nmonitoring for decades. They have been able to continuously adapt to new modeling\\ntechnologies like gradient boosting methods. Given their important impact, the pro‐\\ncesses around the life cycle management of these models are well formalized and even\\nincorporated into many regulations. As a result, they can be a source of best practices\\nfor MLOps in other domains, though adaptations are needed as the trade-off between\\nrobustness  on  one  side  and  cost  efficiency,  time  to  value,  and—importantly—team\\nfrustration on the other may be different in other businesses.\\n\\nClosing Thoughts \\n\\n| \\n\\n133\\n\\n\\x0c\\x0cCHAPTER 10\\nMLOps in Practice: Marketing\\nRecommendation Engines\\n\\nMakoto Miyazaki\\n\\nRecommendation  engines  have  become  very  popular  in  the  last  20  years,  from  the\\nfirst  Amazon  book  recommendations  to  today’s  generalized  use  in  digital  shops,\\nadvertisements, and music and video streaming. We have all become accustomed to\\nthem. However, throughout the years, the underlying technologies behind these rec‐\\nommendation engines have evolved.\\n\\nThis  chapter  covers  a  use  case  that  illustrates  the  adaption  of  and  need  for  MLOps\\nstrategies  given  the  particularities  of  a  fast-paced  and  rapidly  changing  machine\\nlearning model life cycle.\\n\\nThe Rise of Recommendation Engines\\nHistorically,  marketing  recommendations  were  human-built.  Based  on  qualitative\\nand quantitative marketing studies, marketing moguls would set up rules that stati‐\\ncally  defined  the  impression  (in  the  sense  of  advertising  views)  sent  to  a  customer\\nwith  given  characteristics.  This  technique  gave  rise  to  the  marketing  data  mining\\nurban legend that a grocery chain discovered that men who bought diapers on Thurs‐\\ndays  and  Saturdays  were  more  likely  to  buy  beer  as  well  and  hence  placing  the  two\\nnext to each other will increase beer sales.\\n\\nOverall, recommendation engines created manually presented numerous bottlenecks\\nthat  resulted  in  a  significant  amount  of  wasted  money:  it  was  hard  to  build  rules\\nbased  on  many  different  customer  features  because  the  rule  creation  process  was\\nmanual, it was hard to set up experiments to test many different kinds of impressions,\\nand it was hard to update the rules when the behavior of the customers changed.\\n\\n135\\n\\n\\x0cThe Role of Machine Learning\\nThe  rise  of  ML  has  brought  a  new  paradigm  to  recommendation  engines,  allowing\\nfor the elimination of rules based on human insight. A new class of algorithm called\\ncollaborative filtering dominates the field. This algorithm is able to analyze customer\\nand purchase data with millions of customers and tens of thousands of products to\\nperform  recommendations  without  any  prior  marketing  knowledge.  By  identifying\\nefficiently what customers that look like the current customer bought, marketers can\\nrely  on  automatic  strategies  that  outperform  traditional  ones  both  in  cost  and\\nefficiency.\\n\\nBecause the process of building strategies is automatic, it is possible to update them\\nregularly and to compare them using A/B testing or shadow scoring schemes (includ‐\\ning  the  way  to  choose  the  impression  among  all  possibilities).  Note  that  these  algo‐\\nrithms may be combined with more classical business rules for various reasons—e.g.,\\navoiding  the  filtering  bubble,  not  selling  a  product  in  a  given  geographical  area,  or\\npreventing the use of a specific association that is statistically meaningful but unethi‐\\ncal to use (like proposing alcohol to a recovering alcoholic), to name a few.\\n\\nPush or Pull?\\nWhen implementing a recommendation engine, it is important to keep in mind that\\nits  structure  will  depend  on  whether  the  recommendations  are  pushed  or  pulled. \\nPush  channels  are  the  easiest  to  handle;  for  example,  they  can  consist  of  sending\\nemails or making outbound calls. \\n\\nThe recommendation engine can be run on a regular basis in batch mode (typically\\nbetween  once  a  day  and  once  a  month),  and  it  is  easy  to  split  the  customer  dataset\\ninto several parts to perform analysis within a sound experimental design. The regu‐\\nlarity of the process allows for regular review and optimization of the strategy.\\n\\nPull channels are often more effective because they provide information to customers\\nwhen they need it—for example, when doing an online search or when calling a cus‐\\ntomer service line. Specific information from the session can be used (e.g., what the\\nuser has searched for) to precisely tailor the recommendation. Music streaming plat‐\\nforms, for instance, provide pull-channel recommendations for playlists.\\n\\nRecommendations  can  be  prebaked,  as  illustrated  in  the  in-depth  example  in  this\\nchapter, or made in real time. In the latter case, a special architecture has to be set up\\nto compute recommendations on the fly.\\n\\nComparing strategies in a pull context is more challenging. First, the customers who\\ncall  in  on  a  given  channel  are  likely  to  differ  from  the  average  customer.  In  simple\\ncases, it is possible to randomly choose the strategy to use for each recommendation,\\nbut it also happens that the strategy needs to be used consistently over a given period\\nfor  a  given  customer.  This  usually  results  in  an  unbalanced  proportion  of\\n\\n136 \\n\\n| \\n\\nChapter 10: MLOps in Practice: Marketing Recommendation Engines\\n\\n\\x0crecommendations made with each strategy, which makes the statistical treatment to\\ndecide which one is the best more complex. However, once a good framework is set,\\nthis allows a very quick improvement cycle, as many strategies can be tested in real\\ntime.\\n\\nData Preparation\\nThe customer data that is usually accessible to a recommendation engine is composed\\nof the following:\\n\\n• Structural information about the customer (e.g., age, gender, location)\\n\\n• Information about historical activities (e.g., past views, purchases, searches)\\n\\n• Current context (e.g., current search, viewed product)\\n\\nWhatever  the  technique  used,  all  customer  information  has  to  be  aggregated  into  a\\nvector (a list of fixed size) of characteristics. For example, from the historical activi‐\\nties, the following characteristics could be extracted:\\n\\n• Amount of purchases during the last week or the last month\\n\\n• Number of views during past periods\\n\\n• Increase in spending or in views during the last month\\n\\n• Previously seen impressions and customer’s response\\n\\nIn  addition  to  customer  data,  the  recommendation  context  can  also  be  taken  into\\naccount.  For  example,  days  to  summer  for  seasonal  products  like  above-ground\\nswimming pools or days to monthly pay day, as some customers delay purchases for\\ncash flow reasons.\\n\\nOnce the customer and context data is formatted, it is important to define the set of\\npossible recommendations, and there are many choices to make. The same product\\nmay  be  presented  with  different  offers,  which  may  be  communicated  in  different\\nways.\\n\\nIt is of the utmost importance not to forget the “do not recommend anything” option.\\nIndeed, most of us have the personal experience that not all recommendations have a\\npositive  impact.  Sometimes  not  showing  anything  might  be  better  than  the  alterna‐\\ntives. It’s also important to consider that some customers may not be entitled to see\\ncertain recommendations, for example depending on their geographical origin.\\n\\nData Preparation \\n\\n| \\n\\n137\\n\\n\\x0cDesign and Manage Experiments\\nTo leverage the continuous improvement potential of recommendation engines, it is\\nnecessary  to  experiment  with  different  strategies  within  a  sound  framework.  When\\ndesigning a prediction model for a recommendation engine, the data scientist might\\nwell  focus  on  a  simple  strategy,  such  as  predicting  the  probability  that  a  given  cus‐\\ntomer clicks on a given recommendation.\\n\\nThis may seem a reasonable compromise compared to the more precise approach of\\ntrying to gather information about whether the customer purchased the product and\\nwhether to attribute this purchase to a given recommendation. However, this is not\\nadequate from a business perspective, as phenomena like cannibalization may occur\\n(i.e., by showing a low-margin product to a customer, one might prevent them from\\nbuying  a  high-margin  product).  As  a  result,  even  if  the  predictions  were  good  and\\nresulted in increased sales volume, the overall revenue might be reduced.\\n\\nOn the other hand, bluntly promoting the organization’s interest and not the custom‐\\ner’s could also have detrimental long-term consequences. The overarching KPI that is\\nused  to  assess  if  a  given  strategy  yields  better  results  should  be  carefully  chosen,\\ntogether with the time period over which it is evaluated. Choosing the revenue over a\\ntwo-week period after the recommendation as the main KPI is common practice.\\n\\nTo be as close as possible to an experimental setting, also called A/B testing, the con‐\\ntrol  group  and  the  experimental  groups  have  to  be  carefully  chosen.  Ideally,  the\\ngroups are defined before the start of the experiment by randomly splitting the cus‐\\ntomer base. If possible, customers should not have been involved in another experi‐\\nmentation recently so that their historical data is not polluted by its impact. However,\\nthis may not be possible in a pull setting in which many new customers are coming\\nin. In this case, the assignment could be decided on the fly. The size of the groups as\\nwell as the duration of the experiments depend on the expected magnitude of the KPI\\ndifference  between  the  two  groups:  the  larger  the  expected  effect,  the  smaller  the\\ngroup size and the shorter the duration.\\n\\nThe experimentation could also be done in two steps: in the first one, two groups of\\nequal but limited size could be selected. If the experimentation is positive, the deploy‐\\nment could start with 10% on the new strategy, a proportion that can be raised pro‐\\ngressively if results are in line with expectations.\\n\\nModel Training and Deployment\\nTo better illustrate the MLOps process for this type of use case, the following sections\\nfocus  on  the  specific  example  of  a  hypothetical  company  deploying  an  automated\\npipeline to train and deploy recommendation engines. The company is a global soft‐\\nware company (let’s call it MarketCloud) headquartered in Silicon Valley.\\n\\n138 \\n\\n| \\n\\nChapter 10: MLOps in Practice: Marketing Recommendation Engines\\n\\n\\x0cOne  of  MarketCloud’s  products  is  a  software-as-a-service  (SaaS)  platform  called\\nSalesCore. SalesCore is a B2B product that allows its users (businesses) to drive sales\\nto  customers  in  a  simple  manner  by  keeping  track  of  deals,  clearing  tedious\\nadministration tasks off their desks, and creating customized product offers for each\\ncustomer (see Figure 10-1).\\n\\nFrom time to time, MarketCloud’s clients use the cloud-based SalesCore while on a\\ncall with their customers, adjusting their sales strategy by looking at past interactions\\nwith  the  customers  as  well  as  at  the  product  offers  and  discounts  suggested  by\\nSalesCore.\\n\\nMarketCloud is a mid-sized company with an annual revenue of around $200 million\\nand a few thousand employees. From salespeople at a brewing company to those in a\\ntelecommunication entity, MarketCloud’s clients represent a range of businesses.\\n\\nFigure 10-1. Mock-up of the SalesCore platform, the basis of the theoretical company on\\nwhich this section’s example is based\\n\\nMarketCloud would like to automatically display product suggestions on SalesCore to\\nthe salespeople trying to sell products to the customers. Suggestions would be made\\nbased on customers’ information and their past interaction records with the salesper‐\\nson;  suggestions  would  therefore  be  customized  for  each  customer.  In  other  words,\\nSalesCore  is  based  on  a  recommendation  engine  used  in  a  pull  (inbound  calls)  or\\npush (outbound calls) context. Salespeople would be able to incorporate in their sales\\nstrategy the suggested products while on a call with their customers.\\n\\nTo implement this idea, MarketCloud needs to build a recommendation engine and\\nembed  it  into  SalesCore’s  platform,  which,  from  a  model  training  and  deployment\\nstandpoint, presents several challenges. We’ll present these challenges in this section,\\n\\nModel Training and Deployment \\n\\n| \\n\\n139\\n\\n\\x0cand in the next section we’ll show MLOps strategies that allow the company to handle\\neach of them.\\n\\nScalability and Customizability\\nMarketCloud’s business model (selling software for client companies to help them sell\\ntheir  own  products)  presents  an  interesting  situation.  Each  client  company  has  its\\nown  dataset,  mainly  about  its  products  and  customers,  and  it  doesn’t  wish  to  share\\nthe data with other companies.\\n\\nIf MarketCloud has around four thousand clients using SalesCore, that means instead\\nof having a universal recommender system for all the clients, it would need to create\\nfour thousand different systems. MarketCloud needs to come up with a way to build\\nfour  thousand  recommendation  systems  as  efficiently  as  possible  since  there  is  no\\nway it can handcraft that many systems one by one.\\n\\nMonitoring and Retraining Strategy\\nEach  of  the  four  thousand  recommendation  engines  would  be  trained  on  the  cus‐\\ntomer data of the corresponding client. Therefore, each of them would be a different\\nmodel,  yielding  a  different  performance  result  and  making  it  nearly  impossible  for\\nthe company to manually keep an eye on all four thousand. For example, the recom‐\\nmendation engine for client A in the beverage industry might consistently give good\\nproduct  suggestions,  while  the  engine  for  client  B  in  the  telecommunication  sector\\nmight seldom provide good suggestions. MarketCloud needed to come up with a way\\nto automate the monitoring and the subsequent model retraining strategy in case the\\nperformance degraded.\\n\\nReal-Time Scoring\\nIn  many  situations,  MarketCloud’s  clients  use  SalesCore  when  they  are  talking  to\\ntheir customers on the phone. The sales negotiation evolves every single minute dur‐\\ning  the  call,  and  the  salesperson  needs  to  adjust  the  strategy  during  the  interaction\\nwith the customer, so the recommendation engine has to be responsive to real-time\\nrequests.\\n\\nFor  example,  imagine  you  as  a  salesperson  are  on  a  call  with  your  customer  to  sell\\ntelecommunication  devices.  The  customer  tells  you  what  his  office  looks  like,  the\\nexisting infrastructure at the office such as an optic fiber, the type of WiFi network,\\nand so forth. Upon entering this information in SalesCore, you want the platform to\\ngive  you  a  suggestion  for  the  products  that  your  customer  could  feasibly  purchase.\\nThis response from the platform needs to be in real time, not 10 minutes later, after\\nthe call, or on the following day.\\n\\n140 \\n\\n| \\n\\nChapter 10: MLOps in Practice: Marketing Recommendation Engines\\n\\n\\x0cAbility to Turn Recommendations On and Off\\nResponsible AI principles acknowledge that retaining human involvement is impor‐\\ntant. This can be done through a human-in-command design,1 by which it should be\\npossible  not  to  use  the  AI.  In  addition,  adoption  is  likely  to  be  low  if  users  cannot\\noverride  AI  recommendations.  Some  clients  value  using  their  own  intuition  about\\nwhich  products  to  recommend  to  their  customers.  For  this  reason,  MarketCloud\\nwants to give its clients full control to turn the recommendation engine on and off so\\nthat the clients can use the recommendations when they want.\\n\\nPipeline Structure and Deployment Strategy\\nTo efficiently build four thousand recommendation engines, MarketCloud decided to\\nmake one data pipeline as a prototype and duplicate it four thousand times. This pro‐\\ntotype pipeline consists of the necessary data preprocessing steps and a single recom‐\\nmendation  engine,  built  on  an  example  dataset.  The  algorithms  used  in  the\\nrecommendation engines will be the same across all four thousand pipelines, but they\\nwill be trained with the specific data associated with each client (see Figure 10-2).\\n\\nFigure 10-2. Image of data pipeline structure for MarketCloud’s recommendation engine\\nproject\\n\\nIn this way, MarketCloud can efficiently launch four thousand recommendation sys‐\\ntems. The users will still retain some room for customization, because the engine is\\ntrained with their own data, and each algorithm will work with different parameters\\n—i.e., it’s adopted to the customer and product information of each client.\\n\\nWhat makes it possible to scale up a single pipeline to four thousand pipelines is the\\nuniversal schema of the dataset. If a dataset from client A has 100 columns whereas\\n\\n1 For an explanation of human-in-command design, see Karen Yeung, “Responsibility and AI” (Council of\\n\\nEurope study, DGI(2019)05), 64, footnote 229.\\n\\nPipeline Structure and Deployment Strategy \\n\\n| \\n\\n141\\n\\n\\x0cclient  B  has  50,  or  if  the  column  “number  of  purchased  items”  from  client  A  is  an\\ninteger  whereas  the  same  column  from  client  B  is  a  string,  they  would  need  to  go\\nthrough different preprocessing pipelines.\\n\\nAlthough each client has different customer and product data, at the point that this\\ndata is registered on SalesCore, it acquires the same number of columns and the same\\ndata types for each column. This makes things easier, as MarketCloud simply needs\\nto copy a single pipeline four thousand times.\\n\\nEach recommendation system embedded in the four thousand pipelines will have dif‐\\nferent API endpoints. On the surface, it looks like when a user clicks the “show prod‐\\nuct recommendations” button, SalesCore displays a list of suggested products. But in\\nthe background, what is happening is that by clicking the button, the user is hitting\\nthe  specific  API  endpoint  associated  with  the  ranked  product  lists  for  the  specific\\ncustomer.\\n\\nMonitoring and Feedback\\nMaintaining  four  thousand  recommendation  systems  is  not  an  easy  task,  and  while\\nthere  have  already  been  many  MLOps  considerations  until  this  point,  this  is  maybe\\nthe most complex part. Each system’s performance needs to be monitored and upda‐\\nted as needed. To implement this monitoring strategy at a large scale, MarketCloud\\ncan automate the scenario for retraining and updating the models.\\n\\nRetraining Models\\nClients obtain  new customers, some of the customers churn, every once in a while\\nnew  products  are  added  to  or  dropped  from  their  catalogs;  the  bottom  line  is  that\\ncustomer  and  product  data  are  constantly  changing,  and  recommendation  systems\\nhave  to  reflect  the  latest  data.  It’s  the  only  way  they  can  maintain  the  quality  of  the\\nrecommendation, and, more importantly, avoid a situation such as recommending a\\nWiFi router that is outdated and no longer supported.\\n\\nTo reflect the latest data, the team could program a scenario to automatically update\\nthe database with the newest customer and product data, retraining the model with\\nthe  latest  datasets  every  day  at  midnight.  This  automation  scenario  could  then  be\\nimplemented in all four thousand data pipelines.\\n\\nThe  retraining  frequency  can  differ  depending  on  the  use  case.  Thanks  to  the  high\\ndegree of automation, retraining every night in this case is possible. In other contexts,\\nretraining  could  be  triggered  by  various  signals  (e.g.,  signification  volume  of  new\\ninformation or drift in customer behavior, be it aperiodic or seasonal).\\n\\nIn addition, the delay between the recommendation and the point in time at which its\\neffect  is  evaluated  has  to  be  taken  into  account.  If  the  impact  is  only  known  with  a\\n\\n142 \\n\\n| \\n\\nChapter 10: MLOps in Practice: Marketing Recommendation Engines\\n\\n\\x0cdelay of several months, it is unlikely that retraining every day is adequate. Indeed, if\\nthe behavior changes so fast that retraining it every day is needed, it is likely that the\\nmodel  is  completely  outdated  when  it  is  used  to  make  recommendations  several\\nmonths after the most recent ones in the training data.\\n\\nUpdating Models\\nUpdating models is also one of the key features of automation strategies at scale. In\\nthis case, for each of the four thousand pipelines, retrained models must be compared\\nto  the  existing  models.  Their  performances  can  be  compared  using  metrics  such  as\\nRMSE  (root-mean-square  error),  and  only  when  the  performance  of  the  retrained\\nmodel beats the prior one does the retrained model get deployed to SalesCore.\\n\\nRuns Overnight, Sleeps During Daytime\\nAlthough  the  model  is  retrained  every  day,  users  do  not  interact  directly  with  the\\nmodel. Using the updated model, the platform actually finishes calculating the ranked\\nlist of products for all the customers during the night. On the following day, when a\\nuser hits the “show product recommendations” button, the platform simply looks at\\nthe customer ID and returns the ranked list of products for the specific customer.\\n\\nTo the user, it looks as if the recommendation engine is running in real time. In real‐\\nity, however, everything is already prepared overnight, and the engine is sleeping dur‐\\ning daytime. This makes it possible to get the recommendation instantly without any\\ndowntime.\\n\\nOption to Manually Control Models\\nAlthough the monitoring, retraining, and updating of the models is fully automated,\\nMarketCloud still leaves room for its clients to turn the models on and off. More pre‐\\ncisely, MarketCloud allows the users to choose from three options to interact with the\\nmodels:\\n\\n• Turn on to get the recommendation based on the most updated dataset\\n\\n• Freeze to stop retraining with the new data, but keep using the same model\\n\\n• Turn off to completely stop using the recommendation functionality of SalesCore\\n\\nMachine learning algorithms attempt to convert practical knowledge into meaningful\\nalgorithms  to  automate  processing  tasks.  However,  it  is  still  good  practice  to  leave\\nroom  for  users  to  rely  on  their  domain  knowledge,  as  they  are  presumed  to  be  far\\nmore  capable  of  identifying,  articulating,  and  demonstrating  day-to-day  process\\nproblems in business.\\n\\nMonitoring and Feedback \\n\\n| \\n\\n143\\n\\n\\x0cThe second option is important because it allows users to stay in the current quality\\nof  the  recommendation  without  having  the  recommendation  engines  updated  with\\nthe newer data. Whether the current model is replaced with a retrained one depends\\non the mathematical evaluation based on metrics such as the RMSE. However, if users\\nfeel  that  the  product  recommendations  on  SalesCore  are  already  working  well  for\\npushing sales, they have the choice not to risk changing the recommendation quality.\\n\\nOption to Automatically Control Models\\nFor those that don’t want to manually handle the models, the platform could also pro‐\\npose A/B testing so that the impact of new versions is tested before fully switching to\\nthem. Multi-armed bandit algorithms (an algorithm that allows for maximization of\\nthe revenue of a user facing multiple slot machines, each with a different probability\\nto win and a different proportion of the money given back on average) are used for\\nthis purpose.\\n\\nLet’s assume that several model versions are available. The goal is to use the most effi‐\\ncient one, but to do that, the algorithm obviously has to first learn which is the most\\nefficient.  Therefore,  it  balances  these  two  objectives:  sometimes,  it  tries  algorithms\\nthat  may  not  be  the  most  efficient  to  learn  if  they  are  efficient  (exploration),  and\\nsometimes it uses the version that is likely to be the most efficient to maximize the\\nrevenue  (exploitation).  In  addition,  it  forgets  past  information,  as  the  algorithm\\nknows the most efficient today may not be the most efficient tomorrow.\\n\\nThe  most  advanced  option  consists  in  training  different  models  for  different  KPIs\\n(click, buy, expected revenue, etc.). A method inspired from ensemble models would\\nthen allow for the solving of conflicts between models.\\n\\nMonitoring Performance\\nWhen  a  salesperson  suggests  a  customer  buy  the  products  recommended  by  Sales‐\\nCore,  the  interaction  of  the  customer  with  the  recommended  products  as  well  as\\nwhether the customer bought them or not is recorded. This record can then be used\\nto  keep  track  of  the  performance  of  the  recommender  system,  overwriting  the  cus‐\\ntomer and product dataset with this record to feed the most updated information to\\nthe model when it is retrained.\\n\\nThanks  to  this  ground  truth  recording  process,  dashboards  showing  model  perfor‐\\nmance  can  be  presented  to  the  user,  including  performance  comparison  from  A/B\\ntesting. Because the ground truth is obtained quickly, data drift monitoring is secon‐\\ndary. A version of the model is trained every night, but, thanks to the freeze mecha‐\\nnism, the user can choose the active version based on the quantitative information. It\\nis customary to keep the human in the loop on these high-impact decisions where the\\nperformance metrics have a hard time capturing the full context around the decision.\\n\\n144 \\n\\n| \\n\\nChapter 10: MLOps in Practice: Marketing Recommendation Engines\\n\\n\\x0cIn the case of A/B testing, it is important that only one experiment be done at a time\\non a group of customers; the impact of combined strategies cannot be simply added.\\nWith such considerations in mind, it is possible to build a sound baseline to perform\\na  counterfactual  analysis  and  derive  the  increased  revenue  and/or  the  decreased\\nchurn linked to a new strategy.\\n\\nApart  from  this,  MarketCloud  can  also  monitor  the  algorithm  performance  at  a\\nmacro level, by checking how many clients froze or turned off the recommender sys‐\\ntems.  If  many  clients  turned  off  the  recommender  systems,  that’s  a  strong  indicator\\nthat they are not satisfied with the recommendation quality.\\n\\nClosing Thoughts\\nThis  use  case  is  peculiar  in  the  sense  that  MarketCloud  built  a  sales  platform  that\\nmany other companies use to sell products, where the ownership of the data belongs\\nto each company, and the data cannot be shared across companies. This brings a chal‐\\nlenging situation where MarketCloud must create different recommender systems for\\neach of the users instead of pooling all the data to create a universal recommendation\\nengine.\\n\\nMarketCloud can overcome this obstacle by creating a single pipeline into which data\\nfrom many different companies can be fed. By having the data go through an automa‐\\nted  recommendation  engine  training  scenario,  MarketCloud  created  many  recom‐\\nmendation  engines  trained  on  different  datasets.  Good  MLOps  processes  are  what\\nallow the company to do this at scale.\\n\\nIt’s worth noting that though this use case is fictionalized, it is based on reality. The\\nreal-life team that tackled a similar project took around three months to finish. The\\nteam used a data science and machine learning platform to orchestrate the duplica‐\\ntion of a single pipeline to four thousand copies and to automate the processes to feed\\ncorresponding  datasets  to  each  pipeline  and  train  the  models.  Of  necessity,  they\\naccepted trade-offs between the recommendation quality and scalability to efficiently\\nlaunch  the  product.  If  the  team  had  carefully  crafted  a  custom  recommendation\\nengine  for  each  of  the  four  thousand  pipelines  by,  for  example,  choosing  the  best\\nalgorithm for each client, the recommendation engines would have been of a higher\\nquality, but they would have never been able to complete the project with such a small\\nteam in such a short period of time.\\n\\nClosing Thoughts \\n\\n| \\n\\n145\\n\\n\\x0c\\x0cCHAPTER 11\\nMLOps in Practice: Consumption Forecast\\n\\nNicolas Omont\\n\\nPredictions  at  various  times  and  geographical  scales  play  an  important  role  in  the\\noperation of a power grid. They allow for simulation of possible future states of the\\nsystem and for making sure that it can safely operate. This chapter will walk through\\na machine learning model life cycle and MLOps use case for consumption forecast‐\\ning,  including  business  considerations,  data  collection,  and  implementation  deci‐\\nsions.  Though  this  particular  chapter  is  focused  on  power  grids,  the  considerations\\nand particularities of the use case can be generalized to other industrial cases that use\\nconsumption forecasting.\\n\\nPower Systems\\nBulk  power  systems  are  the  backbone  of  power  grids.  Also  called  transmission  net‐\\nworks, they form the core of the system that keeps the lights on. These systems are\\nmainly  composed  of  lines  and  transformers,  which  are  indirectly  connected  with\\nmost  producers  and  consumers  through  distribution  networks  that  take  care  of  the\\nlast few kilometers of transmission. As illustrated in Figure 11-1, only the largest pro‐\\nducers and consumers are directly connected to the bulk system.\\n\\n147\\n\\n\\x0cFigure 11-1. A sample bulk power system, to which only the largest producers and con‐\\nsumers are directly connected\\n\\nThe longer the transmission distance and the larger the energy volume to be trans‐\\nmitted, the higher the voltage used: on the lower end, a few tens of kilovolts for a few\\ntens of megawatts over a few tens of kilometers; on the upper end, one million volts\\nfor a few thousand megawatts over a few thousand kilometers. (A line with a capacity\\nof one megawatt can be used to provide power to around one thousand inhabitants in\\nEurope.) The operation of transmission systems has always required a lot of commu‐\\nnications and computations because of its properties:\\n\\nNo energy storage\\n\\nThe  network  stores  a  meaningless  amount  of  energy—less  than  one  second  of\\nconsumption in the grid and up to 30 seconds in the alternators and motors. By\\nway  of  contrast,  a  gas  network  stores  several  hours  of  consumption  in  its  pipe‐\\nline. Therefore, actions have to be taken very quickly to balance production and\\nconsumption and avoid blackouts.\\n\\nWeak flow control\\n\\nOn  telecommunication  networks,  congestions  are  handled  by  dropping  packets\\nor by not establishing a call. There is no equivalent mechanism in power grids,\\nwhich means the power flow on a grid element can be higher than its operating\\nlimit.  Actions  have  to  be  taken  after  a  few  seconds  to  a  few  hours  of  overload\\ndepending on the technology and the severity. Flow control technologies do exist,\\n\\n148 \\n\\n| \\n\\nChapter 11: MLOps in Practice: Consumption Forecast\\n\\n\\x0cbut  there  is  a  trade-off  between  flow  control  and  instantaneous  balance:  the\\npower has to find a path from generation to consumption.\\n\\nBecause of these two properties, the grid operator always has to anticipate the contin‐\\ngencies: if this grid element fails, will the overload on the remaining elements remain\\nacceptable? The anticipation is done on several timescales, from the next five minutes\\nto the next five decades. The actions to be taken depend on the horizon. For example:\\n\\n• Below  five  minutes:  no  human  action  is  possible.  Automatic  actions  should\\n\\nalready be well defined.\\n\\n• Between five minutes and a few hours ahead: production schedule and grid top‐\\n\\nology adaptation (opening of breakers and other flow control technologies).\\n\\n• A few days ahead: maintenance schedule adaptations.\\n\\n• A few seasons ahead: maintenance schedule adaptations, contracts with produc‐\\ners  or  consumers  to  guarantee  power  capacity  or  limit  power  generation  or\\nconsumption.\\n\\n• From  5  to  50  years  ahead:  investment  in  grid  elements.  Lines  and  transformers\\nhave standard life expectancies of several decades; practically, it is expected that\\nsome grid elements will last over one hundred years.\\n\\nAnother concern is anticipating at different geographical scales. While some contin‐\\ngencies  only  have  effects  on  a  small  part  of  the  grid,  some  may  have  a  continental\\nimpact  and  may  require  coordinated  actions  among  several  countries  to  mitigate\\ntheir effects. As a result, operating the grid requires:\\n\\n1. Collecting data over a wide geographical area with strong time constraints.\\n\\n2. Processing data to anticipate and act accordingly.\\n\\nData Collection\\nCollecting past data is the first step to making forecasts. There are two largely inde‐\\npendent sources of data: the SCADA (supervisory control and data acquisition) sys‐\\ntem and the metering system. Depending on the prediction use case, one or the other\\nmay be used.\\n\\nThe  SCADA  system  collects  data  in  real  time  to  provide  an  up-to-date  view  of  the\\nsystem to the operator. It also allows commands to be sent to network equipment—\\nfor example to open and close a breaker. The most impressive representation of the\\nsystem is the synoptic screen found in most control rooms as shown in Figure 11-2.\\n\\nData Collection \\n\\n| \\n\\n149\\n\\n\\x0cFigure 11-2. The SCADA system typically refreshes thousands of measurements about\\nflows, consumption, and generation on the grid every 10 seconds or less\\n\\nSome  measurements  are  intentionally  redundant,  such  as  measuring  power  loss.  If\\nthe power flow is measured at each end of a line, then the difference between them is\\nequal  to  the  losses  on  the  line.  These  losses  can  be  physically  estimated  so  that  it  is\\npossible to handle the case when one measure is missing, to detect anomalies, or to\\nimprove the precision of the estimates.\\n\\nThe process that uses this redundancy to produce a state of the network is called the\\nstate estimation, and it is run every few minutes. When an operating limit is not satis‐\\nfied, the SCADA system raises an alarm. However, the SCADA cannot raise an alarm\\nwhen an operating limit would not be satisfied if one of the grid elements went out of\\norder.\\n\\nSimulations  of  network  element  loss  (N-1  simulation)  on  the  consistent  state  pro‐\\nduced by the state estimation are run on a regular basis, and the value of SCADA data\\nfades  quickly;  therefore,  when  it  is  historized,  it  is  not  consolidated;  missing  values\\nare usually not input, and anomalies are usually not corrected. State estimations are\\nused by a variety of processes so that they are usually historized over a few months to\\na few years.\\n\\nThe metering system that is used for invoicing does not need to be as reactive as the\\nSCADA system, but should be precise. It focuses on generation and consumption, not\\n\\n150 \\n\\n| \\n\\nChapter 11: MLOps in Practice: Consumption Forecast\\n\\n\\x0cflow. Rather than monitoring instantaneous power, it records the withdrawn or injec‐\\nted energy over a period of time that ranges between a few minutes and one hour.\\n\\nThe  information  it  gathers  was  previously  made  available  after  a  delay  of  a  day  or\\nmore. Newer systems make it available within a few minutes. However, consolidation\\nand validation are usually needed when there are missing measurements or anoma‐\\nlies so that the final data is still usually available within a few working days. This data\\nis well historized.\\n\\nProblem Definition: Machine Learning,\\nor Not Machine Learning?\\nNot  all  use  cases  are  appropriate  for  machine  learning.  Some  can  be  solved  more\\neasily and cheaply in other ways. The techniques used to make forecasts for the type\\nof  use  case  presented  here  are  different  in  these  three  situations  as  shown  in\\nTable 11-1.\\n\\nTable 11-1. Forecasting techniques by use case\\n\\nUse case\\nThe forecast uncertainty\\ncomes from a part of the\\nsystem that the operator\\ncannot change.\\n\\nThe forecast uncertainty\\ncomes from a part of the\\nsystem that the operator\\ncan somehow influence.\\n\\nThe forecast uncertainty\\ncomes from a part of the\\nsystem that some other\\nactors can control and\\nanticipate.\\n\\nForecasting technique\\nChanging the weather is, practically speaking, impossible. As a result, wind and photovoltaic\\n(PV) generation, as well as heating and air conditioning, can safely be considered exogenous.\\nThis makes them good candidates for direct machine learning forecasting. These forecasts can\\nleverage meteorological forecasts or climatic scenarios, depending on the horizon.\\nMeteorological forecasts are available only a few days ahead, though some models now predict\\ntrends over a few months.\\nFor example, strictly speaking, the consumption should not be forecasted, but rather the\\ndemand. The difference between consumption and demand is that the consumption is somehow\\nat the hand of the operator that can choose not to serve the demand by switching off the\\nconsumers. For the same reason, the photovoltaic and wind production potential is forecasted,\\nnot the actual production.\\nFor example, for dispatchable power units where the operator can switch them on or off, it is\\nbetter to ask for the schedules from the operator. If this is not possible, it may be better to\\nreproduce the way the schedules are made—for instance, the operator may start the plant if\\nthe power price is higher than the plant fuel cost. In such cases, the forecasts may rely on\\ntechniques like agent-based modeling.\\nLarge factories are likely to have consumption schedules based on their operational production\\nschedules. Distribution grid topology is also likely to be scheduled ahead of time, as\\nmaintenance operations require advanced planning. In all these cases, it is often better to ask for\\nthe schedules than to use machine learning to forecast them.\\n\\nSpatial and Temporal Resolution\\nDue  to  the  law  of  large  numbers,  the  forecast  uncertainty  decreases  when  the  con‐\\nsumption is spatially or temporally aggregated. While it is hard to forecast the hourly\\n\\nProblem Definition: Machine Learning, or Not Machine Learning? \\n\\n| \\n\\n151\\n\\n\\x0cconsumption of an individual household because people are not machines, it is sur‐\\nprisingly  easy  for  populations  of  a  few  million,  and  is  relatively  easy  to  forecast  the\\nmonthly consumption of such a population as well.\\n\\nAs a result, a forecast system is often hierarchical, with several levels of forecasts that\\nare  linked  together  by  constraints.  That  is,  regional  forecasts  should  sum  up  to  the\\ncountry-wide forecasts, and hourly forecasts should sum up to the daily forecast.\\n\\nLet’s take a striking example to illustrate this. Electric traction trains have a worrying\\nconsumption pattern for grid operators because they move, with a typical train line\\nbeing fed by a different substation every 10 to 50 kilometers. As a result, the operator\\nsees consumption of a few megawatts switching from substation to substation every\\n10 minutes or so. It creates several issues:\\n\\n• The forecast is relatively easy at the line level because the train is always consum‐\\ning somewhere and because trains usually circulate at fixed hours. As a result, a\\nmachine learning approach is likely to work.\\n\\n• The forecast of the energy withdrawn over a long period at a given substation is\\nalso  relatively  easy,  because  the  train  will  go  through  the  corresponding  part  of\\nthe line.\\n\\n• But because the operator wants to know if the train will create an overload when\\n\\ncirculating, a consistent set of forecasts is needed:\\n\\n— The train should withdraw power at one location at a time only.\\n\\n— Each substation should see a consumption spike at some point in time so that\\n\\na fine-grained time resolution is needed.\\n\\nAs a result, the solution depends on the goal of the prediction:\\n\\n• On a day-to-day basis, an average solution that splits the train consumption over\\nall substations is not acceptable, as potential overloads may be missed. A worst-\\ncase solution that assigns the train consumption to all substations may be more\\nacceptable, though it will anticipate spurious overloads as the overall consump‐\\ntion will be too large.\\n\\n• However, to schedule the maintenance of one of the lines that feeds the region,\\nthe exact location of the consumption is likely to have no impact as long as it is\\nnot counted several times.\\n\\nWhen  designing  the  forecast  system,  trade-offs  will  have  to  be  made,  as  the  perfect\\nsystem is unlikely to exist. If the system has a lot of margin, few or no overloads are\\nexpected so that the forecasting system can be coarse. However, if the grid is operated\\nclose to its limits, the system has to be carefully crafted.\\n\\n152 \\n\\n| \\n\\nChapter 11: MLOps in Practice: Consumption Forecast\\n\\n\\x0cImplementation\\nOnce data is collected, either by the SCADA system or by the metering system, it has\\nto be historized. In addition to storing the raw data, some processing is required:\\n\\n• Temporal aggregations, for example over a five-minute period: Either the average\\nvalue or a high quantile value is kept. The average is representative of the energy\\nconsumed over the period, and the high quantile is useful to assess if constraints\\noccurred.\\n\\n• Disaggregations: When only the withdrawal is measured, the production and the\\nconsumption  have  to  be  separately  estimated.  Usually,  consumption  is  what\\nremains  after  removing  the  best  possible  estimation  of  distributed  generation\\n(wind, PV, etc.). Machine learning can be useful to perform these estimations.\\n\\n• Spatial aggregations: As the system is balanced, it is possible to compute the con‐\\nsumption of a region by computing the difference between the local production\\nand the exchanges with the neighboring regions. This was historically very useful\\nbecause the production was easy to monitor because there were only a few very\\nlarge generation units and a few lines with neighboring countries. Nowadays, it\\ntends to be more complex as distributed generation is more widespread.\\n\\n• Missing value imputation: A measurement may be missing. In the SCADA sys‐\\ntem, rules exist to replace a missing value with an older or a typical value in real\\ntime. In the metering system, the imputation is a heavy impact process as it will\\nbe reflected directly on the customer’s invoice.\\n\\nData is then stored in different databases. Data used in short-term critical processes is\\nstored in high-availability systems in which redundancy allows rapid recovery from\\nthe loss of a data center. Data used in longer-term processes (invoicing, reports, ML\\nmodel training) is stored in ordinary IT databases. Overall, the number of monitored\\ngrid elements will range between 1,000 and 100,000. This means that they generate a\\nreasonable volume of data by today’s standards. Scalability is not such an issue either,\\nas bulk power grids do not grow anymore in developed countries.\\n\\nModeling\\nOnce the data preparation has been finished, the data scientist typically has access to\\na  few  hundred  time  series  of  production  and  consumption  at  various  withdrawal\\npoints of the grid. They have to develop methods to predict some of them at various\\nhorizons. Their focus is usually on wind, PV, and sometimes run-of-the river hydro‐\\nelectricity production potential and on demand. While wind and PV mainly depend\\non meteorological factors, the demand is mainly driven by economic activity, but part\\nof it is also dependent on meteorology (for example heating and cooling).\\n\\nImplementation \\n\\n| \\n\\n153\\n\\n\\x0cDepending on the horizon, the modeling might look very different:\\n\\n• Short-term: Up to a few days ahead, the last known values are very important to\\nmake predictions. In addition, for the same reasons, meteorological forecasts are\\navailable. Therefore, methods will leverage this information. In this case, deter‐\\nministic forecasts make sense.\\n\\n• Mid-term:  Between  a  few  days  and  a  few  years,  the  meteorology  is  not  known,\\nbut  the  climate  is.  Statistical  extrapolation  of  past  year  tendencies  make  sense,\\nexcept if an economic crisis occurs. As a result, it is possible to draw scenarios to\\nobtain statistical indicators (mean, confidence intervals, quantiles, etc.) about the\\nfuture consumptions.\\n\\n• Long-term: Investment decisions require forecasts over several decades. On this\\nhorizon, statistical extrapolations of the current trend are not enough, neither on\\nthe  socio-economic  side  nor  on  the  climatic  side  given  global  warming.  As  a\\nresult,  statistical  approaches  have  to  be  completed  with  bottom-up  usage-based\\napproaches and expert-made diversified scenarios about the future.\\n\\nML  and  MLOps  mainly  concern  short-term  and  mid-term  forecasts.  Of  the  two,  in\\nthis case, mid-term models are easier to start with: given a few years of data, the goal\\nis to predict consumption based on:\\n\\n• The calendar, with a superposition of daily, weekly, and annual cycles. Bank holi‐\\ndays and school vacations also have a big impact, in addition to daylight saving\\ntime.\\n\\n• The  meteorological  variables  (temperature,  wind,  sun).  As  buildings  have  very\\nlarge thermal inertia, at least two days and up to three weeks of past temperatures\\nmay be needed.\\n\\nWhile any kind of ML algorithm can be used, the smoothness of the predicted curve\\nis important because the predictions are not used individually, but as daily, weekly, or\\nannual  scenarios.  Many  algorithms  do  not  consider  smoothness  in  their  metrics\\nbecause they rely on the hypothesis that the data is independent and identically dis‐\\ntributed, which in our case is incorrect, since the consumption of a given day is usu‐\\nally correlated with the one of the previous day and the one of the previous week.\\n\\nGeneralized additive models (GAM) are often a good starting point: they are based\\non  splines,  so  that  the  smoothness  is  guaranteed.  In  fact,  consumption  forecasting\\nwas one of the use cases for which they were developed. Combined with climatic sce‐\\nnarios, the ML model is then able to yield yearly consumption scenarios.\\n\\nShort-term forecasts are more complex. The simplest way to proceed is to remove the\\nmid-term forecast from the recent historical data and use standard time series techni‐\\nques,  such  as  ARIMA  (autoregressive  integrated  moving  average)  or  exponential\\n\\n154 \\n\\n| \\n\\nChapter 11: MLOps in Practice: Consumption Forecast\\n\\n\\x0csmoothing, on the residuals. This allows the generation of forecasts over several days.\\nAn integrated short-term model trained on several years of data has potential advan‐\\ntages over this simple approach.\\n\\nFor example, the mid-term model is trained on realized meteorological data and not\\non meteorological forecasts. As a result, it gives too much importance to meteorolog‐\\nical forecasts even though they may be wrong. A short-term model trained on mete‐\\norological  forecasts  would  address  this  issue.  However,  although  new  algorithms,\\nsuch as long short-term memory (LSTM) neural networks, are promising, it is hard\\nto  find  a  method  that  allows  for  forecasting  at  any  time  of  the  day  for  several  time\\nhorizons at once in a consistent way.\\n\\nWhen  the  resolution  is  such  that  the  stochasticity  is  too  large  to  make  meaningful\\npredictions, it is better to aggregate time series spatially or temporally and then use\\nnon-ML heuristics to split the aggregated forecasts:\\n\\n• A sharing key based on past observations in the case of spatial aggregation\\n\\n• An average profile based on past observations in the case of temporal aggregation\\n\\nBecause the grid is under constant evolution, it is likely that new injections and with‐\\ndrawals  appear  for  which  no  historical  data  is  available  and  that  ruptures  occur  in\\nconsumption patterns, so that past data is not relevant anymore. The forecast method\\nhas to take into account these edge cases. Ruptures could be spotted using anomaly\\ndetection  methods.  As  soon  as  a  rupture  is  identified,  a  simplified  model  could  be\\nused for as long as necessary until enough historical data is available.\\n\\nOnce again, neural networks could become an appealing alternative with the promise\\nthat only one model could be trained for all the consumptions instead of one model\\nper consumption with standard methods. Indeed, with only one model, the forecast\\nof a consumption with shallow historical data would be possible provided that its pat‐\\ntern looks similar to an existing pattern.\\n\\nDeployment\\nNowadays, the models are likely to be prototyped by a data scientist in R, Python, or\\nMATLAB scripts. The prototype is able to prepare the data, train the model on one\\ndataset, and score it on another. The operationalization could follow several paths:\\n\\n• The prototype is fully rewritten. This is costly and not flexible but may be neces‐\\n\\nsary if embedding in an operational technology (OT) system is needed.\\n\\n• Only the data preparation and the scoring are rewritten, which allows for train‐\\ning on a different schedule. It makes sense if the training occurs once a year or so\\nbecause it is good practice to regularly perform a model review to ensure that it\\nworks well and that the skills to maintain it are in place.\\n\\nDeployment \\n\\n| \\n\\n155\\n\\n\\x0c• Data  science  and  machine  learning  platforms  can  be  used  to  operationalize  the\\nprototype.  These  platforms  are  flexible  and  allow  the  transfer  of  prototypes  to\\nproduction environments in which security and scalability are guaranteed. Most\\nconsumption  forecast  models  will  be  run  periodically  in  batch  mode.  For  more\\nspecific use cases, these platforms are able to export trained models as JAR files,\\nSQL,  PMML,  PFA,  and  ONNX  so  that  they  can  be  flexibly  integrated  into  any\\nkind of application.\\n\\nMonitoring\\nThis section mainly discusses short-term forecasts. Indeed, mid-term and long-term\\nforecasts are systematically impacted by drift, as the past never looks like the future,\\nso they are almost systematically trained again before being used to make predictions.\\nFor  short-term  forecasts,  besides  IT  monitoring  to  raise  alarms  if  forecasts  are  not\\nproduced on time and warnings for events that may result in missing deadlines, the\\nmodels themselves should be monitored.\\n\\nThe  first  kind  of  monitoring  is  drift  monitoring.  For  electricity  consumption,  it  is\\ncritical that drift monitoring is deployed together with the model. Anomaly detection\\nand rupture detection allow teams to make sure that the trained model can be used. If\\nnot, fallback models based on shallow historical data or normative disaggregation of\\nmultiple  consumption  forecasts  should  be  used.  This  first  layer  will  detect  drastic\\ndrifts online.\\n\\nThough the data scientist will try to design models that are adaptive to the consump‐\\ntion level (like ARIMA), it can be useful to detect that some consumption levels are\\nhigher or lower than in the training period. This may have happened slowly, so that it\\nwas  not  detected  online.  The  offline  analysis  of  the  forecasts,  for  example  once  a\\nmonth if the forecasts are computed every day for the next day, offers the possibility\\nto detect these slow drifts. In these cases, if no additional ground truth is available, it\\nwould make sense to shift to a fallback model for these consumptions.\\n\\nLastly, after the operations, it is possible to assess the performance of the prediction\\nthrough  various  metrics  like  mean  absolute  percentage  error  (MAPE).  If  a  perfor‐\\nmance  drop  is  detected  during  a  significant  amount  of  time  (for  example,  one\\nmonth),  retraining  the  corresponding  models  is  an  option  as  new  data  is  available,\\nand the retrained models may increase performance.\\n\\nThis requires a tight integration of the design and the production environment with\\nCI/CD processes (as discussed at length in Chapter 6). If it is possible to handle man‐\\nually the deployment of new models once a year, it is usually too costly to do so once\\na  month.  With  an  advanced  data  science  and  machine  learning  platform,  it  is  also\\npossible to perform shadow scoring with the new model for a few days before using it\\nfor the forecasts.\\n\\n156 \\n\\n| \\n\\nChapter 11: MLOps in Practice: Consumption Forecast\\n\\n\\x0cClosing Thoughts\\nIn this chapter, we have seen how to make the data speak to assist the operation of a\\ntransmission power grid. Various ML and non-ML techniques can be used to provide\\nforecasts for up to thousands of consumptions on timescales ranging from minutes to\\ndecades.\\n\\nThanks  to  MLOps,  design,  deployment,  and  monitoring  processes  have  been  stan‐\\ndardized across several industries, and data science and machine learning platforms\\nhave been developed to support this process. Designers of consumption forecast sys‐\\ntems can leverage these standard processes and platforms to improve the efficiency of\\nthese systems from the cost, quality, or time to value perspective.\\n\\nTaking  a  larger  step  back,  it’s  clear  that  different  industries  have  a  wide  range  of\\nmachine learning use cases, all of which have their own intricacies when it comes to\\ndefining  the  problem,  building  models,  pushing  to  production—everything  we’ve\\ncovered in this book. But no matter what the industry or use case, MLOps processes\\nare consistently the thread that allows data teams (and more widely, entire organiza‐\\ntions) to scale their machine learning efforts.\\n\\nClosing Thoughts \\n\\n| \\n\\n157\\n\\n\\x0c\\x0cIndex\\n\\nA\\nA/B testing\\n\\ncanary releases, 79\\nconsiderations in MLOps context, 102\\nof new and existing model versions, 33\\nperformance monitoring for marketing rec‐\\n\\nommendation engine, 144\\n\\nfor recommendation engines using collabo‐\\n\\nrative filtering, 136\\n\\nusing in online evaluation of models, 99,\\n\\n101\\n\\naccountability\\n\\nGxP guidelines focus on, 109\\nin Responsible AI, 10, 112\\naccuracy, precision, and recall, 132\\n\\nmetrics collected in preproduction model\\n\\ntesting, 65\\nadversarial attacks, 68\\nAI, 112\\n\\n(see also Responsible AI)\\nnew wave of AI-specific regulations,\\n\\n111-112\\n\\nResponsible AI, MLOps for, 9\\n\\nAIOps versus MLOps, 3\\nalgorithms (machine learning), 23, 44\\n\\ncomputing power considerations, 45\\nMLOps considerations by algorithm type,\\n\\n45\\n\\nonline learning, 88\\nrequirement for tabular input data, 47\\nsmoothness of predicted curve, 154\\n\\nanonymizing or pseudo-anonymizing data, 36\\nApache Spark, 78\\nAPIs\\n\\nmarketing recommendation engine API\\n\\nendpoints, 142\\n\\nREST API for model-as-a-service or live-\\n\\nscoring model, 28\\n\\nARIMA (autoregressive integrated moving\\n\\naverage), 154, 156\\nartifacts (ML), 75-76\\nassumptions (model), 57\\nauditability, 112, 116\\n\\naiding with QA for machine learning, 67\\nand reproducibility, 67\\n\\nautomation\\n\\nautomated feature selection, 48\\nautomated model deployment, 29\\nautomated model documentation, 27\\nautomated model packaging and delivery,\\n\\n14, 18\\n\\nautomated reporting tools on all models, 14\\nautomatically controlling models, marketing\\n\\nrecommendation system, 144\\n\\nin experimentation during model develop‐\\n\\nment, 50\\n\\nfeedback loop, 132\\nof tests in testing pipeline, 76\\nof versioning and reproducibility tasks, 58\\n\\nB\\nbatch scoring, 77\\n\\nanalytics use cases, understanding and classify‐\\n\\nvolume of data becoming too large, distrib‐\\n\\ning, 118\\n\\nanomaly detection, 71\\n\\nuting computation, 82\\n\\nBayesian tests, 34\\n\\n159\\n\\n\\x0cbiases\\n\\nanalyzing models for fairness, 66\\ninappropriate biases in models, 36\\nintroduced by ground truth monitoring, 90\\nin ML black box judgments, 106\\nmodel bias considerations in consumer\\n\\ncredit risk management, 131\\n\\nreputational risk due to, 64\\nResponsible AI position on, 114\\nsample selection bias, 93\\ntuning bias/variance trade-off, 50\\n\\nproblem definition and data acquisition,\\n\\n130\\n\\ndeploying model to production, 132\\nmodel development, 130\\n\\nbias considerations, 131\\n\\npreparing model for production, 131\\nconsumption forecast for power grid, 147-157\\n\\ndata collection, 149-151\\ndeployment of models, 155\\nimplementation, 153-155\\n\\nmodeling, 153\\n\\nblack box models, 54\\nblue-green deployment, 28, 78\\nbounds for retraining frequency, 87\\nbusiness concerns in monitoring ML models,\\n\\n31\\n\\nbusiness decision modeling, 15\\nbusiness metrics for model performance moni‐\\n\\ntoring, 89, 94\\n\\nmonitoring, 156\\npower systems, 147-149\\nproblem definition, using machine learning\\n\\nor not, 151-152\\nspatial and temporal resolution, 151\\n\\ncontainerization, 79-81\\n\\nsolving problems of dependencies in ML\\n\\nmodel deployments, 28\\n\\nbusiness objectives, establishing for ML model,\\n\\ncontinuous integration and continuous delivery\\n\\n24\\n\\n(see CI/CD pipelines)\\n\\nbusiness use case, consumer credit risk man‐\\n\\ncosts\\n\\nagement application, 129\\n\\nC\\ncanary releases, 78\\nCCPA (California Consumer Privacy Act), 35,\\n\\n111\\n\\nchampion/challenger, 66, 100\\nChi-squared test, 94\\nCI/CD pipelines, 73-74\\n\\ncontinuous delivery for end-to-end machine\\n\\nlearning process, 95\\n\\nDevOps role in managing, 20\\nfor different models, 82\\nJenkins build system, 76\\nML artifacts in, 75\\nrobust, for model deployments, 29\\n\\ncollaborative filtering, 136\\ncompression techniques, use in model defini‐\\n\\ntion optimization, 61\\n\\ncomputational metrics from model testing, 65\\ncomputing power\\n\\nML model development and, 45\\nrequred for inference on ML models, 62\\n\\nconcept drift, 92\\nconformal prediction, 72\\nconsumer credit risk management, 129-133\\n\\nbusiness use case, 129\\n\\nfor algorithms that train themselves, 88\\nfor retraining models versus performance\\n\\nimprovement, 86\\ncross-trained models, 88\\ncurse of dimensionality, 71\\ncustomizability, marketing recommendation\\n\\nengine model, 140\\n\\nD\\ndark launch, 100\\n\\n(see also champion/challenger)\\n\\ndata\\n\\ncollection for consumption forecast for\\n\\npower grid, 149-151\\n\\nconsiderations in Responsible AI, 113\\ncustomer data, preparation for marketing\\n\\nrecommendation engine, 137\\n\\nprocessing for consumption forecast system,\\n\\n153\\n\\nreproducibility, 57\\n\\ndata access before validation and launch to pro‐\\n\\nduction, 62\\ndata architects, 21\\ndata cleansing, 25\\ndata drift, 91\\n\\nfor consumer credit risk management\\n\\nmodel, 132\\n\\n160 \\n\\n| \\n\\nIndex\\n\\n\\x0cexample causes of, 93\\n\\ndata engineers, 19\\ndata exploration, 46\\ndata governance, 36\\n\\nquestions for ML model data sources, 25\\ndata pipeline structure for recommendation\\n\\nengine project, 141\\n\\ndata privacy, 35, 108\\n\\nstrategies for, 77-79\\ndeployment, defined, 77\\nmarketing recommendation engine model,\\n\\n138-141\\n\\nmodel deployment types and contents, 28\\nmodel deploymnt requirements, 29\\n\\ndevelopment environments, adaptation to pro‐\\n\\nduction environments, 60-62\\n\\nGDPR and CCPA regulations on, 35, 110\\n\\nDevOps, 20\\n\\ndata scientists, 17-19\\n\\ncollaboration with SMEs in ML model life\\n\\ncycle, 16\\n\\nconcerns in ML model monitoring, 30\\n\\nground truth, 30\\ninput drift, 31\\n\\nrole in and needs from MLOps, 18\\nrole in machine learning model life cycle, 17\\n\\ndata sources for machine learning models, 24\\nDataOps, 7\\ndecision modeling (business), 16\\ndecision-making processes, statistically driven,\\n\\n105\\n\\ndeep learning, 45, 48, 54\\ndegradation of model performance\\n\\nconcerns in ML model monitoring, 30\\nMLOps and, 6\\nmonitoring of ML models, 86\\nrole in and needs from MLOps, 21\\nrole in machine learning model life cycle, 20\\n\\nDI (Data Integrity), 109\\ndimensionality, curse of, 71\\ndimensioning constraints on model develop‐\\n\\nment, 54\\n\\ndisaggregations of data, 153, 156\\ndistances between probability distributions, 132\\ndistillation (model), 61\\ndistributed computation, 82\\ndistribution of data, 92\\n\\ndivergence between training and testing\\n\\ncommon approaches to discovering, 30\\nunderstanding, 89-92\\n\\nphases, 92\\n\\nDocker, 80\\n\\ndelivery, 77\\n\\n(see also CI/CD pipelines)\\ncontinuous delivery versus deployment, 77\\n\\ndependencies\\n\\ndeployment of models through, 132\\nusing Kubernetes with, 80\\n\\ndocumentation of model development, 26\\ndomain knowledge, importance in data explo‐\\n\\npartial dependency plots, 27\\non production environment, reducing, 28\\n\\nration for models, 46\\n\\ndomains\\n\\ndeployment strategies, 77-79\\n\\ncategories of model deployment, 77\\nconcepts and terminology, 77\\nconsiderations in sending models to pro‐\\n\\nduction, 78\\n\\nmaintenance of models in production, 79\\n\\ndeployments\\n\\nbroader model deployment, greater risks\\n\\nfrom, 69\\n\\nconsumption forecast models, 155\\ndeploying to production, 73-84\\nbuilding ML artifacts, 75-76\\nCI/CD pipelines, 73\\nconsumer credit risk management\\n\\nmodel, 132\\n\\ncontainerization, 79-81\\nscaling deployments, 81-83\\n\\ndomain classsifier, 94\\nmodel retraining frequency and, 86\\n\\ndrift, 91\\n\\n(see also input drift)\\ndetection in practice, 92-95\\n\\nexample causes of data drift, 93\\ninput drift detection techniques, 93\\nmeasuring for consumer credit risk assess‐\\n\\nment model, 132\\n\\nmonitoring, 156\\nmonitoring and mitigation measures, 103\\n\\nE\\nEDA (exploratory data analysis), 24, 25, 46\\neffienciency, data scientists' need for from\\n\\nMLOps, 19\\nelastic systems, 81\\n\\nIndex \\n\\n| \\n\\n161\\n\\n\\x0cembedded models, 28\\nembeddings, 48\\nengaging and educating groups responsible for\\n\\ngovernance, 123\\n\\nenvironmental, social, and governance (ESG)\\n\\nperformance indicators, 36\\n\\nenvironments\\n\\nchanging rapidly, multiplying model risks,\\n\\n70\\n\\nfederated learning approach to model retrain‐\\n\\ning, 34\\n\\nfeedback loop, 95-103\\n\\nautomating or not, 132\\ninfrastructure, main components of, 96\\nlogging, 96\\nmodel evaluation, 97-99\\nlogical model, 97\\nmodel evaluation store, 98\\n\\ninformation needed by data scientists, 53\\nproviding exact description for model ver‐\\n\\nsion management, 79\\n\\nreproducibility and, 58\\n\\nethical position, establishing in MLOps, 118\\nEU\\n\\nonline evaluation of models in production,\\n\\n99-102\\nA/B testing, 101\\nchampion/challenger, 100\\nfinancial crisis of 2007-2008, 109\\nfinancial model risk management regulation,\\n\\nGeneral Data Protection Regulation\\n\\n109\\n\\n(GDPR), 35, 110\\n\\nkey requirements for trustworthy AI appli‐\\n\\ncations, 111\\n\\nevaluation datasets, 44\\nexperimentation in model development, 49-51\\n\\nimpacts on MLOps strategy, 50\\nmarketing recommendation engines, 138\\n\\nexplainability, 27\\n\\nfor decisions made by ML models affecting\\n\\nhumans, 54\\n\\nin Responsible AI, 113\\n\\nexploratory data analysis (see EDA)\\n\\nF\\nFacebook-Cambridge Analytica affair, 106\\nfairness\\n\\nreassuring public that ML is fair, 106\\nrequirements having dimensioning con‐\\nstraints on model development, 54\\nsubpopulation analysis and model fairness,\\n\\n66\\nfeature drift, 92\\nfeature stores, 49\\nfeatures, 24\\n\\ncontrolling feature-value intervals, 71\\ndrift attributed to, use in mitigating impact\\n\\nof drift, 95\\n\\nengineering and selection, 25, 47-49\\n\\nfeature engineering techniques, 47\\nimpacts of feature selection on MLOps\\n\\nstrategy, 48\\n\\nfinancial risk management techniques, 130\\nFlask framework, 29\\nforecasting, 147\\n\\n(see also consumption forecast for power\\n\\ngrid)\\n\\nforecasting techniques by use case, 151\\nspatial and temporal resolution of consump‐\\n\\ntion, 151\\n\\nG\\nGAM (generalized additive models), 154\\nGDPR (General Data Protection Regulation),\\n\\n35, 110\\n\\ngeneralization capacity (models), 42\\nGit, 75\\nGoogle, smartphone keyboard software,\\n\\nGBoard, 34\\n\\ngovernance, 34-38, 105-125\\n\\nAI-specific regulations, new wave of,\\n\\n111-112\\nstatus of AI governance initiatives\\n\\nworldwide, 111\\n\\napplication to MLOps, 36\\ndata governance, 36\\nprocess governance, 37\\n\\ncritical role in machine learning security, 69\\ncurrent regulations driving MLOps gover‐\\n\\nnance, 108-111\\nfinancial model risk management, 109\\nGDPR and CCPA data privacy regula‐\\n\\ntions, 110\\n\\nstatistical test on data from source and tar‐\\n\\npharmaceutical regulation in US, GxP,\\n\\nget distribution, 94\\n\\n109\\n\\n162 \\n\\n| \\n\\nIndex\\n\\n\\x0ckey elements of Responsible AI, 112-116\\n\\nbias, 114\\ndata, 113\\ngovernance techniques, 116\\ninclusiveness, 115\\nmodel management at scale, 116\\n\\nmatching with risk level, 107\\ntemplate for MLOps, 117-125\\n\\ndetermining governance policies, 120\\nengaging end educating, 123\\nestablishing an ethical position, 118\\nestablishing responsibilities, 119\\nintegrating governance policies into\\n\\ndetection of, 91\\ndetection techniques, 93\\ndomain classifier, 94\\ninterpretation of results, 94\\nunivariate statistical tests, 93\\nmodel retraining motivated by, 33\\n\\nintegration, 77\\n\\n(see also CI/CD pipelines)\\n\\nintentionality, Responsible AI, 9, 112\\ninteractions between models, risks generated\\n\\nby, 70\\n\\ninterpretability (ML), bias detection through,\\n\\n114\\n\\nMLOps process, 121\\n\\niterations of ML models, 32\\n\\nmonitoring and refining governance,\\n\\n124\\n\\nfeedback loop, 33\\niterating on model deployed to millions of\\n\\nselecting tools for centralized gover‐\\n\\ndevices, 34\\n\\nnance management, 122\\n\\nunderstanding/classifying analytics use\\n\\ncases, 118\\n\\nwho decides on organization needs, 105\\n\\ngradient boosting algorithms, 130\\nground truth, 30\\n\\nevaluation of, in monitoring of performance\\n\\ndegradation, 89\\n\\nGxP pharmaceutical regulations in US, 109\\n\\nH\\nhealth checks, 79\\nhealth diagnosis or prediction example, 43\\nhigh bias model (underfitting), 50\\nhigh variance model (overfitting), 50\\nhistorizing data, 153\\nhouse price example, model prediction and\\n\\ngeneralization, 43\\n\\nhuman-centered approach, 113\\nhuman-in-command, 141\\nhuman-in-the-loop (HITL), 115\\nhyperparameters of ML algorithms, 44\\n\\nI\\nimpact coding, 47\\nimplementation, reproducibility of models and,\\n\\n57\\n\\ninclusiveness, 115\\nindividual conditional expectation (ICE) com‐\\n\\nputations, 54\\n\\ninference, computing power required for, 62\\ninput drift, 31\\n\\nJ\\nJava-based environment, model developed in\\n\\nPython, 61\\n\\nJenkins build system, 76\\n\\nK\\nKolmogorov-Smirnov test, 94, 132\\nKPIs (key performance indicators), 24, 107\\n\\nin business scoping, 122\\nfor governance, 124\\nmeasurement of, 25\\nmonitoring for deployed ML models, 31\\nsubject matter experts contributing, 15\\ntracking model performance against, 123\\ntraining different models for different KPIs,\\n\\n144\\nKubernetes, 28, 80\\n\\nautoscaling properties, 81\\nusing with Spark, 82\\n\\nL\\nlabeled sample subset, problems in ground\\n\\ntruth monitoring, 90\\n\\nlatency\\n\\nmetrics on, collection in preproduction\\n\\nmodel testing, 65\\n\\nmodel conversions and, 61\\n\\nlinear algorithms, 45\\nlinear models, advanced, 130\\nLinux cgroups, 29\\n\\nIndex \\n\\n| \\n\\n163\\n\\n\\x0clive-scoring model, 28\\nlogging\\n\\nactivity logging of deployed model, 132\\ncomponent in ML feedback loop, 96\\nusing to generate centralized datasets for use\\n\\nby model designer, 83\\n\\nlogical model, 98\\n\\nMLOps\\n\\nconsiderations with A/B testing, 102\\ndefining, and its challenges, 4-7\\ngovernance template for, 117-125\\nmitigating risks of ML models, 7-10\\nversus ModelOps versus AIOps, 3\\nreal-world examples\\n\\ncomparing performance between different\\n\\nconsumer credit risk management,\\n\\nversions, 99\\n\\nversioning evolution of, 99\\n\\nLTSM (long short-term memory) neural net‐\\n\\nworks, 155\\n\\nM\\nmachine learning, 41\\n(see also models)\\ncontinuous delivery for end-to-end process,\\n\\n95\\n\\ndeciding whether to use or not, 151-152\\nprimer, 23\\nquality assurance for, 64-66\\nrole in recommendation engines, 136\\nsecurity issues, 67-69\\n\\nmachine learning architects, 21\\n\\nrole in and needs from MLOps, 22\\nrole in machine learning model life cycle, 22\\n\\nmachine learning metrics, monitoring, 79\\nmachine learning models (see models)\\nmaintenance of models in production, 79\\nmanual control of models, 143\\nMAPE (mean absolute percentage error), 156\\nmarketing data mining urban legend, 135\\nmarketing recommendation engines, 135-145\\n\\ndata preparation for, 137\\ndesigning and managing experiments on,\\n\\n138\\n\\nmodel training and deployment, 138-141\\n\\nchallenges in, 139\\n\\nmonitoring and feedback, 142-145\\npipeline structure and deployment strategy,\\n\\n141\\n\\nrise of recommendation engines, 135-137\\npush or pull recommendations, 136\\nrole of machine learning, 136\\n\\nMATLAB scripts, 155\\nminority populations, representation in ML\\n\\nmodel data sources, 25\\nmisbehavior of models, 71\\nmissing value imputation, 153\\n\\n164 \\n\\n| \\n\\nIndex\\n\\n129-133\\n\\nconsumption forecast, 147-157\\nmarketing recommendation engines,\\n\\n135-145\\n\\nfor scale, 10\\n\\nmodel evaluation stores, 97, 98\\nmodel risk manager/auditor, 21\\n\\nrole in and needs from MLOps, 21\\nrole in machine learning model life cycle, 21\\n\\nmodel-as-a-service, or live-scoring model, 28\\nmodels\\n\\ndeveloping, 24-27, 41-58\\n\\nconsumer credit risk management\\n\\nmodel, 130\\ndata exploration, 46\\ndata sources and exploratory data analy‐\\n\\nsis, 24\\n\\nestablishing business objectives, 24\\nevaluating and comparing models, 51-56\\nexperimentation, 49-51\\nfeature engineering and selection, 25,\\n\\n47-49\\nin practice, 43\\nin theory, 42\\nMLOps considerations by algorithm\\n\\ntype, 45\\n\\nrequired components, 44\\nResponsible AI, 26\\ntraining and evaluation, 26\\nversion management and reproducibil‐\\n\\nity, 56-58\\n\\niterations and life cycle, 32-34\\nlife cycle, 4\\nmitigating risks with MLOps, 7-10\\nmonitoring after deployment to production,\\n\\n29-32\\n\\nproductionalization and deploymeent,\\n\\n27-29\\n\\nmonitoring and refining governance, 124\\nmonitoring machine learning models, 29-32,\\n\\n85-103\\n\\n\\x0cbusiness concerns, 31\\nconsumer credit risk management model,\\n\\n132, 132\\n\\nconsumption forecast for power grid model,\\n\\n156\\n\\ndata scientist concerns, 18, 30\\ndeciding how often to retrain models, 86-89\\ndegradation of performance, understanding,\\n\\n89-92\\nground truth evaluation, 89\\ninput drift detection, 91\\n\\nDevOps concerns, 30\\ndrift detection in practice, 92-95\\n\\ninput drift detection techniques, 93\\n\\nfeedback loop, 95-103\\n\\nlogging, 96\\nmodel evaluation, 97-99\\nonline evaluation of models in produc‐\\n\\ntion, 99-102\\n\\nmarketing recommendation engine, 140,\\n\\n142-145\\nmodel runs overnight, sleeps in daytime,\\n\\n143\\n\\nmonitoring performance, 144\\noption to automatically control models,\\n\\n144\\n\\noption to manually control models, 143\\nretraining models, 142\\nupdating models, 143\\n\\nmodel risk manager/auditor, role in, 21\\nin production, 79\\n\\nmonotonicity constraints, 130\\nMRM (model risk management), 21\\n\\n(see also model risk manager/auditor)\\nprinciples for good MRM, defined by UK\\n\\nPRA, 109\\n\\nregulation for fianancial models, 109\\n\\nmulti-armed bandit testing, 34\\n\\nN\\nneural networks, 45\\n\\nadversarial attacks on, 68\\nlong short-term memory (LSTM), 155\\nNLP (natural language processing) pretrained\\n\\nmodels, compression used with, 61\\n\\nnull hypothesis to p-values, 90\\n\\nO\\none-hot encoding, 47\\n\\nonline evaluation of models in production,\\n\\n99-102\\nA/B testing, 101\\nchampion/challenger, 100\\n\\nonline machine learning, 87\\noperationalization and MLOps, 18\\norchestration of containers, 28\\n\\nP\\np-values, 89\\n\\nadvantages and drawbacks of, 94\\n\\nparallelization of batch scoring, 78\\npartitioning (or sharding), 78\\n\\ndistributing batch processing by partition‐\\n\\ning data, 82\\n\\npeople in MLOps, roles and responsibilities,\\n\\n13-22\\ndata engineers, 19\\ndata scientists, 17-19\\nDevOps, 20\\nmachine learning architects, 21\\nmodel risk manager/auditor, 21\\noverview, 13\\nsoftware engineers, 20\\nsubject matter experts (SMEs), 15-17\\n\\nperformance\\n\\nconsiderations when converting from devel‐\\nopment to production environments, 61\\n\\ndegradation of model performance, moni‐\\n\\ntoring, 89-92\\n\\ndrift, assessing for subpopulations in credit\\n\\nrisk management model, 132\\n\\nmodel retraining and, 86\\nmonitoring for marketing recommendation\\n\\nengines, 144\\n\\nmonitoring ML models for, 86\\ntesting on out-of-sample datasets for con‐\\nsumer credit risk management model,\\n131\\n\\nperformance metrics, 44\\npersonally identifiable information (PII), 25\\nmaking sure no PII used to train a model,\\n\\n36\\n\\npharmaceutical regulations in US, GxP, 109\\npoisoning attacks, 68\\nportable formats for models, 28, 156\\npower systems, 147-149\\npredictions\\n\\nconformal, 72\\n\\nIndex \\n\\n| \\n\\n165\\n\\n\\x0cdecoupled ground truth and prediction, 90\\nexamples of model prediction and generali‐\\n\\nzation, 43\\n\\nadaptation from development to produc‐\\n\\ntion environments, 60-62\\n\\ndata access before validation and launch\\n\\nground truth for model predictions, 31\\ninput drift and, 31\\n\\nto poduction, 62\\nfinal thoughts on, 62\\n\\nprobabilities\\n\\ndistances between probability distributions,\\n\\nmeasuring, 132\\n\\nproduced by models used for risk assess‐\\n\\nment, 131\\n\\nproblem definition and data acquisition, com‐\\nsumer credit risk management MLOps\\napplication, 130\\n\\nproblem definition, using machine learning or\\n\\nnot, 151-152\\n\\nprocess governance, 37\\n\\neffective implementation, difficulties of, 37\\n\\nproduction, deploying to, 73-84\\nbuilding ML artifacts, 75-76\\n\\nusing testing pipeline on model, 75\\n\\nCI/CD pipelines, 73\\nconsumer credit risk management model,\\n\\n132\\n\\nconsumption forecast prototype models,\\n\\n155\\n\\ncontainerization, 79-81\\ndeployment strategies, 77-79\\n\\ncategories of model deployment, 77\\nconsiderations in sending models to pro‐\\n\\nduction, 78\\n\\nmaintenance of models in production,\\n\\n79\\n\\nscaling deployments, 81-83\\n\\nrequirements and challenges, 83\\n\\nproduction, preparing for, 59-72\\n\\nconsumer credit risk management model,\\n\\n131\\n\\nkey ideas, summary of, 72\\nmachine learning security, 67-69\\nmodel risk evaluation, 63-64\\nmodel risk mitigation, 69-72\\n\\nchanging environments, 70\\ninteractions between models, 70\\nmodel misbehavior, 71\\n\\nquality assurance for machine learning,\\n\\n64-66\\n\\nreproducibility and auditability for models,\\n\\n66\\n\\nruntime environments, 60-63\\n\\n166 \\n\\n| \\n\\nIndex\\n\\nproductionalization and deploymnt of models,\\n\\n27-29\\nmodel deployment requirements, 29\\nmodel deployment types and contents, 28\\n\\nprogressive or canary rollouts, 69\\nproject criticality and operationalization\\napproaches to risk assessment, 107\\n\\nprovenance of data, 113\\npruning models, 61\\npublic opinion, influence on ML governance,\\n\\n106\\n\\npush or pull recommendations, 136\\nPython, 9, 61, 155\\n\\nQ\\nQA (quality assurance) for machine learning,\\n\\n64-66\\nkey testing considerations, 65\\nproviding clear view of model performance\\n\\nand facilitating auditability, 67\\n\\nquantization, 61\\n\\nR\\nR language, 155\\nRACI (responsible, accountable, consulted,\\n\\ninformed), 119\\n\\nrandomness, 57\\n\\nrandom sampling, 91\\n\\nreal-time scoring, 77\\n\\nlogging streaming data, 83\\nmarketing recommendation engine model,\\n\\n140\\n\\nrecommendation engines, 135\\n\\n(see also marketing recommendation\\n\\nengines)\\nrise of, 135-137\\n\\ndeciding on push or pull recommenda‐\\n\\ntions, 136\\n\\nturning recommendations on/off, 141\\n\\nred-black deployment, 78\\nregulations, 118\\n\\nAI-specific, new wave of, 111-112\\ncurrent, driving MLOps governance,\\n\\n108-111\\n\\n\\x0cfinancial model risk management, 109\\nGDPR and CCPA data privacy regula‐\\n\\ntions, 110\\n\\n(see also biases)\\n\\nin consumer credit risk management model\\n\\nbias, 131\\n\\npharmaceutical regulation in US, GxP,\\n\\nfeature that causes drift, 95\\n\\n109\\n\\nrisks\\n\\ngovernance and regulatory checks in model\\n\\ndeployments, 29\\n\\nassessing, considerations in MLOps, 107\\ncredit risk modeling, 130\\n\\ngovernment regulations on ML to mitigate\\n\\n(see also consumer credit risk manage‐\\n\\nnegative impact of its use, 35\\n\\nment)\\n\\ngovernment regulations on use of personal\\n\\nfinancial model risk management regula‐\\n\\ndata by businesses, 35\\nprocess governance and, 38\\n\\nreleases, 77\\nreproducibility\\n\\nauditability and, 67\\nof experiments with ML models, 26\\nimportance as model property, 56\\nin MLOps versus academia, 66\\n\\nresource demands of models\\n\\ncapped, 29\\ncomputing power, 45\\nscalability of compute resources for\\n\\ndeployed models, 30\\n\\nresource monitoring, 79\\nresources\\n\\nmonitoring for ML models, 85\\noptimizing usage with elastic systems, 81\\nresponsibilities, establishing in MLOps, 119\\nResponsible AI, 112-116\\n\\nbusinesses engaging with, 36\\nimpacts on modeling, 53\\nkey elements of, 113-116\\n\\nbias, 114\\ndata, 113\\ngovernance, 116\\ninclusiveness, 115\\nmodel management at scale, 116\\n\\nin marketing recommendation engine, 141\\nin ML model development, 26\\nMLOps for, 9\\nmodel explainability, 27\\n\\nresults of in-depth analysis of models, compar‐\\n\\ning, 57\\n\\nretraining models\\n\\ndeciding how often to retrain, 86-89\\nmarketing recommendation engine, 140,\\n\\n142\\nreweighting, 93\\n\\nfor biased samples, 91, 93\\n\\ntion, 109\\n\\nmatching governance with risk level, 107\\nmodel risk evaluation, 63-64\\n\\norigins of ML model risk, 64\\npurpose of model validation, 63\\n\\nmodel risk manager/auditor, 21\\nmodel risk mitigation, 69-72\\n\\nchanging environments, 70\\ninteractions between models, 70\\nmodel misbehavior, 71\\n\\nneed for proactively addressing in ML, 106\\nrisk assessment, 8\\nrisk mitigation, MLOps for, 9\\nRMSE (root-mean-square error), 143\\nrollbacks to previous model versions, 79\\nruntime environments, 60-63\\n\\nadaptation from development to production\\n\\nenvironments, 60-62\\nperformane considerations, 61\\ntooling considerations, 61\\n\\ndata access before validation and launch to\\n\\nproduction, 62\\nfinal thoughts on, 62\\n\\nS\\nsampling\\n\\nrandom, 91\\nselection bias, 93\\n\\nSCADA (supervisory control and data acquisi‐\\n\\ntion) system, 149\\n\\nscale\\n\\nMLOps for, 10\\nmodel management at scale (Responsible\\n\\nAI), 116\\n\\nscalability of compute resources for\\n\\ndeployed models, 30\\n\\nscalability of marketing recommendation\\n\\nengine model, 140\\nscaling deployments, 81-83\\n\\nIndex \\n\\n| \\n\\n167\\n\\n\\x0crequirements and challenges, 83\\nscalable and elastic systems, 81\\nscaling the number of models, 82\\n\\nscaling inference on models, 62\\n\\nscikit-learn, 9\\n\\ntemporal aggegrations of data, 153\\ntesting\\n\\nkey testing considerations, 65\\nfor models using self-training algorithm, 88\\ntesting pipeline for ML artifacts, 75\\n\\nmodels developed with, adpatation to pro‐\\n\\ntools\\n\\nducton environments, 61\\n\\nsecurity in machine learning, 67-69\\n\\nadversarial attacks, 68\\nother vulnerabilities, 68\\n\\nselection bias, 93\\nsettings, reproducibility and, 57\\nshadow testing, 99\\n\\n(see also champion/challenger)\\nof new model version deployed alongside\\n\\nexisting model, 33\\n\\nShapley values, 27, 53, 130\\nsharding, 78\\nSMEs (see subject matter experts)\\nsoftware engineers, 20\\n\\nrole in and needs from MLOps, 20\\nrole in machine learning model life cycle, 20\\n\\nSpark, 82\\nspatial aggregations of data, 153\\nspatial and temporal resolution, forecast uncer‐\\n\\ntainty and, 151\\n\\nstatistics\\n\\nfrom null hypothesis to p-values, 90\\nstatistical metrics for model performance\\n\\nmonitoring, 89\\n\\nstatistical results on model testing, 65\\nstatistically driven decision-making pro‐\\n\\ncesses in ML, 105\\n\\nunivariate statistical tests, 93\\n\\nstochasticity, 155\\nsubject matter experts (SMEs), 15-17\\nrole in and needs from MLOps, 16\\nrole in machine learning model life cycle, 15\\n\\nsubpopulations\\n\\nanalyses of, 27\\nanalysis and model fairness, 66\\ninvesting model performance for in con‐\\nsumer credit risk management, 131\\nstatistical tests on results in preproduction\\n\\nmodel testing, 66\\nsupervised learning, 44\\n\\nT\\ntechnical debt in machine learning, 49\\n\\n168 \\n\\n| \\n\\nIndex\\n\\nconsiderations in adaptation from develop‐\\nment to production environments, 61\\nselecting for centralized governance man‐\\n\\nagement, 122\\n\\ntraceability in pharmaceutical industry, 109\\ntraining data, 23, 44\\n\\nadversarial attacks on, 68\\ndata governance concerns, 36\\nkeeping up to date for deployed models, 30\\nnon-stationary environment, 93\\nquality of, determining model performance,\\n\\n86\\n\\ntraining models, 26\\n\\nautomation in, 50\\ndeciding how often to retrain models, 86-89\\nmarketing recommendation engine model,\\n\\n138-141\\n\\nretraining existing model with latest train‐\\n\\ning data, 32\\n\\ntransfer learning, 48\\ntransparency\\n\\ndata scientists' need for from MLOps, 18\\nsubject matter experts, role in MLOps, 17\\ntransparent strategies for machine learning,\\n\\n11\\n\\ntree-based algorithms, 45\\ntrust\\n\\nEU's requirements for trustworthy AI appli‐\\n\\ncations, 111\\n\\nimportance to consumers and businesses, 36\\n\\nTweedie distribution, 130\\nTwitter chatbot by Microsoft, 68\\ntwo-stage models with offset, 130\\n\\nU\\nU.S. Food and Drug Administration (FDA), 109\\nUCI Wine Quality dataset, 91\\nUK Prudential Regulation Authority’s (PRA)\\n\\nregulation, 109\\n\\nunderfitting, 50\\nunivariate statistical tests, 93\\nupdating models, 143\\n\\n\\x0cV\\nvalidation\\n\\ndata access before model validation, 62\\nmodel for consumer credit risk manage‐\\n\\nment, 131\\n\\npurpose of model validation, 63\\ntesting on recent production data from\\n\\nmodels, 76\\nvariance in models, 50\\nversion control systems, central, 75\\nversion management, 79\\n\\nversion management and reproducibility, 26,\\n\\n56-58\\n\\nVMs (virtual machines), containers versus, 80\\n\\nW\\nWasserstein distance, 132\\nwhat-if analysis, 27\\n\\nX\\nXGBoost algorithm, 130\\n\\nIndex \\n\\n| \\n\\n169\\n\\n\\x0cAbout the Authors\\n\\nMark Treveil has designed products in fields as diverse as telecommunications, bank‐\\ning, and online trading. His own startup led a revolution in governance in UK local\\ngovernment,  where  it  still  dominates.  He  is  now  part  of  the  Dataiku  Product  Team\\nbased in Paris.\\n\\nNicolas Omont is VP of operations at Artelys, where he is developing mathematical\\noptimization solutions for energy and transport. He previously held the role of Data‐\\niku  product  manager  for  ML  and  advanced  analytics.  He  holds  a  PhD  in  computer\\nscience,  and  he’s  been  working  in  operations  research  and  statistics  for  the  past  15\\nyears, mainly in the telecommunications and energy utility sectors.\\n\\nClément  Stenac  is  a  passionate  software  engineer,  CTO,  and  cofounder  at  Dataiku.\\nHe oversees the design and development of the Dataiku DSS Enterprise AI Platform.\\nClément was previously head of product development at Exalead, leading the design\\nand implementation of web-scale search engine software. He also has extensive expe‐\\nrience with open source software, as a former developer of the VideoLAN (VLC) and\\nDebian projects.\\n\\nKenji Lefèvre is VP of product at Dataiku. He oversees the product road map and the\\nuser experience of the Dataiku DSS Enterprise AI Platform. He holds a PhD in pure\\nmathematics  from  University  of  Paris  VII,  and  he  directed  documentary  movies\\nbefore switching to data science and product management.\\n\\nDu Phan is a machine learning engineer at Dataiku, where he works in democratiz‐\\ning data science. In the past few years, he has been dealing with a variety of data prob‐\\nlems,  from  geospatial  analysis  to  deep  learning.  His  work  now  focuses  on  different\\nfacets and challenges of MLOps.\\n\\nJoachim Zentici is an engineering director at Dataiku. Joachim graduated in applied\\nmathematics  from  Ecole  Centrale  Paris.  Prior  to  joining  Dataiku  in  2014,  he  was  a\\nresearch  engineer  in  computer  vision  at  Siemens  Molecular  Imaging  and  Inria.  He\\nhas also been a teacher and a lecturer. At Dataiku, Joachim has made multiple contri‐\\nbutions including managing the engineers in charge of the core infrastructure, build‐\\ning the team for the plug-ins and ecosystem efforts, and leading the global technology\\ntraining program for customer-facing engineers.\\n\\nAdrien  Lavoillotte  is  an  engineering  director  at  Dataiku  where  he  leads  the  team\\nresponsible for machine learning and statistics features in the software. He studied at\\nECE Paris, a graduate school of engineering, and worked for several startups before\\njoining Dataiku in 2015.\\n\\n\\x0cMakoto Miyazaki is a data scientist at Dataiku and responsible for delivering hands-\\non consulting services using Dataiku DSS for European and Japanese clients. Makoto\\nholds a bachelor’s degree in economics and a master’s degree in data science, and he\\nwas  also  a  former  financial  journalist  with  a  wide  range  of  beats,  including  nuclear\\nenergy and economic recoveries from the tsunami.\\n\\nLynn Heidmann received her bachelor’s degree in journalism/mass communications\\nand anthropology from the University of Wisconsin-Madison in 2008 and decided to\\nbring  her  passion  for  research  and  writing  into  the  world  of  tech.  She  spent  seven\\nyears in the San Francisco Bay Area writing and running operations with Google and\\nsubsequently Niantic before moving to Paris to head content initiatives at Dataiku. In\\nher  current  role,  Lynn  follows  and  writes  about  technological  trends  and  develop‐\\nments in the world of data and AI.\\n\\nColophon\\n\\nThe animal on the cover of Introducing MLOps is an African moth called Bunaeopsis\\noubie, also known as Zaddach’s Emperor, that can be found across central and eastern\\nAfrica,  from  Angola  to  Eritrea.  It  is  a  member  of  the  Saturniidae  family,  which\\nincludes one thousand species of the world’s largest moths.\\n\\nThis African moth has one of the largest wingspans, stretching up to 10 inches, mak‐\\ning it bigger than some birds. Its wings have distinctive markings: one reddish brown\\ncircle  on  each  of  the  four  wings,  dark  brown  stripes  underneath,  and  white  strokes\\nbordering  the  thorax  and  along  the  outer  edges  of  each  wing.  Moth  antennae  are\\nthick  and  feathered.  Their  entire  bodies  repel  water  with  a  wax  coating  that  covers\\ntheir hairs and the scales on their wings.\\n\\nMoths  tend  to  be  attracted  to  white,  fragrant  flowers,  which  they  sniff  out  easily  at\\nnight  and  pollinate  well  with  their  fuzzy,  sticky  bodies.  Many  animals  and  birds\\ndepend on moths in their diets, including owls and bats. Moth caterpillars are prey to\\nlizards, birds, and many small mammals.\\n\\nMany of the animals on O’Reilly’s covers are endangered; all of them are important to\\nthe world.\\n\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom  Encyclopedie  D’Histoire  Naturelle.  The  cover  fonts  are  Gilroy  Semibold  and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\\n\\n\\x0c\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4043b3",
   "metadata": {},
   "source": [
    "# Extract Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebe08eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\achyu\\anaconda3\\lib\\site-packages (1.22.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\achyu\\anaconda3\\lib\\site-packages (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF \n",
    "!pip install pillow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ebd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF  # to read the image page by page\n",
    "#!pip install pillow   # to extract the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a25bc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # for PyMuPDF\n",
    "\n",
    "import PIL.Image   # PIL -Python image library  #for pillow\n",
    "import io          # to read each byte  #for pillow    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14bdd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(\"D:\\Learnbay\\Books & Reference Material\\Books\\Introduction to ML ops.pdf\")\n",
    "counter = 1 # one after one image\n",
    "\n",
    "for i in range(len(pdf)):\n",
    "    page = pdf[i]\n",
    "    images = page.get_images()\n",
    "    for image in images:\n",
    "        base_img = pdf.extract_image(image[0])\n",
    "        image_data = base_img[\"image\"]  # naming the every image as image\n",
    "        img = PIL.Image.open(io.BytesIO(image_data)) # inbuilt function to extract image\n",
    "        extension = base_img['ext']\n",
    "        img.save(open(f'image{counter}.{extension}',\"wb\")) # wb =read & write\n",
    "        counter +=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a10d9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\achyu\\\\Learnbay\\\\NLP'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b68e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Learnbay\\Books & Reference Material\\Books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c8d04",
   "metadata": {},
   "source": [
    "# Extract Table from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ae9179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camelot-py[cv]\n",
      "  Downloading camelot_py-0.11.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openpyxl>=2.5.8 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (3.0.10)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (1.4.4)\n",
      "Requirement already satisfied: click>=6.7 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (8.0.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (1.26.3)\n",
      "Requirement already satisfied: pdfminer.six>=20200726 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (20221105)\n",
      "Requirement already satisfied: chardet>=3.0.4 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (4.0.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (0.8.10)\n",
      "Collecting pypdf>=3.0.0\n",
      "  Downloading pypdf-4.0.0-py3-none-any.whl (283 kB)\n",
      "     -------------------------------------- 283.9/283.9 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting ghostscript>=0.7\n",
      "  Downloading ghostscript-0.7-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: opencv-python>=3.4.2.17 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (4.9.0.80)\n",
      "Collecting pdftopng>=0.2.3\n",
      "  Downloading pdftopng-0.2.3-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from click>=6.7->camelot-py[cv]) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=38.6.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from ghostscript>=0.7->camelot-py[cv]) (63.4.1)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from openpyxl>=2.5.8->camelot-py[cv]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py[cv]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py[cv]) (2022.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py[cv]) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py[cv]) (2.0.4)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from pypdf>=3.0.0->camelot-py[cv]) (4.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv]) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.23.4->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv]) (2.21)\n",
      "Installing collected packages: pypdf, ghostscript, pdftopng, camelot-py\n",
      "Successfully installed camelot-py-0.11.0 ghostscript-0.7 pdftopng-0.2.3 pypdf-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install camelot-py[cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa17b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cda7c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\anaconda3\\lib\\site-packages\\camelot\\parsers\\lattice.py:411: UserWarning: page-1 is image-based, camelot only works on text-based pages.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Ghostscript is not installed. You can install it using the instructions here: https://camelot-py.readthedocs.io/en/master/user/install-deps.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48332\\1480321097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcamelot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\Learnbay\\\\Books & Reference Material\\\\Books\\\\Introduction to ML ops.pdf\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Table {i + 1}:\\n{table.df}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\camelot\\io.py\u001b[0m in \u001b[0;36mread_pdf\u001b[1;34m(filepath, pages, password, flavor, suppress_stdout, layout_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_extra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         tables = p.parse(\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[0mflavor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0msuppress_stdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuppress_stdout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\camelot\\handlers.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, flavor, suppress_stdout, layout_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLattice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mflavor\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lattice\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 t = parser.extract_tables(\n\u001b[0m\u001b[0;32m    177\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress_stdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuppress_stdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\camelot\\parsers\\lattice.py\u001b[0m in \u001b[0;36mextract_tables\u001b[1;34m(self, filename, suppress_stdout, layout_kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimagename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_table_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\camelot\\backends\\ghostscript_backend.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, pdf_path, png_path, resolution)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpng_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             raise OSError(\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[1;34m\"Ghostscript is not installed. You can install it using the instructions\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;34m\" here: https://camelot-py.readthedocs.io/en/master/user/install-deps.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Ghostscript is not installed. You can install it using the instructions here: https://camelot-py.readthedocs.io/en/master/user/install-deps.html"
     ]
    }
   ],
   "source": [
    "tables = camelot.read_pdf(\"D:\\\\Learnbay\\\\Books & Reference Material\\\\Books\\\\Introduction to ML ops.pdf\",pages ='all')\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i + 1}:\\n{table.df}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8768ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83076e94",
   "metadata": {},
   "source": [
    "# Convert Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "138131c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gTTS in c:\\users\\achyu\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from gTTS) (8.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from gTTS) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achyu\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222ee606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4b42dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence:\n",
      ">Hi everyone, hope india will become third largest economy by 2030 and will become super power too\n"
     ]
    }
   ],
   "source": [
    "my_sentence = input(\"Enter the sentence:\\n>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae606ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = gTTS(my_sentence, lang ='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b6be84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.save(\"english.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "శీతాకాలంలో కేదార్ నాథ్ ఆలయ తలుపులు మూసివేసిన తర్వాత.. ఈ సమయంలో కేదార్‌నాథ్ ధామ్ లో పూర్తిగా జన సంచారం ఆగిపోతుంది. కానీ బాబా కేదార్ భక్తుడైన సాధు లలిత్ మహారాజ్ ఇప్పటికీ తపస్సులో మునిగి ఉన్నాడు. ఎంత హిమపాతం వచ్చినా లలిత్ మహారాజ్ కేదార్‌నాథ్ ధామ్ నుండి కిందకు రారు."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a666370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence:\n",
      ">శీతాకాలంలో కేదార్ నాథ్ ఆలయ తలుపులు మూసివేసిన తర్వాత.. ఈ సమయంలో కేదార్‌నాథ్ ధామ్ లో పూర్తిగా జన సంచారం ఆగిపోతుంది. కానీ బాబా కేదార్ భక్తుడైన సాధు లలిత్ మహారాజ్ ఇప్పటికీ తపస్సులో మునిగి ఉన్నాడు. ఎంత హిమపాతం వచ్చినా లలిత్ మహారాజ్ కేదార్‌నాథ్ ధామ్ నుండి కిందకు రారు.\n"
     ]
    }
   ],
   "source": [
    "my_sentence1 = input(\"Enter the sentence:\\n>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1628c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = gTTS(my_sentence1, lang ='te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaadbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.save(\"telugu.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd40f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = gTTS(my_sentence1, lang ='hi')  # how hindi person says telugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa16a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1.save(\"hindi.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db28431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
